{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": null,
  "global_step": 11107,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 9.003128587184046e-05,
      "grad_norm": 4.193701773416133,
      "learning_rate": 5.98802395209581e-08,
      "loss": 2.6242,
      "step": 1
    },
    {
      "epoch": 0.00018006257174368092,
      "grad_norm": 4.583127153085649,
      "learning_rate": 1.197604790419162e-07,
      "loss": 2.7575,
      "step": 2
    },
    {
      "epoch": 0.0002700938576155214,
      "grad_norm": 4.80619216687548,
      "learning_rate": 1.7964071856287425e-07,
      "loss": 2.6246,
      "step": 3
    },
    {
      "epoch": 0.00036012514348736184,
      "grad_norm": 4.829229908566871,
      "learning_rate": 2.395209580838324e-07,
      "loss": 2.8203,
      "step": 4
    },
    {
      "epoch": 0.00045015642935920233,
      "grad_norm": 5.879511041617817,
      "learning_rate": 2.9940119760479047e-07,
      "loss": 2.6255,
      "step": 5
    },
    {
      "epoch": 0.0005401877152310428,
      "grad_norm": 4.548333667264882,
      "learning_rate": 3.592814371257485e-07,
      "loss": 2.5771,
      "step": 6
    },
    {
      "epoch": 0.0006302190011028833,
      "grad_norm": 5.23071207380344,
      "learning_rate": 4.191616766467066e-07,
      "loss": 2.3762,
      "step": 7
    },
    {
      "epoch": 0.0007202502869747237,
      "grad_norm": 5.280018543025292,
      "learning_rate": 4.790419161676648e-07,
      "loss": 2.6145,
      "step": 8
    },
    {
      "epoch": 0.0008102815728465642,
      "grad_norm": 4.683681401836624,
      "learning_rate": 5.389221556886228e-07,
      "loss": 2.5522,
      "step": 9
    },
    {
      "epoch": 0.0009003128587184047,
      "grad_norm": 4.9083026930748055,
      "learning_rate": 5.988023952095809e-07,
      "loss": 2.6007,
      "step": 10
    },
    {
      "epoch": 0.0009903441445902451,
      "grad_norm": 4.309355869412944,
      "learning_rate": 6.586826347305391e-07,
      "loss": 2.3763,
      "step": 11
    },
    {
      "epoch": 0.0010803754304620856,
      "grad_norm": 4.339288485368901,
      "learning_rate": 7.18562874251497e-07,
      "loss": 2.3903,
      "step": 12
    },
    {
      "epoch": 0.0011704067163339261,
      "grad_norm": 4.305003109641405,
      "learning_rate": 7.784431137724552e-07,
      "loss": 2.4126,
      "step": 13
    },
    {
      "epoch": 0.0012604380022057666,
      "grad_norm": 5.210459218121963,
      "learning_rate": 8.383233532934132e-07,
      "loss": 2.6012,
      "step": 14
    },
    {
      "epoch": 0.0013504692880776069,
      "grad_norm": 4.753727315870391,
      "learning_rate": 8.982035928143713e-07,
      "loss": 2.5979,
      "step": 15
    },
    {
      "epoch": 0.0014405005739494474,
      "grad_norm": 4.9964465208940645,
      "learning_rate": 9.580838323353295e-07,
      "loss": 2.5312,
      "step": 16
    },
    {
      "epoch": 0.0015305318598212879,
      "grad_norm": 4.402953957476213,
      "learning_rate": 1.0179640718562875e-06,
      "loss": 2.3591,
      "step": 17
    },
    {
      "epoch": 0.0016205631456931283,
      "grad_norm": 6.060997716033515,
      "learning_rate": 1.0778443113772456e-06,
      "loss": 2.9532,
      "step": 18
    },
    {
      "epoch": 0.0017105944315649688,
      "grad_norm": 4.078248517037523,
      "learning_rate": 1.1377245508982037e-06,
      "loss": 2.2037,
      "step": 19
    },
    {
      "epoch": 0.0018006257174368093,
      "grad_norm": 6.303440278595254,
      "learning_rate": 1.1976047904191619e-06,
      "loss": 2.756,
      "step": 20
    },
    {
      "epoch": 0.0018906570033086498,
      "grad_norm": 4.517815812664685,
      "learning_rate": 1.2574850299401198e-06,
      "loss": 2.4646,
      "step": 21
    },
    {
      "epoch": 0.0019806882891804903,
      "grad_norm": 3.7132386050110187,
      "learning_rate": 1.3173652694610781e-06,
      "loss": 2.1908,
      "step": 22
    },
    {
      "epoch": 0.002070719575052331,
      "grad_norm": 4.3375003297536665,
      "learning_rate": 1.377245508982036e-06,
      "loss": 2.3799,
      "step": 23
    },
    {
      "epoch": 0.0021607508609241713,
      "grad_norm": 5.042784587803069,
      "learning_rate": 1.437125748502994e-06,
      "loss": 2.3572,
      "step": 24
    },
    {
      "epoch": 0.0022507821467960118,
      "grad_norm": 4.775929145492194,
      "learning_rate": 1.4970059880239521e-06,
      "loss": 2.3464,
      "step": 25
    },
    {
      "epoch": 0.0023408134326678522,
      "grad_norm": 3.5031643587622887,
      "learning_rate": 1.5568862275449105e-06,
      "loss": 2.1073,
      "step": 26
    },
    {
      "epoch": 0.0024308447185396927,
      "grad_norm": 4.7962603168885956,
      "learning_rate": 1.6167664670658684e-06,
      "loss": 2.2374,
      "step": 27
    },
    {
      "epoch": 0.0025208760044115332,
      "grad_norm": 5.869519617138849,
      "learning_rate": 1.6766467065868263e-06,
      "loss": 2.5009,
      "step": 28
    },
    {
      "epoch": 0.0026109072902833733,
      "grad_norm": 4.419561623080792,
      "learning_rate": 1.7365269461077847e-06,
      "loss": 2.2546,
      "step": 29
    },
    {
      "epoch": 0.0027009385761552138,
      "grad_norm": 4.429998734022076,
      "learning_rate": 1.7964071856287426e-06,
      "loss": 2.1306,
      "step": 30
    },
    {
      "epoch": 0.0027909698620270543,
      "grad_norm": 5.937506862784642,
      "learning_rate": 1.8562874251497007e-06,
      "loss": 2.272,
      "step": 31
    },
    {
      "epoch": 0.0028810011478988947,
      "grad_norm": 6.744591953694104,
      "learning_rate": 1.916167664670659e-06,
      "loss": 2.2756,
      "step": 32
    },
    {
      "epoch": 0.0029710324337707352,
      "grad_norm": 6.516954109212153,
      "learning_rate": 1.976047904191617e-06,
      "loss": 2.6149,
      "step": 33
    },
    {
      "epoch": 0.0030610637196425757,
      "grad_norm": 6.227341029831034,
      "learning_rate": 2.035928143712575e-06,
      "loss": 2.2707,
      "step": 34
    },
    {
      "epoch": 0.003151095005514416,
      "grad_norm": 7.863432994544156,
      "learning_rate": 2.095808383233533e-06,
      "loss": 2.2141,
      "step": 35
    },
    {
      "epoch": 0.0032411262913862567,
      "grad_norm": 7.1740599902017,
      "learning_rate": 2.155688622754491e-06,
      "loss": 2.158,
      "step": 36
    },
    {
      "epoch": 0.003331157577258097,
      "grad_norm": 5.595024329747163,
      "learning_rate": 2.215568862275449e-06,
      "loss": 1.7937,
      "step": 37
    },
    {
      "epoch": 0.0034211888631299377,
      "grad_norm": 7.1587342331304304,
      "learning_rate": 2.2754491017964075e-06,
      "loss": 1.904,
      "step": 38
    },
    {
      "epoch": 0.003511220149001778,
      "grad_norm": 5.314330142732934,
      "learning_rate": 2.3353293413173654e-06,
      "loss": 1.7755,
      "step": 39
    },
    {
      "epoch": 0.0036012514348736186,
      "grad_norm": 4.994616403010728,
      "learning_rate": 2.3952095808383237e-06,
      "loss": 1.6937,
      "step": 40
    },
    {
      "epoch": 0.003691282720745459,
      "grad_norm": 5.356098784162705,
      "learning_rate": 2.4550898203592817e-06,
      "loss": 1.7909,
      "step": 41
    },
    {
      "epoch": 0.0037813140066172996,
      "grad_norm": 2.881285923140914,
      "learning_rate": 2.5149700598802396e-06,
      "loss": 1.7329,
      "step": 42
    },
    {
      "epoch": 0.00387134529248914,
      "grad_norm": 2.756738729610379,
      "learning_rate": 2.5748502994011975e-06,
      "loss": 1.6601,
      "step": 43
    },
    {
      "epoch": 0.003961376578360981,
      "grad_norm": 3.8882312559681225,
      "learning_rate": 2.6347305389221563e-06,
      "loss": 1.6175,
      "step": 44
    },
    {
      "epoch": 0.004051407864232821,
      "grad_norm": 3.076916189489693,
      "learning_rate": 2.694610778443114e-06,
      "loss": 1.5477,
      "step": 45
    },
    {
      "epoch": 0.004141439150104662,
      "grad_norm": 2.6306760052854994,
      "learning_rate": 2.754491017964072e-06,
      "loss": 1.4543,
      "step": 46
    },
    {
      "epoch": 0.004231470435976502,
      "grad_norm": 2.7893683400623575,
      "learning_rate": 2.81437125748503e-06,
      "loss": 1.3393,
      "step": 47
    },
    {
      "epoch": 0.0043215017218483425,
      "grad_norm": 3.207978850481441,
      "learning_rate": 2.874251497005988e-06,
      "loss": 1.4402,
      "step": 48
    },
    {
      "epoch": 0.004411533007720183,
      "grad_norm": 2.8770641108857786,
      "learning_rate": 2.9341317365269463e-06,
      "loss": 1.4723,
      "step": 49
    },
    {
      "epoch": 0.0045015642935920235,
      "grad_norm": 3.8150038077365185,
      "learning_rate": 2.9940119760479042e-06,
      "loss": 1.5293,
      "step": 50
    },
    {
      "epoch": 0.004591595579463864,
      "grad_norm": 3.5372235055378796,
      "learning_rate": 3.0538922155688626e-06,
      "loss": 1.484,
      "step": 51
    },
    {
      "epoch": 0.0046816268653357045,
      "grad_norm": 2.3084326387972753,
      "learning_rate": 3.113772455089821e-06,
      "loss": 1.3476,
      "step": 52
    },
    {
      "epoch": 0.0047716581512075446,
      "grad_norm": 2.8377435023808903,
      "learning_rate": 3.173652694610779e-06,
      "loss": 1.4071,
      "step": 53
    },
    {
      "epoch": 0.0048616894370793855,
      "grad_norm": 2.182638482682246,
      "learning_rate": 3.2335329341317368e-06,
      "loss": 1.3556,
      "step": 54
    },
    {
      "epoch": 0.0049517207229512255,
      "grad_norm": 2.0532804526360224,
      "learning_rate": 3.2934131736526947e-06,
      "loss": 1.2027,
      "step": 55
    },
    {
      "epoch": 0.0050417520088230664,
      "grad_norm": 2.5245447292972827,
      "learning_rate": 3.3532934131736526e-06,
      "loss": 1.2958,
      "step": 56
    },
    {
      "epoch": 0.0051317832946949065,
      "grad_norm": 2.2378718002071585,
      "learning_rate": 3.4131736526946114e-06,
      "loss": 1.3721,
      "step": 57
    },
    {
      "epoch": 0.0052218145805667466,
      "grad_norm": 1.472527963695073,
      "learning_rate": 3.4730538922155693e-06,
      "loss": 1.3542,
      "step": 58
    },
    {
      "epoch": 0.0053118458664385875,
      "grad_norm": 1.5101274851679753,
      "learning_rate": 3.5329341317365273e-06,
      "loss": 1.2086,
      "step": 59
    },
    {
      "epoch": 0.0054018771523104275,
      "grad_norm": 1.174861750420621,
      "learning_rate": 3.592814371257485e-06,
      "loss": 1.098,
      "step": 60
    },
    {
      "epoch": 0.0054919084381822685,
      "grad_norm": 1.3732596884359742,
      "learning_rate": 3.6526946107784435e-06,
      "loss": 1.1729,
      "step": 61
    },
    {
      "epoch": 0.0055819397240541085,
      "grad_norm": 1.280812973520695,
      "learning_rate": 3.7125748502994014e-06,
      "loss": 1.2366,
      "step": 62
    },
    {
      "epoch": 0.005671971009925949,
      "grad_norm": 1.2766424313567752,
      "learning_rate": 3.7724550898203594e-06,
      "loss": 1.12,
      "step": 63
    },
    {
      "epoch": 0.0057620022957977895,
      "grad_norm": 1.2967963718441684,
      "learning_rate": 3.832335329341318e-06,
      "loss": 1.343,
      "step": 64
    },
    {
      "epoch": 0.00585203358166963,
      "grad_norm": 1.0346238666069674,
      "learning_rate": 3.892215568862276e-06,
      "loss": 1.2243,
      "step": 65
    },
    {
      "epoch": 0.0059420648675414705,
      "grad_norm": 1.5113259532266023,
      "learning_rate": 3.952095808383234e-06,
      "loss": 1.1506,
      "step": 66
    },
    {
      "epoch": 0.006032096153413311,
      "grad_norm": 1.1676529873782573,
      "learning_rate": 4.011976047904192e-06,
      "loss": 1.131,
      "step": 67
    },
    {
      "epoch": 0.0061221274392851514,
      "grad_norm": 1.2988218245356833,
      "learning_rate": 4.07185628742515e-06,
      "loss": 1.1536,
      "step": 68
    },
    {
      "epoch": 0.006212158725156992,
      "grad_norm": 1.6953398261317372,
      "learning_rate": 4.131736526946108e-06,
      "loss": 1.0955,
      "step": 69
    },
    {
      "epoch": 0.006302190011028832,
      "grad_norm": 1.0200271608513911,
      "learning_rate": 4.191616766467066e-06,
      "loss": 1.0242,
      "step": 70
    },
    {
      "epoch": 0.006392221296900673,
      "grad_norm": 1.666929898768839,
      "learning_rate": 4.251497005988025e-06,
      "loss": 1.234,
      "step": 71
    },
    {
      "epoch": 0.006482252582772513,
      "grad_norm": 1.273989471844235,
      "learning_rate": 4.311377245508982e-06,
      "loss": 1.195,
      "step": 72
    },
    {
      "epoch": 0.006572283868644354,
      "grad_norm": 1.2434791956898277,
      "learning_rate": 4.371257485029941e-06,
      "loss": 1.2107,
      "step": 73
    },
    {
      "epoch": 0.006662315154516194,
      "grad_norm": 1.4092639910239446,
      "learning_rate": 4.431137724550898e-06,
      "loss": 1.1642,
      "step": 74
    },
    {
      "epoch": 0.006752346440388034,
      "grad_norm": 1.3349068051847537,
      "learning_rate": 4.4910179640718566e-06,
      "loss": 1.1988,
      "step": 75
    },
    {
      "epoch": 0.006842377726259875,
      "grad_norm": 1.1450852327627632,
      "learning_rate": 4.550898203592815e-06,
      "loss": 1.0949,
      "step": 76
    },
    {
      "epoch": 0.006932409012131715,
      "grad_norm": 1.284326086763607,
      "learning_rate": 4.610778443113773e-06,
      "loss": 1.0851,
      "step": 77
    },
    {
      "epoch": 0.007022440298003556,
      "grad_norm": 1.1899948937045677,
      "learning_rate": 4.670658682634731e-06,
      "loss": 1.1799,
      "step": 78
    },
    {
      "epoch": 0.007112471583875396,
      "grad_norm": 1.3738234150702233,
      "learning_rate": 4.730538922155689e-06,
      "loss": 1.2126,
      "step": 79
    },
    {
      "epoch": 0.007202502869747237,
      "grad_norm": 1.0327798307947793,
      "learning_rate": 4.7904191616766475e-06,
      "loss": 1.0594,
      "step": 80
    },
    {
      "epoch": 0.007292534155619077,
      "grad_norm": 1.0520829569410148,
      "learning_rate": 4.850299401197605e-06,
      "loss": 1.1144,
      "step": 81
    },
    {
      "epoch": 0.007382565441490918,
      "grad_norm": 0.9442166438813985,
      "learning_rate": 4.910179640718563e-06,
      "loss": 0.9749,
      "step": 82
    },
    {
      "epoch": 0.007472596727362758,
      "grad_norm": 1.1262553794312713,
      "learning_rate": 4.970059880239521e-06,
      "loss": 1.1692,
      "step": 83
    },
    {
      "epoch": 0.007562628013234599,
      "grad_norm": 1.18151087268671,
      "learning_rate": 5.029940119760479e-06,
      "loss": 0.9307,
      "step": 84
    },
    {
      "epoch": 0.007652659299106439,
      "grad_norm": 1.1460559836533926,
      "learning_rate": 5.0898203592814375e-06,
      "loss": 1.0065,
      "step": 85
    },
    {
      "epoch": 0.00774269058497828,
      "grad_norm": 1.1642102412318647,
      "learning_rate": 5.149700598802395e-06,
      "loss": 1.0117,
      "step": 86
    },
    {
      "epoch": 0.00783272187085012,
      "grad_norm": 1.0170991308151802,
      "learning_rate": 5.209580838323353e-06,
      "loss": 1.0543,
      "step": 87
    },
    {
      "epoch": 0.007922753156721961,
      "grad_norm": 1.3317256494328051,
      "learning_rate": 5.2694610778443125e-06,
      "loss": 1.2066,
      "step": 88
    },
    {
      "epoch": 0.008012784442593802,
      "grad_norm": 1.0636975832279558,
      "learning_rate": 5.32934131736527e-06,
      "loss": 1.087,
      "step": 89
    },
    {
      "epoch": 0.008102815728465641,
      "grad_norm": 0.9305346860547821,
      "learning_rate": 5.389221556886228e-06,
      "loss": 1.0796,
      "step": 90
    },
    {
      "epoch": 0.008192847014337482,
      "grad_norm": 1.2279566994069289,
      "learning_rate": 5.449101796407186e-06,
      "loss": 0.9921,
      "step": 91
    },
    {
      "epoch": 0.008282878300209323,
      "grad_norm": 1.045817521349893,
      "learning_rate": 5.508982035928144e-06,
      "loss": 1.037,
      "step": 92
    },
    {
      "epoch": 0.008372909586081164,
      "grad_norm": 0.812313961553316,
      "learning_rate": 5.568862275449102e-06,
      "loss": 0.9427,
      "step": 93
    },
    {
      "epoch": 0.008462940871953003,
      "grad_norm": 0.9949166900329669,
      "learning_rate": 5.62874251497006e-06,
      "loss": 1.0408,
      "step": 94
    },
    {
      "epoch": 0.008552972157824844,
      "grad_norm": 1.0540435806361463,
      "learning_rate": 5.6886227544910184e-06,
      "loss": 1.0697,
      "step": 95
    },
    {
      "epoch": 0.008643003443696685,
      "grad_norm": 1.2652826242092636,
      "learning_rate": 5.748502994011976e-06,
      "loss": 1.0165,
      "step": 96
    },
    {
      "epoch": 0.008733034729568524,
      "grad_norm": 0.9946745265754758,
      "learning_rate": 5.808383233532935e-06,
      "loss": 1.0413,
      "step": 97
    },
    {
      "epoch": 0.008823066015440365,
      "grad_norm": 1.1333207991108247,
      "learning_rate": 5.868263473053893e-06,
      "loss": 1.0948,
      "step": 98
    },
    {
      "epoch": 0.008913097301312206,
      "grad_norm": 0.9166270824704921,
      "learning_rate": 5.928143712574851e-06,
      "loss": 1.0621,
      "step": 99
    },
    {
      "epoch": 0.009003128587184047,
      "grad_norm": 1.04615378705572,
      "learning_rate": 5.9880239520958085e-06,
      "loss": 1.0489,
      "step": 100
    },
    {
      "epoch": 0.009093159873055886,
      "grad_norm": 0.878176622098636,
      "learning_rate": 6.047904191616767e-06,
      "loss": 0.9232,
      "step": 101
    },
    {
      "epoch": 0.009183191158927727,
      "grad_norm": 0.9028500436165344,
      "learning_rate": 6.107784431137725e-06,
      "loss": 0.9986,
      "step": 102
    },
    {
      "epoch": 0.009273222444799568,
      "grad_norm": 0.8259212622022809,
      "learning_rate": 6.167664670658683e-06,
      "loss": 1.0089,
      "step": 103
    },
    {
      "epoch": 0.009363253730671409,
      "grad_norm": 1.1769285981128892,
      "learning_rate": 6.227544910179642e-06,
      "loss": 1.0674,
      "step": 104
    },
    {
      "epoch": 0.009453285016543248,
      "grad_norm": 0.9059083098172436,
      "learning_rate": 6.2874251497005985e-06,
      "loss": 1.0253,
      "step": 105
    },
    {
      "epoch": 0.009543316302415089,
      "grad_norm": 0.8468885101030618,
      "learning_rate": 6.347305389221558e-06,
      "loss": 0.9226,
      "step": 106
    },
    {
      "epoch": 0.00963334758828693,
      "grad_norm": 0.8928171096738744,
      "learning_rate": 6.407185628742516e-06,
      "loss": 0.8921,
      "step": 107
    },
    {
      "epoch": 0.009723378874158771,
      "grad_norm": 0.9957419237325356,
      "learning_rate": 6.4670658682634736e-06,
      "loss": 1.0062,
      "step": 108
    },
    {
      "epoch": 0.00981341016003061,
      "grad_norm": 0.9321797104994787,
      "learning_rate": 6.526946107784432e-06,
      "loss": 0.8826,
      "step": 109
    },
    {
      "epoch": 0.009903441445902451,
      "grad_norm": 0.8723159144551725,
      "learning_rate": 6.586826347305389e-06,
      "loss": 0.9797,
      "step": 110
    },
    {
      "epoch": 0.009993472731774292,
      "grad_norm": 0.7869297278551333,
      "learning_rate": 6.646706586826348e-06,
      "loss": 0.9007,
      "step": 111
    },
    {
      "epoch": 0.010083504017646133,
      "grad_norm": 0.8797182977770206,
      "learning_rate": 6.706586826347305e-06,
      "loss": 1.0257,
      "step": 112
    },
    {
      "epoch": 0.010173535303517972,
      "grad_norm": 0.9805842000045569,
      "learning_rate": 6.7664670658682645e-06,
      "loss": 0.9376,
      "step": 113
    },
    {
      "epoch": 0.010263566589389813,
      "grad_norm": 1.096979037026994,
      "learning_rate": 6.826347305389223e-06,
      "loss": 1.0361,
      "step": 114
    },
    {
      "epoch": 0.010353597875261654,
      "grad_norm": 0.959562254159314,
      "learning_rate": 6.88622754491018e-06,
      "loss": 0.9655,
      "step": 115
    },
    {
      "epoch": 0.010443629161133493,
      "grad_norm": 0.96295865805689,
      "learning_rate": 6.946107784431139e-06,
      "loss": 1.0368,
      "step": 116
    },
    {
      "epoch": 0.010533660447005334,
      "grad_norm": 1.0625736040919742,
      "learning_rate": 7.005988023952096e-06,
      "loss": 0.9465,
      "step": 117
    },
    {
      "epoch": 0.010623691732877175,
      "grad_norm": 0.9948185375959336,
      "learning_rate": 7.0658682634730545e-06,
      "loss": 0.9341,
      "step": 118
    },
    {
      "epoch": 0.010713723018749016,
      "grad_norm": 1.0512970471021619,
      "learning_rate": 7.125748502994012e-06,
      "loss": 1.0412,
      "step": 119
    },
    {
      "epoch": 0.010803754304620855,
      "grad_norm": 1.1251323410441572,
      "learning_rate": 7.18562874251497e-06,
      "loss": 0.8822,
      "step": 120
    },
    {
      "epoch": 0.010893785590492696,
      "grad_norm": 1.325617480042701,
      "learning_rate": 7.2455089820359295e-06,
      "loss": 0.9771,
      "step": 121
    },
    {
      "epoch": 0.010983816876364537,
      "grad_norm": 1.1449482759375134,
      "learning_rate": 7.305389221556887e-06,
      "loss": 0.9751,
      "step": 122
    },
    {
      "epoch": 0.011073848162236378,
      "grad_norm": 1.0653388753013011,
      "learning_rate": 7.365269461077845e-06,
      "loss": 0.8521,
      "step": 123
    },
    {
      "epoch": 0.011163879448108217,
      "grad_norm": 0.8290575203384121,
      "learning_rate": 7.425149700598803e-06,
      "loss": 0.936,
      "step": 124
    },
    {
      "epoch": 0.011253910733980058,
      "grad_norm": 1.213837340284003,
      "learning_rate": 7.485029940119761e-06,
      "loss": 0.9588,
      "step": 125
    },
    {
      "epoch": 0.011343942019851899,
      "grad_norm": 0.9286600017131632,
      "learning_rate": 7.544910179640719e-06,
      "loss": 0.8328,
      "step": 126
    },
    {
      "epoch": 0.01143397330572374,
      "grad_norm": 1.059653027234402,
      "learning_rate": 7.604790419161677e-06,
      "loss": 1.0059,
      "step": 127
    },
    {
      "epoch": 0.011524004591595579,
      "grad_norm": 0.841103212548472,
      "learning_rate": 7.664670658682636e-06,
      "loss": 0.8675,
      "step": 128
    },
    {
      "epoch": 0.01161403587746742,
      "grad_norm": 1.2259770063343414,
      "learning_rate": 7.724550898203594e-06,
      "loss": 0.9781,
      "step": 129
    },
    {
      "epoch": 0.01170406716333926,
      "grad_norm": 1.0101392544808205,
      "learning_rate": 7.784431137724551e-06,
      "loss": 0.7873,
      "step": 130
    },
    {
      "epoch": 0.0117940984492111,
      "grad_norm": 0.7907615787142983,
      "learning_rate": 7.844311377245509e-06,
      "loss": 0.8757,
      "step": 131
    },
    {
      "epoch": 0.011884129735082941,
      "grad_norm": 1.0940062157763515,
      "learning_rate": 7.904191616766468e-06,
      "loss": 0.907,
      "step": 132
    },
    {
      "epoch": 0.011974161020954782,
      "grad_norm": 1.1890848519211235,
      "learning_rate": 7.964071856287425e-06,
      "loss": 0.9218,
      "step": 133
    },
    {
      "epoch": 0.012064192306826623,
      "grad_norm": 0.8582257741495702,
      "learning_rate": 8.023952095808385e-06,
      "loss": 1.0014,
      "step": 134
    },
    {
      "epoch": 0.012154223592698462,
      "grad_norm": 0.825113598615717,
      "learning_rate": 8.083832335329342e-06,
      "loss": 0.8107,
      "step": 135
    },
    {
      "epoch": 0.012244254878570303,
      "grad_norm": 1.3904999580466455,
      "learning_rate": 8.1437125748503e-06,
      "loss": 0.8974,
      "step": 136
    },
    {
      "epoch": 0.012334286164442144,
      "grad_norm": 0.9196187414717055,
      "learning_rate": 8.203592814371259e-06,
      "loss": 0.8599,
      "step": 137
    },
    {
      "epoch": 0.012424317450313985,
      "grad_norm": 0.8992035393545763,
      "learning_rate": 8.263473053892216e-06,
      "loss": 0.7898,
      "step": 138
    },
    {
      "epoch": 0.012514348736185824,
      "grad_norm": 1.010031479362244,
      "learning_rate": 8.323353293413174e-06,
      "loss": 0.9898,
      "step": 139
    },
    {
      "epoch": 0.012604380022057665,
      "grad_norm": 0.9671562175742774,
      "learning_rate": 8.383233532934131e-06,
      "loss": 0.951,
      "step": 140
    },
    {
      "epoch": 0.012694411307929506,
      "grad_norm": 0.8933044274533456,
      "learning_rate": 8.44311377245509e-06,
      "loss": 0.8369,
      "step": 141
    },
    {
      "epoch": 0.012784442593801347,
      "grad_norm": 1.2896526857444957,
      "learning_rate": 8.50299401197605e-06,
      "loss": 0.859,
      "step": 142
    },
    {
      "epoch": 0.012874473879673186,
      "grad_norm": 0.876020194069114,
      "learning_rate": 8.562874251497007e-06,
      "loss": 0.8776,
      "step": 143
    },
    {
      "epoch": 0.012964505165545027,
      "grad_norm": 1.1741767898881628,
      "learning_rate": 8.622754491017965e-06,
      "loss": 0.8659,
      "step": 144
    },
    {
      "epoch": 0.013054536451416868,
      "grad_norm": 1.0126264821356012,
      "learning_rate": 8.682634730538922e-06,
      "loss": 0.9028,
      "step": 145
    },
    {
      "epoch": 0.013144567737288709,
      "grad_norm": 1.1307593124475996,
      "learning_rate": 8.742514970059881e-06,
      "loss": 0.8779,
      "step": 146
    },
    {
      "epoch": 0.013234599023160548,
      "grad_norm": 0.9285839619637988,
      "learning_rate": 8.802395209580839e-06,
      "loss": 0.8602,
      "step": 147
    },
    {
      "epoch": 0.013324630309032389,
      "grad_norm": 0.9002960288252152,
      "learning_rate": 8.862275449101796e-06,
      "loss": 0.8696,
      "step": 148
    },
    {
      "epoch": 0.01341466159490423,
      "grad_norm": 0.9479615085543118,
      "learning_rate": 8.922155688622756e-06,
      "loss": 0.8455,
      "step": 149
    },
    {
      "epoch": 0.013504692880776069,
      "grad_norm": 1.0719869936352149,
      "learning_rate": 8.982035928143713e-06,
      "loss": 0.9602,
      "step": 150
    },
    {
      "epoch": 0.01359472416664791,
      "grad_norm": 1.0143300022293478,
      "learning_rate": 9.041916167664672e-06,
      "loss": 0.7481,
      "step": 151
    },
    {
      "epoch": 0.01368475545251975,
      "grad_norm": 1.475475070234184,
      "learning_rate": 9.10179640718563e-06,
      "loss": 0.8194,
      "step": 152
    },
    {
      "epoch": 0.013774786738391592,
      "grad_norm": 1.3233547979151248,
      "learning_rate": 9.161676646706587e-06,
      "loss": 0.9078,
      "step": 153
    },
    {
      "epoch": 0.01386481802426343,
      "grad_norm": 1.202975363230472,
      "learning_rate": 9.221556886227547e-06,
      "loss": 0.7716,
      "step": 154
    },
    {
      "epoch": 0.013954849310135272,
      "grad_norm": 0.9312035090170386,
      "learning_rate": 9.281437125748504e-06,
      "loss": 0.7894,
      "step": 155
    },
    {
      "epoch": 0.014044880596007113,
      "grad_norm": 1.0336636776326342,
      "learning_rate": 9.341317365269462e-06,
      "loss": 0.8041,
      "step": 156
    },
    {
      "epoch": 0.014134911881878954,
      "grad_norm": 1.2093733534520925,
      "learning_rate": 9.401197604790419e-06,
      "loss": 0.8421,
      "step": 157
    },
    {
      "epoch": 0.014224943167750793,
      "grad_norm": 0.9887631396246,
      "learning_rate": 9.461077844311378e-06,
      "loss": 0.8658,
      "step": 158
    },
    {
      "epoch": 0.014314974453622634,
      "grad_norm": 1.0055107151290583,
      "learning_rate": 9.520958083832336e-06,
      "loss": 0.7934,
      "step": 159
    },
    {
      "epoch": 0.014405005739494475,
      "grad_norm": 1.116914512061121,
      "learning_rate": 9.580838323353295e-06,
      "loss": 0.7359,
      "step": 160
    },
    {
      "epoch": 0.014495037025366315,
      "grad_norm": 1.3045175851735207,
      "learning_rate": 9.640718562874252e-06,
      "loss": 0.8671,
      "step": 161
    },
    {
      "epoch": 0.014585068311238155,
      "grad_norm": 0.8099324629990159,
      "learning_rate": 9.70059880239521e-06,
      "loss": 0.9094,
      "step": 162
    },
    {
      "epoch": 0.014675099597109996,
      "grad_norm": 1.5862214752565962,
      "learning_rate": 9.760479041916169e-06,
      "loss": 0.667,
      "step": 163
    },
    {
      "epoch": 0.014765130882981837,
      "grad_norm": 1.3605857738311584,
      "learning_rate": 9.820359281437127e-06,
      "loss": 0.9383,
      "step": 164
    },
    {
      "epoch": 0.014855162168853677,
      "grad_norm": 1.0850035251753674,
      "learning_rate": 9.880239520958084e-06,
      "loss": 0.8391,
      "step": 165
    },
    {
      "epoch": 0.014945193454725517,
      "grad_norm": 1.0668064431261008,
      "learning_rate": 9.940119760479042e-06,
      "loss": 0.8374,
      "step": 166
    },
    {
      "epoch": 0.015035224740597358,
      "grad_norm": 1.9987405485296355,
      "learning_rate": 1e-05,
      "loss": 0.82,
      "step": 167
    },
    {
      "epoch": 0.015125256026469198,
      "grad_norm": 1.10607707902198,
      "learning_rate": 1.0059880239520958e-05,
      "loss": 0.9055,
      "step": 168
    },
    {
      "epoch": 0.015215287312341038,
      "grad_norm": 1.0191714722174117,
      "learning_rate": 1.0119760479041918e-05,
      "loss": 0.923,
      "step": 169
    },
    {
      "epoch": 0.015305318598212879,
      "grad_norm": 1.25165507754716,
      "learning_rate": 1.0179640718562875e-05,
      "loss": 0.8829,
      "step": 170
    },
    {
      "epoch": 0.01539534988408472,
      "grad_norm": 0.9892926694161912,
      "learning_rate": 1.0239520958083833e-05,
      "loss": 0.8236,
      "step": 171
    },
    {
      "epoch": 0.01548538116995656,
      "grad_norm": 1.021936760345529,
      "learning_rate": 1.029940119760479e-05,
      "loss": 0.827,
      "step": 172
    },
    {
      "epoch": 0.0155754124558284,
      "grad_norm": 0.8899462999113099,
      "learning_rate": 1.035928143712575e-05,
      "loss": 0.7655,
      "step": 173
    },
    {
      "epoch": 0.01566544374170024,
      "grad_norm": 1.1088335253876083,
      "learning_rate": 1.0419161676646707e-05,
      "loss": 0.7799,
      "step": 174
    },
    {
      "epoch": 0.01575547502757208,
      "grad_norm": 1.2544284215256984,
      "learning_rate": 1.0479041916167664e-05,
      "loss": 0.7992,
      "step": 175
    },
    {
      "epoch": 0.015845506313443922,
      "grad_norm": 1.332367015044952,
      "learning_rate": 1.0538922155688625e-05,
      "loss": 0.763,
      "step": 176
    },
    {
      "epoch": 0.015935537599315763,
      "grad_norm": 1.8153329302511247,
      "learning_rate": 1.0598802395209583e-05,
      "loss": 0.7249,
      "step": 177
    },
    {
      "epoch": 0.016025568885187604,
      "grad_norm": 1.3706820687598775,
      "learning_rate": 1.065868263473054e-05,
      "loss": 0.7556,
      "step": 178
    },
    {
      "epoch": 0.01611560017105944,
      "grad_norm": 0.9465966233556655,
      "learning_rate": 1.0718562874251498e-05,
      "loss": 0.7991,
      "step": 179
    },
    {
      "epoch": 0.016205631456931283,
      "grad_norm": 1.0498622388437062,
      "learning_rate": 1.0778443113772457e-05,
      "loss": 0.8449,
      "step": 180
    },
    {
      "epoch": 0.016295662742803124,
      "grad_norm": 1.039449792060584,
      "learning_rate": 1.0838323353293414e-05,
      "loss": 0.819,
      "step": 181
    },
    {
      "epoch": 0.016385694028674964,
      "grad_norm": 1.0678175796547544,
      "learning_rate": 1.0898203592814372e-05,
      "loss": 0.7566,
      "step": 182
    },
    {
      "epoch": 0.016475725314546805,
      "grad_norm": 1.033246771424281,
      "learning_rate": 1.0958083832335331e-05,
      "loss": 0.8698,
      "step": 183
    },
    {
      "epoch": 0.016565756600418646,
      "grad_norm": 0.8913535880889834,
      "learning_rate": 1.1017964071856288e-05,
      "loss": 0.7664,
      "step": 184
    },
    {
      "epoch": 0.016655787886290487,
      "grad_norm": 1.1529870634885302,
      "learning_rate": 1.1077844311377246e-05,
      "loss": 0.7556,
      "step": 185
    },
    {
      "epoch": 0.016745819172162328,
      "grad_norm": 1.0203694134467716,
      "learning_rate": 1.1137724550898203e-05,
      "loss": 0.7044,
      "step": 186
    },
    {
      "epoch": 0.016835850458034166,
      "grad_norm": 1.2913129267518086,
      "learning_rate": 1.1197604790419163e-05,
      "loss": 0.8688,
      "step": 187
    },
    {
      "epoch": 0.016925881743906006,
      "grad_norm": 1.0335822172924736,
      "learning_rate": 1.125748502994012e-05,
      "loss": 0.8442,
      "step": 188
    },
    {
      "epoch": 0.017015913029777847,
      "grad_norm": 1.3324760299114962,
      "learning_rate": 1.1317365269461078e-05,
      "loss": 0.6356,
      "step": 189
    },
    {
      "epoch": 0.01710594431564969,
      "grad_norm": 1.0559518283344824,
      "learning_rate": 1.1377245508982037e-05,
      "loss": 0.8407,
      "step": 190
    },
    {
      "epoch": 0.01719597560152153,
      "grad_norm": 1.2372573085155179,
      "learning_rate": 1.1437125748502994e-05,
      "loss": 0.7884,
      "step": 191
    },
    {
      "epoch": 0.01728600688739337,
      "grad_norm": 1.6252964414378663,
      "learning_rate": 1.1497005988023952e-05,
      "loss": 0.6976,
      "step": 192
    },
    {
      "epoch": 0.01737603817326521,
      "grad_norm": 1.179508435932061,
      "learning_rate": 1.155688622754491e-05,
      "loss": 0.8093,
      "step": 193
    },
    {
      "epoch": 0.01746606945913705,
      "grad_norm": 1.2256190108518643,
      "learning_rate": 1.161676646706587e-05,
      "loss": 0.8698,
      "step": 194
    },
    {
      "epoch": 0.01755610074500889,
      "grad_norm": 1.0611114298713977,
      "learning_rate": 1.1676646706586828e-05,
      "loss": 0.7538,
      "step": 195
    },
    {
      "epoch": 0.01764613203088073,
      "grad_norm": 0.9950395386762348,
      "learning_rate": 1.1736526946107785e-05,
      "loss": 0.7663,
      "step": 196
    },
    {
      "epoch": 0.01773616331675257,
      "grad_norm": 0.9345413103886416,
      "learning_rate": 1.1796407185628744e-05,
      "loss": 0.8042,
      "step": 197
    },
    {
      "epoch": 0.017826194602624412,
      "grad_norm": 1.1513624523271637,
      "learning_rate": 1.1856287425149702e-05,
      "loss": 0.7476,
      "step": 198
    },
    {
      "epoch": 0.017916225888496253,
      "grad_norm": 0.9917885573121306,
      "learning_rate": 1.191616766467066e-05,
      "loss": 0.6764,
      "step": 199
    },
    {
      "epoch": 0.018006257174368094,
      "grad_norm": 1.1455076994614117,
      "learning_rate": 1.1976047904191617e-05,
      "loss": 0.7641,
      "step": 200
    },
    {
      "epoch": 0.018096288460239935,
      "grad_norm": 1.079280943614145,
      "learning_rate": 1.2035928143712576e-05,
      "loss": 0.7699,
      "step": 201
    },
    {
      "epoch": 0.018186319746111772,
      "grad_norm": 1.6282281220460955,
      "learning_rate": 1.2095808383233534e-05,
      "loss": 0.7711,
      "step": 202
    },
    {
      "epoch": 0.018276351031983613,
      "grad_norm": 1.1911660000987887,
      "learning_rate": 1.2155688622754491e-05,
      "loss": 0.7274,
      "step": 203
    },
    {
      "epoch": 0.018366382317855454,
      "grad_norm": 1.4952145915438109,
      "learning_rate": 1.221556886227545e-05,
      "loss": 0.7791,
      "step": 204
    },
    {
      "epoch": 0.018456413603727295,
      "grad_norm": 0.917109616944876,
      "learning_rate": 1.2275449101796408e-05,
      "loss": 0.8326,
      "step": 205
    },
    {
      "epoch": 0.018546444889599136,
      "grad_norm": 0.8996773049694357,
      "learning_rate": 1.2335329341317365e-05,
      "loss": 0.792,
      "step": 206
    },
    {
      "epoch": 0.018636476175470977,
      "grad_norm": 1.0518188904755785,
      "learning_rate": 1.2395209580838323e-05,
      "loss": 0.6833,
      "step": 207
    },
    {
      "epoch": 0.018726507461342818,
      "grad_norm": 0.8468937171711203,
      "learning_rate": 1.2455089820359284e-05,
      "loss": 0.765,
      "step": 208
    },
    {
      "epoch": 0.018816538747214655,
      "grad_norm": 1.242710711136054,
      "learning_rate": 1.251497005988024e-05,
      "loss": 0.7173,
      "step": 209
    },
    {
      "epoch": 0.018906570033086496,
      "grad_norm": 1.5088258776577963,
      "learning_rate": 1.2574850299401197e-05,
      "loss": 0.7702,
      "step": 210
    },
    {
      "epoch": 0.018996601318958337,
      "grad_norm": 1.110204992068546,
      "learning_rate": 1.2634730538922158e-05,
      "loss": 0.7519,
      "step": 211
    },
    {
      "epoch": 0.019086632604830178,
      "grad_norm": 0.9559808058068264,
      "learning_rate": 1.2694610778443115e-05,
      "loss": 0.7472,
      "step": 212
    },
    {
      "epoch": 0.01917666389070202,
      "grad_norm": 1.0332379184987872,
      "learning_rate": 1.2754491017964073e-05,
      "loss": 0.7906,
      "step": 213
    },
    {
      "epoch": 0.01926669517657386,
      "grad_norm": 1.1095106514418114,
      "learning_rate": 1.2814371257485032e-05,
      "loss": 0.7236,
      "step": 214
    },
    {
      "epoch": 0.0193567264624457,
      "grad_norm": 1.13015902224156,
      "learning_rate": 1.287425149700599e-05,
      "loss": 0.7618,
      "step": 215
    },
    {
      "epoch": 0.019446757748317542,
      "grad_norm": 1.1659587271251333,
      "learning_rate": 1.2934131736526947e-05,
      "loss": 0.833,
      "step": 216
    },
    {
      "epoch": 0.01953678903418938,
      "grad_norm": 1.0230752093600786,
      "learning_rate": 1.2994011976047905e-05,
      "loss": 0.6812,
      "step": 217
    },
    {
      "epoch": 0.01962682032006122,
      "grad_norm": 1.7348583283074859,
      "learning_rate": 1.3053892215568864e-05,
      "loss": 0.8869,
      "step": 218
    },
    {
      "epoch": 0.01971685160593306,
      "grad_norm": 1.299584940644756,
      "learning_rate": 1.3113772455089821e-05,
      "loss": 0.7576,
      "step": 219
    },
    {
      "epoch": 0.019806882891804902,
      "grad_norm": 1.0002789425307905,
      "learning_rate": 1.3173652694610779e-05,
      "loss": 0.6886,
      "step": 220
    },
    {
      "epoch": 0.019896914177676743,
      "grad_norm": 1.136615376958967,
      "learning_rate": 1.3233532934131738e-05,
      "loss": 0.8495,
      "step": 221
    },
    {
      "epoch": 0.019986945463548584,
      "grad_norm": 0.9540952441465145,
      "learning_rate": 1.3293413173652696e-05,
      "loss": 0.803,
      "step": 222
    },
    {
      "epoch": 0.020076976749420425,
      "grad_norm": 1.181992482800437,
      "learning_rate": 1.3353293413173653e-05,
      "loss": 0.7078,
      "step": 223
    },
    {
      "epoch": 0.020167008035292266,
      "grad_norm": 1.055158559242383,
      "learning_rate": 1.341317365269461e-05,
      "loss": 0.6684,
      "step": 224
    },
    {
      "epoch": 0.020257039321164103,
      "grad_norm": 1.0086104893618624,
      "learning_rate": 1.3473053892215571e-05,
      "loss": 0.824,
      "step": 225
    },
    {
      "epoch": 0.020347070607035944,
      "grad_norm": 1.1220704695068715,
      "learning_rate": 1.3532934131736529e-05,
      "loss": 0.7232,
      "step": 226
    },
    {
      "epoch": 0.020437101892907785,
      "grad_norm": 0.9991829252365991,
      "learning_rate": 1.3592814371257486e-05,
      "loss": 0.7362,
      "step": 227
    },
    {
      "epoch": 0.020527133178779626,
      "grad_norm": 1.227927550440298,
      "learning_rate": 1.3652694610778446e-05,
      "loss": 0.6417,
      "step": 228
    },
    {
      "epoch": 0.020617164464651467,
      "grad_norm": 1.0821755863481939,
      "learning_rate": 1.3712574850299403e-05,
      "loss": 0.7349,
      "step": 229
    },
    {
      "epoch": 0.020707195750523308,
      "grad_norm": 1.2201610404663925,
      "learning_rate": 1.377245508982036e-05,
      "loss": 0.7202,
      "step": 230
    },
    {
      "epoch": 0.02079722703639515,
      "grad_norm": 1.1012493982821618,
      "learning_rate": 1.3832335329341318e-05,
      "loss": 0.7133,
      "step": 231
    },
    {
      "epoch": 0.020887258322266986,
      "grad_norm": 1.8589906634791553,
      "learning_rate": 1.3892215568862277e-05,
      "loss": 0.6954,
      "step": 232
    },
    {
      "epoch": 0.020977289608138827,
      "grad_norm": 1.292403702654239,
      "learning_rate": 1.3952095808383235e-05,
      "loss": 0.8225,
      "step": 233
    },
    {
      "epoch": 0.021067320894010668,
      "grad_norm": 0.9991476286614815,
      "learning_rate": 1.4011976047904192e-05,
      "loss": 0.7401,
      "step": 234
    },
    {
      "epoch": 0.02115735217988251,
      "grad_norm": 1.057707280314252,
      "learning_rate": 1.4071856287425152e-05,
      "loss": 0.7047,
      "step": 235
    },
    {
      "epoch": 0.02124738346575435,
      "grad_norm": 1.6174773304396937,
      "learning_rate": 1.4131736526946109e-05,
      "loss": 0.7638,
      "step": 236
    },
    {
      "epoch": 0.02133741475162619,
      "grad_norm": 0.9770869703582785,
      "learning_rate": 1.4191616766467067e-05,
      "loss": 0.6879,
      "step": 237
    },
    {
      "epoch": 0.021427446037498032,
      "grad_norm": 1.2044931738095577,
      "learning_rate": 1.4251497005988024e-05,
      "loss": 0.7093,
      "step": 238
    },
    {
      "epoch": 0.021517477323369873,
      "grad_norm": 1.2591650425402083,
      "learning_rate": 1.4311377245508983e-05,
      "loss": 0.7047,
      "step": 239
    },
    {
      "epoch": 0.02160750860924171,
      "grad_norm": 1.3325849323922347,
      "learning_rate": 1.437125748502994e-05,
      "loss": 0.8119,
      "step": 240
    },
    {
      "epoch": 0.02169753989511355,
      "grad_norm": 1.044619105620737,
      "learning_rate": 1.4431137724550898e-05,
      "loss": 0.7145,
      "step": 241
    },
    {
      "epoch": 0.021787571180985392,
      "grad_norm": 0.9108638636478845,
      "learning_rate": 1.4491017964071859e-05,
      "loss": 0.7488,
      "step": 242
    },
    {
      "epoch": 0.021877602466857233,
      "grad_norm": 0.9067176060824301,
      "learning_rate": 1.4550898203592817e-05,
      "loss": 0.7692,
      "step": 243
    },
    {
      "epoch": 0.021967633752729074,
      "grad_norm": 1.0757758710260932,
      "learning_rate": 1.4610778443113774e-05,
      "loss": 0.733,
      "step": 244
    },
    {
      "epoch": 0.022057665038600915,
      "grad_norm": 1.0817092748791945,
      "learning_rate": 1.4670658682634732e-05,
      "loss": 0.7518,
      "step": 245
    },
    {
      "epoch": 0.022147696324472756,
      "grad_norm": 1.2665769118935861,
      "learning_rate": 1.473053892215569e-05,
      "loss": 0.7131,
      "step": 246
    },
    {
      "epoch": 0.022237727610344593,
      "grad_norm": 0.8782813624198264,
      "learning_rate": 1.4790419161676648e-05,
      "loss": 0.7223,
      "step": 247
    },
    {
      "epoch": 0.022327758896216434,
      "grad_norm": 1.1299990880021398,
      "learning_rate": 1.4850299401197606e-05,
      "loss": 0.7932,
      "step": 248
    },
    {
      "epoch": 0.022417790182088275,
      "grad_norm": 1.4487218409017544,
      "learning_rate": 1.4910179640718565e-05,
      "loss": 0.7243,
      "step": 249
    },
    {
      "epoch": 0.022507821467960116,
      "grad_norm": 0.9658256639758063,
      "learning_rate": 1.4970059880239522e-05,
      "loss": 0.6858,
      "step": 250
    },
    {
      "epoch": 0.022597852753831957,
      "grad_norm": 1.1990940738458113,
      "learning_rate": 1.502994011976048e-05,
      "loss": 0.7275,
      "step": 251
    },
    {
      "epoch": 0.022687884039703798,
      "grad_norm": 1.0167921170121765,
      "learning_rate": 1.5089820359281437e-05,
      "loss": 0.7005,
      "step": 252
    },
    {
      "epoch": 0.02277791532557564,
      "grad_norm": 1.0989668819241578,
      "learning_rate": 1.5149700598802397e-05,
      "loss": 0.747,
      "step": 253
    },
    {
      "epoch": 0.02286794661144748,
      "grad_norm": 1.0008037624856836,
      "learning_rate": 1.5209580838323354e-05,
      "loss": 0.7252,
      "step": 254
    },
    {
      "epoch": 0.022957977897319317,
      "grad_norm": 0.891801830379952,
      "learning_rate": 1.5269461077844313e-05,
      "loss": 0.7142,
      "step": 255
    },
    {
      "epoch": 0.023048009183191158,
      "grad_norm": 1.2525732594495722,
      "learning_rate": 1.5329341317365273e-05,
      "loss": 0.7225,
      "step": 256
    },
    {
      "epoch": 0.023138040469063,
      "grad_norm": 1.3714691753549015,
      "learning_rate": 1.538922155688623e-05,
      "loss": 0.7212,
      "step": 257
    },
    {
      "epoch": 0.02322807175493484,
      "grad_norm": 1.0383267661378086,
      "learning_rate": 1.5449101796407188e-05,
      "loss": 0.6694,
      "step": 258
    },
    {
      "epoch": 0.02331810304080668,
      "grad_norm": 1.3167750012922486,
      "learning_rate": 1.5508982035928143e-05,
      "loss": 0.6936,
      "step": 259
    },
    {
      "epoch": 0.02340813432667852,
      "grad_norm": 1.1179701939529567,
      "learning_rate": 1.5568862275449103e-05,
      "loss": 0.6869,
      "step": 260
    },
    {
      "epoch": 0.023498165612550363,
      "grad_norm": 0.9698952203268933,
      "learning_rate": 1.5628742514970062e-05,
      "loss": 0.7312,
      "step": 261
    },
    {
      "epoch": 0.0235881968984222,
      "grad_norm": 1.1503402955718152,
      "learning_rate": 1.5688622754491018e-05,
      "loss": 0.6427,
      "step": 262
    },
    {
      "epoch": 0.02367822818429404,
      "grad_norm": 0.8927721043687864,
      "learning_rate": 1.5748502994011977e-05,
      "loss": 0.7727,
      "step": 263
    },
    {
      "epoch": 0.023768259470165882,
      "grad_norm": 1.021226899857566,
      "learning_rate": 1.5808383233532936e-05,
      "loss": 0.7845,
      "step": 264
    },
    {
      "epoch": 0.023858290756037723,
      "grad_norm": 1.1516245193679466,
      "learning_rate": 1.5868263473053892e-05,
      "loss": 0.5849,
      "step": 265
    },
    {
      "epoch": 0.023948322041909564,
      "grad_norm": 1.2120505790877731,
      "learning_rate": 1.592814371257485e-05,
      "loss": 0.7211,
      "step": 266
    },
    {
      "epoch": 0.024038353327781405,
      "grad_norm": 1.363436723415705,
      "learning_rate": 1.598802395209581e-05,
      "loss": 0.7059,
      "step": 267
    },
    {
      "epoch": 0.024128384613653246,
      "grad_norm": 1.5738444975048604,
      "learning_rate": 1.604790419161677e-05,
      "loss": 0.7291,
      "step": 268
    },
    {
      "epoch": 0.024218415899525086,
      "grad_norm": 1.2906585059458622,
      "learning_rate": 1.6107784431137725e-05,
      "loss": 0.6326,
      "step": 269
    },
    {
      "epoch": 0.024308447185396924,
      "grad_norm": 1.192829953868401,
      "learning_rate": 1.6167664670658684e-05,
      "loss": 0.7975,
      "step": 270
    },
    {
      "epoch": 0.024398478471268765,
      "grad_norm": 0.9849306650247308,
      "learning_rate": 1.6227544910179644e-05,
      "loss": 0.7736,
      "step": 271
    },
    {
      "epoch": 0.024488509757140606,
      "grad_norm": 1.3052315501461815,
      "learning_rate": 1.62874251497006e-05,
      "loss": 0.7471,
      "step": 272
    },
    {
      "epoch": 0.024578541043012447,
      "grad_norm": 1.0590858570492434,
      "learning_rate": 1.634730538922156e-05,
      "loss": 0.674,
      "step": 273
    },
    {
      "epoch": 0.024668572328884288,
      "grad_norm": 1.5515912020559677,
      "learning_rate": 1.6407185628742518e-05,
      "loss": 0.6721,
      "step": 274
    },
    {
      "epoch": 0.02475860361475613,
      "grad_norm": 1.321746609018337,
      "learning_rate": 1.6467065868263474e-05,
      "loss": 0.7915,
      "step": 275
    },
    {
      "epoch": 0.02484863490062797,
      "grad_norm": 1.104794932722275,
      "learning_rate": 1.6526946107784433e-05,
      "loss": 0.7211,
      "step": 276
    },
    {
      "epoch": 0.02493866618649981,
      "grad_norm": 0.9699249187911474,
      "learning_rate": 1.6586826347305392e-05,
      "loss": 0.6404,
      "step": 277
    },
    {
      "epoch": 0.025028697472371648,
      "grad_norm": 1.3607653849038666,
      "learning_rate": 1.6646706586826348e-05,
      "loss": 0.6676,
      "step": 278
    },
    {
      "epoch": 0.02511872875824349,
      "grad_norm": 1.1909879848960925,
      "learning_rate": 1.6706586826347307e-05,
      "loss": 0.7132,
      "step": 279
    },
    {
      "epoch": 0.02520876004411533,
      "grad_norm": 1.118554397271032,
      "learning_rate": 1.6766467065868263e-05,
      "loss": 0.6999,
      "step": 280
    },
    {
      "epoch": 0.02529879132998717,
      "grad_norm": 1.072400822288408,
      "learning_rate": 1.6826347305389222e-05,
      "loss": 0.7683,
      "step": 281
    },
    {
      "epoch": 0.02538882261585901,
      "grad_norm": 1.242049462903517,
      "learning_rate": 1.688622754491018e-05,
      "loss": 0.838,
      "step": 282
    },
    {
      "epoch": 0.025478853901730852,
      "grad_norm": 1.0337484093695724,
      "learning_rate": 1.6946107784431137e-05,
      "loss": 0.714,
      "step": 283
    },
    {
      "epoch": 0.025568885187602693,
      "grad_norm": 1.5243742928821093,
      "learning_rate": 1.70059880239521e-05,
      "loss": 0.7024,
      "step": 284
    },
    {
      "epoch": 0.02565891647347453,
      "grad_norm": 1.8869285797792146,
      "learning_rate": 1.7065868263473055e-05,
      "loss": 0.7747,
      "step": 285
    },
    {
      "epoch": 0.02574894775934637,
      "grad_norm": 1.448865057294533,
      "learning_rate": 1.7125748502994015e-05,
      "loss": 0.663,
      "step": 286
    },
    {
      "epoch": 0.025838979045218213,
      "grad_norm": 1.0594209047849696,
      "learning_rate": 1.718562874251497e-05,
      "loss": 0.7208,
      "step": 287
    },
    {
      "epoch": 0.025929010331090054,
      "grad_norm": 1.0035033922056396,
      "learning_rate": 1.724550898203593e-05,
      "loss": 0.7366,
      "step": 288
    },
    {
      "epoch": 0.026019041616961894,
      "grad_norm": 1.3204407937512068,
      "learning_rate": 1.730538922155689e-05,
      "loss": 0.7096,
      "step": 289
    },
    {
      "epoch": 0.026109072902833735,
      "grad_norm": 0.957790865196517,
      "learning_rate": 1.7365269461077845e-05,
      "loss": 0.6958,
      "step": 290
    },
    {
      "epoch": 0.026199104188705576,
      "grad_norm": 1.0122879884508678,
      "learning_rate": 1.7425149700598804e-05,
      "loss": 0.6794,
      "step": 291
    },
    {
      "epoch": 0.026289135474577417,
      "grad_norm": 1.04660373434976,
      "learning_rate": 1.7485029940119763e-05,
      "loss": 0.7053,
      "step": 292
    },
    {
      "epoch": 0.026379166760449255,
      "grad_norm": 0.9046876398949344,
      "learning_rate": 1.754491017964072e-05,
      "loss": 0.659,
      "step": 293
    },
    {
      "epoch": 0.026469198046321096,
      "grad_norm": 1.238830855199633,
      "learning_rate": 1.7604790419161678e-05,
      "loss": 0.6581,
      "step": 294
    },
    {
      "epoch": 0.026559229332192937,
      "grad_norm": 1.0513968276651116,
      "learning_rate": 1.7664670658682637e-05,
      "loss": 0.7141,
      "step": 295
    },
    {
      "epoch": 0.026649260618064777,
      "grad_norm": 1.3891947839669683,
      "learning_rate": 1.7724550898203593e-05,
      "loss": 0.6673,
      "step": 296
    },
    {
      "epoch": 0.02673929190393662,
      "grad_norm": 1.3376600470554183,
      "learning_rate": 1.7784431137724552e-05,
      "loss": 0.7344,
      "step": 297
    },
    {
      "epoch": 0.02682932318980846,
      "grad_norm": 1.1107597442485515,
      "learning_rate": 1.784431137724551e-05,
      "loss": 0.6909,
      "step": 298
    },
    {
      "epoch": 0.0269193544756803,
      "grad_norm": 0.8617283025990297,
      "learning_rate": 1.7904191616766467e-05,
      "loss": 0.6629,
      "step": 299
    },
    {
      "epoch": 0.027009385761552138,
      "grad_norm": 0.9382321880744721,
      "learning_rate": 1.7964071856287426e-05,
      "loss": 0.7629,
      "step": 300
    },
    {
      "epoch": 0.02709941704742398,
      "grad_norm": 0.8373180856630095,
      "learning_rate": 1.8023952095808385e-05,
      "loss": 0.6908,
      "step": 301
    },
    {
      "epoch": 0.02718944833329582,
      "grad_norm": 1.0704589360761505,
      "learning_rate": 1.8083832335329345e-05,
      "loss": 0.7898,
      "step": 302
    },
    {
      "epoch": 0.02727947961916766,
      "grad_norm": 1.566921632807856,
      "learning_rate": 1.81437125748503e-05,
      "loss": 0.7124,
      "step": 303
    },
    {
      "epoch": 0.0273695109050395,
      "grad_norm": 1.563290260773413,
      "learning_rate": 1.820359281437126e-05,
      "loss": 0.6925,
      "step": 304
    },
    {
      "epoch": 0.027459542190911342,
      "grad_norm": 1.0009710130577676,
      "learning_rate": 1.826347305389222e-05,
      "loss": 0.6528,
      "step": 305
    },
    {
      "epoch": 0.027549573476783183,
      "grad_norm": 1.2382406317474601,
      "learning_rate": 1.8323353293413175e-05,
      "loss": 0.6829,
      "step": 306
    },
    {
      "epoch": 0.027639604762655024,
      "grad_norm": 1.0214734229510793,
      "learning_rate": 1.8383233532934134e-05,
      "loss": 0.6983,
      "step": 307
    },
    {
      "epoch": 0.02772963604852686,
      "grad_norm": 1.1396426731812606,
      "learning_rate": 1.8443113772455093e-05,
      "loss": 0.7751,
      "step": 308
    },
    {
      "epoch": 0.027819667334398702,
      "grad_norm": 1.1838060588552302,
      "learning_rate": 1.850299401197605e-05,
      "loss": 0.7722,
      "step": 309
    },
    {
      "epoch": 0.027909698620270543,
      "grad_norm": 1.1561364459269983,
      "learning_rate": 1.8562874251497008e-05,
      "loss": 0.728,
      "step": 310
    },
    {
      "epoch": 0.027999729906142384,
      "grad_norm": 1.0126642873006217,
      "learning_rate": 1.8622754491017964e-05,
      "loss": 0.7175,
      "step": 311
    },
    {
      "epoch": 0.028089761192014225,
      "grad_norm": 0.8772161008762296,
      "learning_rate": 1.8682634730538923e-05,
      "loss": 0.7072,
      "step": 312
    },
    {
      "epoch": 0.028179792477886066,
      "grad_norm": 1.1617991491474906,
      "learning_rate": 1.8742514970059882e-05,
      "loss": 0.7294,
      "step": 313
    },
    {
      "epoch": 0.028269823763757907,
      "grad_norm": 1.3450053061731657,
      "learning_rate": 1.8802395209580838e-05,
      "loss": 0.7581,
      "step": 314
    },
    {
      "epoch": 0.028359855049629748,
      "grad_norm": 1.0506053979375112,
      "learning_rate": 1.88622754491018e-05,
      "loss": 0.767,
      "step": 315
    },
    {
      "epoch": 0.028449886335501585,
      "grad_norm": 1.2710645025103533,
      "learning_rate": 1.8922155688622756e-05,
      "loss": 0.6798,
      "step": 316
    },
    {
      "epoch": 0.028539917621373426,
      "grad_norm": 1.0450450201658972,
      "learning_rate": 1.8982035928143712e-05,
      "loss": 0.7806,
      "step": 317
    },
    {
      "epoch": 0.028629948907245267,
      "grad_norm": 1.5216882277260866,
      "learning_rate": 1.904191616766467e-05,
      "loss": 0.6587,
      "step": 318
    },
    {
      "epoch": 0.028719980193117108,
      "grad_norm": 1.2400912090548855,
      "learning_rate": 1.910179640718563e-05,
      "loss": 0.7079,
      "step": 319
    },
    {
      "epoch": 0.02881001147898895,
      "grad_norm": 1.4074377068837494,
      "learning_rate": 1.916167664670659e-05,
      "loss": 0.7407,
      "step": 320
    },
    {
      "epoch": 0.02890004276486079,
      "grad_norm": 1.2993173525755881,
      "learning_rate": 1.9221556886227546e-05,
      "loss": 0.6251,
      "step": 321
    },
    {
      "epoch": 0.02899007405073263,
      "grad_norm": 1.2545121452794654,
      "learning_rate": 1.9281437125748505e-05,
      "loss": 0.719,
      "step": 322
    },
    {
      "epoch": 0.02908010533660447,
      "grad_norm": 1.2558828141687788,
      "learning_rate": 1.9341317365269464e-05,
      "loss": 0.7275,
      "step": 323
    },
    {
      "epoch": 0.02917013662247631,
      "grad_norm": 0.8384655923003596,
      "learning_rate": 1.940119760479042e-05,
      "loss": 0.789,
      "step": 324
    },
    {
      "epoch": 0.02926016790834815,
      "grad_norm": 1.1995110410397083,
      "learning_rate": 1.946107784431138e-05,
      "loss": 0.7482,
      "step": 325
    },
    {
      "epoch": 0.02935019919421999,
      "grad_norm": 1.0514167433633306,
      "learning_rate": 1.9520958083832338e-05,
      "loss": 0.7621,
      "step": 326
    },
    {
      "epoch": 0.029440230480091832,
      "grad_norm": 1.0318702506809339,
      "learning_rate": 1.9580838323353294e-05,
      "loss": 0.8033,
      "step": 327
    },
    {
      "epoch": 0.029530261765963673,
      "grad_norm": 1.3196737365034539,
      "learning_rate": 1.9640718562874253e-05,
      "loss": 0.736,
      "step": 328
    },
    {
      "epoch": 0.029620293051835514,
      "grad_norm": 1.131981192849851,
      "learning_rate": 1.9700598802395212e-05,
      "loss": 0.6405,
      "step": 329
    },
    {
      "epoch": 0.029710324337707355,
      "grad_norm": 1.6420591104599187,
      "learning_rate": 1.9760479041916168e-05,
      "loss": 0.743,
      "step": 330
    },
    {
      "epoch": 0.029800355623579192,
      "grad_norm": 0.9833596339960043,
      "learning_rate": 1.9820359281437127e-05,
      "loss": 0.6992,
      "step": 331
    },
    {
      "epoch": 0.029890386909451033,
      "grad_norm": 1.1167266672865632,
      "learning_rate": 1.9880239520958083e-05,
      "loss": 0.6881,
      "step": 332
    },
    {
      "epoch": 0.029980418195322874,
      "grad_norm": 1.035638407346431,
      "learning_rate": 1.9940119760479046e-05,
      "loss": 0.7416,
      "step": 333
    },
    {
      "epoch": 0.030070449481194715,
      "grad_norm": 1.36039525813233,
      "learning_rate": 2e-05,
      "loss": 0.6478,
      "step": 334
    },
    {
      "epoch": 0.030160480767066556,
      "grad_norm": 0.8664694922522777,
      "learning_rate": 1.99999995747969e-05,
      "loss": 0.7071,
      "step": 335
    },
    {
      "epoch": 0.030250512052938397,
      "grad_norm": 1.033357622247727,
      "learning_rate": 1.999999829918762e-05,
      "loss": 0.7406,
      "step": 336
    },
    {
      "epoch": 0.030340543338810238,
      "grad_norm": 1.5140088391436606,
      "learning_rate": 1.9999996173172275e-05,
      "loss": 0.7726,
      "step": 337
    },
    {
      "epoch": 0.030430574624682075,
      "grad_norm": 1.1523211192769354,
      "learning_rate": 1.999999319675105e-05,
      "loss": 0.7298,
      "step": 338
    },
    {
      "epoch": 0.030520605910553916,
      "grad_norm": 1.1360940041191967,
      "learning_rate": 1.9999989369924192e-05,
      "loss": 0.7212,
      "step": 339
    },
    {
      "epoch": 0.030610637196425757,
      "grad_norm": 1.2093574449043472,
      "learning_rate": 1.999998469269203e-05,
      "loss": 0.7729,
      "step": 340
    },
    {
      "epoch": 0.030700668482297598,
      "grad_norm": 1.3604914453688315,
      "learning_rate": 1.999997916505496e-05,
      "loss": 0.921,
      "step": 341
    },
    {
      "epoch": 0.03079069976816944,
      "grad_norm": 1.0050887442614929,
      "learning_rate": 1.9999972787013448e-05,
      "loss": 0.8153,
      "step": 342
    },
    {
      "epoch": 0.03088073105404128,
      "grad_norm": 1.2754701541936775,
      "learning_rate": 1.9999965558568046e-05,
      "loss": 0.6083,
      "step": 343
    },
    {
      "epoch": 0.03097076233991312,
      "grad_norm": 1.5121364336404322,
      "learning_rate": 1.9999957479719363e-05,
      "loss": 0.7519,
      "step": 344
    },
    {
      "epoch": 0.031060793625784962,
      "grad_norm": 1.241560363313832,
      "learning_rate": 1.9999948550468084e-05,
      "loss": 0.8027,
      "step": 345
    },
    {
      "epoch": 0.0311508249116568,
      "grad_norm": 1.049797028122728,
      "learning_rate": 1.9999938770814973e-05,
      "loss": 0.7535,
      "step": 346
    },
    {
      "epoch": 0.03124085619752864,
      "grad_norm": 1.011938030408643,
      "learning_rate": 1.999992814076086e-05,
      "loss": 0.6088,
      "step": 347
    },
    {
      "epoch": 0.03133088748340048,
      "grad_norm": 0.886449385474628,
      "learning_rate": 1.9999916660306647e-05,
      "loss": 0.6761,
      "step": 348
    },
    {
      "epoch": 0.031420918769272325,
      "grad_norm": 1.2226198782834363,
      "learning_rate": 1.999990432945331e-05,
      "loss": 0.7313,
      "step": 349
    },
    {
      "epoch": 0.03151095005514416,
      "grad_norm": 1.3659823799357695,
      "learning_rate": 1.9999891148201906e-05,
      "loss": 0.6992,
      "step": 350
    },
    {
      "epoch": 0.031600981341016,
      "grad_norm": 1.1512317560282594,
      "learning_rate": 1.9999877116553545e-05,
      "loss": 0.8793,
      "step": 351
    },
    {
      "epoch": 0.031691012626887845,
      "grad_norm": 1.227072896968124,
      "learning_rate": 1.999986223450942e-05,
      "loss": 0.7375,
      "step": 352
    },
    {
      "epoch": 0.03178104391275968,
      "grad_norm": 1.0604153405021655,
      "learning_rate": 1.9999846502070808e-05,
      "loss": 0.6474,
      "step": 353
    },
    {
      "epoch": 0.03187107519863153,
      "grad_norm": 1.097120624952212,
      "learning_rate": 1.9999829919239037e-05,
      "loss": 0.6204,
      "step": 354
    },
    {
      "epoch": 0.031961106484503364,
      "grad_norm": 1.001599236776631,
      "learning_rate": 1.9999812486015525e-05,
      "loss": 0.7004,
      "step": 355
    },
    {
      "epoch": 0.03205113777037521,
      "grad_norm": 0.9660011165797198,
      "learning_rate": 1.9999794202401746e-05,
      "loss": 0.678,
      "step": 356
    },
    {
      "epoch": 0.032141169056247046,
      "grad_norm": 0.9816157647806146,
      "learning_rate": 1.999977506839926e-05,
      "loss": 0.7038,
      "step": 357
    },
    {
      "epoch": 0.03223120034211888,
      "grad_norm": 0.9813313418184272,
      "learning_rate": 1.999975508400969e-05,
      "loss": 0.7469,
      "step": 358
    },
    {
      "epoch": 0.03232123162799073,
      "grad_norm": 1.2546502976902816,
      "learning_rate": 1.9999734249234743e-05,
      "loss": 0.6814,
      "step": 359
    },
    {
      "epoch": 0.032411262913862565,
      "grad_norm": 1.0944264300824627,
      "learning_rate": 1.999971256407618e-05,
      "loss": 0.6403,
      "step": 360
    },
    {
      "epoch": 0.03250129419973441,
      "grad_norm": 1.2604334505707044,
      "learning_rate": 1.9999690028535856e-05,
      "loss": 0.7952,
      "step": 361
    },
    {
      "epoch": 0.03259132548560625,
      "grad_norm": 1.1143774936881077,
      "learning_rate": 1.9999666642615682e-05,
      "loss": 0.7035,
      "step": 362
    },
    {
      "epoch": 0.03268135677147809,
      "grad_norm": 1.1853868235924514,
      "learning_rate": 1.9999642406317648e-05,
      "loss": 0.7614,
      "step": 363
    },
    {
      "epoch": 0.03277138805734993,
      "grad_norm": 1.070955251088583,
      "learning_rate": 1.9999617319643814e-05,
      "loss": 0.7057,
      "step": 364
    },
    {
      "epoch": 0.032861419343221766,
      "grad_norm": 1.225498419576497,
      "learning_rate": 1.999959138259631e-05,
      "loss": 0.7676,
      "step": 365
    },
    {
      "epoch": 0.03295145062909361,
      "grad_norm": 1.2037598509286898,
      "learning_rate": 1.9999564595177352e-05,
      "loss": 0.8606,
      "step": 366
    },
    {
      "epoch": 0.03304148191496545,
      "grad_norm": 1.1229650100025825,
      "learning_rate": 1.999953695738921e-05,
      "loss": 0.6718,
      "step": 367
    },
    {
      "epoch": 0.03313151320083729,
      "grad_norm": 0.837134407431475,
      "learning_rate": 1.9999508469234236e-05,
      "loss": 0.6425,
      "step": 368
    },
    {
      "epoch": 0.03322154448670913,
      "grad_norm": 1.1172102360619958,
      "learning_rate": 1.999947913071485e-05,
      "loss": 0.7089,
      "step": 369
    },
    {
      "epoch": 0.033311575772580974,
      "grad_norm": 1.3985185891802863,
      "learning_rate": 1.9999448941833554e-05,
      "loss": 0.733,
      "step": 370
    },
    {
      "epoch": 0.03340160705845281,
      "grad_norm": 1.231579332736978,
      "learning_rate": 1.9999417902592908e-05,
      "loss": 0.6866,
      "step": 371
    },
    {
      "epoch": 0.033491638344324656,
      "grad_norm": 0.9027881420808563,
      "learning_rate": 1.9999386012995554e-05,
      "loss": 0.7374,
      "step": 372
    },
    {
      "epoch": 0.033581669630196494,
      "grad_norm": 1.2840431963863042,
      "learning_rate": 1.9999353273044203e-05,
      "loss": 0.6165,
      "step": 373
    },
    {
      "epoch": 0.03367170091606833,
      "grad_norm": 0.8809649747194482,
      "learning_rate": 1.9999319682741646e-05,
      "loss": 0.6498,
      "step": 374
    },
    {
      "epoch": 0.033761732201940176,
      "grad_norm": 1.1176097325639944,
      "learning_rate": 1.999928524209073e-05,
      "loss": 0.6525,
      "step": 375
    },
    {
      "epoch": 0.03385176348781201,
      "grad_norm": 1.1660402779680152,
      "learning_rate": 1.9999249951094388e-05,
      "loss": 0.6807,
      "step": 376
    },
    {
      "epoch": 0.03394179477368386,
      "grad_norm": 1.0391478865139645,
      "learning_rate": 1.9999213809755626e-05,
      "loss": 0.6828,
      "step": 377
    },
    {
      "epoch": 0.034031826059555695,
      "grad_norm": 0.9559226681264685,
      "learning_rate": 1.9999176818077506e-05,
      "loss": 0.7372,
      "step": 378
    },
    {
      "epoch": 0.03412185734542754,
      "grad_norm": 0.9834952032660582,
      "learning_rate": 1.9999138976063184e-05,
      "loss": 0.6785,
      "step": 379
    },
    {
      "epoch": 0.03421188863129938,
      "grad_norm": 0.8817064035762529,
      "learning_rate": 1.9999100283715874e-05,
      "loss": 0.7053,
      "step": 380
    },
    {
      "epoch": 0.034301919917171214,
      "grad_norm": 1.1374678313151823,
      "learning_rate": 1.9999060741038866e-05,
      "loss": 0.738,
      "step": 381
    },
    {
      "epoch": 0.03439195120304306,
      "grad_norm": 0.8174510021850083,
      "learning_rate": 1.9999020348035525e-05,
      "loss": 0.72,
      "step": 382
    },
    {
      "epoch": 0.034481982488914896,
      "grad_norm": 1.0864211868740832,
      "learning_rate": 1.9998979104709283e-05,
      "loss": 0.6609,
      "step": 383
    },
    {
      "epoch": 0.03457201377478674,
      "grad_norm": 0.8089525867021431,
      "learning_rate": 1.9998937011063653e-05,
      "loss": 0.5729,
      "step": 384
    },
    {
      "epoch": 0.03466204506065858,
      "grad_norm": 1.2529905919028907,
      "learning_rate": 1.9998894067102207e-05,
      "loss": 0.7818,
      "step": 385
    },
    {
      "epoch": 0.03475207634653042,
      "grad_norm": 1.26006284742647,
      "learning_rate": 1.9998850272828603e-05,
      "loss": 0.5513,
      "step": 386
    },
    {
      "epoch": 0.03484210763240226,
      "grad_norm": 1.1269223451562422,
      "learning_rate": 1.9998805628246566e-05,
      "loss": 0.6507,
      "step": 387
    },
    {
      "epoch": 0.0349321389182741,
      "grad_norm": 1.708101296606514,
      "learning_rate": 1.9998760133359886e-05,
      "loss": 0.6871,
      "step": 388
    },
    {
      "epoch": 0.03502217020414594,
      "grad_norm": 0.9302080984951715,
      "learning_rate": 1.9998713788172432e-05,
      "loss": 0.6196,
      "step": 389
    },
    {
      "epoch": 0.03511220149001778,
      "grad_norm": 1.3517178603158877,
      "learning_rate": 1.9998666592688153e-05,
      "loss": 0.619,
      "step": 390
    },
    {
      "epoch": 0.03520223277588962,
      "grad_norm": 1.1373667467944153,
      "learning_rate": 1.999861854691106e-05,
      "loss": 0.7474,
      "step": 391
    },
    {
      "epoch": 0.03529226406176146,
      "grad_norm": 1.0417997709909075,
      "learning_rate": 1.9998569650845233e-05,
      "loss": 0.711,
      "step": 392
    },
    {
      "epoch": 0.035382295347633305,
      "grad_norm": 0.9807508340973097,
      "learning_rate": 1.9998519904494836e-05,
      "loss": 0.7123,
      "step": 393
    },
    {
      "epoch": 0.03547232663350514,
      "grad_norm": 1.2141684574373972,
      "learning_rate": 1.99984693078641e-05,
      "loss": 0.7303,
      "step": 394
    },
    {
      "epoch": 0.03556235791937698,
      "grad_norm": 1.0516625649284161,
      "learning_rate": 1.9998417860957323e-05,
      "loss": 0.676,
      "step": 395
    },
    {
      "epoch": 0.035652389205248824,
      "grad_norm": 1.0883959017974805,
      "learning_rate": 1.9998365563778884e-05,
      "loss": 0.6957,
      "step": 396
    },
    {
      "epoch": 0.03574242049112066,
      "grad_norm": 1.4029331937071077,
      "learning_rate": 1.999831241633323e-05,
      "loss": 0.6097,
      "step": 397
    },
    {
      "epoch": 0.035832451776992506,
      "grad_norm": 1.0787631943727622,
      "learning_rate": 1.999825841862488e-05,
      "loss": 0.6033,
      "step": 398
    },
    {
      "epoch": 0.035922483062864344,
      "grad_norm": 1.1161599106413456,
      "learning_rate": 1.9998203570658422e-05,
      "loss": 0.717,
      "step": 399
    },
    {
      "epoch": 0.03601251434873619,
      "grad_norm": 1.1199486924493793,
      "learning_rate": 1.9998147872438528e-05,
      "loss": 0.7984,
      "step": 400
    },
    {
      "epoch": 0.036102545634608026,
      "grad_norm": 1.047583439009767,
      "learning_rate": 1.9998091323969926e-05,
      "loss": 0.6562,
      "step": 401
    },
    {
      "epoch": 0.03619257692047987,
      "grad_norm": 1.0411746622689697,
      "learning_rate": 1.999803392525743e-05,
      "loss": 0.6433,
      "step": 402
    },
    {
      "epoch": 0.03628260820635171,
      "grad_norm": 1.5914733930172236,
      "learning_rate": 1.9997975676305924e-05,
      "loss": 0.7602,
      "step": 403
    },
    {
      "epoch": 0.036372639492223545,
      "grad_norm": 1.3198446274386821,
      "learning_rate": 1.999791657712036e-05,
      "loss": 0.6029,
      "step": 404
    },
    {
      "epoch": 0.03646267077809539,
      "grad_norm": 1.027209700989422,
      "learning_rate": 1.999785662770576e-05,
      "loss": 0.6956,
      "step": 405
    },
    {
      "epoch": 0.03655270206396723,
      "grad_norm": 1.0775473129589825,
      "learning_rate": 1.9997795828067222e-05,
      "loss": 0.6864,
      "step": 406
    },
    {
      "epoch": 0.03664273334983907,
      "grad_norm": 1.153432114549929,
      "learning_rate": 1.9997734178209924e-05,
      "loss": 0.7824,
      "step": 407
    },
    {
      "epoch": 0.03673276463571091,
      "grad_norm": 1.587511012991218,
      "learning_rate": 1.9997671678139098e-05,
      "loss": 0.7029,
      "step": 408
    },
    {
      "epoch": 0.03682279592158275,
      "grad_norm": 1.1499526420162585,
      "learning_rate": 1.999760832786007e-05,
      "loss": 0.7438,
      "step": 409
    },
    {
      "epoch": 0.03691282720745459,
      "grad_norm": 1.1622149998420663,
      "learning_rate": 1.9997544127378217e-05,
      "loss": 0.8071,
      "step": 410
    },
    {
      "epoch": 0.03700285849332643,
      "grad_norm": 1.4853790498597825,
      "learning_rate": 1.9997479076699005e-05,
      "loss": 0.7252,
      "step": 411
    },
    {
      "epoch": 0.03709288977919827,
      "grad_norm": 0.8825161997237293,
      "learning_rate": 1.9997413175827965e-05,
      "loss": 0.6835,
      "step": 412
    },
    {
      "epoch": 0.03718292106507011,
      "grad_norm": 0.9765527454611753,
      "learning_rate": 1.99973464247707e-05,
      "loss": 0.7107,
      "step": 413
    },
    {
      "epoch": 0.037272952350941954,
      "grad_norm": 0.8934199044296427,
      "learning_rate": 1.9997278823532888e-05,
      "loss": 0.5869,
      "step": 414
    },
    {
      "epoch": 0.03736298363681379,
      "grad_norm": 1.1215349270675523,
      "learning_rate": 1.9997210372120276e-05,
      "loss": 0.6516,
      "step": 415
    },
    {
      "epoch": 0.037453014922685636,
      "grad_norm": 1.2502380413387533,
      "learning_rate": 1.9997141070538685e-05,
      "loss": 0.7286,
      "step": 416
    },
    {
      "epoch": 0.03754304620855747,
      "grad_norm": 0.8646044612124596,
      "learning_rate": 1.9997070918794016e-05,
      "loss": 0.6465,
      "step": 417
    },
    {
      "epoch": 0.03763307749442931,
      "grad_norm": 1.6062957649608947,
      "learning_rate": 1.9996999916892222e-05,
      "loss": 0.6777,
      "step": 418
    },
    {
      "epoch": 0.037723108780301155,
      "grad_norm": 0.9067945605753994,
      "learning_rate": 1.999692806483935e-05,
      "loss": 0.6019,
      "step": 419
    },
    {
      "epoch": 0.03781314006617299,
      "grad_norm": 2.009814144716663,
      "learning_rate": 1.9996855362641505e-05,
      "loss": 0.6729,
      "step": 420
    },
    {
      "epoch": 0.03790317135204484,
      "grad_norm": 1.1256413774468526,
      "learning_rate": 1.9996781810304876e-05,
      "loss": 0.7441,
      "step": 421
    },
    {
      "epoch": 0.037993202637916675,
      "grad_norm": 1.070116116024383,
      "learning_rate": 1.9996707407835712e-05,
      "loss": 0.6678,
      "step": 422
    },
    {
      "epoch": 0.03808323392378852,
      "grad_norm": 0.9886105201123638,
      "learning_rate": 1.9996632155240348e-05,
      "loss": 0.6801,
      "step": 423
    },
    {
      "epoch": 0.038173265209660356,
      "grad_norm": 1.418983138142281,
      "learning_rate": 1.9996556052525174e-05,
      "loss": 0.691,
      "step": 424
    },
    {
      "epoch": 0.0382632964955322,
      "grad_norm": 1.504804390141723,
      "learning_rate": 1.9996479099696666e-05,
      "loss": 0.6403,
      "step": 425
    },
    {
      "epoch": 0.03835332778140404,
      "grad_norm": 1.016905823061016,
      "learning_rate": 1.9996401296761368e-05,
      "loss": 0.6797,
      "step": 426
    },
    {
      "epoch": 0.038443359067275876,
      "grad_norm": 0.9581457847691897,
      "learning_rate": 1.9996322643725897e-05,
      "loss": 0.6153,
      "step": 427
    },
    {
      "epoch": 0.03853339035314772,
      "grad_norm": 0.9428623291178809,
      "learning_rate": 1.999624314059694e-05,
      "loss": 0.621,
      "step": 428
    },
    {
      "epoch": 0.03862342163901956,
      "grad_norm": 1.0133201895693935,
      "learning_rate": 1.999616278738126e-05,
      "loss": 0.7382,
      "step": 429
    },
    {
      "epoch": 0.0387134529248914,
      "grad_norm": 0.9939211481737708,
      "learning_rate": 1.999608158408569e-05,
      "loss": 0.73,
      "step": 430
    },
    {
      "epoch": 0.03880348421076324,
      "grad_norm": 1.112865291307395,
      "learning_rate": 1.9995999530717136e-05,
      "loss": 0.6887,
      "step": 431
    },
    {
      "epoch": 0.038893515496635084,
      "grad_norm": 0.9399231386248004,
      "learning_rate": 1.9995916627282575e-05,
      "loss": 0.613,
      "step": 432
    },
    {
      "epoch": 0.03898354678250692,
      "grad_norm": 1.1323449642020351,
      "learning_rate": 1.9995832873789055e-05,
      "loss": 0.6949,
      "step": 433
    },
    {
      "epoch": 0.03907357806837876,
      "grad_norm": 1.3734278531909563,
      "learning_rate": 1.9995748270243704e-05,
      "loss": 0.6871,
      "step": 434
    },
    {
      "epoch": 0.0391636093542506,
      "grad_norm": 1.0531294782139016,
      "learning_rate": 1.999566281665371e-05,
      "loss": 0.5959,
      "step": 435
    },
    {
      "epoch": 0.03925364064012244,
      "grad_norm": 0.9465852083170496,
      "learning_rate": 1.9995576513026345e-05,
      "loss": 0.8051,
      "step": 436
    },
    {
      "epoch": 0.039343671925994285,
      "grad_norm": 0.9034553083790637,
      "learning_rate": 1.9995489359368946e-05,
      "loss": 0.6816,
      "step": 437
    },
    {
      "epoch": 0.03943370321186612,
      "grad_norm": 1.021029325791693,
      "learning_rate": 1.9995401355688922e-05,
      "loss": 0.7238,
      "step": 438
    },
    {
      "epoch": 0.03952373449773797,
      "grad_norm": 0.9021784892861724,
      "learning_rate": 1.9995312501993765e-05,
      "loss": 0.7019,
      "step": 439
    },
    {
      "epoch": 0.039613765783609804,
      "grad_norm": 1.082552472113047,
      "learning_rate": 1.9995222798291024e-05,
      "loss": 0.684,
      "step": 440
    },
    {
      "epoch": 0.03970379706948164,
      "grad_norm": 1.1582488252153353,
      "learning_rate": 1.999513224458833e-05,
      "loss": 0.7008,
      "step": 441
    },
    {
      "epoch": 0.039793828355353486,
      "grad_norm": 0.8420496509267656,
      "learning_rate": 1.999504084089339e-05,
      "loss": 0.7035,
      "step": 442
    },
    {
      "epoch": 0.039883859641225324,
      "grad_norm": 0.9026735507885668,
      "learning_rate": 1.999494858721396e-05,
      "loss": 0.5908,
      "step": 443
    },
    {
      "epoch": 0.03997389092709717,
      "grad_norm": 0.8966942990540633,
      "learning_rate": 1.9994855483557904e-05,
      "loss": 0.7001,
      "step": 444
    },
    {
      "epoch": 0.040063922212969005,
      "grad_norm": 0.8959214067765483,
      "learning_rate": 1.9994761529933123e-05,
      "loss": 0.6612,
      "step": 445
    },
    {
      "epoch": 0.04015395349884085,
      "grad_norm": 0.9998474655012215,
      "learning_rate": 1.9994666726347622e-05,
      "loss": 0.5901,
      "step": 446
    },
    {
      "epoch": 0.04024398478471269,
      "grad_norm": 0.8761966152059218,
      "learning_rate": 1.9994571072809455e-05,
      "loss": 0.6245,
      "step": 447
    },
    {
      "epoch": 0.04033401607058453,
      "grad_norm": 0.9963920378775526,
      "learning_rate": 1.999447456932676e-05,
      "loss": 0.6505,
      "step": 448
    },
    {
      "epoch": 0.04042404735645637,
      "grad_norm": 1.105643099562763,
      "learning_rate": 1.999437721590774e-05,
      "loss": 0.8173,
      "step": 449
    },
    {
      "epoch": 0.040514078642328206,
      "grad_norm": 0.8413442015783614,
      "learning_rate": 1.999427901256067e-05,
      "loss": 0.639,
      "step": 450
    },
    {
      "epoch": 0.04060410992820005,
      "grad_norm": 1.0368558738562512,
      "learning_rate": 1.9994179959293915e-05,
      "loss": 0.7016,
      "step": 451
    },
    {
      "epoch": 0.04069414121407189,
      "grad_norm": 0.9857937092843523,
      "learning_rate": 1.9994080056115886e-05,
      "loss": 0.5923,
      "step": 452
    },
    {
      "epoch": 0.04078417249994373,
      "grad_norm": 0.9301624656100255,
      "learning_rate": 1.9993979303035084e-05,
      "loss": 0.7425,
      "step": 453
    },
    {
      "epoch": 0.04087420378581557,
      "grad_norm": 0.9697391204210539,
      "learning_rate": 1.999387770006008e-05,
      "loss": 0.6927,
      "step": 454
    },
    {
      "epoch": 0.040964235071687415,
      "grad_norm": 1.1231348529982985,
      "learning_rate": 1.9993775247199508e-05,
      "loss": 0.7713,
      "step": 455
    },
    {
      "epoch": 0.04105426635755925,
      "grad_norm": 1.2258483702337664,
      "learning_rate": 1.999367194446208e-05,
      "loss": 0.6759,
      "step": 456
    },
    {
      "epoch": 0.04114429764343109,
      "grad_norm": 0.9140499157872259,
      "learning_rate": 1.999356779185659e-05,
      "loss": 0.6902,
      "step": 457
    },
    {
      "epoch": 0.041234328929302934,
      "grad_norm": 0.995380214372689,
      "learning_rate": 1.9993462789391885e-05,
      "loss": 0.6057,
      "step": 458
    },
    {
      "epoch": 0.04132436021517477,
      "grad_norm": 1.0751444878703769,
      "learning_rate": 1.9993356937076904e-05,
      "loss": 0.6501,
      "step": 459
    },
    {
      "epoch": 0.041414391501046616,
      "grad_norm": 1.0564834803357324,
      "learning_rate": 1.9993250234920638e-05,
      "loss": 0.6464,
      "step": 460
    },
    {
      "epoch": 0.04150442278691845,
      "grad_norm": 0.9178286226121195,
      "learning_rate": 1.999314268293217e-05,
      "loss": 0.6694,
      "step": 461
    },
    {
      "epoch": 0.0415944540727903,
      "grad_norm": 0.9806830724102641,
      "learning_rate": 1.9993034281120643e-05,
      "loss": 0.6961,
      "step": 462
    },
    {
      "epoch": 0.041684485358662135,
      "grad_norm": 0.8987377460810849,
      "learning_rate": 1.9992925029495274e-05,
      "loss": 0.6669,
      "step": 463
    },
    {
      "epoch": 0.04177451664453397,
      "grad_norm": 1.2494639747211882,
      "learning_rate": 1.9992814928065358e-05,
      "loss": 0.6905,
      "step": 464
    },
    {
      "epoch": 0.04186454793040582,
      "grad_norm": 0.6654391149100214,
      "learning_rate": 1.9992703976840256e-05,
      "loss": 0.5933,
      "step": 465
    },
    {
      "epoch": 0.041954579216277654,
      "grad_norm": 0.8183196797421748,
      "learning_rate": 1.99925921758294e-05,
      "loss": 0.7079,
      "step": 466
    },
    {
      "epoch": 0.0420446105021495,
      "grad_norm": 0.8718956545895206,
      "learning_rate": 1.9992479525042305e-05,
      "loss": 0.7061,
      "step": 467
    },
    {
      "epoch": 0.042134641788021336,
      "grad_norm": 0.9681127827084279,
      "learning_rate": 1.999236602448854e-05,
      "loss": 0.6365,
      "step": 468
    },
    {
      "epoch": 0.04222467307389318,
      "grad_norm": 1.1334253417339764,
      "learning_rate": 1.9992251674177766e-05,
      "loss": 0.6519,
      "step": 469
    },
    {
      "epoch": 0.04231470435976502,
      "grad_norm": 1.1828824541045295,
      "learning_rate": 1.999213647411971e-05,
      "loss": 0.693,
      "step": 470
    },
    {
      "epoch": 0.042404735645636855,
      "grad_norm": 1.0436803253048061,
      "learning_rate": 1.999202042432416e-05,
      "loss": 0.683,
      "step": 471
    },
    {
      "epoch": 0.0424947669315087,
      "grad_norm": 0.9361562787906543,
      "learning_rate": 1.9991903524800986e-05,
      "loss": 0.6659,
      "step": 472
    },
    {
      "epoch": 0.04258479821738054,
      "grad_norm": 0.9151571286965566,
      "learning_rate": 1.9991785775560134e-05,
      "loss": 0.7019,
      "step": 473
    },
    {
      "epoch": 0.04267482950325238,
      "grad_norm": 0.851917957778355,
      "learning_rate": 1.9991667176611614e-05,
      "loss": 0.7795,
      "step": 474
    },
    {
      "epoch": 0.04276486078912422,
      "grad_norm": 0.9504579812962651,
      "learning_rate": 1.9991547727965518e-05,
      "loss": 0.6403,
      "step": 475
    },
    {
      "epoch": 0.042854892074996064,
      "grad_norm": 1.2736694609739891,
      "learning_rate": 1.9991427429631994e-05,
      "loss": 0.7621,
      "step": 476
    },
    {
      "epoch": 0.0429449233608679,
      "grad_norm": 0.9713425982647214,
      "learning_rate": 1.999130628162128e-05,
      "loss": 0.5832,
      "step": 477
    },
    {
      "epoch": 0.043034954646739745,
      "grad_norm": 1.0043303303703448,
      "learning_rate": 1.9991184283943674e-05,
      "loss": 0.7206,
      "step": 478
    },
    {
      "epoch": 0.04312498593261158,
      "grad_norm": 0.8742757523286906,
      "learning_rate": 1.9991061436609552e-05,
      "loss": 0.7179,
      "step": 479
    },
    {
      "epoch": 0.04321501721848342,
      "grad_norm": 1.0196282041914175,
      "learning_rate": 1.9990937739629363e-05,
      "loss": 0.6033,
      "step": 480
    },
    {
      "epoch": 0.043305048504355265,
      "grad_norm": 1.0505544841791976,
      "learning_rate": 1.9990813193013625e-05,
      "loss": 0.7134,
      "step": 481
    },
    {
      "epoch": 0.0433950797902271,
      "grad_norm": 0.7896548551773784,
      "learning_rate": 1.999068779677293e-05,
      "loss": 0.6371,
      "step": 482
    },
    {
      "epoch": 0.043485111076098946,
      "grad_norm": 1.052023432028744,
      "learning_rate": 1.9990561550917938e-05,
      "loss": 0.6251,
      "step": 483
    },
    {
      "epoch": 0.043575142361970784,
      "grad_norm": 0.9838356766930417,
      "learning_rate": 1.999043445545939e-05,
      "loss": 0.6704,
      "step": 484
    },
    {
      "epoch": 0.04366517364784263,
      "grad_norm": 0.8971803060112706,
      "learning_rate": 1.9990306510408092e-05,
      "loss": 0.6261,
      "step": 485
    },
    {
      "epoch": 0.043755204933714466,
      "grad_norm": 0.9633158487785012,
      "learning_rate": 1.9990177715774927e-05,
      "loss": 0.6822,
      "step": 486
    },
    {
      "epoch": 0.0438452362195863,
      "grad_norm": 1.0397193057948557,
      "learning_rate": 1.9990048071570846e-05,
      "loss": 0.7083,
      "step": 487
    },
    {
      "epoch": 0.04393526750545815,
      "grad_norm": 1.046869916391184,
      "learning_rate": 1.998991757780687e-05,
      "loss": 0.6035,
      "step": 488
    },
    {
      "epoch": 0.044025298791329985,
      "grad_norm": 0.8833374007123506,
      "learning_rate": 1.9989786234494103e-05,
      "loss": 0.632,
      "step": 489
    },
    {
      "epoch": 0.04411533007720183,
      "grad_norm": 0.858866734175456,
      "learning_rate": 1.998965404164371e-05,
      "loss": 0.7122,
      "step": 490
    },
    {
      "epoch": 0.04420536136307367,
      "grad_norm": 1.1333119370030296,
      "learning_rate": 1.9989520999266937e-05,
      "loss": 0.5824,
      "step": 491
    },
    {
      "epoch": 0.04429539264894551,
      "grad_norm": 0.8377510337523905,
      "learning_rate": 1.9989387107375094e-05,
      "loss": 0.6543,
      "step": 492
    },
    {
      "epoch": 0.04438542393481735,
      "grad_norm": 1.049544529432566,
      "learning_rate": 1.998925236597957e-05,
      "loss": 0.7494,
      "step": 493
    },
    {
      "epoch": 0.044475455220689186,
      "grad_norm": 0.9441241776065153,
      "learning_rate": 1.9989116775091815e-05,
      "loss": 0.7226,
      "step": 494
    },
    {
      "epoch": 0.04456548650656103,
      "grad_norm": 0.8212473694145963,
      "learning_rate": 1.9988980334723375e-05,
      "loss": 0.7658,
      "step": 495
    },
    {
      "epoch": 0.04465551779243287,
      "grad_norm": 1.1021747300545202,
      "learning_rate": 1.998884304488584e-05,
      "loss": 0.6446,
      "step": 496
    },
    {
      "epoch": 0.04474554907830471,
      "grad_norm": 1.0631323565462774,
      "learning_rate": 1.998870490559089e-05,
      "loss": 0.6863,
      "step": 497
    },
    {
      "epoch": 0.04483558036417655,
      "grad_norm": 0.9721347377767973,
      "learning_rate": 1.9988565916850273e-05,
      "loss": 0.6284,
      "step": 498
    },
    {
      "epoch": 0.044925611650048394,
      "grad_norm": 0.8983375015371062,
      "learning_rate": 1.9988426078675813e-05,
      "loss": 0.6714,
      "step": 499
    },
    {
      "epoch": 0.04501564293592023,
      "grad_norm": 0.8049189482156953,
      "learning_rate": 1.998828539107939e-05,
      "loss": 0.6655,
      "step": 500
    },
    {
      "epoch": 0.045105674221792076,
      "grad_norm": 0.8901530025213471,
      "learning_rate": 1.998814385407298e-05,
      "loss": 0.717,
      "step": 501
    },
    {
      "epoch": 0.045195705507663914,
      "grad_norm": 0.8849566612096134,
      "learning_rate": 1.9988001467668613e-05,
      "loss": 0.638,
      "step": 502
    },
    {
      "epoch": 0.04528573679353575,
      "grad_norm": 0.9377738991140314,
      "learning_rate": 1.9987858231878397e-05,
      "loss": 0.6312,
      "step": 503
    },
    {
      "epoch": 0.045375768079407595,
      "grad_norm": 0.8309881298445201,
      "learning_rate": 1.998771414671452e-05,
      "loss": 0.7615,
      "step": 504
    },
    {
      "epoch": 0.04546579936527943,
      "grad_norm": 0.8780676104415728,
      "learning_rate": 1.9987569212189224e-05,
      "loss": 0.6728,
      "step": 505
    },
    {
      "epoch": 0.04555583065115128,
      "grad_norm": 0.8634593503283106,
      "learning_rate": 1.9987423428314845e-05,
      "loss": 0.6251,
      "step": 506
    },
    {
      "epoch": 0.045645861937023115,
      "grad_norm": 0.9998187287414346,
      "learning_rate": 1.9987276795103775e-05,
      "loss": 0.6491,
      "step": 507
    },
    {
      "epoch": 0.04573589322289496,
      "grad_norm": 0.927187715667778,
      "learning_rate": 1.9987129312568486e-05,
      "loss": 0.643,
      "step": 508
    },
    {
      "epoch": 0.0458259245087668,
      "grad_norm": 0.9470246968739763,
      "learning_rate": 1.9986980980721515e-05,
      "loss": 0.6735,
      "step": 509
    },
    {
      "epoch": 0.045915955794638634,
      "grad_norm": 0.849705590610303,
      "learning_rate": 1.9986831799575485e-05,
      "loss": 0.6497,
      "step": 510
    },
    {
      "epoch": 0.04600598708051048,
      "grad_norm": 0.9658156683174514,
      "learning_rate": 1.998668176914307e-05,
      "loss": 0.6551,
      "step": 511
    },
    {
      "epoch": 0.046096018366382316,
      "grad_norm": 0.9387031077038368,
      "learning_rate": 1.9986530889437044e-05,
      "loss": 0.6401,
      "step": 512
    },
    {
      "epoch": 0.04618604965225416,
      "grad_norm": 0.8572708771394062,
      "learning_rate": 1.9986379160470227e-05,
      "loss": 0.5716,
      "step": 513
    },
    {
      "epoch": 0.046276080938126,
      "grad_norm": 1.1480128818124224,
      "learning_rate": 1.9986226582255526e-05,
      "loss": 0.6659,
      "step": 514
    },
    {
      "epoch": 0.04636611222399784,
      "grad_norm": 0.7684919239633269,
      "learning_rate": 1.9986073154805918e-05,
      "loss": 0.6644,
      "step": 515
    },
    {
      "epoch": 0.04645614350986968,
      "grad_norm": 0.8593168302817845,
      "learning_rate": 1.9985918878134446e-05,
      "loss": 0.6586,
      "step": 516
    },
    {
      "epoch": 0.04654617479574152,
      "grad_norm": 0.9586843656300948,
      "learning_rate": 1.998576375225423e-05,
      "loss": 0.7369,
      "step": 517
    },
    {
      "epoch": 0.04663620608161336,
      "grad_norm": 1.3382086272424243,
      "learning_rate": 1.9985607777178465e-05,
      "loss": 0.6997,
      "step": 518
    },
    {
      "epoch": 0.0467262373674852,
      "grad_norm": 0.9622749082862032,
      "learning_rate": 1.9985450952920414e-05,
      "loss": 0.5946,
      "step": 519
    },
    {
      "epoch": 0.04681626865335704,
      "grad_norm": 0.894435918352855,
      "learning_rate": 1.9985293279493413e-05,
      "loss": 0.6634,
      "step": 520
    },
    {
      "epoch": 0.04690629993922888,
      "grad_norm": 0.9186461153409916,
      "learning_rate": 1.998513475691087e-05,
      "loss": 0.6307,
      "step": 521
    },
    {
      "epoch": 0.046996331225100725,
      "grad_norm": 0.8460407057946874,
      "learning_rate": 1.9984975385186272e-05,
      "loss": 0.7655,
      "step": 522
    },
    {
      "epoch": 0.04708636251097256,
      "grad_norm": 0.9406577640526411,
      "learning_rate": 1.9984815164333163e-05,
      "loss": 0.6302,
      "step": 523
    },
    {
      "epoch": 0.0471763937968444,
      "grad_norm": 0.9152665387451864,
      "learning_rate": 1.9984654094365175e-05,
      "loss": 0.6708,
      "step": 524
    },
    {
      "epoch": 0.047266425082716244,
      "grad_norm": 1.0232333143146561,
      "learning_rate": 1.9984492175296002e-05,
      "loss": 0.6211,
      "step": 525
    },
    {
      "epoch": 0.04735645636858808,
      "grad_norm": 0.9625939327541845,
      "learning_rate": 1.9984329407139414e-05,
      "loss": 0.6736,
      "step": 526
    },
    {
      "epoch": 0.047446487654459926,
      "grad_norm": 1.2335242830634683,
      "learning_rate": 1.9984165789909254e-05,
      "loss": 0.6787,
      "step": 527
    },
    {
      "epoch": 0.047536518940331764,
      "grad_norm": 0.9312233291173948,
      "learning_rate": 1.9984001323619434e-05,
      "loss": 0.6559,
      "step": 528
    },
    {
      "epoch": 0.04762655022620361,
      "grad_norm": 1.1809989232145846,
      "learning_rate": 1.9983836008283943e-05,
      "loss": 0.7048,
      "step": 529
    },
    {
      "epoch": 0.047716581512075446,
      "grad_norm": 1.1068858034258073,
      "learning_rate": 1.9983669843916843e-05,
      "loss": 0.7535,
      "step": 530
    },
    {
      "epoch": 0.04780661279794729,
      "grad_norm": 0.9574123251170391,
      "learning_rate": 1.9983502830532252e-05,
      "loss": 0.6876,
      "step": 531
    },
    {
      "epoch": 0.04789664408381913,
      "grad_norm": 0.9542096589780461,
      "learning_rate": 1.9983334968144386e-05,
      "loss": 0.6561,
      "step": 532
    },
    {
      "epoch": 0.047986675369690965,
      "grad_norm": 0.8396158993102436,
      "learning_rate": 1.9983166256767513e-05,
      "loss": 0.6714,
      "step": 533
    },
    {
      "epoch": 0.04807670665556281,
      "grad_norm": 1.039164040332002,
      "learning_rate": 1.9982996696415985e-05,
      "loss": 0.7885,
      "step": 534
    },
    {
      "epoch": 0.04816673794143465,
      "grad_norm": 1.2545990262496678,
      "learning_rate": 1.998282628710422e-05,
      "loss": 0.7764,
      "step": 535
    },
    {
      "epoch": 0.04825676922730649,
      "grad_norm": 1.0175322217866838,
      "learning_rate": 1.9982655028846708e-05,
      "loss": 0.6166,
      "step": 536
    },
    {
      "epoch": 0.04834680051317833,
      "grad_norm": 1.0889093749839507,
      "learning_rate": 1.998248292165801e-05,
      "loss": 0.6364,
      "step": 537
    },
    {
      "epoch": 0.04843683179905017,
      "grad_norm": 1.0865376451733828,
      "learning_rate": 1.998230996555277e-05,
      "loss": 0.5625,
      "step": 538
    },
    {
      "epoch": 0.04852686308492201,
      "grad_norm": 0.9600636949441312,
      "learning_rate": 1.998213616054569e-05,
      "loss": 0.6755,
      "step": 539
    },
    {
      "epoch": 0.04861689437079385,
      "grad_norm": 0.8886665378997495,
      "learning_rate": 1.9981961506651554e-05,
      "loss": 0.624,
      "step": 540
    },
    {
      "epoch": 0.04870692565666569,
      "grad_norm": 1.229744475906215,
      "learning_rate": 1.9981786003885212e-05,
      "loss": 0.6532,
      "step": 541
    },
    {
      "epoch": 0.04879695694253753,
      "grad_norm": 0.9682649747196338,
      "learning_rate": 1.998160965226159e-05,
      "loss": 0.6784,
      "step": 542
    },
    {
      "epoch": 0.048886988228409374,
      "grad_norm": 0.9135313818530864,
      "learning_rate": 1.9981432451795687e-05,
      "loss": 0.6786,
      "step": 543
    },
    {
      "epoch": 0.04897701951428121,
      "grad_norm": 0.9728053386053397,
      "learning_rate": 1.9981254402502568e-05,
      "loss": 0.696,
      "step": 544
    },
    {
      "epoch": 0.049067050800153056,
      "grad_norm": 1.063626940209177,
      "learning_rate": 1.998107550439738e-05,
      "loss": 0.7192,
      "step": 545
    },
    {
      "epoch": 0.04915708208602489,
      "grad_norm": 1.1734323251823795,
      "learning_rate": 1.9980895757495327e-05,
      "loss": 0.741,
      "step": 546
    },
    {
      "epoch": 0.04924711337189673,
      "grad_norm": 0.858209284238923,
      "learning_rate": 1.9980715161811707e-05,
      "loss": 0.5285,
      "step": 547
    },
    {
      "epoch": 0.049337144657768575,
      "grad_norm": 0.9555811125093184,
      "learning_rate": 1.998053371736187e-05,
      "loss": 0.7613,
      "step": 548
    },
    {
      "epoch": 0.04942717594364041,
      "grad_norm": 1.0396101354720233,
      "learning_rate": 1.998035142416125e-05,
      "loss": 0.7279,
      "step": 549
    },
    {
      "epoch": 0.04951720722951226,
      "grad_norm": 0.8208653088702476,
      "learning_rate": 1.998016828222535e-05,
      "loss": 0.6441,
      "step": 550
    },
    {
      "epoch": 0.049607238515384094,
      "grad_norm": 0.965079719968593,
      "learning_rate": 1.9979984291569735e-05,
      "loss": 0.7822,
      "step": 551
    },
    {
      "epoch": 0.04969726980125594,
      "grad_norm": 0.9260010311882247,
      "learning_rate": 1.9979799452210066e-05,
      "loss": 0.6314,
      "step": 552
    },
    {
      "epoch": 0.049787301087127776,
      "grad_norm": 1.0510946875527114,
      "learning_rate": 1.997961376416205e-05,
      "loss": 0.6798,
      "step": 553
    },
    {
      "epoch": 0.04987733237299962,
      "grad_norm": 1.0393581004219434,
      "learning_rate": 1.9979427227441483e-05,
      "loss": 0.6891,
      "step": 554
    },
    {
      "epoch": 0.04996736365887146,
      "grad_norm": 1.0885106979383918,
      "learning_rate": 1.997923984206423e-05,
      "loss": 0.6597,
      "step": 555
    },
    {
      "epoch": 0.050057394944743296,
      "grad_norm": 1.0318878125934146,
      "learning_rate": 1.9979051608046222e-05,
      "loss": 0.6662,
      "step": 556
    },
    {
      "epoch": 0.05014742623061514,
      "grad_norm": 0.8504985299357243,
      "learning_rate": 1.9978862525403474e-05,
      "loss": 0.5675,
      "step": 557
    },
    {
      "epoch": 0.05023745751648698,
      "grad_norm": 0.848518704043584,
      "learning_rate": 1.9978672594152053e-05,
      "loss": 0.6224,
      "step": 558
    },
    {
      "epoch": 0.05032748880235882,
      "grad_norm": 0.9515004047483961,
      "learning_rate": 1.9978481814308124e-05,
      "loss": 0.6093,
      "step": 559
    },
    {
      "epoch": 0.05041752008823066,
      "grad_norm": 1.0268745831836459,
      "learning_rate": 1.9978290185887903e-05,
      "loss": 0.7544,
      "step": 560
    },
    {
      "epoch": 0.050507551374102504,
      "grad_norm": 0.9891540342005642,
      "learning_rate": 1.9978097708907687e-05,
      "loss": 0.6299,
      "step": 561
    },
    {
      "epoch": 0.05059758265997434,
      "grad_norm": 0.8653641172744945,
      "learning_rate": 1.997790438338385e-05,
      "loss": 0.6461,
      "step": 562
    },
    {
      "epoch": 0.05068761394584618,
      "grad_norm": 1.1033624363445091,
      "learning_rate": 1.9977710209332827e-05,
      "loss": 0.7647,
      "step": 563
    },
    {
      "epoch": 0.05077764523171802,
      "grad_norm": 1.2577635455802045,
      "learning_rate": 1.9977515186771135e-05,
      "loss": 0.5387,
      "step": 564
    },
    {
      "epoch": 0.05086767651758986,
      "grad_norm": 1.1607049361520752,
      "learning_rate": 1.997731931571535e-05,
      "loss": 0.6708,
      "step": 565
    },
    {
      "epoch": 0.050957707803461705,
      "grad_norm": 0.9860619462631725,
      "learning_rate": 1.997712259618214e-05,
      "loss": 0.6978,
      "step": 566
    },
    {
      "epoch": 0.05104773908933354,
      "grad_norm": 1.0637101242876439,
      "learning_rate": 1.9976925028188227e-05,
      "loss": 0.5857,
      "step": 567
    },
    {
      "epoch": 0.05113777037520539,
      "grad_norm": 0.8781071514188507,
      "learning_rate": 1.9976726611750416e-05,
      "loss": 0.6979,
      "step": 568
    },
    {
      "epoch": 0.051227801661077224,
      "grad_norm": 1.0137205667016695,
      "learning_rate": 1.9976527346885577e-05,
      "loss": 0.5972,
      "step": 569
    },
    {
      "epoch": 0.05131783294694906,
      "grad_norm": 0.949363731610826,
      "learning_rate": 1.9976327233610658e-05,
      "loss": 0.6091,
      "step": 570
    },
    {
      "epoch": 0.051407864232820906,
      "grad_norm": 1.268145994916681,
      "learning_rate": 1.9976126271942677e-05,
      "loss": 0.6328,
      "step": 571
    },
    {
      "epoch": 0.05149789551869274,
      "grad_norm": 1.072219312995851,
      "learning_rate": 1.9975924461898723e-05,
      "loss": 0.6216,
      "step": 572
    },
    {
      "epoch": 0.05158792680456459,
      "grad_norm": 1.028930428507802,
      "learning_rate": 1.997572180349596e-05,
      "loss": 0.6575,
      "step": 573
    },
    {
      "epoch": 0.051677958090436425,
      "grad_norm": 0.9460565238287372,
      "learning_rate": 1.9975518296751618e-05,
      "loss": 0.6314,
      "step": 574
    },
    {
      "epoch": 0.05176798937630827,
      "grad_norm": 0.9441832587970062,
      "learning_rate": 1.9975313941683008e-05,
      "loss": 0.6202,
      "step": 575
    },
    {
      "epoch": 0.05185802066218011,
      "grad_norm": 1.017957501281129,
      "learning_rate": 1.9975108738307503e-05,
      "loss": 0.6246,
      "step": 576
    },
    {
      "epoch": 0.05194805194805195,
      "grad_norm": 0.8013725664682866,
      "learning_rate": 1.997490268664256e-05,
      "loss": 0.6724,
      "step": 577
    },
    {
      "epoch": 0.05203808323392379,
      "grad_norm": 0.9786077663716547,
      "learning_rate": 1.9974695786705695e-05,
      "loss": 0.6027,
      "step": 578
    },
    {
      "epoch": 0.052128114519795626,
      "grad_norm": 0.956015252700455,
      "learning_rate": 1.9974488038514514e-05,
      "loss": 0.6733,
      "step": 579
    },
    {
      "epoch": 0.05221814580566747,
      "grad_norm": 0.8915191374990732,
      "learning_rate": 1.997427944208667e-05,
      "loss": 0.6045,
      "step": 580
    },
    {
      "epoch": 0.05230817709153931,
      "grad_norm": 0.8199136666858605,
      "learning_rate": 1.997406999743991e-05,
      "loss": 0.6252,
      "step": 581
    },
    {
      "epoch": 0.05239820837741115,
      "grad_norm": 0.8525471536257114,
      "learning_rate": 1.9973859704592045e-05,
      "loss": 0.619,
      "step": 582
    },
    {
      "epoch": 0.05248823966328299,
      "grad_norm": 0.862487547594644,
      "learning_rate": 1.9973648563560957e-05,
      "loss": 0.634,
      "step": 583
    },
    {
      "epoch": 0.052578270949154834,
      "grad_norm": 1.1901269858637467,
      "learning_rate": 1.99734365743646e-05,
      "loss": 0.7638,
      "step": 584
    },
    {
      "epoch": 0.05266830223502667,
      "grad_norm": 1.0614228857473889,
      "learning_rate": 1.997322373702101e-05,
      "loss": 0.6669,
      "step": 585
    },
    {
      "epoch": 0.05275833352089851,
      "grad_norm": 0.7977263941491247,
      "learning_rate": 1.9973010051548274e-05,
      "loss": 0.6167,
      "step": 586
    },
    {
      "epoch": 0.052848364806770354,
      "grad_norm": 0.7653910869456148,
      "learning_rate": 1.9972795517964573e-05,
      "loss": 0.651,
      "step": 587
    },
    {
      "epoch": 0.05293839609264219,
      "grad_norm": 1.2790563436365054,
      "learning_rate": 1.997258013628815e-05,
      "loss": 0.7346,
      "step": 588
    },
    {
      "epoch": 0.053028427378514036,
      "grad_norm": 1.1475574278907223,
      "learning_rate": 1.9972363906537318e-05,
      "loss": 0.7494,
      "step": 589
    },
    {
      "epoch": 0.05311845866438587,
      "grad_norm": 0.9564836940153705,
      "learning_rate": 1.9972146828730467e-05,
      "loss": 0.6431,
      "step": 590
    },
    {
      "epoch": 0.05320848995025772,
      "grad_norm": 1.2111935071716147,
      "learning_rate": 1.9971928902886064e-05,
      "loss": 0.7066,
      "step": 591
    },
    {
      "epoch": 0.053298521236129555,
      "grad_norm": 0.7818498491768185,
      "learning_rate": 1.9971710129022628e-05,
      "loss": 0.7332,
      "step": 592
    },
    {
      "epoch": 0.05338855252200139,
      "grad_norm": 0.9425005096734103,
      "learning_rate": 1.997149050715877e-05,
      "loss": 0.7498,
      "step": 593
    },
    {
      "epoch": 0.05347858380787324,
      "grad_norm": 0.7113817542535253,
      "learning_rate": 1.997127003731317e-05,
      "loss": 0.6628,
      "step": 594
    },
    {
      "epoch": 0.053568615093745074,
      "grad_norm": 0.8341710781881045,
      "learning_rate": 1.9971048719504575e-05,
      "loss": 0.5231,
      "step": 595
    },
    {
      "epoch": 0.05365864637961692,
      "grad_norm": 0.8273869616916403,
      "learning_rate": 1.9970826553751804e-05,
      "loss": 0.7284,
      "step": 596
    },
    {
      "epoch": 0.053748677665488756,
      "grad_norm": 0.8351534085078623,
      "learning_rate": 1.9970603540073753e-05,
      "loss": 0.5804,
      "step": 597
    },
    {
      "epoch": 0.0538387089513606,
      "grad_norm": 1.002981306035734,
      "learning_rate": 1.9970379678489386e-05,
      "loss": 0.6551,
      "step": 598
    },
    {
      "epoch": 0.05392874023723244,
      "grad_norm": 0.9002742231804793,
      "learning_rate": 1.9970154969017738e-05,
      "loss": 0.7184,
      "step": 599
    },
    {
      "epoch": 0.054018771523104275,
      "grad_norm": 1.0590665590256718,
      "learning_rate": 1.996992941167792e-05,
      "loss": 0.6384,
      "step": 600
    },
    {
      "epoch": 0.05410880280897612,
      "grad_norm": 0.949785836567555,
      "learning_rate": 1.9969703006489117e-05,
      "loss": 0.6463,
      "step": 601
    },
    {
      "epoch": 0.05419883409484796,
      "grad_norm": 1.0376886714784084,
      "learning_rate": 1.996947575347058e-05,
      "loss": 0.6367,
      "step": 602
    },
    {
      "epoch": 0.0542888653807198,
      "grad_norm": 0.7274495694573833,
      "learning_rate": 1.996924765264163e-05,
      "loss": 0.6241,
      "step": 603
    },
    {
      "epoch": 0.05437889666659164,
      "grad_norm": 1.176602838037287,
      "learning_rate": 1.9969018704021677e-05,
      "loss": 0.6804,
      "step": 604
    },
    {
      "epoch": 0.05446892795246348,
      "grad_norm": 0.9235816172183249,
      "learning_rate": 1.9968788907630177e-05,
      "loss": 0.7281,
      "step": 605
    },
    {
      "epoch": 0.05455895923833532,
      "grad_norm": 1.0154504360841843,
      "learning_rate": 1.996855826348668e-05,
      "loss": 0.6211,
      "step": 606
    },
    {
      "epoch": 0.054648990524207165,
      "grad_norm": 0.7858479472794757,
      "learning_rate": 1.9968326771610797e-05,
      "loss": 0.6172,
      "step": 607
    },
    {
      "epoch": 0.054739021810079,
      "grad_norm": 0.7762919362832079,
      "learning_rate": 1.996809443202222e-05,
      "loss": 0.6841,
      "step": 608
    },
    {
      "epoch": 0.05482905309595084,
      "grad_norm": 0.8809557575123234,
      "learning_rate": 1.9967861244740696e-05,
      "loss": 0.7008,
      "step": 609
    },
    {
      "epoch": 0.054919084381822685,
      "grad_norm": 0.8243176185050882,
      "learning_rate": 1.9967627209786065e-05,
      "loss": 0.6923,
      "step": 610
    },
    {
      "epoch": 0.05500911566769452,
      "grad_norm": 1.0673680511106491,
      "learning_rate": 1.996739232717823e-05,
      "loss": 0.5697,
      "step": 611
    },
    {
      "epoch": 0.055099146953566366,
      "grad_norm": 0.8986767036960253,
      "learning_rate": 1.996715659693716e-05,
      "loss": 0.711,
      "step": 612
    },
    {
      "epoch": 0.055189178239438204,
      "grad_norm": 0.9323085987212774,
      "learning_rate": 1.9966920019082905e-05,
      "loss": 0.6804,
      "step": 613
    },
    {
      "epoch": 0.05527920952531005,
      "grad_norm": 0.9683050997249132,
      "learning_rate": 1.996668259363558e-05,
      "loss": 0.6243,
      "step": 614
    },
    {
      "epoch": 0.055369240811181886,
      "grad_norm": 0.9540342818044111,
      "learning_rate": 1.996644432061538e-05,
      "loss": 0.6003,
      "step": 615
    },
    {
      "epoch": 0.05545927209705372,
      "grad_norm": 1.158867710239679,
      "learning_rate": 1.9966205200042567e-05,
      "loss": 0.7319,
      "step": 616
    },
    {
      "epoch": 0.05554930338292557,
      "grad_norm": 0.9172472840897494,
      "learning_rate": 1.9965965231937478e-05,
      "loss": 0.7041,
      "step": 617
    },
    {
      "epoch": 0.055639334668797405,
      "grad_norm": 1.1712998797938379,
      "learning_rate": 1.9965724416320512e-05,
      "loss": 0.6639,
      "step": 618
    },
    {
      "epoch": 0.05572936595466925,
      "grad_norm": 1.0949601150595363,
      "learning_rate": 1.9965482753212154e-05,
      "loss": 0.6367,
      "step": 619
    },
    {
      "epoch": 0.05581939724054109,
      "grad_norm": 0.7343282837898878,
      "learning_rate": 1.996524024263296e-05,
      "loss": 0.6058,
      "step": 620
    },
    {
      "epoch": 0.05590942852641293,
      "grad_norm": 0.982166497683241,
      "learning_rate": 1.9964996884603543e-05,
      "loss": 0.6918,
      "step": 621
    },
    {
      "epoch": 0.05599945981228477,
      "grad_norm": 1.0166226355366728,
      "learning_rate": 1.9964752679144604e-05,
      "loss": 0.7114,
      "step": 622
    },
    {
      "epoch": 0.056089491098156606,
      "grad_norm": 0.9147047570072673,
      "learning_rate": 1.996450762627691e-05,
      "loss": 0.6788,
      "step": 623
    },
    {
      "epoch": 0.05617952238402845,
      "grad_norm": 1.0620408243953845,
      "learning_rate": 1.9964261726021302e-05,
      "loss": 0.6813,
      "step": 624
    },
    {
      "epoch": 0.05626955366990029,
      "grad_norm": 0.9599802541182414,
      "learning_rate": 1.9964014978398686e-05,
      "loss": 0.7537,
      "step": 625
    },
    {
      "epoch": 0.05635958495577213,
      "grad_norm": 1.0522473994193584,
      "learning_rate": 1.9963767383430053e-05,
      "loss": 0.6144,
      "step": 626
    },
    {
      "epoch": 0.05644961624164397,
      "grad_norm": 1.2776715063826731,
      "learning_rate": 1.9963518941136453e-05,
      "loss": 0.588,
      "step": 627
    },
    {
      "epoch": 0.056539647527515814,
      "grad_norm": 0.8967545019303536,
      "learning_rate": 1.9963269651539018e-05,
      "loss": 0.6365,
      "step": 628
    },
    {
      "epoch": 0.05662967881338765,
      "grad_norm": 0.93684161744961,
      "learning_rate": 1.996301951465894e-05,
      "loss": 0.5815,
      "step": 629
    },
    {
      "epoch": 0.056719710099259496,
      "grad_norm": 0.9393832257652056,
      "learning_rate": 1.99627685305175e-05,
      "loss": 0.7551,
      "step": 630
    },
    {
      "epoch": 0.056809741385131333,
      "grad_norm": 0.8861074930401021,
      "learning_rate": 1.9962516699136036e-05,
      "loss": 0.612,
      "step": 631
    },
    {
      "epoch": 0.05689977267100317,
      "grad_norm": 0.8628333498464491,
      "learning_rate": 1.9962264020535967e-05,
      "loss": 0.7736,
      "step": 632
    },
    {
      "epoch": 0.056989803956875015,
      "grad_norm": 0.955021226881684,
      "learning_rate": 1.996201049473878e-05,
      "loss": 0.6749,
      "step": 633
    },
    {
      "epoch": 0.05707983524274685,
      "grad_norm": 0.8884360634892634,
      "learning_rate": 1.9961756121766033e-05,
      "loss": 0.6521,
      "step": 634
    },
    {
      "epoch": 0.0571698665286187,
      "grad_norm": 1.0013550319844666,
      "learning_rate": 1.996150090163936e-05,
      "loss": 0.6362,
      "step": 635
    },
    {
      "epoch": 0.057259897814490535,
      "grad_norm": 0.7949013768178217,
      "learning_rate": 1.9961244834380465e-05,
      "loss": 0.6421,
      "step": 636
    },
    {
      "epoch": 0.05734992910036238,
      "grad_norm": 0.893447443945852,
      "learning_rate": 1.9960987920011123e-05,
      "loss": 0.6203,
      "step": 637
    },
    {
      "epoch": 0.057439960386234216,
      "grad_norm": 0.9912512061012145,
      "learning_rate": 1.9960730158553186e-05,
      "loss": 0.7178,
      "step": 638
    },
    {
      "epoch": 0.057529991672106054,
      "grad_norm": 0.9751159982055746,
      "learning_rate": 1.996047155002857e-05,
      "loss": 0.7112,
      "step": 639
    },
    {
      "epoch": 0.0576200229579779,
      "grad_norm": 0.9425245219562107,
      "learning_rate": 1.996021209445927e-05,
      "loss": 0.567,
      "step": 640
    },
    {
      "epoch": 0.057710054243849736,
      "grad_norm": 0.815445477050289,
      "learning_rate": 1.9959951791867346e-05,
      "loss": 0.5094,
      "step": 641
    },
    {
      "epoch": 0.05780008552972158,
      "grad_norm": 0.8037518504438739,
      "learning_rate": 1.9959690642274937e-05,
      "loss": 0.5949,
      "step": 642
    },
    {
      "epoch": 0.05789011681559342,
      "grad_norm": 0.8814047818347812,
      "learning_rate": 1.995942864570425e-05,
      "loss": 0.6818,
      "step": 643
    },
    {
      "epoch": 0.05798014810146526,
      "grad_norm": 0.9549903000543171,
      "learning_rate": 1.995916580217757e-05,
      "loss": 0.6448,
      "step": 644
    },
    {
      "epoch": 0.0580701793873371,
      "grad_norm": 0.9259128161088334,
      "learning_rate": 1.9958902111717247e-05,
      "loss": 0.6218,
      "step": 645
    },
    {
      "epoch": 0.05816021067320894,
      "grad_norm": 0.9567100082688164,
      "learning_rate": 1.9958637574345703e-05,
      "loss": 0.6631,
      "step": 646
    },
    {
      "epoch": 0.05825024195908078,
      "grad_norm": 0.93067627781872,
      "learning_rate": 1.9958372190085434e-05,
      "loss": 0.6297,
      "step": 647
    },
    {
      "epoch": 0.05834027324495262,
      "grad_norm": 1.11925646587846,
      "learning_rate": 1.9958105958959012e-05,
      "loss": 0.7648,
      "step": 648
    },
    {
      "epoch": 0.05843030453082446,
      "grad_norm": 1.0896183387883667,
      "learning_rate": 1.9957838880989076e-05,
      "loss": 0.5815,
      "step": 649
    },
    {
      "epoch": 0.0585203358166963,
      "grad_norm": 0.8365050912821017,
      "learning_rate": 1.9957570956198338e-05,
      "loss": 0.5776,
      "step": 650
    },
    {
      "epoch": 0.058610367102568145,
      "grad_norm": 1.0371654997643553,
      "learning_rate": 1.9957302184609582e-05,
      "loss": 0.6697,
      "step": 651
    },
    {
      "epoch": 0.05870039838843998,
      "grad_norm": 0.901658670953868,
      "learning_rate": 1.995703256624567e-05,
      "loss": 0.7121,
      "step": 652
    },
    {
      "epoch": 0.05879042967431183,
      "grad_norm": 0.8607812604192697,
      "learning_rate": 1.995676210112952e-05,
      "loss": 0.5812,
      "step": 653
    },
    {
      "epoch": 0.058880460960183664,
      "grad_norm": 0.7327724602663231,
      "learning_rate": 1.995649078928414e-05,
      "loss": 0.6437,
      "step": 654
    },
    {
      "epoch": 0.0589704922460555,
      "grad_norm": 0.9367260693198731,
      "learning_rate": 1.99562186307326e-05,
      "loss": 0.622,
      "step": 655
    },
    {
      "epoch": 0.059060523531927346,
      "grad_norm": 0.9128673384151462,
      "learning_rate": 1.995594562549805e-05,
      "loss": 0.6513,
      "step": 656
    },
    {
      "epoch": 0.059150554817799184,
      "grad_norm": 0.8483548362548965,
      "learning_rate": 1.99556717736037e-05,
      "loss": 0.5914,
      "step": 657
    },
    {
      "epoch": 0.05924058610367103,
      "grad_norm": 0.9830085327164084,
      "learning_rate": 1.995539707507284e-05,
      "loss": 0.6859,
      "step": 658
    },
    {
      "epoch": 0.059330617389542865,
      "grad_norm": 1.15869841472906,
      "learning_rate": 1.995512152992883e-05,
      "loss": 0.7126,
      "step": 659
    },
    {
      "epoch": 0.05942064867541471,
      "grad_norm": 0.7342703529440637,
      "learning_rate": 1.99548451381951e-05,
      "loss": 0.6694,
      "step": 660
    },
    {
      "epoch": 0.05951067996128655,
      "grad_norm": 0.6904002197353167,
      "learning_rate": 1.9954567899895162e-05,
      "loss": 0.6131,
      "step": 661
    },
    {
      "epoch": 0.059600711247158385,
      "grad_norm": 1.023548166554696,
      "learning_rate": 1.995428981505259e-05,
      "loss": 0.6149,
      "step": 662
    },
    {
      "epoch": 0.05969074253303023,
      "grad_norm": 1.031940923182114,
      "learning_rate": 1.9954010883691032e-05,
      "loss": 0.6582,
      "step": 663
    },
    {
      "epoch": 0.05978077381890207,
      "grad_norm": 0.8911957971984166,
      "learning_rate": 1.9953731105834202e-05,
      "loss": 0.6651,
      "step": 664
    },
    {
      "epoch": 0.05987080510477391,
      "grad_norm": 0.9088061757928111,
      "learning_rate": 1.9953450481505902e-05,
      "loss": 0.5775,
      "step": 665
    },
    {
      "epoch": 0.05996083639064575,
      "grad_norm": 0.9875755436595317,
      "learning_rate": 1.9953169010729992e-05,
      "loss": 0.6154,
      "step": 666
    },
    {
      "epoch": 0.06005086767651759,
      "grad_norm": 0.8621381360127315,
      "learning_rate": 1.9952886693530406e-05,
      "loss": 0.647,
      "step": 667
    },
    {
      "epoch": 0.06014089896238943,
      "grad_norm": 0.9084632282045156,
      "learning_rate": 1.9952603529931162e-05,
      "loss": 0.6081,
      "step": 668
    },
    {
      "epoch": 0.06023093024826127,
      "grad_norm": 0.7920628016714899,
      "learning_rate": 1.9952319519956325e-05,
      "loss": 0.6893,
      "step": 669
    },
    {
      "epoch": 0.06032096153413311,
      "grad_norm": 0.9882477788336507,
      "learning_rate": 1.9952034663630064e-05,
      "loss": 0.5972,
      "step": 670
    },
    {
      "epoch": 0.06041099282000495,
      "grad_norm": 0.856010875641953,
      "learning_rate": 1.9951748960976594e-05,
      "loss": 0.6782,
      "step": 671
    },
    {
      "epoch": 0.060501024105876794,
      "grad_norm": 1.0544834270461045,
      "learning_rate": 1.995146241202021e-05,
      "loss": 0.6386,
      "step": 672
    },
    {
      "epoch": 0.06059105539174863,
      "grad_norm": 0.7500339102550944,
      "learning_rate": 1.9951175016785287e-05,
      "loss": 0.632,
      "step": 673
    },
    {
      "epoch": 0.060681086677620476,
      "grad_norm": 0.9270811402313248,
      "learning_rate": 1.9950886775296257e-05,
      "loss": 0.6615,
      "step": 674
    },
    {
      "epoch": 0.06077111796349231,
      "grad_norm": 1.1424032857487507,
      "learning_rate": 1.995059768757764e-05,
      "loss": 0.7228,
      "step": 675
    },
    {
      "epoch": 0.06086114924936415,
      "grad_norm": 0.8468277183391748,
      "learning_rate": 1.9950307753654016e-05,
      "loss": 0.6399,
      "step": 676
    },
    {
      "epoch": 0.060951180535235995,
      "grad_norm": 1.2081750522132098,
      "learning_rate": 1.995001697355004e-05,
      "loss": 0.6926,
      "step": 677
    },
    {
      "epoch": 0.06104121182110783,
      "grad_norm": 1.3368059398138863,
      "learning_rate": 1.9949725347290445e-05,
      "loss": 0.6969,
      "step": 678
    },
    {
      "epoch": 0.06113124310697968,
      "grad_norm": 0.9782016810561651,
      "learning_rate": 1.9949432874900026e-05,
      "loss": 0.5871,
      "step": 679
    },
    {
      "epoch": 0.061221274392851514,
      "grad_norm": 0.8476375228446376,
      "learning_rate": 1.994913955640366e-05,
      "loss": 0.6919,
      "step": 680
    },
    {
      "epoch": 0.06131130567872336,
      "grad_norm": 1.0976136431472536,
      "learning_rate": 1.994884539182629e-05,
      "loss": 0.6183,
      "step": 681
    },
    {
      "epoch": 0.061401336964595196,
      "grad_norm": 1.248471500386518,
      "learning_rate": 1.9948550381192925e-05,
      "loss": 0.6045,
      "step": 682
    },
    {
      "epoch": 0.06149136825046704,
      "grad_norm": 0.7677472064229989,
      "learning_rate": 1.994825452452866e-05,
      "loss": 0.6435,
      "step": 683
    },
    {
      "epoch": 0.06158139953633888,
      "grad_norm": 0.9330536238384372,
      "learning_rate": 1.9947957821858656e-05,
      "loss": 0.6547,
      "step": 684
    },
    {
      "epoch": 0.061671430822210715,
      "grad_norm": 0.8557327785124581,
      "learning_rate": 1.9947660273208135e-05,
      "loss": 0.5913,
      "step": 685
    },
    {
      "epoch": 0.06176146210808256,
      "grad_norm": 1.0598634769968418,
      "learning_rate": 1.9947361878602412e-05,
      "loss": 0.595,
      "step": 686
    },
    {
      "epoch": 0.0618514933939544,
      "grad_norm": 1.1322347047333021,
      "learning_rate": 1.994706263806686e-05,
      "loss": 0.6218,
      "step": 687
    },
    {
      "epoch": 0.06194152467982624,
      "grad_norm": 1.0246435353281003,
      "learning_rate": 1.994676255162692e-05,
      "loss": 0.5292,
      "step": 688
    },
    {
      "epoch": 0.06203155596569808,
      "grad_norm": 0.9128529638700046,
      "learning_rate": 1.994646161930812e-05,
      "loss": 0.6991,
      "step": 689
    },
    {
      "epoch": 0.062121587251569924,
      "grad_norm": 0.8868337140156307,
      "learning_rate": 1.9946159841136046e-05,
      "loss": 0.6058,
      "step": 690
    },
    {
      "epoch": 0.06221161853744176,
      "grad_norm": 1.2691356899825406,
      "learning_rate": 1.9945857217136365e-05,
      "loss": 0.6904,
      "step": 691
    },
    {
      "epoch": 0.0623016498233136,
      "grad_norm": 1.25318741911506,
      "learning_rate": 1.9945553747334807e-05,
      "loss": 0.6568,
      "step": 692
    },
    {
      "epoch": 0.06239168110918544,
      "grad_norm": 0.8991926255911026,
      "learning_rate": 1.9945249431757187e-05,
      "loss": 0.5931,
      "step": 693
    },
    {
      "epoch": 0.06248171239505728,
      "grad_norm": 1.1092077013831352,
      "learning_rate": 1.9944944270429376e-05,
      "loss": 0.6493,
      "step": 694
    },
    {
      "epoch": 0.06257174368092912,
      "grad_norm": 1.299972603118042,
      "learning_rate": 1.9944638263377332e-05,
      "loss": 0.7339,
      "step": 695
    },
    {
      "epoch": 0.06266177496680096,
      "grad_norm": 1.172293068146599,
      "learning_rate": 1.994433141062707e-05,
      "loss": 0.6641,
      "step": 696
    },
    {
      "epoch": 0.0627518062526728,
      "grad_norm": 0.8924364628716703,
      "learning_rate": 1.9944023712204696e-05,
      "loss": 0.6075,
      "step": 697
    },
    {
      "epoch": 0.06284183753854465,
      "grad_norm": 0.9468333967663096,
      "learning_rate": 1.9943715168136366e-05,
      "loss": 0.6346,
      "step": 698
    },
    {
      "epoch": 0.06293186882441648,
      "grad_norm": 1.2161680242597395,
      "learning_rate": 1.9943405778448327e-05,
      "loss": 0.7083,
      "step": 699
    },
    {
      "epoch": 0.06302190011028833,
      "grad_norm": 1.3080732063800742,
      "learning_rate": 1.9943095543166886e-05,
      "loss": 0.5161,
      "step": 700
    },
    {
      "epoch": 0.06311193139616017,
      "grad_norm": 0.9102558056524025,
      "learning_rate": 1.9942784462318424e-05,
      "loss": 0.6023,
      "step": 701
    },
    {
      "epoch": 0.063201962682032,
      "grad_norm": 0.808034451521208,
      "learning_rate": 1.9942472535929402e-05,
      "loss": 0.6491,
      "step": 702
    },
    {
      "epoch": 0.06329199396790385,
      "grad_norm": 0.8911102698508747,
      "learning_rate": 1.9942159764026336e-05,
      "loss": 0.6789,
      "step": 703
    },
    {
      "epoch": 0.06338202525377569,
      "grad_norm": 1.2034700236062201,
      "learning_rate": 1.9941846146635832e-05,
      "loss": 0.6354,
      "step": 704
    },
    {
      "epoch": 0.06347205653964753,
      "grad_norm": 0.8010848721443733,
      "learning_rate": 1.9941531683784562e-05,
      "loss": 0.716,
      "step": 705
    },
    {
      "epoch": 0.06356208782551936,
      "grad_norm": 0.8868460167065081,
      "learning_rate": 1.994121637549926e-05,
      "loss": 0.7064,
      "step": 706
    },
    {
      "epoch": 0.06365211911139121,
      "grad_norm": 0.8293037659596695,
      "learning_rate": 1.994090022180675e-05,
      "loss": 0.6863,
      "step": 707
    },
    {
      "epoch": 0.06374215039726305,
      "grad_norm": 0.9392018635761618,
      "learning_rate": 1.9940583222733904e-05,
      "loss": 0.6494,
      "step": 708
    },
    {
      "epoch": 0.06383218168313488,
      "grad_norm": 0.9597805995552541,
      "learning_rate": 1.9940265378307693e-05,
      "loss": 0.6533,
      "step": 709
    },
    {
      "epoch": 0.06392221296900673,
      "grad_norm": 0.9355473747098405,
      "learning_rate": 1.993994668855514e-05,
      "loss": 0.5898,
      "step": 710
    },
    {
      "epoch": 0.06401224425487857,
      "grad_norm": 0.885746502451759,
      "learning_rate": 1.993962715350335e-05,
      "loss": 0.6364,
      "step": 711
    },
    {
      "epoch": 0.06410227554075042,
      "grad_norm": 0.8382112305426266,
      "learning_rate": 1.9939306773179498e-05,
      "loss": 0.6675,
      "step": 712
    },
    {
      "epoch": 0.06419230682662225,
      "grad_norm": 0.7896265610948616,
      "learning_rate": 1.9938985547610824e-05,
      "loss": 0.6659,
      "step": 713
    },
    {
      "epoch": 0.06428233811249409,
      "grad_norm": 0.9228327804516717,
      "learning_rate": 1.9938663476824646e-05,
      "loss": 0.5437,
      "step": 714
    },
    {
      "epoch": 0.06437236939836594,
      "grad_norm": 0.9050710774481164,
      "learning_rate": 1.9938340560848353e-05,
      "loss": 0.6626,
      "step": 715
    },
    {
      "epoch": 0.06446240068423777,
      "grad_norm": 0.8558301176362058,
      "learning_rate": 1.993801679970941e-05,
      "loss": 0.7052,
      "step": 716
    },
    {
      "epoch": 0.06455243197010961,
      "grad_norm": 0.9236623574785964,
      "learning_rate": 1.993769219343535e-05,
      "loss": 0.7174,
      "step": 717
    },
    {
      "epoch": 0.06464246325598146,
      "grad_norm": 1.105895674345732,
      "learning_rate": 1.993736674205377e-05,
      "loss": 0.6484,
      "step": 718
    },
    {
      "epoch": 0.0647324945418533,
      "grad_norm": 0.9573984656054453,
      "learning_rate": 1.9937040445592356e-05,
      "loss": 0.784,
      "step": 719
    },
    {
      "epoch": 0.06482252582772513,
      "grad_norm": 0.9203556561097299,
      "learning_rate": 1.993671330407885e-05,
      "loss": 0.6384,
      "step": 720
    },
    {
      "epoch": 0.06491255711359697,
      "grad_norm": 0.8643812549514113,
      "learning_rate": 1.9936385317541075e-05,
      "loss": 0.615,
      "step": 721
    },
    {
      "epoch": 0.06500258839946882,
      "grad_norm": 0.9614239812236118,
      "learning_rate": 1.993605648600692e-05,
      "loss": 0.5853,
      "step": 722
    },
    {
      "epoch": 0.06509261968534065,
      "grad_norm": 0.9232834425428058,
      "learning_rate": 1.9935726809504357e-05,
      "loss": 0.6468,
      "step": 723
    },
    {
      "epoch": 0.0651826509712125,
      "grad_norm": 0.8409394710093815,
      "learning_rate": 1.9935396288061412e-05,
      "loss": 0.582,
      "step": 724
    },
    {
      "epoch": 0.06527268225708434,
      "grad_norm": 0.8047166345520703,
      "learning_rate": 1.99350649217062e-05,
      "loss": 0.5863,
      "step": 725
    },
    {
      "epoch": 0.06536271354295618,
      "grad_norm": 0.9385700768462824,
      "learning_rate": 1.9934732710466897e-05,
      "loss": 0.6198,
      "step": 726
    },
    {
      "epoch": 0.06545274482882801,
      "grad_norm": 0.9812142700306185,
      "learning_rate": 1.9934399654371757e-05,
      "loss": 0.7486,
      "step": 727
    },
    {
      "epoch": 0.06554277611469986,
      "grad_norm": 0.8853893129335281,
      "learning_rate": 1.99340657534491e-05,
      "loss": 0.6368,
      "step": 728
    },
    {
      "epoch": 0.0656328074005717,
      "grad_norm": 0.800031261904951,
      "learning_rate": 1.9933731007727322e-05,
      "loss": 0.5225,
      "step": 729
    },
    {
      "epoch": 0.06572283868644353,
      "grad_norm": 1.0823596646476665,
      "learning_rate": 1.9933395417234892e-05,
      "loss": 0.7476,
      "step": 730
    },
    {
      "epoch": 0.06581286997231538,
      "grad_norm": 0.8667231033326653,
      "learning_rate": 1.993305898200035e-05,
      "loss": 0.6245,
      "step": 731
    },
    {
      "epoch": 0.06590290125818722,
      "grad_norm": 0.7805161469037438,
      "learning_rate": 1.99327217020523e-05,
      "loss": 0.6454,
      "step": 732
    },
    {
      "epoch": 0.06599293254405907,
      "grad_norm": 0.91119718378148,
      "learning_rate": 1.9932383577419432e-05,
      "loss": 0.6286,
      "step": 733
    },
    {
      "epoch": 0.0660829638299309,
      "grad_norm": 0.7015847099919208,
      "learning_rate": 1.9932044608130495e-05,
      "loss": 0.6925,
      "step": 734
    },
    {
      "epoch": 0.06617299511580274,
      "grad_norm": 0.9709731734715782,
      "learning_rate": 1.9931704794214316e-05,
      "loss": 0.7039,
      "step": 735
    },
    {
      "epoch": 0.06626302640167459,
      "grad_norm": 0.9948059750836087,
      "learning_rate": 1.99313641356998e-05,
      "loss": 0.6123,
      "step": 736
    },
    {
      "epoch": 0.06635305768754642,
      "grad_norm": 0.9136840130072706,
      "learning_rate": 1.9931022632615908e-05,
      "loss": 0.6107,
      "step": 737
    },
    {
      "epoch": 0.06644308897341826,
      "grad_norm": 0.8069887063017938,
      "learning_rate": 1.9930680284991683e-05,
      "loss": 0.6009,
      "step": 738
    },
    {
      "epoch": 0.0665331202592901,
      "grad_norm": 0.9005625223666468,
      "learning_rate": 1.9930337092856243e-05,
      "loss": 0.5605,
      "step": 739
    },
    {
      "epoch": 0.06662315154516195,
      "grad_norm": 0.9729343095899563,
      "learning_rate": 1.9929993056238768e-05,
      "loss": 0.6833,
      "step": 740
    },
    {
      "epoch": 0.06671318283103378,
      "grad_norm": 0.8872974553433142,
      "learning_rate": 1.992964817516852e-05,
      "loss": 0.5855,
      "step": 741
    },
    {
      "epoch": 0.06680321411690562,
      "grad_norm": 0.961344230857516,
      "learning_rate": 1.9929302449674824e-05,
      "loss": 0.6291,
      "step": 742
    },
    {
      "epoch": 0.06689324540277747,
      "grad_norm": 0.8262111313636582,
      "learning_rate": 1.992895587978708e-05,
      "loss": 0.5976,
      "step": 743
    },
    {
      "epoch": 0.06698327668864931,
      "grad_norm": 0.646332400812623,
      "learning_rate": 1.9928608465534766e-05,
      "loss": 0.6041,
      "step": 744
    },
    {
      "epoch": 0.06707330797452114,
      "grad_norm": 0.9067609679600324,
      "learning_rate": 1.992826020694742e-05,
      "loss": 0.6445,
      "step": 745
    },
    {
      "epoch": 0.06716333926039299,
      "grad_norm": 0.8754388787877325,
      "learning_rate": 1.9927911104054663e-05,
      "loss": 0.6274,
      "step": 746
    },
    {
      "epoch": 0.06725337054626483,
      "grad_norm": 0.9199528502317471,
      "learning_rate": 1.9927561156886184e-05,
      "loss": 0.6497,
      "step": 747
    },
    {
      "epoch": 0.06734340183213666,
      "grad_norm": 0.7668348917890503,
      "learning_rate": 1.9927210365471734e-05,
      "loss": 0.6562,
      "step": 748
    },
    {
      "epoch": 0.0674334331180085,
      "grad_norm": 1.1204411946060908,
      "learning_rate": 1.9926858729841154e-05,
      "loss": 0.615,
      "step": 749
    },
    {
      "epoch": 0.06752346440388035,
      "grad_norm": 0.906640598920881,
      "learning_rate": 1.9926506250024342e-05,
      "loss": 0.5797,
      "step": 750
    },
    {
      "epoch": 0.0676134956897522,
      "grad_norm": 1.0152276115025174,
      "learning_rate": 1.9926152926051276e-05,
      "loss": 0.6212,
      "step": 751
    },
    {
      "epoch": 0.06770352697562403,
      "grad_norm": 0.8126398054196898,
      "learning_rate": 1.9925798757952003e-05,
      "loss": 0.5984,
      "step": 752
    },
    {
      "epoch": 0.06779355826149587,
      "grad_norm": 1.0695501832125924,
      "learning_rate": 1.9925443745756635e-05,
      "loss": 0.6974,
      "step": 753
    },
    {
      "epoch": 0.06788358954736771,
      "grad_norm": 0.9840263110009709,
      "learning_rate": 1.9925087889495374e-05,
      "loss": 0.6594,
      "step": 754
    },
    {
      "epoch": 0.06797362083323955,
      "grad_norm": 0.9120307513618242,
      "learning_rate": 1.9924731189198474e-05,
      "loss": 0.6993,
      "step": 755
    },
    {
      "epoch": 0.06806365211911139,
      "grad_norm": 0.9939179667783352,
      "learning_rate": 1.9924373644896267e-05,
      "loss": 0.602,
      "step": 756
    },
    {
      "epoch": 0.06815368340498323,
      "grad_norm": 0.8406347778387762,
      "learning_rate": 1.9924015256619167e-05,
      "loss": 0.6008,
      "step": 757
    },
    {
      "epoch": 0.06824371469085508,
      "grad_norm": 0.8635263851052599,
      "learning_rate": 1.9923656024397646e-05,
      "loss": 0.5604,
      "step": 758
    },
    {
      "epoch": 0.06833374597672691,
      "grad_norm": 1.183498224117018,
      "learning_rate": 1.9923295948262257e-05,
      "loss": 0.6761,
      "step": 759
    },
    {
      "epoch": 0.06842377726259875,
      "grad_norm": 1.0927316965395095,
      "learning_rate": 1.9922935028243617e-05,
      "loss": 0.7177,
      "step": 760
    },
    {
      "epoch": 0.0685138085484706,
      "grad_norm": 0.8606408254735465,
      "learning_rate": 1.9922573264372418e-05,
      "loss": 0.6993,
      "step": 761
    },
    {
      "epoch": 0.06860383983434243,
      "grad_norm": 0.8952033603051093,
      "learning_rate": 1.992221065667943e-05,
      "loss": 0.7519,
      "step": 762
    },
    {
      "epoch": 0.06869387112021427,
      "grad_norm": 0.7277349075889421,
      "learning_rate": 1.9921847205195485e-05,
      "loss": 0.5661,
      "step": 763
    },
    {
      "epoch": 0.06878390240608612,
      "grad_norm": 0.900067052815185,
      "learning_rate": 1.9921482909951496e-05,
      "loss": 0.6464,
      "step": 764
    },
    {
      "epoch": 0.06887393369195796,
      "grad_norm": 1.2687957996962054,
      "learning_rate": 1.9921117770978436e-05,
      "loss": 0.6419,
      "step": 765
    },
    {
      "epoch": 0.06896396497782979,
      "grad_norm": 0.9166786273519146,
      "learning_rate": 1.9920751788307364e-05,
      "loss": 0.6478,
      "step": 766
    },
    {
      "epoch": 0.06905399626370164,
      "grad_norm": 0.9486299328350304,
      "learning_rate": 1.9920384961969395e-05,
      "loss": 0.6078,
      "step": 767
    },
    {
      "epoch": 0.06914402754957348,
      "grad_norm": 1.0549877260323468,
      "learning_rate": 1.9920017291995732e-05,
      "loss": 0.7292,
      "step": 768
    },
    {
      "epoch": 0.06923405883544531,
      "grad_norm": 0.7570425910073765,
      "learning_rate": 1.9919648778417637e-05,
      "loss": 0.6439,
      "step": 769
    },
    {
      "epoch": 0.06932409012131716,
      "grad_norm": 1.366461180404449,
      "learning_rate": 1.9919279421266453e-05,
      "loss": 0.6613,
      "step": 770
    },
    {
      "epoch": 0.069414121407189,
      "grad_norm": 0.8116310286447795,
      "learning_rate": 1.9918909220573588e-05,
      "loss": 0.6017,
      "step": 771
    },
    {
      "epoch": 0.06950415269306084,
      "grad_norm": 0.7478677831901427,
      "learning_rate": 1.991853817637052e-05,
      "loss": 0.6993,
      "step": 772
    },
    {
      "epoch": 0.06959418397893267,
      "grad_norm": 0.9419596033503236,
      "learning_rate": 1.991816628868881e-05,
      "loss": 0.7188,
      "step": 773
    },
    {
      "epoch": 0.06968421526480452,
      "grad_norm": 0.747521772083581,
      "learning_rate": 1.9917793557560076e-05,
      "loss": 0.5748,
      "step": 774
    },
    {
      "epoch": 0.06977424655067636,
      "grad_norm": 0.9058912031760946,
      "learning_rate": 1.9917419983016025e-05,
      "loss": 0.7385,
      "step": 775
    },
    {
      "epoch": 0.0698642778365482,
      "grad_norm": 0.9888608471845093,
      "learning_rate": 1.9917045565088418e-05,
      "loss": 0.6222,
      "step": 776
    },
    {
      "epoch": 0.06995430912242004,
      "grad_norm": 0.9349120255219355,
      "learning_rate": 1.99166703038091e-05,
      "loss": 0.6539,
      "step": 777
    },
    {
      "epoch": 0.07004434040829188,
      "grad_norm": 1.0024825098140775,
      "learning_rate": 1.9916294199209977e-05,
      "loss": 0.6857,
      "step": 778
    },
    {
      "epoch": 0.07013437169416373,
      "grad_norm": 1.0293391973615122,
      "learning_rate": 1.9915917251323044e-05,
      "loss": 0.5652,
      "step": 779
    },
    {
      "epoch": 0.07022440298003556,
      "grad_norm": 0.8325976258183879,
      "learning_rate": 1.9915539460180345e-05,
      "loss": 0.6462,
      "step": 780
    },
    {
      "epoch": 0.0703144342659074,
      "grad_norm": 1.1294794799617498,
      "learning_rate": 1.9915160825814016e-05,
      "loss": 0.6419,
      "step": 781
    },
    {
      "epoch": 0.07040446555177925,
      "grad_norm": 0.9781619353427522,
      "learning_rate": 1.9914781348256254e-05,
      "loss": 0.6287,
      "step": 782
    },
    {
      "epoch": 0.07049449683765108,
      "grad_norm": 1.1353764451775683,
      "learning_rate": 1.991440102753933e-05,
      "loss": 0.6001,
      "step": 783
    },
    {
      "epoch": 0.07058452812352292,
      "grad_norm": 1.5640676129425293,
      "learning_rate": 1.9914019863695584e-05,
      "loss": 0.6265,
      "step": 784
    },
    {
      "epoch": 0.07067455940939477,
      "grad_norm": 1.248470217952301,
      "learning_rate": 1.9913637856757434e-05,
      "loss": 0.6083,
      "step": 785
    },
    {
      "epoch": 0.07076459069526661,
      "grad_norm": 1.275872642352166,
      "learning_rate": 1.9913255006757363e-05,
      "loss": 0.627,
      "step": 786
    },
    {
      "epoch": 0.07085462198113844,
      "grad_norm": 0.986091580569602,
      "learning_rate": 1.9912871313727937e-05,
      "loss": 0.631,
      "step": 787
    },
    {
      "epoch": 0.07094465326701029,
      "grad_norm": 1.0575872307093306,
      "learning_rate": 1.991248677770177e-05,
      "loss": 0.6126,
      "step": 788
    },
    {
      "epoch": 0.07103468455288213,
      "grad_norm": 1.175362977780415,
      "learning_rate": 1.991210139871158e-05,
      "loss": 0.6887,
      "step": 789
    },
    {
      "epoch": 0.07112471583875396,
      "grad_norm": 0.8303823499015206,
      "learning_rate": 1.991171517679013e-05,
      "loss": 0.676,
      "step": 790
    },
    {
      "epoch": 0.0712147471246258,
      "grad_norm": 1.0449315042496294,
      "learning_rate": 1.9911328111970265e-05,
      "loss": 0.6778,
      "step": 791
    },
    {
      "epoch": 0.07130477841049765,
      "grad_norm": 0.676257927704992,
      "learning_rate": 1.9910940204284905e-05,
      "loss": 0.6198,
      "step": 792
    },
    {
      "epoch": 0.0713948096963695,
      "grad_norm": 0.8000448525880827,
      "learning_rate": 1.9910551453767034e-05,
      "loss": 0.5831,
      "step": 793
    },
    {
      "epoch": 0.07148484098224132,
      "grad_norm": 0.8620959666252962,
      "learning_rate": 1.9910161860449713e-05,
      "loss": 0.662,
      "step": 794
    },
    {
      "epoch": 0.07157487226811317,
      "grad_norm": 0.995653675280295,
      "learning_rate": 1.990977142436608e-05,
      "loss": 0.6425,
      "step": 795
    },
    {
      "epoch": 0.07166490355398501,
      "grad_norm": 0.8143320083612581,
      "learning_rate": 1.9909380145549325e-05,
      "loss": 0.619,
      "step": 796
    },
    {
      "epoch": 0.07175493483985686,
      "grad_norm": 0.7909598175112007,
      "learning_rate": 1.990898802403273e-05,
      "loss": 0.5974,
      "step": 797
    },
    {
      "epoch": 0.07184496612572869,
      "grad_norm": 0.8150013918094265,
      "learning_rate": 1.990859505984964e-05,
      "loss": 0.6977,
      "step": 798
    },
    {
      "epoch": 0.07193499741160053,
      "grad_norm": 0.8143633107582622,
      "learning_rate": 1.9908201253033478e-05,
      "loss": 0.6377,
      "step": 799
    },
    {
      "epoch": 0.07202502869747238,
      "grad_norm": 0.9189798392116322,
      "learning_rate": 1.9907806603617724e-05,
      "loss": 0.6088,
      "step": 800
    },
    {
      "epoch": 0.0721150599833442,
      "grad_norm": 1.2509316953750516,
      "learning_rate": 1.990741111163595e-05,
      "loss": 0.6818,
      "step": 801
    },
    {
      "epoch": 0.07220509126921605,
      "grad_norm": 0.7774808307396729,
      "learning_rate": 1.990701477712178e-05,
      "loss": 0.622,
      "step": 802
    },
    {
      "epoch": 0.0722951225550879,
      "grad_norm": 0.7959955891024826,
      "learning_rate": 1.9906617600108918e-05,
      "loss": 0.5918,
      "step": 803
    },
    {
      "epoch": 0.07238515384095974,
      "grad_norm": 0.7955524742871011,
      "learning_rate": 1.990621958063115e-05,
      "loss": 0.6394,
      "step": 804
    },
    {
      "epoch": 0.07247518512683157,
      "grad_norm": 1.0007085120818604,
      "learning_rate": 1.9905820718722312e-05,
      "loss": 0.6379,
      "step": 805
    },
    {
      "epoch": 0.07256521641270341,
      "grad_norm": 0.8718309115246565,
      "learning_rate": 1.9905421014416336e-05,
      "loss": 0.6209,
      "step": 806
    },
    {
      "epoch": 0.07265524769857526,
      "grad_norm": 0.9625641853225991,
      "learning_rate": 1.99050204677472e-05,
      "loss": 0.5335,
      "step": 807
    },
    {
      "epoch": 0.07274527898444709,
      "grad_norm": 0.9644477358571792,
      "learning_rate": 1.9904619078748976e-05,
      "loss": 0.5751,
      "step": 808
    },
    {
      "epoch": 0.07283531027031893,
      "grad_norm": 0.9647056347575116,
      "learning_rate": 1.9904216847455795e-05,
      "loss": 0.6229,
      "step": 809
    },
    {
      "epoch": 0.07292534155619078,
      "grad_norm": 0.8961342617066971,
      "learning_rate": 1.990381377390186e-05,
      "loss": 0.5388,
      "step": 810
    },
    {
      "epoch": 0.07301537284206262,
      "grad_norm": 0.8177227489783463,
      "learning_rate": 1.9903409858121457e-05,
      "loss": 0.6704,
      "step": 811
    },
    {
      "epoch": 0.07310540412793445,
      "grad_norm": 0.8282225888252737,
      "learning_rate": 1.990300510014893e-05,
      "loss": 0.5852,
      "step": 812
    },
    {
      "epoch": 0.0731954354138063,
      "grad_norm": 0.9176612025177313,
      "learning_rate": 1.99025995000187e-05,
      "loss": 0.6648,
      "step": 813
    },
    {
      "epoch": 0.07328546669967814,
      "grad_norm": 1.5988015103519235,
      "learning_rate": 1.9902193057765253e-05,
      "loss": 0.6356,
      "step": 814
    },
    {
      "epoch": 0.07337549798554997,
      "grad_norm": 0.8420423908684179,
      "learning_rate": 1.9901785773423167e-05,
      "loss": 0.5399,
      "step": 815
    },
    {
      "epoch": 0.07346552927142182,
      "grad_norm": 0.8238959706759954,
      "learning_rate": 1.9901377647027068e-05,
      "loss": 0.5873,
      "step": 816
    },
    {
      "epoch": 0.07355556055729366,
      "grad_norm": 0.8125609505644813,
      "learning_rate": 1.9900968678611664e-05,
      "loss": 0.65,
      "step": 817
    },
    {
      "epoch": 0.0736455918431655,
      "grad_norm": 1.0613777800821573,
      "learning_rate": 1.9900558868211738e-05,
      "loss": 0.6659,
      "step": 818
    },
    {
      "epoch": 0.07373562312903734,
      "grad_norm": 1.1553240521120638,
      "learning_rate": 1.990014821586214e-05,
      "loss": 0.7188,
      "step": 819
    },
    {
      "epoch": 0.07382565441490918,
      "grad_norm": 0.7893607529891743,
      "learning_rate": 1.9899736721597787e-05,
      "loss": 0.6293,
      "step": 820
    },
    {
      "epoch": 0.07391568570078103,
      "grad_norm": 1.1981323168206837,
      "learning_rate": 1.9899324385453676e-05,
      "loss": 0.675,
      "step": 821
    },
    {
      "epoch": 0.07400571698665286,
      "grad_norm": 0.9292171189948579,
      "learning_rate": 1.989891120746487e-05,
      "loss": 0.6794,
      "step": 822
    },
    {
      "epoch": 0.0740957482725247,
      "grad_norm": 0.9897658196620981,
      "learning_rate": 1.9898497187666514e-05,
      "loss": 0.5987,
      "step": 823
    },
    {
      "epoch": 0.07418577955839654,
      "grad_norm": 0.9514559631438293,
      "learning_rate": 1.989808232609381e-05,
      "loss": 0.7145,
      "step": 824
    },
    {
      "epoch": 0.07427581084426839,
      "grad_norm": 1.0695089729235898,
      "learning_rate": 1.9897666622782036e-05,
      "loss": 0.5956,
      "step": 825
    },
    {
      "epoch": 0.07436584213014022,
      "grad_norm": 0.8863932035730235,
      "learning_rate": 1.9897250077766545e-05,
      "loss": 0.6133,
      "step": 826
    },
    {
      "epoch": 0.07445587341601206,
      "grad_norm": 0.9875416517917907,
      "learning_rate": 1.9896832691082767e-05,
      "loss": 0.7281,
      "step": 827
    },
    {
      "epoch": 0.07454590470188391,
      "grad_norm": 0.8137707162245958,
      "learning_rate": 1.9896414462766188e-05,
      "loss": 0.5641,
      "step": 828
    },
    {
      "epoch": 0.07463593598775574,
      "grad_norm": 0.9313063783231573,
      "learning_rate": 1.989599539285238e-05,
      "loss": 0.6605,
      "step": 829
    },
    {
      "epoch": 0.07472596727362758,
      "grad_norm": 1.3042095309906097,
      "learning_rate": 1.9895575481376977e-05,
      "loss": 0.7341,
      "step": 830
    },
    {
      "epoch": 0.07481599855949943,
      "grad_norm": 1.0186737521768916,
      "learning_rate": 1.9895154728375693e-05,
      "loss": 0.657,
      "step": 831
    },
    {
      "epoch": 0.07490602984537127,
      "grad_norm": 0.8530725647650926,
      "learning_rate": 1.9894733133884302e-05,
      "loss": 0.6634,
      "step": 832
    },
    {
      "epoch": 0.0749960611312431,
      "grad_norm": 0.9188521096722377,
      "learning_rate": 1.9894310697938666e-05,
      "loss": 0.6445,
      "step": 833
    },
    {
      "epoch": 0.07508609241711495,
      "grad_norm": 0.7854629629302243,
      "learning_rate": 1.98938874205747e-05,
      "loss": 0.5761,
      "step": 834
    },
    {
      "epoch": 0.07517612370298679,
      "grad_norm": 0.8928404872162481,
      "learning_rate": 1.9893463301828407e-05,
      "loss": 0.605,
      "step": 835
    },
    {
      "epoch": 0.07526615498885862,
      "grad_norm": 0.8630574587593948,
      "learning_rate": 1.9893038341735854e-05,
      "loss": 0.6113,
      "step": 836
    },
    {
      "epoch": 0.07535618627473047,
      "grad_norm": 0.7520101353540561,
      "learning_rate": 1.9892612540333175e-05,
      "loss": 0.5314,
      "step": 837
    },
    {
      "epoch": 0.07544621756060231,
      "grad_norm": 0.8795486155156033,
      "learning_rate": 1.989218589765658e-05,
      "loss": 0.6105,
      "step": 838
    },
    {
      "epoch": 0.07553624884647415,
      "grad_norm": 1.0215826677679798,
      "learning_rate": 1.9891758413742357e-05,
      "loss": 0.7102,
      "step": 839
    },
    {
      "epoch": 0.07562628013234599,
      "grad_norm": 0.9567397882841462,
      "learning_rate": 1.9891330088626856e-05,
      "loss": 0.6015,
      "step": 840
    },
    {
      "epoch": 0.07571631141821783,
      "grad_norm": 0.8526233939684906,
      "learning_rate": 1.9890900922346502e-05,
      "loss": 0.6782,
      "step": 841
    },
    {
      "epoch": 0.07580634270408967,
      "grad_norm": 0.8309389599048341,
      "learning_rate": 1.9890470914937793e-05,
      "loss": 0.6362,
      "step": 842
    },
    {
      "epoch": 0.0758963739899615,
      "grad_norm": 0.8848001498974757,
      "learning_rate": 1.9890040066437294e-05,
      "loss": 0.7447,
      "step": 843
    },
    {
      "epoch": 0.07598640527583335,
      "grad_norm": 0.8065322468055697,
      "learning_rate": 1.9889608376881647e-05,
      "loss": 0.5927,
      "step": 844
    },
    {
      "epoch": 0.0760764365617052,
      "grad_norm": 0.9226844809524466,
      "learning_rate": 1.9889175846307566e-05,
      "loss": 0.5636,
      "step": 845
    },
    {
      "epoch": 0.07616646784757704,
      "grad_norm": 0.8781919642375389,
      "learning_rate": 1.9888742474751826e-05,
      "loss": 0.6875,
      "step": 846
    },
    {
      "epoch": 0.07625649913344887,
      "grad_norm": 1.003871515647589,
      "learning_rate": 1.9888308262251286e-05,
      "loss": 0.5779,
      "step": 847
    },
    {
      "epoch": 0.07634653041932071,
      "grad_norm": 1.0040581745489174,
      "learning_rate": 1.9887873208842874e-05,
      "loss": 0.7101,
      "step": 848
    },
    {
      "epoch": 0.07643656170519256,
      "grad_norm": 1.1614390050084347,
      "learning_rate": 1.988743731456358e-05,
      "loss": 0.5625,
      "step": 849
    },
    {
      "epoch": 0.0765265929910644,
      "grad_norm": 0.8161232927501173,
      "learning_rate": 1.988700057945048e-05,
      "loss": 0.6725,
      "step": 850
    },
    {
      "epoch": 0.07661662427693623,
      "grad_norm": 1.142961140096786,
      "learning_rate": 1.9886563003540715e-05,
      "loss": 0.6699,
      "step": 851
    },
    {
      "epoch": 0.07670665556280808,
      "grad_norm": 0.9800919605654786,
      "learning_rate": 1.9886124586871492e-05,
      "loss": 0.6734,
      "step": 852
    },
    {
      "epoch": 0.07679668684867992,
      "grad_norm": 0.9632840717479723,
      "learning_rate": 1.9885685329480093e-05,
      "loss": 0.6115,
      "step": 853
    },
    {
      "epoch": 0.07688671813455175,
      "grad_norm": 1.0096898353723156,
      "learning_rate": 1.9885245231403876e-05,
      "loss": 0.6756,
      "step": 854
    },
    {
      "epoch": 0.0769767494204236,
      "grad_norm": 0.9426440522550007,
      "learning_rate": 1.9884804292680265e-05,
      "loss": 0.6801,
      "step": 855
    },
    {
      "epoch": 0.07706678070629544,
      "grad_norm": 0.8032809806471042,
      "learning_rate": 1.988436251334676e-05,
      "loss": 0.6579,
      "step": 856
    },
    {
      "epoch": 0.07715681199216728,
      "grad_norm": 1.349525344568918,
      "learning_rate": 1.9883919893440927e-05,
      "loss": 0.7488,
      "step": 857
    },
    {
      "epoch": 0.07724684327803912,
      "grad_norm": 0.8733951335004324,
      "learning_rate": 1.988347643300041e-05,
      "loss": 0.5641,
      "step": 858
    },
    {
      "epoch": 0.07733687456391096,
      "grad_norm": 0.8597281756676218,
      "learning_rate": 1.9883032132062926e-05,
      "loss": 0.6638,
      "step": 859
    },
    {
      "epoch": 0.0774269058497828,
      "grad_norm": 1.0925784461324963,
      "learning_rate": 1.9882586990666245e-05,
      "loss": 0.7337,
      "step": 860
    },
    {
      "epoch": 0.07751693713565463,
      "grad_norm": 0.8691057150513877,
      "learning_rate": 1.9882141008848234e-05,
      "loss": 0.59,
      "step": 861
    },
    {
      "epoch": 0.07760696842152648,
      "grad_norm": 0.9433559984386501,
      "learning_rate": 1.9881694186646813e-05,
      "loss": 0.6659,
      "step": 862
    },
    {
      "epoch": 0.07769699970739832,
      "grad_norm": 0.7881807841739749,
      "learning_rate": 1.9881246524099987e-05,
      "loss": 0.6596,
      "step": 863
    },
    {
      "epoch": 0.07778703099327017,
      "grad_norm": 0.7590985444637965,
      "learning_rate": 1.9880798021245815e-05,
      "loss": 0.6099,
      "step": 864
    },
    {
      "epoch": 0.077877062279142,
      "grad_norm": 0.7908354630531053,
      "learning_rate": 1.9880348678122446e-05,
      "loss": 0.6362,
      "step": 865
    },
    {
      "epoch": 0.07796709356501384,
      "grad_norm": 0.8962297136550583,
      "learning_rate": 1.9879898494768093e-05,
      "loss": 0.6072,
      "step": 866
    },
    {
      "epoch": 0.07805712485088569,
      "grad_norm": 0.7325607743580437,
      "learning_rate": 1.987944747122103e-05,
      "loss": 0.6684,
      "step": 867
    },
    {
      "epoch": 0.07814715613675752,
      "grad_norm": 0.8438812156140584,
      "learning_rate": 1.987899560751963e-05,
      "loss": 0.6165,
      "step": 868
    },
    {
      "epoch": 0.07823718742262936,
      "grad_norm": 0.9509335086090785,
      "learning_rate": 1.9878542903702302e-05,
      "loss": 0.608,
      "step": 869
    },
    {
      "epoch": 0.0783272187085012,
      "grad_norm": 0.8007108386951783,
      "learning_rate": 1.987808935980755e-05,
      "loss": 0.5949,
      "step": 870
    },
    {
      "epoch": 0.07841724999437305,
      "grad_norm": 0.7609553393796017,
      "learning_rate": 1.987763497587395e-05,
      "loss": 0.6978,
      "step": 871
    },
    {
      "epoch": 0.07850728128024488,
      "grad_norm": 0.7775268819279901,
      "learning_rate": 1.9877179751940135e-05,
      "loss": 0.6574,
      "step": 872
    },
    {
      "epoch": 0.07859731256611673,
      "grad_norm": 0.7369908983406486,
      "learning_rate": 1.9876723688044824e-05,
      "loss": 0.6326,
      "step": 873
    },
    {
      "epoch": 0.07868734385198857,
      "grad_norm": 0.8520467750228328,
      "learning_rate": 1.9876266784226798e-05,
      "loss": 0.681,
      "step": 874
    },
    {
      "epoch": 0.0787773751378604,
      "grad_norm": 0.8955687366200978,
      "learning_rate": 1.9875809040524907e-05,
      "loss": 0.6275,
      "step": 875
    },
    {
      "epoch": 0.07886740642373224,
      "grad_norm": 1.0348076600128344,
      "learning_rate": 1.9875350456978086e-05,
      "loss": 0.6553,
      "step": 876
    },
    {
      "epoch": 0.07895743770960409,
      "grad_norm": 0.9461666536498211,
      "learning_rate": 1.987489103362533e-05,
      "loss": 0.7206,
      "step": 877
    },
    {
      "epoch": 0.07904746899547593,
      "grad_norm": 0.8556426044751503,
      "learning_rate": 1.987443077050571e-05,
      "loss": 0.5738,
      "step": 878
    },
    {
      "epoch": 0.07913750028134776,
      "grad_norm": 0.9163428601340107,
      "learning_rate": 1.9873969667658364e-05,
      "loss": 0.6818,
      "step": 879
    },
    {
      "epoch": 0.07922753156721961,
      "grad_norm": 0.9251412315095768,
      "learning_rate": 1.9873507725122505e-05,
      "loss": 0.6442,
      "step": 880
    },
    {
      "epoch": 0.07931756285309145,
      "grad_norm": 1.0604363729582953,
      "learning_rate": 1.987304494293742e-05,
      "loss": 0.6855,
      "step": 881
    },
    {
      "epoch": 0.07940759413896328,
      "grad_norm": 0.818630248098342,
      "learning_rate": 1.9872581321142464e-05,
      "loss": 0.5154,
      "step": 882
    },
    {
      "epoch": 0.07949762542483513,
      "grad_norm": 1.2391195804420916,
      "learning_rate": 1.987211685977706e-05,
      "loss": 0.6289,
      "step": 883
    },
    {
      "epoch": 0.07958765671070697,
      "grad_norm": 1.21001214607886,
      "learning_rate": 1.9871651558880708e-05,
      "loss": 0.6157,
      "step": 884
    },
    {
      "epoch": 0.07967768799657882,
      "grad_norm": 0.9941041897270447,
      "learning_rate": 1.9871185418492978e-05,
      "loss": 0.6719,
      "step": 885
    },
    {
      "epoch": 0.07976771928245065,
      "grad_norm": 1.0498934094993753,
      "learning_rate": 1.9870718438653513e-05,
      "loss": 0.6651,
      "step": 886
    },
    {
      "epoch": 0.07985775056832249,
      "grad_norm": 1.012608769180435,
      "learning_rate": 1.987025061940202e-05,
      "loss": 0.6159,
      "step": 887
    },
    {
      "epoch": 0.07994778185419434,
      "grad_norm": 0.8142721480902614,
      "learning_rate": 1.9869781960778287e-05,
      "loss": 0.5906,
      "step": 888
    },
    {
      "epoch": 0.08003781314006617,
      "grad_norm": 0.8642759285483899,
      "learning_rate": 1.986931246282217e-05,
      "loss": 0.6378,
      "step": 889
    },
    {
      "epoch": 0.08012784442593801,
      "grad_norm": 1.3761282634201408,
      "learning_rate": 1.9868842125573587e-05,
      "loss": 0.6493,
      "step": 890
    },
    {
      "epoch": 0.08021787571180986,
      "grad_norm": 1.0093574413851918,
      "learning_rate": 1.9868370949072546e-05,
      "loss": 0.6625,
      "step": 891
    },
    {
      "epoch": 0.0803079069976817,
      "grad_norm": 0.8966969810652483,
      "learning_rate": 1.9867898933359107e-05,
      "loss": 0.6685,
      "step": 892
    },
    {
      "epoch": 0.08039793828355353,
      "grad_norm": 0.7739819147371259,
      "learning_rate": 1.9867426078473418e-05,
      "loss": 0.571,
      "step": 893
    },
    {
      "epoch": 0.08048796956942537,
      "grad_norm": 1.058297136084188,
      "learning_rate": 1.986695238445569e-05,
      "loss": 0.7138,
      "step": 894
    },
    {
      "epoch": 0.08057800085529722,
      "grad_norm": 0.9990549779426106,
      "learning_rate": 1.9866477851346206e-05,
      "loss": 0.6494,
      "step": 895
    },
    {
      "epoch": 0.08066803214116906,
      "grad_norm": 1.1933988344515722,
      "learning_rate": 1.9866002479185313e-05,
      "loss": 0.5913,
      "step": 896
    },
    {
      "epoch": 0.0807580634270409,
      "grad_norm": 0.9355493474073532,
      "learning_rate": 1.9865526268013448e-05,
      "loss": 0.6568,
      "step": 897
    },
    {
      "epoch": 0.08084809471291274,
      "grad_norm": 0.932758289691096,
      "learning_rate": 1.98650492178711e-05,
      "loss": 0.6695,
      "step": 898
    },
    {
      "epoch": 0.08093812599878458,
      "grad_norm": 0.877292359536184,
      "learning_rate": 1.9864571328798844e-05,
      "loss": 0.6218,
      "step": 899
    },
    {
      "epoch": 0.08102815728465641,
      "grad_norm": 0.9709167712405099,
      "learning_rate": 1.9864092600837312e-05,
      "loss": 0.7243,
      "step": 900
    },
    {
      "epoch": 0.08111818857052826,
      "grad_norm": 0.7976325897444305,
      "learning_rate": 1.9863613034027224e-05,
      "loss": 0.5812,
      "step": 901
    },
    {
      "epoch": 0.0812082198564001,
      "grad_norm": 0.8262204790752982,
      "learning_rate": 1.986313262840936e-05,
      "loss": 0.6072,
      "step": 902
    },
    {
      "epoch": 0.08129825114227195,
      "grad_norm": 0.8230163890673291,
      "learning_rate": 1.986265138402457e-05,
      "loss": 0.689,
      "step": 903
    },
    {
      "epoch": 0.08138828242814378,
      "grad_norm": 0.7751718694038953,
      "learning_rate": 1.9862169300913784e-05,
      "loss": 0.5896,
      "step": 904
    },
    {
      "epoch": 0.08147831371401562,
      "grad_norm": 1.1229419714715125,
      "learning_rate": 1.9861686379117997e-05,
      "loss": 0.639,
      "step": 905
    },
    {
      "epoch": 0.08156834499988747,
      "grad_norm": 0.9371979051236237,
      "learning_rate": 1.9861202618678273e-05,
      "loss": 0.7069,
      "step": 906
    },
    {
      "epoch": 0.0816583762857593,
      "grad_norm": 1.1848882260628022,
      "learning_rate": 1.9860718019635762e-05,
      "loss": 0.6333,
      "step": 907
    },
    {
      "epoch": 0.08174840757163114,
      "grad_norm": 1.2384469471652622,
      "learning_rate": 1.986023258203166e-05,
      "loss": 0.7089,
      "step": 908
    },
    {
      "epoch": 0.08183843885750298,
      "grad_norm": 1.0102452944800582,
      "learning_rate": 1.985974630590726e-05,
      "loss": 0.6297,
      "step": 909
    },
    {
      "epoch": 0.08192847014337483,
      "grad_norm": 0.7520930596939789,
      "learning_rate": 1.9859259191303915e-05,
      "loss": 0.6511,
      "step": 910
    },
    {
      "epoch": 0.08201850142924666,
      "grad_norm": 1.1067519876363392,
      "learning_rate": 1.985877123826304e-05,
      "loss": 0.6218,
      "step": 911
    },
    {
      "epoch": 0.0821085327151185,
      "grad_norm": 0.8437022165645034,
      "learning_rate": 1.9858282446826143e-05,
      "loss": 0.5781,
      "step": 912
    },
    {
      "epoch": 0.08219856400099035,
      "grad_norm": 1.199787177170283,
      "learning_rate": 1.9857792817034784e-05,
      "loss": 0.6055,
      "step": 913
    },
    {
      "epoch": 0.08228859528686218,
      "grad_norm": 0.9584645051892803,
      "learning_rate": 1.98573023489306e-05,
      "loss": 0.6401,
      "step": 914
    },
    {
      "epoch": 0.08237862657273402,
      "grad_norm": 0.8341507874947748,
      "learning_rate": 1.9856811042555307e-05,
      "loss": 0.6185,
      "step": 915
    },
    {
      "epoch": 0.08246865785860587,
      "grad_norm": 0.6751525453187563,
      "learning_rate": 1.9856318897950677e-05,
      "loss": 0.5797,
      "step": 916
    },
    {
      "epoch": 0.08255868914447771,
      "grad_norm": 1.0428977423986618,
      "learning_rate": 1.9855825915158573e-05,
      "loss": 0.6741,
      "step": 917
    },
    {
      "epoch": 0.08264872043034954,
      "grad_norm": 0.9484047959876337,
      "learning_rate": 1.9855332094220913e-05,
      "loss": 0.5929,
      "step": 918
    },
    {
      "epoch": 0.08273875171622139,
      "grad_norm": 0.780939901632877,
      "learning_rate": 1.9854837435179687e-05,
      "loss": 0.6738,
      "step": 919
    },
    {
      "epoch": 0.08282878300209323,
      "grad_norm": 0.957482787818729,
      "learning_rate": 1.985434193807697e-05,
      "loss": 0.6265,
      "step": 920
    },
    {
      "epoch": 0.08291881428796506,
      "grad_norm": 0.9025164353703081,
      "learning_rate": 1.9853845602954894e-05,
      "loss": 0.6246,
      "step": 921
    },
    {
      "epoch": 0.0830088455738369,
      "grad_norm": 0.7678424596642107,
      "learning_rate": 1.985334842985567e-05,
      "loss": 0.5757,
      "step": 922
    },
    {
      "epoch": 0.08309887685970875,
      "grad_norm": 0.8181944516093921,
      "learning_rate": 1.985285041882158e-05,
      "loss": 0.6999,
      "step": 923
    },
    {
      "epoch": 0.0831889081455806,
      "grad_norm": 0.7667760590796712,
      "learning_rate": 1.985235156989497e-05,
      "loss": 0.589,
      "step": 924
    },
    {
      "epoch": 0.08327893943145243,
      "grad_norm": 1.0216127674235007,
      "learning_rate": 1.9851851883118267e-05,
      "loss": 0.6233,
      "step": 925
    },
    {
      "epoch": 0.08336897071732427,
      "grad_norm": 0.9225841491340371,
      "learning_rate": 1.985135135853396e-05,
      "loss": 0.6762,
      "step": 926
    },
    {
      "epoch": 0.08345900200319611,
      "grad_norm": 0.8896974791125475,
      "learning_rate": 1.9850849996184614e-05,
      "loss": 0.6611,
      "step": 927
    },
    {
      "epoch": 0.08354903328906794,
      "grad_norm": 0.7442627430661132,
      "learning_rate": 1.985034779611287e-05,
      "loss": 0.5372,
      "step": 928
    },
    {
      "epoch": 0.08363906457493979,
      "grad_norm": 1.07971568484656,
      "learning_rate": 1.9849844758361433e-05,
      "loss": 0.6613,
      "step": 929
    },
    {
      "epoch": 0.08372909586081163,
      "grad_norm": 0.8569650660207562,
      "learning_rate": 1.9849340882973083e-05,
      "loss": 0.6203,
      "step": 930
    },
    {
      "epoch": 0.08381912714668348,
      "grad_norm": 0.9769351808166785,
      "learning_rate": 1.9848836169990667e-05,
      "loss": 0.6424,
      "step": 931
    },
    {
      "epoch": 0.08390915843255531,
      "grad_norm": 0.8340101157000354,
      "learning_rate": 1.984833061945711e-05,
      "loss": 0.6362,
      "step": 932
    },
    {
      "epoch": 0.08399918971842715,
      "grad_norm": 0.7909567070108694,
      "learning_rate": 1.9847824231415397e-05,
      "loss": 0.5851,
      "step": 933
    },
    {
      "epoch": 0.084089221004299,
      "grad_norm": 1.0178198211453675,
      "learning_rate": 1.98473170059086e-05,
      "loss": 0.6115,
      "step": 934
    },
    {
      "epoch": 0.08417925229017083,
      "grad_norm": 0.9668703215463895,
      "learning_rate": 1.984680894297985e-05,
      "loss": 0.5502,
      "step": 935
    },
    {
      "epoch": 0.08426928357604267,
      "grad_norm": 0.9218622298374001,
      "learning_rate": 1.9846300042672354e-05,
      "loss": 0.614,
      "step": 936
    },
    {
      "epoch": 0.08435931486191452,
      "grad_norm": 0.9649757294414496,
      "learning_rate": 1.9845790305029383e-05,
      "loss": 0.6997,
      "step": 937
    },
    {
      "epoch": 0.08444934614778636,
      "grad_norm": 0.68252506981783,
      "learning_rate": 1.9845279730094296e-05,
      "loss": 0.5795,
      "step": 938
    },
    {
      "epoch": 0.08453937743365819,
      "grad_norm": 0.9647033385764298,
      "learning_rate": 1.9844768317910506e-05,
      "loss": 0.6391,
      "step": 939
    },
    {
      "epoch": 0.08462940871953004,
      "grad_norm": 1.063923861149834,
      "learning_rate": 1.9844256068521505e-05,
      "loss": 0.7158,
      "step": 940
    },
    {
      "epoch": 0.08471944000540188,
      "grad_norm": 0.7525821290659247,
      "learning_rate": 1.9843742981970855e-05,
      "loss": 0.6269,
      "step": 941
    },
    {
      "epoch": 0.08480947129127371,
      "grad_norm": 0.9733526428315987,
      "learning_rate": 1.9843229058302192e-05,
      "loss": 0.6372,
      "step": 942
    },
    {
      "epoch": 0.08489950257714556,
      "grad_norm": 1.0005872497149406,
      "learning_rate": 1.9842714297559212e-05,
      "loss": 0.6208,
      "step": 943
    },
    {
      "epoch": 0.0849895338630174,
      "grad_norm": 0.9894017640386661,
      "learning_rate": 1.98421986997857e-05,
      "loss": 0.6785,
      "step": 944
    },
    {
      "epoch": 0.08507956514888924,
      "grad_norm": 0.9853076523408149,
      "learning_rate": 1.98416822650255e-05,
      "loss": 0.7512,
      "step": 945
    },
    {
      "epoch": 0.08516959643476107,
      "grad_norm": 0.9320686436353547,
      "learning_rate": 1.9841164993322528e-05,
      "loss": 0.587,
      "step": 946
    },
    {
      "epoch": 0.08525962772063292,
      "grad_norm": 1.0722602637069036,
      "learning_rate": 1.9840646884720777e-05,
      "loss": 0.5323,
      "step": 947
    },
    {
      "epoch": 0.08534965900650476,
      "grad_norm": 0.6561617190509789,
      "learning_rate": 1.98401279392643e-05,
      "loss": 0.5646,
      "step": 948
    },
    {
      "epoch": 0.08543969029237661,
      "grad_norm": 0.9501301427779898,
      "learning_rate": 1.9839608156997236e-05,
      "loss": 0.6461,
      "step": 949
    },
    {
      "epoch": 0.08552972157824844,
      "grad_norm": 0.7691379817197943,
      "learning_rate": 1.9839087537963783e-05,
      "loss": 0.5526,
      "step": 950
    },
    {
      "epoch": 0.08561975286412028,
      "grad_norm": 0.8389063061518838,
      "learning_rate": 1.9838566082208217e-05,
      "loss": 0.6269,
      "step": 951
    },
    {
      "epoch": 0.08570978414999213,
      "grad_norm": 0.9198699409941286,
      "learning_rate": 1.9838043789774882e-05,
      "loss": 0.6886,
      "step": 952
    },
    {
      "epoch": 0.08579981543586396,
      "grad_norm": 0.7426516661285701,
      "learning_rate": 1.9837520660708192e-05,
      "loss": 0.5699,
      "step": 953
    },
    {
      "epoch": 0.0858898467217358,
      "grad_norm": 0.7731757222497914,
      "learning_rate": 1.983699669505264e-05,
      "loss": 0.5955,
      "step": 954
    },
    {
      "epoch": 0.08597987800760765,
      "grad_norm": 0.8618071528578267,
      "learning_rate": 1.9836471892852777e-05,
      "loss": 0.6628,
      "step": 955
    },
    {
      "epoch": 0.08606990929347949,
      "grad_norm": 0.7790326202188222,
      "learning_rate": 1.983594625415324e-05,
      "loss": 0.5634,
      "step": 956
    },
    {
      "epoch": 0.08615994057935132,
      "grad_norm": 1.0670818041604595,
      "learning_rate": 1.9835419778998724e-05,
      "loss": 0.6914,
      "step": 957
    },
    {
      "epoch": 0.08624997186522317,
      "grad_norm": 0.6776433212909351,
      "learning_rate": 1.9834892467434004e-05,
      "loss": 0.5716,
      "step": 958
    },
    {
      "epoch": 0.08634000315109501,
      "grad_norm": 0.9893297452514418,
      "learning_rate": 1.9834364319503924e-05,
      "loss": 0.6429,
      "step": 959
    },
    {
      "epoch": 0.08643003443696684,
      "grad_norm": 0.6818715398598811,
      "learning_rate": 1.983383533525339e-05,
      "loss": 0.5655,
      "step": 960
    },
    {
      "epoch": 0.08652006572283868,
      "grad_norm": 0.9341081968006899,
      "learning_rate": 1.9833305514727396e-05,
      "loss": 0.6326,
      "step": 961
    },
    {
      "epoch": 0.08661009700871053,
      "grad_norm": 0.8777987724842188,
      "learning_rate": 1.9832774857970992e-05,
      "loss": 0.635,
      "step": 962
    },
    {
      "epoch": 0.08670012829458237,
      "grad_norm": 0.8251024699480577,
      "learning_rate": 1.9832243365029313e-05,
      "loss": 0.5068,
      "step": 963
    },
    {
      "epoch": 0.0867901595804542,
      "grad_norm": 0.8867488902946127,
      "learning_rate": 1.9831711035947552e-05,
      "loss": 0.574,
      "step": 964
    },
    {
      "epoch": 0.08688019086632605,
      "grad_norm": 1.0921533851069065,
      "learning_rate": 1.983117787077098e-05,
      "loss": 0.6478,
      "step": 965
    },
    {
      "epoch": 0.08697022215219789,
      "grad_norm": 0.6999585639605608,
      "learning_rate": 1.9830643869544935e-05,
      "loss": 0.5526,
      "step": 966
    },
    {
      "epoch": 0.08706025343806972,
      "grad_norm": 0.8539871614235257,
      "learning_rate": 1.9830109032314832e-05,
      "loss": 0.6055,
      "step": 967
    },
    {
      "epoch": 0.08715028472394157,
      "grad_norm": 0.9441904679865437,
      "learning_rate": 1.9829573359126154e-05,
      "loss": 0.6098,
      "step": 968
    },
    {
      "epoch": 0.08724031600981341,
      "grad_norm": 0.6645421049813082,
      "learning_rate": 1.9829036850024452e-05,
      "loss": 0.5923,
      "step": 969
    },
    {
      "epoch": 0.08733034729568526,
      "grad_norm": 0.8799760194162279,
      "learning_rate": 1.9828499505055354e-05,
      "loss": 0.5535,
      "step": 970
    },
    {
      "epoch": 0.08742037858155709,
      "grad_norm": 0.823071331978621,
      "learning_rate": 1.982796132426456e-05,
      "loss": 0.5605,
      "step": 971
    },
    {
      "epoch": 0.08751040986742893,
      "grad_norm": 1.0506387482880688,
      "learning_rate": 1.9827422307697823e-05,
      "loss": 0.6517,
      "step": 972
    },
    {
      "epoch": 0.08760044115330078,
      "grad_norm": 0.9263215433493629,
      "learning_rate": 1.9826882455400995e-05,
      "loss": 0.6009,
      "step": 973
    },
    {
      "epoch": 0.0876904724391726,
      "grad_norm": 0.8437360907724151,
      "learning_rate": 1.982634176741998e-05,
      "loss": 0.622,
      "step": 974
    },
    {
      "epoch": 0.08778050372504445,
      "grad_norm": 0.8846017260013697,
      "learning_rate": 1.9825800243800764e-05,
      "loss": 0.616,
      "step": 975
    },
    {
      "epoch": 0.0878705350109163,
      "grad_norm": 0.8691001167603507,
      "learning_rate": 1.982525788458939e-05,
      "loss": 0.6784,
      "step": 976
    },
    {
      "epoch": 0.08796056629678814,
      "grad_norm": 0.836512756293381,
      "learning_rate": 1.9824714689831986e-05,
      "loss": 0.6515,
      "step": 977
    },
    {
      "epoch": 0.08805059758265997,
      "grad_norm": 0.8652670585829362,
      "learning_rate": 1.982417065957474e-05,
      "loss": 0.7144,
      "step": 978
    },
    {
      "epoch": 0.08814062886853181,
      "grad_norm": 0.7914968369549633,
      "learning_rate": 1.9823625793863926e-05,
      "loss": 0.6664,
      "step": 979
    },
    {
      "epoch": 0.08823066015440366,
      "grad_norm": 0.8808628243477405,
      "learning_rate": 1.9823080092745878e-05,
      "loss": 0.6204,
      "step": 980
    },
    {
      "epoch": 0.08832069144027549,
      "grad_norm": 1.0450421212110472,
      "learning_rate": 1.9822533556266993e-05,
      "loss": 0.6712,
      "step": 981
    },
    {
      "epoch": 0.08841072272614733,
      "grad_norm": 1.0380674471828486,
      "learning_rate": 1.9821986184473757e-05,
      "loss": 0.6455,
      "step": 982
    },
    {
      "epoch": 0.08850075401201918,
      "grad_norm": 0.9732023276463252,
      "learning_rate": 1.9821437977412715e-05,
      "loss": 0.5567,
      "step": 983
    },
    {
      "epoch": 0.08859078529789102,
      "grad_norm": 0.8567384988952491,
      "learning_rate": 1.9820888935130492e-05,
      "loss": 0.5556,
      "step": 984
    },
    {
      "epoch": 0.08868081658376285,
      "grad_norm": 0.6746824686773402,
      "learning_rate": 1.9820339057673773e-05,
      "loss": 0.5993,
      "step": 985
    },
    {
      "epoch": 0.0887708478696347,
      "grad_norm": 0.8401180270182468,
      "learning_rate": 1.9819788345089324e-05,
      "loss": 0.7241,
      "step": 986
    },
    {
      "epoch": 0.08886087915550654,
      "grad_norm": 0.8973743543623487,
      "learning_rate": 1.9819236797423976e-05,
      "loss": 0.5528,
      "step": 987
    },
    {
      "epoch": 0.08895091044137837,
      "grad_norm": 0.7979642297072345,
      "learning_rate": 1.9818684414724638e-05,
      "loss": 0.5544,
      "step": 988
    },
    {
      "epoch": 0.08904094172725022,
      "grad_norm": 0.9335438273553481,
      "learning_rate": 1.9818131197038272e-05,
      "loss": 0.6372,
      "step": 989
    },
    {
      "epoch": 0.08913097301312206,
      "grad_norm": 0.7507426262298669,
      "learning_rate": 1.981757714441194e-05,
      "loss": 0.6696,
      "step": 990
    },
    {
      "epoch": 0.0892210042989939,
      "grad_norm": 0.9292267173554994,
      "learning_rate": 1.9817022256892747e-05,
      "loss": 0.5587,
      "step": 991
    },
    {
      "epoch": 0.08931103558486574,
      "grad_norm": 0.8643688366022453,
      "learning_rate": 1.9816466534527886e-05,
      "loss": 0.6314,
      "step": 992
    },
    {
      "epoch": 0.08940106687073758,
      "grad_norm": 0.8644795624285015,
      "learning_rate": 1.981590997736462e-05,
      "loss": 0.5561,
      "step": 993
    },
    {
      "epoch": 0.08949109815660942,
      "grad_norm": 0.8135448755435578,
      "learning_rate": 1.981535258545027e-05,
      "loss": 0.5193,
      "step": 994
    },
    {
      "epoch": 0.08958112944248126,
      "grad_norm": 0.9727356892186804,
      "learning_rate": 1.9814794358832243e-05,
      "loss": 0.7595,
      "step": 995
    },
    {
      "epoch": 0.0896711607283531,
      "grad_norm": 0.6839538073126745,
      "learning_rate": 1.9814235297558005e-05,
      "loss": 0.574,
      "step": 996
    },
    {
      "epoch": 0.08976119201422494,
      "grad_norm": 1.0248515255052255,
      "learning_rate": 1.981367540167511e-05,
      "loss": 0.6165,
      "step": 997
    },
    {
      "epoch": 0.08985122330009679,
      "grad_norm": 0.7034832560926253,
      "learning_rate": 1.981311467123116e-05,
      "loss": 0.6073,
      "step": 998
    },
    {
      "epoch": 0.08994125458596862,
      "grad_norm": 1.1288484119008804,
      "learning_rate": 1.9812553106273848e-05,
      "loss": 0.6796,
      "step": 999
    },
    {
      "epoch": 0.09003128587184046,
      "grad_norm": 0.9664000142274433,
      "learning_rate": 1.9811990706850928e-05,
      "loss": 0.6417,
      "step": 1000
    },
    {
      "epoch": 0.09012131715771231,
      "grad_norm": 0.9280458421879633,
      "learning_rate": 1.9811427473010223e-05,
      "loss": 0.6551,
      "step": 1001
    },
    {
      "epoch": 0.09021134844358415,
      "grad_norm": 1.4354232740602897,
      "learning_rate": 1.9810863404799633e-05,
      "loss": 0.6735,
      "step": 1002
    },
    {
      "epoch": 0.09030137972945598,
      "grad_norm": 0.8224189628794554,
      "learning_rate": 1.9810298502267126e-05,
      "loss": 0.5928,
      "step": 1003
    },
    {
      "epoch": 0.09039141101532783,
      "grad_norm": 0.7305387896549669,
      "learning_rate": 1.980973276546075e-05,
      "loss": 0.5936,
      "step": 1004
    },
    {
      "epoch": 0.09048144230119967,
      "grad_norm": 0.9916749704050977,
      "learning_rate": 1.98091661944286e-05,
      "loss": 0.5865,
      "step": 1005
    },
    {
      "epoch": 0.0905714735870715,
      "grad_norm": 1.0548293256355472,
      "learning_rate": 1.9808598789218866e-05,
      "loss": 0.6778,
      "step": 1006
    },
    {
      "epoch": 0.09066150487294335,
      "grad_norm": 0.9576068757834943,
      "learning_rate": 1.9808030549879803e-05,
      "loss": 0.6273,
      "step": 1007
    },
    {
      "epoch": 0.09075153615881519,
      "grad_norm": 0.9046592202933682,
      "learning_rate": 1.9807461476459736e-05,
      "loss": 0.5653,
      "step": 1008
    },
    {
      "epoch": 0.09084156744468704,
      "grad_norm": 0.9159074952164918,
      "learning_rate": 1.980689156900705e-05,
      "loss": 0.6236,
      "step": 1009
    },
    {
      "epoch": 0.09093159873055887,
      "grad_norm": 0.9213085184412575,
      "learning_rate": 1.980632082757022e-05,
      "loss": 0.541,
      "step": 1010
    },
    {
      "epoch": 0.09102163001643071,
      "grad_norm": 0.7963473875525069,
      "learning_rate": 1.980574925219777e-05,
      "loss": 0.6152,
      "step": 1011
    },
    {
      "epoch": 0.09111166130230255,
      "grad_norm": 0.9724165523276949,
      "learning_rate": 1.980517684293832e-05,
      "loss": 0.6226,
      "step": 1012
    },
    {
      "epoch": 0.09120169258817439,
      "grad_norm": 0.7674968479721811,
      "learning_rate": 1.9804603599840542e-05,
      "loss": 0.6393,
      "step": 1013
    },
    {
      "epoch": 0.09129172387404623,
      "grad_norm": 0.8309945342599979,
      "learning_rate": 1.9804029522953186e-05,
      "loss": 0.561,
      "step": 1014
    },
    {
      "epoch": 0.09138175515991807,
      "grad_norm": 0.7314614869397748,
      "learning_rate": 1.9803454612325073e-05,
      "loss": 0.6802,
      "step": 1015
    },
    {
      "epoch": 0.09147178644578992,
      "grad_norm": 0.8689486053653569,
      "learning_rate": 1.980287886800509e-05,
      "loss": 0.6097,
      "step": 1016
    },
    {
      "epoch": 0.09156181773166175,
      "grad_norm": 0.9671816508645964,
      "learning_rate": 1.98023022900422e-05,
      "loss": 0.5612,
      "step": 1017
    },
    {
      "epoch": 0.0916518490175336,
      "grad_norm": 0.6871404032667154,
      "learning_rate": 1.9801724878485438e-05,
      "loss": 0.5927,
      "step": 1018
    },
    {
      "epoch": 0.09174188030340544,
      "grad_norm": 0.8605622968632326,
      "learning_rate": 1.980114663338391e-05,
      "loss": 0.5737,
      "step": 1019
    },
    {
      "epoch": 0.09183191158927727,
      "grad_norm": 0.8828588126326684,
      "learning_rate": 1.980056755478678e-05,
      "loss": 0.6078,
      "step": 1020
    },
    {
      "epoch": 0.09192194287514911,
      "grad_norm": 0.9210013945099177,
      "learning_rate": 1.9799987642743303e-05,
      "loss": 0.566,
      "step": 1021
    },
    {
      "epoch": 0.09201197416102096,
      "grad_norm": 1.0220714869897436,
      "learning_rate": 1.9799406897302793e-05,
      "loss": 0.5783,
      "step": 1022
    },
    {
      "epoch": 0.0921020054468928,
      "grad_norm": 0.7453079692616144,
      "learning_rate": 1.9798825318514633e-05,
      "loss": 0.5591,
      "step": 1023
    },
    {
      "epoch": 0.09219203673276463,
      "grad_norm": 1.3549806892871679,
      "learning_rate": 1.9798242906428284e-05,
      "loss": 0.6413,
      "step": 1024
    },
    {
      "epoch": 0.09228206801863648,
      "grad_norm": 0.8020343322727947,
      "learning_rate": 1.9797659661093274e-05,
      "loss": 0.7033,
      "step": 1025
    },
    {
      "epoch": 0.09237209930450832,
      "grad_norm": 1.0040974328493164,
      "learning_rate": 1.97970755825592e-05,
      "loss": 0.6333,
      "step": 1026
    },
    {
      "epoch": 0.09246213059038015,
      "grad_norm": 0.8989853705726009,
      "learning_rate": 1.979649067087574e-05,
      "loss": 0.5598,
      "step": 1027
    },
    {
      "epoch": 0.092552161876252,
      "grad_norm": 0.7841956953729455,
      "learning_rate": 1.979590492609263e-05,
      "loss": 0.5754,
      "step": 1028
    },
    {
      "epoch": 0.09264219316212384,
      "grad_norm": 0.7365400647910139,
      "learning_rate": 1.979531834825968e-05,
      "loss": 0.608,
      "step": 1029
    },
    {
      "epoch": 0.09273222444799568,
      "grad_norm": 0.7649589734549572,
      "learning_rate": 1.9794730937426776e-05,
      "loss": 0.6579,
      "step": 1030
    },
    {
      "epoch": 0.09282225573386751,
      "grad_norm": 0.6746179327212063,
      "learning_rate": 1.9794142693643872e-05,
      "loss": 0.657,
      "step": 1031
    },
    {
      "epoch": 0.09291228701973936,
      "grad_norm": 0.8581431446844727,
      "learning_rate": 1.979355361696099e-05,
      "loss": 0.7662,
      "step": 1032
    },
    {
      "epoch": 0.0930023183056112,
      "grad_norm": 0.9855296536391173,
      "learning_rate": 1.9792963707428227e-05,
      "loss": 0.6062,
      "step": 1033
    },
    {
      "epoch": 0.09309234959148303,
      "grad_norm": 0.9700533266985941,
      "learning_rate": 1.979237296509575e-05,
      "loss": 0.6853,
      "step": 1034
    },
    {
      "epoch": 0.09318238087735488,
      "grad_norm": 0.6510101335955518,
      "learning_rate": 1.9791781390013797e-05,
      "loss": 0.534,
      "step": 1035
    },
    {
      "epoch": 0.09327241216322672,
      "grad_norm": 0.7975188695615917,
      "learning_rate": 1.9791188982232672e-05,
      "loss": 0.7139,
      "step": 1036
    },
    {
      "epoch": 0.09336244344909857,
      "grad_norm": 0.6943713018281065,
      "learning_rate": 1.9790595741802757e-05,
      "loss": 0.6259,
      "step": 1037
    },
    {
      "epoch": 0.0934524747349704,
      "grad_norm": 1.1190487780346021,
      "learning_rate": 1.9790001668774502e-05,
      "loss": 0.6263,
      "step": 1038
    },
    {
      "epoch": 0.09354250602084224,
      "grad_norm": 0.8528985767905548,
      "learning_rate": 1.9789406763198427e-05,
      "loss": 0.5632,
      "step": 1039
    },
    {
      "epoch": 0.09363253730671409,
      "grad_norm": 0.8453770815777519,
      "learning_rate": 1.9788811025125118e-05,
      "loss": 0.6918,
      "step": 1040
    },
    {
      "epoch": 0.09372256859258592,
      "grad_norm": 0.9457986142594211,
      "learning_rate": 1.9788214454605246e-05,
      "loss": 0.7411,
      "step": 1041
    },
    {
      "epoch": 0.09381259987845776,
      "grad_norm": 0.9405161520699847,
      "learning_rate": 1.9787617051689535e-05,
      "loss": 0.5451,
      "step": 1042
    },
    {
      "epoch": 0.0939026311643296,
      "grad_norm": 1.0998648781565254,
      "learning_rate": 1.9787018816428794e-05,
      "loss": 0.703,
      "step": 1043
    },
    {
      "epoch": 0.09399266245020145,
      "grad_norm": 0.833987868708641,
      "learning_rate": 1.9786419748873897e-05,
      "loss": 0.6148,
      "step": 1044
    },
    {
      "epoch": 0.09408269373607328,
      "grad_norm": 0.6945597121763428,
      "learning_rate": 1.9785819849075786e-05,
      "loss": 0.5639,
      "step": 1045
    },
    {
      "epoch": 0.09417272502194513,
      "grad_norm": 0.7056700269000603,
      "learning_rate": 1.978521911708548e-05,
      "loss": 0.5772,
      "step": 1046
    },
    {
      "epoch": 0.09426275630781697,
      "grad_norm": 0.602620507491321,
      "learning_rate": 1.9784617552954062e-05,
      "loss": 0.5312,
      "step": 1047
    },
    {
      "epoch": 0.0943527875936888,
      "grad_norm": 0.6536895807590545,
      "learning_rate": 1.9784015156732693e-05,
      "loss": 0.5025,
      "step": 1048
    },
    {
      "epoch": 0.09444281887956064,
      "grad_norm": 0.6501435081564088,
      "learning_rate": 1.97834119284726e-05,
      "loss": 0.5434,
      "step": 1049
    },
    {
      "epoch": 0.09453285016543249,
      "grad_norm": 0.72321283322678,
      "learning_rate": 1.9782807868225084e-05,
      "loss": 0.594,
      "step": 1050
    },
    {
      "epoch": 0.09462288145130433,
      "grad_norm": 0.8090799868296841,
      "learning_rate": 1.9782202976041507e-05,
      "loss": 0.5756,
      "step": 1051
    },
    {
      "epoch": 0.09471291273717616,
      "grad_norm": 0.9624371614713011,
      "learning_rate": 1.978159725197332e-05,
      "loss": 0.5356,
      "step": 1052
    },
    {
      "epoch": 0.09480294402304801,
      "grad_norm": 0.7599983344432695,
      "learning_rate": 1.9780990696072023e-05,
      "loss": 0.556,
      "step": 1053
    },
    {
      "epoch": 0.09489297530891985,
      "grad_norm": 0.9219570390729342,
      "learning_rate": 1.9780383308389207e-05,
      "loss": 0.5992,
      "step": 1054
    },
    {
      "epoch": 0.0949830065947917,
      "grad_norm": 0.8661556975750637,
      "learning_rate": 1.9779775088976523e-05,
      "loss": 0.5694,
      "step": 1055
    },
    {
      "epoch": 0.09507303788066353,
      "grad_norm": 0.8661883866005208,
      "learning_rate": 1.9779166037885692e-05,
      "loss": 0.6016,
      "step": 1056
    },
    {
      "epoch": 0.09516306916653537,
      "grad_norm": 0.7876965063085987,
      "learning_rate": 1.9778556155168508e-05,
      "loss": 0.61,
      "step": 1057
    },
    {
      "epoch": 0.09525310045240722,
      "grad_norm": 0.76305759571396,
      "learning_rate": 1.9777945440876836e-05,
      "loss": 0.6709,
      "step": 1058
    },
    {
      "epoch": 0.09534313173827905,
      "grad_norm": 0.9555270234587596,
      "learning_rate": 1.9777333895062614e-05,
      "loss": 0.6371,
      "step": 1059
    },
    {
      "epoch": 0.09543316302415089,
      "grad_norm": 0.8014604647935145,
      "learning_rate": 1.9776721517777845e-05,
      "loss": 0.6107,
      "step": 1060
    },
    {
      "epoch": 0.09552319431002274,
      "grad_norm": 0.9731255301923585,
      "learning_rate": 1.9776108309074605e-05,
      "loss": 0.7629,
      "step": 1061
    },
    {
      "epoch": 0.09561322559589458,
      "grad_norm": 0.7969367684112758,
      "learning_rate": 1.9775494269005048e-05,
      "loss": 0.6165,
      "step": 1062
    },
    {
      "epoch": 0.09570325688176641,
      "grad_norm": 0.8243556998162409,
      "learning_rate": 1.9774879397621387e-05,
      "loss": 0.613,
      "step": 1063
    },
    {
      "epoch": 0.09579328816763825,
      "grad_norm": 0.9234317006317049,
      "learning_rate": 1.9774263694975914e-05,
      "loss": 0.5484,
      "step": 1064
    },
    {
      "epoch": 0.0958833194535101,
      "grad_norm": 0.686601519630014,
      "learning_rate": 1.9773647161120983e-05,
      "loss": 0.6095,
      "step": 1065
    },
    {
      "epoch": 0.09597335073938193,
      "grad_norm": 0.8478547986379735,
      "learning_rate": 1.977302979610903e-05,
      "loss": 0.5445,
      "step": 1066
    },
    {
      "epoch": 0.09606338202525377,
      "grad_norm": 0.7740090172269725,
      "learning_rate": 1.9772411599992557e-05,
      "loss": 0.4931,
      "step": 1067
    },
    {
      "epoch": 0.09615341331112562,
      "grad_norm": 0.820750456296364,
      "learning_rate": 1.9771792572824134e-05,
      "loss": 0.6492,
      "step": 1068
    },
    {
      "epoch": 0.09624344459699746,
      "grad_norm": 0.9564877843397931,
      "learning_rate": 1.97711727146564e-05,
      "loss": 0.6232,
      "step": 1069
    },
    {
      "epoch": 0.0963334758828693,
      "grad_norm": 0.8188286901176137,
      "learning_rate": 1.9770552025542074e-05,
      "loss": 0.6832,
      "step": 1070
    },
    {
      "epoch": 0.09642350716874114,
      "grad_norm": 0.869745503241065,
      "learning_rate": 1.9769930505533933e-05,
      "loss": 0.6729,
      "step": 1071
    },
    {
      "epoch": 0.09651353845461298,
      "grad_norm": 0.7281896580365106,
      "learning_rate": 1.9769308154684837e-05,
      "loss": 0.565,
      "step": 1072
    },
    {
      "epoch": 0.09660356974048481,
      "grad_norm": 0.9886660705261641,
      "learning_rate": 1.9768684973047708e-05,
      "loss": 0.6257,
      "step": 1073
    },
    {
      "epoch": 0.09669360102635666,
      "grad_norm": 0.9520646852481929,
      "learning_rate": 1.9768060960675548e-05,
      "loss": 0.5883,
      "step": 1074
    },
    {
      "epoch": 0.0967836323122285,
      "grad_norm": 0.7148604230496873,
      "learning_rate": 1.9767436117621416e-05,
      "loss": 0.6092,
      "step": 1075
    },
    {
      "epoch": 0.09687366359810035,
      "grad_norm": 0.9999347910255189,
      "learning_rate": 1.9766810443938447e-05,
      "loss": 0.6469,
      "step": 1076
    },
    {
      "epoch": 0.09696369488397218,
      "grad_norm": 0.8229364793482632,
      "learning_rate": 1.9766183939679857e-05,
      "loss": 0.6015,
      "step": 1077
    },
    {
      "epoch": 0.09705372616984402,
      "grad_norm": 1.0562175944014653,
      "learning_rate": 1.9765556604898922e-05,
      "loss": 0.6708,
      "step": 1078
    },
    {
      "epoch": 0.09714375745571587,
      "grad_norm": 0.9461422148097505,
      "learning_rate": 1.976492843964899e-05,
      "loss": 0.598,
      "step": 1079
    },
    {
      "epoch": 0.0972337887415877,
      "grad_norm": 0.9545304268209429,
      "learning_rate": 1.9764299443983477e-05,
      "loss": 0.7054,
      "step": 1080
    },
    {
      "epoch": 0.09732382002745954,
      "grad_norm": 1.1949536658710722,
      "learning_rate": 1.9763669617955875e-05,
      "loss": 0.6492,
      "step": 1081
    },
    {
      "epoch": 0.09741385131333138,
      "grad_norm": 0.856408404213034,
      "learning_rate": 1.9763038961619748e-05,
      "loss": 0.6236,
      "step": 1082
    },
    {
      "epoch": 0.09750388259920323,
      "grad_norm": 0.7760300643357102,
      "learning_rate": 1.9762407475028726e-05,
      "loss": 0.6529,
      "step": 1083
    },
    {
      "epoch": 0.09759391388507506,
      "grad_norm": 0.6822560294211699,
      "learning_rate": 1.9761775158236508e-05,
      "loss": 0.5863,
      "step": 1084
    },
    {
      "epoch": 0.0976839451709469,
      "grad_norm": 0.7488028878837908,
      "learning_rate": 1.9761142011296873e-05,
      "loss": 0.6066,
      "step": 1085
    },
    {
      "epoch": 0.09777397645681875,
      "grad_norm": 0.8993147663913302,
      "learning_rate": 1.976050803426366e-05,
      "loss": 0.6741,
      "step": 1086
    },
    {
      "epoch": 0.09786400774269058,
      "grad_norm": 0.7742726265578649,
      "learning_rate": 1.975987322719078e-05,
      "loss": 0.6085,
      "step": 1087
    },
    {
      "epoch": 0.09795403902856242,
      "grad_norm": 0.7294461541264663,
      "learning_rate": 1.975923759013222e-05,
      "loss": 0.568,
      "step": 1088
    },
    {
      "epoch": 0.09804407031443427,
      "grad_norm": 0.7460221411203731,
      "learning_rate": 1.975860112314204e-05,
      "loss": 0.5597,
      "step": 1089
    },
    {
      "epoch": 0.09813410160030611,
      "grad_norm": 0.9727532319459683,
      "learning_rate": 1.9757963826274357e-05,
      "loss": 0.6332,
      "step": 1090
    },
    {
      "epoch": 0.09822413288617794,
      "grad_norm": 0.7467729398345457,
      "learning_rate": 1.9757325699583373e-05,
      "loss": 0.7012,
      "step": 1091
    },
    {
      "epoch": 0.09831416417204979,
      "grad_norm": 0.997623694310346,
      "learning_rate": 1.975668674312335e-05,
      "loss": 0.6328,
      "step": 1092
    },
    {
      "epoch": 0.09840419545792163,
      "grad_norm": 0.8199630743555193,
      "learning_rate": 1.9756046956948633e-05,
      "loss": 0.5697,
      "step": 1093
    },
    {
      "epoch": 0.09849422674379346,
      "grad_norm": 0.7647775431784708,
      "learning_rate": 1.9755406341113622e-05,
      "loss": 0.6131,
      "step": 1094
    },
    {
      "epoch": 0.0985842580296653,
      "grad_norm": 0.7547850640382389,
      "learning_rate": 1.97547648956728e-05,
      "loss": 0.6154,
      "step": 1095
    },
    {
      "epoch": 0.09867428931553715,
      "grad_norm": 0.95302236918873,
      "learning_rate": 1.975412262068071e-05,
      "loss": 0.5906,
      "step": 1096
    },
    {
      "epoch": 0.098764320601409,
      "grad_norm": 0.9650696775625897,
      "learning_rate": 1.9753479516191977e-05,
      "loss": 0.5829,
      "step": 1097
    },
    {
      "epoch": 0.09885435188728083,
      "grad_norm": 0.8567270300058684,
      "learning_rate": 1.975283558226129e-05,
      "loss": 0.5984,
      "step": 1098
    },
    {
      "epoch": 0.09894438317315267,
      "grad_norm": 0.699224328131709,
      "learning_rate": 1.975219081894341e-05,
      "loss": 0.59,
      "step": 1099
    },
    {
      "epoch": 0.09903441445902451,
      "grad_norm": 0.7731455943517882,
      "learning_rate": 1.975154522629317e-05,
      "loss": 0.6567,
      "step": 1100
    },
    {
      "epoch": 0.09912444574489636,
      "grad_norm": 0.827635304422,
      "learning_rate": 1.9750898804365464e-05,
      "loss": 0.6539,
      "step": 1101
    },
    {
      "epoch": 0.09921447703076819,
      "grad_norm": 0.6917668849173371,
      "learning_rate": 1.975025155321527e-05,
      "loss": 0.5759,
      "step": 1102
    },
    {
      "epoch": 0.09930450831664003,
      "grad_norm": 1.3228352354096706,
      "learning_rate": 1.974960347289763e-05,
      "loss": 0.57,
      "step": 1103
    },
    {
      "epoch": 0.09939453960251188,
      "grad_norm": 1.1796437956265762,
      "learning_rate": 1.9748954563467656e-05,
      "loss": 0.7082,
      "step": 1104
    },
    {
      "epoch": 0.09948457088838371,
      "grad_norm": 0.8431555307036231,
      "learning_rate": 1.9748304824980536e-05,
      "loss": 0.5224,
      "step": 1105
    },
    {
      "epoch": 0.09957460217425555,
      "grad_norm": 0.8314676773294655,
      "learning_rate": 1.974765425749152e-05,
      "loss": 0.6286,
      "step": 1106
    },
    {
      "epoch": 0.0996646334601274,
      "grad_norm": 1.2784443119117017,
      "learning_rate": 1.9747002861055925e-05,
      "loss": 0.7048,
      "step": 1107
    },
    {
      "epoch": 0.09975466474599924,
      "grad_norm": 0.8028175113159377,
      "learning_rate": 1.974635063572916e-05,
      "loss": 0.6331,
      "step": 1108
    },
    {
      "epoch": 0.09984469603187107,
      "grad_norm": 0.8251748160817808,
      "learning_rate": 1.9745697581566687e-05,
      "loss": 0.586,
      "step": 1109
    },
    {
      "epoch": 0.09993472731774292,
      "grad_norm": 0.810384058340102,
      "learning_rate": 1.974504369862404e-05,
      "loss": 0.599,
      "step": 1110
    },
    {
      "epoch": 0.10002475860361476,
      "grad_norm": 0.9653730938653291,
      "learning_rate": 1.9744388986956824e-05,
      "loss": 0.567,
      "step": 1111
    },
    {
      "epoch": 0.10011478988948659,
      "grad_norm": 1.041740039685878,
      "learning_rate": 1.974373344662072e-05,
      "loss": 0.6528,
      "step": 1112
    },
    {
      "epoch": 0.10020482117535844,
      "grad_norm": 1.2211765062610205,
      "learning_rate": 1.974307707767147e-05,
      "loss": 0.5308,
      "step": 1113
    },
    {
      "epoch": 0.10029485246123028,
      "grad_norm": 0.8067129813425412,
      "learning_rate": 1.97424198801649e-05,
      "loss": 0.6235,
      "step": 1114
    },
    {
      "epoch": 0.10038488374710212,
      "grad_norm": 0.9731907162844672,
      "learning_rate": 1.9741761854156885e-05,
      "loss": 0.7017,
      "step": 1115
    },
    {
      "epoch": 0.10047491503297395,
      "grad_norm": 0.7864704030707789,
      "learning_rate": 1.97411029997034e-05,
      "loss": 0.5567,
      "step": 1116
    },
    {
      "epoch": 0.1005649463188458,
      "grad_norm": 0.8804516090133798,
      "learning_rate": 1.9740443316860466e-05,
      "loss": 0.5533,
      "step": 1117
    },
    {
      "epoch": 0.10065497760471764,
      "grad_norm": 1.1178442506967161,
      "learning_rate": 1.9739782805684188e-05,
      "loss": 0.6775,
      "step": 1118
    },
    {
      "epoch": 0.10074500889058947,
      "grad_norm": 0.9311870097498092,
      "learning_rate": 1.9739121466230725e-05,
      "loss": 0.5449,
      "step": 1119
    },
    {
      "epoch": 0.10083504017646132,
      "grad_norm": 0.7701833785663172,
      "learning_rate": 1.973845929855633e-05,
      "loss": 0.6603,
      "step": 1120
    },
    {
      "epoch": 0.10092507146233316,
      "grad_norm": 1.0738254921809873,
      "learning_rate": 1.9737796302717305e-05,
      "loss": 0.6623,
      "step": 1121
    },
    {
      "epoch": 0.10101510274820501,
      "grad_norm": 1.0105587499774886,
      "learning_rate": 1.973713247877004e-05,
      "loss": 0.6756,
      "step": 1122
    },
    {
      "epoch": 0.10110513403407684,
      "grad_norm": 0.7008993046043035,
      "learning_rate": 1.9736467826770984e-05,
      "loss": 0.6162,
      "step": 1123
    },
    {
      "epoch": 0.10119516531994868,
      "grad_norm": 0.7982754647885055,
      "learning_rate": 1.9735802346776654e-05,
      "loss": 0.5211,
      "step": 1124
    },
    {
      "epoch": 0.10128519660582053,
      "grad_norm": 0.7029149462046539,
      "learning_rate": 1.973513603884365e-05,
      "loss": 0.6038,
      "step": 1125
    },
    {
      "epoch": 0.10137522789169236,
      "grad_norm": 0.8725307466209854,
      "learning_rate": 1.973446890302863e-05,
      "loss": 0.5926,
      "step": 1126
    },
    {
      "epoch": 0.1014652591775642,
      "grad_norm": 0.9185504440141579,
      "learning_rate": 1.973380093938833e-05,
      "loss": 0.6067,
      "step": 1127
    },
    {
      "epoch": 0.10155529046343605,
      "grad_norm": 1.152163375195602,
      "learning_rate": 1.9733132147979556e-05,
      "loss": 0.6048,
      "step": 1128
    },
    {
      "epoch": 0.10164532174930789,
      "grad_norm": 1.0154896131673674,
      "learning_rate": 1.9732462528859177e-05,
      "loss": 0.6371,
      "step": 1129
    },
    {
      "epoch": 0.10173535303517972,
      "grad_norm": 0.8009564113833435,
      "learning_rate": 1.9731792082084148e-05,
      "loss": 0.6994,
      "step": 1130
    },
    {
      "epoch": 0.10182538432105157,
      "grad_norm": 0.7355000386602557,
      "learning_rate": 1.9731120807711472e-05,
      "loss": 0.6082,
      "step": 1131
    },
    {
      "epoch": 0.10191541560692341,
      "grad_norm": 0.8077887126377535,
      "learning_rate": 1.973044870579824e-05,
      "loss": 0.6495,
      "step": 1132
    },
    {
      "epoch": 0.10200544689279524,
      "grad_norm": 0.9299091091860298,
      "learning_rate": 1.972977577640161e-05,
      "loss": 0.6311,
      "step": 1133
    },
    {
      "epoch": 0.10209547817866708,
      "grad_norm": 1.2770657434150003,
      "learning_rate": 1.9729102019578807e-05,
      "loss": 0.6327,
      "step": 1134
    },
    {
      "epoch": 0.10218550946453893,
      "grad_norm": 0.7334672096673603,
      "learning_rate": 1.972842743538712e-05,
      "loss": 0.5917,
      "step": 1135
    },
    {
      "epoch": 0.10227554075041077,
      "grad_norm": 0.7192657647887652,
      "learning_rate": 1.9727752023883933e-05,
      "loss": 0.5552,
      "step": 1136
    },
    {
      "epoch": 0.1023655720362826,
      "grad_norm": 0.8787974480374652,
      "learning_rate": 1.9727075785126668e-05,
      "loss": 0.5885,
      "step": 1137
    },
    {
      "epoch": 0.10245560332215445,
      "grad_norm": 0.8173098984714616,
      "learning_rate": 1.972639871917284e-05,
      "loss": 0.5798,
      "step": 1138
    },
    {
      "epoch": 0.10254563460802629,
      "grad_norm": 0.8130206276847738,
      "learning_rate": 1.9725720826080023e-05,
      "loss": 0.6138,
      "step": 1139
    },
    {
      "epoch": 0.10263566589389812,
      "grad_norm": 0.8035527604304109,
      "learning_rate": 1.972504210590587e-05,
      "loss": 0.5522,
      "step": 1140
    },
    {
      "epoch": 0.10272569717976997,
      "grad_norm": 0.757872915885963,
      "learning_rate": 1.97243625587081e-05,
      "loss": 0.549,
      "step": 1141
    },
    {
      "epoch": 0.10281572846564181,
      "grad_norm": 0.5739080006027457,
      "learning_rate": 1.9723682184544498e-05,
      "loss": 0.6026,
      "step": 1142
    },
    {
      "epoch": 0.10290575975151366,
      "grad_norm": 0.7890852767554551,
      "learning_rate": 1.972300098347292e-05,
      "loss": 0.5959,
      "step": 1143
    },
    {
      "epoch": 0.10299579103738549,
      "grad_norm": 0.8719700394704956,
      "learning_rate": 1.9722318955551307e-05,
      "loss": 0.6365,
      "step": 1144
    },
    {
      "epoch": 0.10308582232325733,
      "grad_norm": 0.7752348984068494,
      "learning_rate": 1.9721636100837652e-05,
      "loss": 0.5592,
      "step": 1145
    },
    {
      "epoch": 0.10317585360912918,
      "grad_norm": 0.908303449209516,
      "learning_rate": 1.9720952419390026e-05,
      "loss": 0.6093,
      "step": 1146
    },
    {
      "epoch": 0.103265884895001,
      "grad_norm": 0.9534405211775134,
      "learning_rate": 1.9720267911266567e-05,
      "loss": 0.5481,
      "step": 1147
    },
    {
      "epoch": 0.10335591618087285,
      "grad_norm": 0.7396804032246922,
      "learning_rate": 1.9719582576525492e-05,
      "loss": 0.624,
      "step": 1148
    },
    {
      "epoch": 0.1034459474667447,
      "grad_norm": 1.270307397659184,
      "learning_rate": 1.9718896415225083e-05,
      "loss": 0.6257,
      "step": 1149
    },
    {
      "epoch": 0.10353597875261654,
      "grad_norm": 0.6981040133775708,
      "learning_rate": 1.9718209427423682e-05,
      "loss": 0.5336,
      "step": 1150
    },
    {
      "epoch": 0.10362601003848837,
      "grad_norm": 0.7282464882447064,
      "learning_rate": 1.971752161317972e-05,
      "loss": 0.5831,
      "step": 1151
    },
    {
      "epoch": 0.10371604132436021,
      "grad_norm": 0.7973358499691766,
      "learning_rate": 1.971683297255168e-05,
      "loss": 0.6012,
      "step": 1152
    },
    {
      "epoch": 0.10380607261023206,
      "grad_norm": 0.6039671780082267,
      "learning_rate": 1.971614350559814e-05,
      "loss": 0.5582,
      "step": 1153
    },
    {
      "epoch": 0.1038961038961039,
      "grad_norm": 0.8188767943080806,
      "learning_rate": 1.971545321237772e-05,
      "loss": 0.5729,
      "step": 1154
    },
    {
      "epoch": 0.10398613518197573,
      "grad_norm": 0.9003404568714652,
      "learning_rate": 1.9714762092949122e-05,
      "loss": 0.6382,
      "step": 1155
    },
    {
      "epoch": 0.10407616646784758,
      "grad_norm": 0.6464760043197995,
      "learning_rate": 1.971407014737113e-05,
      "loss": 0.5665,
      "step": 1156
    },
    {
      "epoch": 0.10416619775371942,
      "grad_norm": 0.686415162386494,
      "learning_rate": 1.9713377375702572e-05,
      "loss": 0.6178,
      "step": 1157
    },
    {
      "epoch": 0.10425622903959125,
      "grad_norm": 0.7531437046513596,
      "learning_rate": 1.9712683778002376e-05,
      "loss": 0.5713,
      "step": 1158
    },
    {
      "epoch": 0.1043462603254631,
      "grad_norm": 1.0914315740979172,
      "learning_rate": 1.971198935432952e-05,
      "loss": 0.588,
      "step": 1159
    },
    {
      "epoch": 0.10443629161133494,
      "grad_norm": 0.8882594623154765,
      "learning_rate": 1.971129410474306e-05,
      "loss": 0.6844,
      "step": 1160
    },
    {
      "epoch": 0.10452632289720679,
      "grad_norm": 0.9418977640625741,
      "learning_rate": 1.9710598029302117e-05,
      "loss": 0.5766,
      "step": 1161
    },
    {
      "epoch": 0.10461635418307862,
      "grad_norm": 0.8637844486323837,
      "learning_rate": 1.9709901128065888e-05,
      "loss": 0.5668,
      "step": 1162
    },
    {
      "epoch": 0.10470638546895046,
      "grad_norm": 0.858689608261603,
      "learning_rate": 1.970920340109364e-05,
      "loss": 0.693,
      "step": 1163
    },
    {
      "epoch": 0.1047964167548223,
      "grad_norm": 0.7659575535116256,
      "learning_rate": 1.9708504848444704e-05,
      "loss": 0.6614,
      "step": 1164
    },
    {
      "epoch": 0.10488644804069414,
      "grad_norm": 0.8667406047506837,
      "learning_rate": 1.9707805470178487e-05,
      "loss": 0.6093,
      "step": 1165
    },
    {
      "epoch": 0.10497647932656598,
      "grad_norm": 0.6918918188205933,
      "learning_rate": 1.9707105266354464e-05,
      "loss": 0.562,
      "step": 1166
    },
    {
      "epoch": 0.10506651061243782,
      "grad_norm": 0.9691855395271418,
      "learning_rate": 1.9706404237032185e-05,
      "loss": 0.5373,
      "step": 1167
    },
    {
      "epoch": 0.10515654189830967,
      "grad_norm": 0.8922075315323597,
      "learning_rate": 1.970570238227126e-05,
      "loss": 0.6164,
      "step": 1168
    },
    {
      "epoch": 0.1052465731841815,
      "grad_norm": 0.7693063961965628,
      "learning_rate": 1.970499970213138e-05,
      "loss": 0.5839,
      "step": 1169
    },
    {
      "epoch": 0.10533660447005334,
      "grad_norm": 1.1091419658078832,
      "learning_rate": 1.9704296196672298e-05,
      "loss": 0.593,
      "step": 1170
    },
    {
      "epoch": 0.10542663575592519,
      "grad_norm": 0.7935427858751268,
      "learning_rate": 1.9703591865953838e-05,
      "loss": 0.5382,
      "step": 1171
    },
    {
      "epoch": 0.10551666704179702,
      "grad_norm": 0.81593827110214,
      "learning_rate": 1.9702886710035904e-05,
      "loss": 0.6956,
      "step": 1172
    },
    {
      "epoch": 0.10560669832766886,
      "grad_norm": 0.7886745450372431,
      "learning_rate": 1.970218072897846e-05,
      "loss": 0.507,
      "step": 1173
    },
    {
      "epoch": 0.10569672961354071,
      "grad_norm": 0.7662777201539285,
      "learning_rate": 1.970147392284154e-05,
      "loss": 0.6495,
      "step": 1174
    },
    {
      "epoch": 0.10578676089941255,
      "grad_norm": 0.8175215417360199,
      "learning_rate": 1.9700766291685255e-05,
      "loss": 0.6073,
      "step": 1175
    },
    {
      "epoch": 0.10587679218528438,
      "grad_norm": 0.6585635883840164,
      "learning_rate": 1.970005783556978e-05,
      "loss": 0.5332,
      "step": 1176
    },
    {
      "epoch": 0.10596682347115623,
      "grad_norm": 0.7415275165742494,
      "learning_rate": 1.9699348554555365e-05,
      "loss": 0.5276,
      "step": 1177
    },
    {
      "epoch": 0.10605685475702807,
      "grad_norm": 0.7617304627480079,
      "learning_rate": 1.9698638448702326e-05,
      "loss": 0.6895,
      "step": 1178
    },
    {
      "epoch": 0.1061468860428999,
      "grad_norm": 0.6947580395833772,
      "learning_rate": 1.969792751807105e-05,
      "loss": 0.663,
      "step": 1179
    },
    {
      "epoch": 0.10623691732877175,
      "grad_norm": 0.7532617904455193,
      "learning_rate": 1.9697215762721994e-05,
      "loss": 0.5585,
      "step": 1180
    },
    {
      "epoch": 0.10632694861464359,
      "grad_norm": 1.102571159219808,
      "learning_rate": 1.969650318271569e-05,
      "loss": 0.5823,
      "step": 1181
    },
    {
      "epoch": 0.10641697990051543,
      "grad_norm": 0.8409689504602498,
      "learning_rate": 1.9695789778112738e-05,
      "loss": 0.6676,
      "step": 1182
    },
    {
      "epoch": 0.10650701118638727,
      "grad_norm": 0.9569816258471718,
      "learning_rate": 1.96950755489738e-05,
      "loss": 0.647,
      "step": 1183
    },
    {
      "epoch": 0.10659704247225911,
      "grad_norm": 0.9381427192191993,
      "learning_rate": 1.9694360495359616e-05,
      "loss": 0.5922,
      "step": 1184
    },
    {
      "epoch": 0.10668707375813095,
      "grad_norm": 0.7821597712813265,
      "learning_rate": 1.9693644617330996e-05,
      "loss": 0.6861,
      "step": 1185
    },
    {
      "epoch": 0.10677710504400278,
      "grad_norm": 0.753263760151244,
      "learning_rate": 1.9692927914948818e-05,
      "loss": 0.6064,
      "step": 1186
    },
    {
      "epoch": 0.10686713632987463,
      "grad_norm": 0.8989818546710996,
      "learning_rate": 1.9692210388274033e-05,
      "loss": 0.616,
      "step": 1187
    },
    {
      "epoch": 0.10695716761574647,
      "grad_norm": 0.9685638575479538,
      "learning_rate": 1.9691492037367656e-05,
      "loss": 0.6132,
      "step": 1188
    },
    {
      "epoch": 0.10704719890161832,
      "grad_norm": 0.8973549634108395,
      "learning_rate": 1.969077286229078e-05,
      "loss": 0.6133,
      "step": 1189
    },
    {
      "epoch": 0.10713723018749015,
      "grad_norm": 0.9869698087782611,
      "learning_rate": 1.9690052863104566e-05,
      "loss": 0.5225,
      "step": 1190
    },
    {
      "epoch": 0.10722726147336199,
      "grad_norm": 0.8386111873018225,
      "learning_rate": 1.9689332039870233e-05,
      "loss": 0.5768,
      "step": 1191
    },
    {
      "epoch": 0.10731729275923384,
      "grad_norm": 0.7564918550523394,
      "learning_rate": 1.968861039264909e-05,
      "loss": 0.6325,
      "step": 1192
    },
    {
      "epoch": 0.10740732404510567,
      "grad_norm": 0.7484774166284224,
      "learning_rate": 1.9687887921502505e-05,
      "loss": 0.5858,
      "step": 1193
    },
    {
      "epoch": 0.10749735533097751,
      "grad_norm": 0.7463915965039135,
      "learning_rate": 1.9687164626491912e-05,
      "loss": 0.5762,
      "step": 1194
    },
    {
      "epoch": 0.10758738661684936,
      "grad_norm": 0.8556260556266019,
      "learning_rate": 1.9686440507678827e-05,
      "loss": 0.5096,
      "step": 1195
    },
    {
      "epoch": 0.1076774179027212,
      "grad_norm": 0.9983266671735875,
      "learning_rate": 1.9685715565124822e-05,
      "loss": 0.6517,
      "step": 1196
    },
    {
      "epoch": 0.10776744918859303,
      "grad_norm": 0.782575458528968,
      "learning_rate": 1.9684989798891557e-05,
      "loss": 0.605,
      "step": 1197
    },
    {
      "epoch": 0.10785748047446488,
      "grad_norm": 0.8484531116798041,
      "learning_rate": 1.968426320904074e-05,
      "loss": 0.6006,
      "step": 1198
    },
    {
      "epoch": 0.10794751176033672,
      "grad_norm": 0.8860123602230944,
      "learning_rate": 1.968353579563417e-05,
      "loss": 0.5717,
      "step": 1199
    },
    {
      "epoch": 0.10803754304620855,
      "grad_norm": 1.1761537631915935,
      "learning_rate": 1.9682807558733703e-05,
      "loss": 0.6369,
      "step": 1200
    },
    {
      "epoch": 0.1081275743320804,
      "grad_norm": 0.8696963628052242,
      "learning_rate": 1.9682078498401266e-05,
      "loss": 0.622,
      "step": 1201
    },
    {
      "epoch": 0.10821760561795224,
      "grad_norm": 0.8768570866110089,
      "learning_rate": 1.9681348614698863e-05,
      "loss": 0.5925,
      "step": 1202
    },
    {
      "epoch": 0.10830763690382408,
      "grad_norm": 0.8255582496174714,
      "learning_rate": 1.9680617907688562e-05,
      "loss": 0.6247,
      "step": 1203
    },
    {
      "epoch": 0.10839766818969591,
      "grad_norm": 0.7448548555487362,
      "learning_rate": 1.9679886377432504e-05,
      "loss": 0.6065,
      "step": 1204
    },
    {
      "epoch": 0.10848769947556776,
      "grad_norm": 1.1729763760496423,
      "learning_rate": 1.96791540239929e-05,
      "loss": 0.6458,
      "step": 1205
    },
    {
      "epoch": 0.1085777307614396,
      "grad_norm": 0.8880341304039578,
      "learning_rate": 1.9678420847432024e-05,
      "loss": 0.5327,
      "step": 1206
    },
    {
      "epoch": 0.10866776204731145,
      "grad_norm": 0.7669232221492931,
      "learning_rate": 1.967768684781223e-05,
      "loss": 0.6949,
      "step": 1207
    },
    {
      "epoch": 0.10875779333318328,
      "grad_norm": 1.01943713316176,
      "learning_rate": 1.9676952025195937e-05,
      "loss": 0.5906,
      "step": 1208
    },
    {
      "epoch": 0.10884782461905512,
      "grad_norm": 0.9292913585461927,
      "learning_rate": 1.9676216379645636e-05,
      "loss": 0.6641,
      "step": 1209
    },
    {
      "epoch": 0.10893785590492697,
      "grad_norm": 0.7607288346464219,
      "learning_rate": 1.9675479911223886e-05,
      "loss": 0.5977,
      "step": 1210
    },
    {
      "epoch": 0.1090278871907988,
      "grad_norm": 0.8274888759778052,
      "learning_rate": 1.9674742619993316e-05,
      "loss": 0.8099,
      "step": 1211
    },
    {
      "epoch": 0.10911791847667064,
      "grad_norm": 1.2612053367076448,
      "learning_rate": 1.9674004506016625e-05,
      "loss": 0.7069,
      "step": 1212
    },
    {
      "epoch": 0.10920794976254249,
      "grad_norm": 0.7754969696313877,
      "learning_rate": 1.9673265569356585e-05,
      "loss": 0.6834,
      "step": 1213
    },
    {
      "epoch": 0.10929798104841433,
      "grad_norm": 0.8148137758231391,
      "learning_rate": 1.9672525810076033e-05,
      "loss": 0.651,
      "step": 1214
    },
    {
      "epoch": 0.10938801233428616,
      "grad_norm": 0.7322717873454508,
      "learning_rate": 1.9671785228237882e-05,
      "loss": 0.5546,
      "step": 1215
    },
    {
      "epoch": 0.109478043620158,
      "grad_norm": 0.7184054252936318,
      "learning_rate": 1.967104382390511e-05,
      "loss": 0.6112,
      "step": 1216
    },
    {
      "epoch": 0.10956807490602985,
      "grad_norm": 0.8434748201034966,
      "learning_rate": 1.9670301597140765e-05,
      "loss": 0.7687,
      "step": 1217
    },
    {
      "epoch": 0.10965810619190168,
      "grad_norm": 0.7830088920144175,
      "learning_rate": 1.9669558548007966e-05,
      "loss": 0.6062,
      "step": 1218
    },
    {
      "epoch": 0.10974813747777352,
      "grad_norm": 0.8347829776607976,
      "learning_rate": 1.9668814676569907e-05,
      "loss": 0.7023,
      "step": 1219
    },
    {
      "epoch": 0.10983816876364537,
      "grad_norm": 1.2466691661767506,
      "learning_rate": 1.966806998288984e-05,
      "loss": 0.7207,
      "step": 1220
    },
    {
      "epoch": 0.10992820004951721,
      "grad_norm": 0.8108203750698493,
      "learning_rate": 1.96673244670311e-05,
      "loss": 0.5635,
      "step": 1221
    },
    {
      "epoch": 0.11001823133538904,
      "grad_norm": 0.736097506933485,
      "learning_rate": 1.9666578129057086e-05,
      "loss": 0.4994,
      "step": 1222
    },
    {
      "epoch": 0.11010826262126089,
      "grad_norm": 0.9029621239357428,
      "learning_rate": 1.9665830969031265e-05,
      "loss": 0.5642,
      "step": 1223
    },
    {
      "epoch": 0.11019829390713273,
      "grad_norm": 0.9750545345147343,
      "learning_rate": 1.9665082987017173e-05,
      "loss": 0.7275,
      "step": 1224
    },
    {
      "epoch": 0.11028832519300456,
      "grad_norm": 0.7133192628968031,
      "learning_rate": 1.966433418307843e-05,
      "loss": 0.5704,
      "step": 1225
    },
    {
      "epoch": 0.11037835647887641,
      "grad_norm": 0.7720310494490575,
      "learning_rate": 1.96635845572787e-05,
      "loss": 0.5806,
      "step": 1226
    },
    {
      "epoch": 0.11046838776474825,
      "grad_norm": 0.7585025525107211,
      "learning_rate": 1.966283410968174e-05,
      "loss": 0.5034,
      "step": 1227
    },
    {
      "epoch": 0.1105584190506201,
      "grad_norm": 0.8825665728208325,
      "learning_rate": 1.966208284035137e-05,
      "loss": 0.7084,
      "step": 1228
    },
    {
      "epoch": 0.11064845033649193,
      "grad_norm": 0.8140566782137519,
      "learning_rate": 1.966133074935147e-05,
      "loss": 0.5988,
      "step": 1229
    },
    {
      "epoch": 0.11073848162236377,
      "grad_norm": 0.7352255834315723,
      "learning_rate": 1.966057783674601e-05,
      "loss": 0.6822,
      "step": 1230
    },
    {
      "epoch": 0.11082851290823562,
      "grad_norm": 0.7960436231699685,
      "learning_rate": 1.9659824102599014e-05,
      "loss": 0.5847,
      "step": 1231
    },
    {
      "epoch": 0.11091854419410745,
      "grad_norm": 0.7663187899091005,
      "learning_rate": 1.9659069546974573e-05,
      "loss": 0.604,
      "step": 1232
    },
    {
      "epoch": 0.11100857547997929,
      "grad_norm": 0.8350855073135499,
      "learning_rate": 1.9658314169936862e-05,
      "loss": 0.6143,
      "step": 1233
    },
    {
      "epoch": 0.11109860676585114,
      "grad_norm": 0.9353194137327339,
      "learning_rate": 1.9657557971550116e-05,
      "loss": 0.6951,
      "step": 1234
    },
    {
      "epoch": 0.11118863805172298,
      "grad_norm": 0.7527413126055372,
      "learning_rate": 1.9656800951878643e-05,
      "loss": 0.5511,
      "step": 1235
    },
    {
      "epoch": 0.11127866933759481,
      "grad_norm": 0.7057261314340475,
      "learning_rate": 1.965604311098682e-05,
      "loss": 0.5759,
      "step": 1236
    },
    {
      "epoch": 0.11136870062346665,
      "grad_norm": 0.9799219370376082,
      "learning_rate": 1.9655284448939094e-05,
      "loss": 0.7009,
      "step": 1237
    },
    {
      "epoch": 0.1114587319093385,
      "grad_norm": 0.9620958481652954,
      "learning_rate": 1.9654524965799986e-05,
      "loss": 0.5377,
      "step": 1238
    },
    {
      "epoch": 0.11154876319521033,
      "grad_norm": 0.8659478865866428,
      "learning_rate": 1.965376466163408e-05,
      "loss": 0.5378,
      "step": 1239
    },
    {
      "epoch": 0.11163879448108217,
      "grad_norm": 0.7659213600045387,
      "learning_rate": 1.9653003536506032e-05,
      "loss": 0.683,
      "step": 1240
    },
    {
      "epoch": 0.11172882576695402,
      "grad_norm": 1.1812448620709104,
      "learning_rate": 1.965224159048057e-05,
      "loss": 0.678,
      "step": 1241
    },
    {
      "epoch": 0.11181885705282586,
      "grad_norm": 0.8269445887784166,
      "learning_rate": 1.9651478823622488e-05,
      "loss": 0.6075,
      "step": 1242
    },
    {
      "epoch": 0.11190888833869769,
      "grad_norm": 0.768057468310048,
      "learning_rate": 1.9650715235996656e-05,
      "loss": 0.6061,
      "step": 1243
    },
    {
      "epoch": 0.11199891962456954,
      "grad_norm": 0.8668967553680786,
      "learning_rate": 1.964995082766801e-05,
      "loss": 0.7111,
      "step": 1244
    },
    {
      "epoch": 0.11208895091044138,
      "grad_norm": 0.8333024965578687,
      "learning_rate": 1.9649185598701546e-05,
      "loss": 0.6597,
      "step": 1245
    },
    {
      "epoch": 0.11217898219631321,
      "grad_norm": 0.963106700078249,
      "learning_rate": 1.964841954916235e-05,
      "loss": 0.6416,
      "step": 1246
    },
    {
      "epoch": 0.11226901348218506,
      "grad_norm": 0.8943825974512719,
      "learning_rate": 1.9647652679115564e-05,
      "loss": 0.6811,
      "step": 1247
    },
    {
      "epoch": 0.1123590447680569,
      "grad_norm": 0.6662810237389027,
      "learning_rate": 1.964688498862641e-05,
      "loss": 0.617,
      "step": 1248
    },
    {
      "epoch": 0.11244907605392875,
      "grad_norm": 1.0105330049309034,
      "learning_rate": 1.964611647776016e-05,
      "loss": 0.64,
      "step": 1249
    },
    {
      "epoch": 0.11253910733980058,
      "grad_norm": 0.6471985472229259,
      "learning_rate": 1.9645347146582177e-05,
      "loss": 0.593,
      "step": 1250
    },
    {
      "epoch": 0.11262913862567242,
      "grad_norm": 0.9694192931422762,
      "learning_rate": 1.9644576995157884e-05,
      "loss": 0.6223,
      "step": 1251
    },
    {
      "epoch": 0.11271916991154426,
      "grad_norm": 0.861088730489495,
      "learning_rate": 1.964380602355277e-05,
      "loss": 0.6636,
      "step": 1252
    },
    {
      "epoch": 0.1128092011974161,
      "grad_norm": 0.7747452262312198,
      "learning_rate": 1.9643034231832407e-05,
      "loss": 0.6341,
      "step": 1253
    },
    {
      "epoch": 0.11289923248328794,
      "grad_norm": 0.6626240715346028,
      "learning_rate": 1.964226162006243e-05,
      "loss": 0.5831,
      "step": 1254
    },
    {
      "epoch": 0.11298926376915978,
      "grad_norm": 0.7661144529989228,
      "learning_rate": 1.964148818830853e-05,
      "loss": 0.4647,
      "step": 1255
    },
    {
      "epoch": 0.11307929505503163,
      "grad_norm": 0.8643934029021599,
      "learning_rate": 1.9640713936636492e-05,
      "loss": 0.6691,
      "step": 1256
    },
    {
      "epoch": 0.11316932634090346,
      "grad_norm": 0.754597067937118,
      "learning_rate": 1.9639938865112153e-05,
      "loss": 0.6521,
      "step": 1257
    },
    {
      "epoch": 0.1132593576267753,
      "grad_norm": 0.970138179518765,
      "learning_rate": 1.9639162973801426e-05,
      "loss": 0.5999,
      "step": 1258
    },
    {
      "epoch": 0.11334938891264715,
      "grad_norm": 0.8716534099523746,
      "learning_rate": 1.9638386262770295e-05,
      "loss": 0.6288,
      "step": 1259
    },
    {
      "epoch": 0.11343942019851899,
      "grad_norm": 0.8048082791788473,
      "learning_rate": 1.963760873208481e-05,
      "loss": 0.6272,
      "step": 1260
    },
    {
      "epoch": 0.11352945148439082,
      "grad_norm": 0.6207193084680314,
      "learning_rate": 1.96368303818111e-05,
      "loss": 0.5681,
      "step": 1261
    },
    {
      "epoch": 0.11361948277026267,
      "grad_norm": 0.8824377483739431,
      "learning_rate": 1.9636051212015344e-05,
      "loss": 0.6213,
      "step": 1262
    },
    {
      "epoch": 0.11370951405613451,
      "grad_norm": 1.1037442774979067,
      "learning_rate": 1.9635271222763813e-05,
      "loss": 0.6179,
      "step": 1263
    },
    {
      "epoch": 0.11379954534200634,
      "grad_norm": 0.903612732201387,
      "learning_rate": 1.9634490414122833e-05,
      "loss": 0.698,
      "step": 1264
    },
    {
      "epoch": 0.11388957662787819,
      "grad_norm": 0.6794426595795933,
      "learning_rate": 1.9633708786158803e-05,
      "loss": 0.6031,
      "step": 1265
    },
    {
      "epoch": 0.11397960791375003,
      "grad_norm": 0.7596415170832718,
      "learning_rate": 1.9632926338938202e-05,
      "loss": 0.6634,
      "step": 1266
    },
    {
      "epoch": 0.11406963919962188,
      "grad_norm": 0.8045842320880475,
      "learning_rate": 1.9632143072527558e-05,
      "loss": 0.5554,
      "step": 1267
    },
    {
      "epoch": 0.1141596704854937,
      "grad_norm": 0.7578031304529266,
      "learning_rate": 1.9631358986993488e-05,
      "loss": 0.5619,
      "step": 1268
    },
    {
      "epoch": 0.11424970177136555,
      "grad_norm": 0.710595703441802,
      "learning_rate": 1.9630574082402672e-05,
      "loss": 0.6301,
      "step": 1269
    },
    {
      "epoch": 0.1143397330572374,
      "grad_norm": 0.7890490680218533,
      "learning_rate": 1.9629788358821852e-05,
      "loss": 0.6603,
      "step": 1270
    },
    {
      "epoch": 0.11442976434310922,
      "grad_norm": 0.6878799362693968,
      "learning_rate": 1.9629001816317853e-05,
      "loss": 0.6831,
      "step": 1271
    },
    {
      "epoch": 0.11451979562898107,
      "grad_norm": 0.7554700023961017,
      "learning_rate": 1.962821445495756e-05,
      "loss": 0.534,
      "step": 1272
    },
    {
      "epoch": 0.11460982691485291,
      "grad_norm": 0.8170619607649684,
      "learning_rate": 1.962742627480793e-05,
      "loss": 0.6254,
      "step": 1273
    },
    {
      "epoch": 0.11469985820072476,
      "grad_norm": 0.7210072924614894,
      "learning_rate": 1.9626637275935993e-05,
      "loss": 0.5084,
      "step": 1274
    },
    {
      "epoch": 0.11478988948659659,
      "grad_norm": 0.9339782642531201,
      "learning_rate": 1.9625847458408844e-05,
      "loss": 0.586,
      "step": 1275
    },
    {
      "epoch": 0.11487992077246843,
      "grad_norm": 0.7275847072151993,
      "learning_rate": 1.962505682229365e-05,
      "loss": 0.5823,
      "step": 1276
    },
    {
      "epoch": 0.11496995205834028,
      "grad_norm": 0.8134159660261246,
      "learning_rate": 1.9624265367657648e-05,
      "loss": 0.6181,
      "step": 1277
    },
    {
      "epoch": 0.11505998334421211,
      "grad_norm": 0.7489713882182825,
      "learning_rate": 1.962347309456814e-05,
      "loss": 0.5214,
      "step": 1278
    },
    {
      "epoch": 0.11515001463008395,
      "grad_norm": 1.0576711783605726,
      "learning_rate": 1.9622680003092503e-05,
      "loss": 0.6955,
      "step": 1279
    },
    {
      "epoch": 0.1152400459159558,
      "grad_norm": 0.877267818263942,
      "learning_rate": 1.962188609329819e-05,
      "loss": 0.5934,
      "step": 1280
    },
    {
      "epoch": 0.11533007720182764,
      "grad_norm": 0.7846085860386321,
      "learning_rate": 1.9621091365252705e-05,
      "loss": 0.6204,
      "step": 1281
    },
    {
      "epoch": 0.11542010848769947,
      "grad_norm": 0.8154001773979127,
      "learning_rate": 1.9620295819023637e-05,
      "loss": 0.6071,
      "step": 1282
    },
    {
      "epoch": 0.11551013977357132,
      "grad_norm": 0.6967895448277818,
      "learning_rate": 1.9619499454678638e-05,
      "loss": 0.6402,
      "step": 1283
    },
    {
      "epoch": 0.11560017105944316,
      "grad_norm": 0.8275916089540664,
      "learning_rate": 1.9618702272285434e-05,
      "loss": 0.5897,
      "step": 1284
    },
    {
      "epoch": 0.11569020234531499,
      "grad_norm": 0.7388705462590034,
      "learning_rate": 1.9617904271911815e-05,
      "loss": 0.591,
      "step": 1285
    },
    {
      "epoch": 0.11578023363118684,
      "grad_norm": 0.7412564890040636,
      "learning_rate": 1.9617105453625648e-05,
      "loss": 0.5909,
      "step": 1286
    },
    {
      "epoch": 0.11587026491705868,
      "grad_norm": 0.648327437375855,
      "learning_rate": 1.9616305817494856e-05,
      "loss": 0.5557,
      "step": 1287
    },
    {
      "epoch": 0.11596029620293052,
      "grad_norm": 0.6769948185640386,
      "learning_rate": 1.961550536358745e-05,
      "loss": 0.5343,
      "step": 1288
    },
    {
      "epoch": 0.11605032748880235,
      "grad_norm": 0.9326612866077677,
      "learning_rate": 1.9614704091971496e-05,
      "loss": 0.6156,
      "step": 1289
    },
    {
      "epoch": 0.1161403587746742,
      "grad_norm": 0.7366319555146961,
      "learning_rate": 1.9613902002715134e-05,
      "loss": 0.5752,
      "step": 1290
    },
    {
      "epoch": 0.11623039006054604,
      "grad_norm": 0.8443561213929435,
      "learning_rate": 1.9613099095886577e-05,
      "loss": 0.5828,
      "step": 1291
    },
    {
      "epoch": 0.11632042134641787,
      "grad_norm": 0.9509340226289296,
      "learning_rate": 1.9612295371554104e-05,
      "loss": 0.7198,
      "step": 1292
    },
    {
      "epoch": 0.11641045263228972,
      "grad_norm": 1.0663436807881024,
      "learning_rate": 1.9611490829786065e-05,
      "loss": 0.5637,
      "step": 1293
    },
    {
      "epoch": 0.11650048391816156,
      "grad_norm": 0.8131445476638832,
      "learning_rate": 1.9610685470650876e-05,
      "loss": 0.5841,
      "step": 1294
    },
    {
      "epoch": 0.11659051520403341,
      "grad_norm": 0.7863970404201963,
      "learning_rate": 1.9609879294217026e-05,
      "loss": 0.5361,
      "step": 1295
    },
    {
      "epoch": 0.11668054648990524,
      "grad_norm": 0.9909081137332311,
      "learning_rate": 1.9609072300553075e-05,
      "loss": 0.6617,
      "step": 1296
    },
    {
      "epoch": 0.11677057777577708,
      "grad_norm": 0.7298357008721038,
      "learning_rate": 1.960826448972765e-05,
      "loss": 0.5808,
      "step": 1297
    },
    {
      "epoch": 0.11686060906164893,
      "grad_norm": 1.114568100200963,
      "learning_rate": 1.9607455861809446e-05,
      "loss": 0.7077,
      "step": 1298
    },
    {
      "epoch": 0.11695064034752076,
      "grad_norm": 0.7385242007576996,
      "learning_rate": 1.960664641686723e-05,
      "loss": 0.589,
      "step": 1299
    },
    {
      "epoch": 0.1170406716333926,
      "grad_norm": 0.6932812723245554,
      "learning_rate": 1.960583615496984e-05,
      "loss": 0.5801,
      "step": 1300
    },
    {
      "epoch": 0.11713070291926445,
      "grad_norm": 0.7958089617300419,
      "learning_rate": 1.9605025076186176e-05,
      "loss": 0.6446,
      "step": 1301
    },
    {
      "epoch": 0.11722073420513629,
      "grad_norm": 1.3608373447342184,
      "learning_rate": 1.9604213180585212e-05,
      "loss": 0.586,
      "step": 1302
    },
    {
      "epoch": 0.11731076549100812,
      "grad_norm": 0.7833391652181252,
      "learning_rate": 1.9603400468236e-05,
      "loss": 0.6072,
      "step": 1303
    },
    {
      "epoch": 0.11740079677687996,
      "grad_norm": 0.8578425178236371,
      "learning_rate": 1.9602586939207648e-05,
      "loss": 0.568,
      "step": 1304
    },
    {
      "epoch": 0.11749082806275181,
      "grad_norm": 0.6709211486488489,
      "learning_rate": 1.960177259356934e-05,
      "loss": 0.5592,
      "step": 1305
    },
    {
      "epoch": 0.11758085934862365,
      "grad_norm": 0.9999770754212326,
      "learning_rate": 1.960095743139033e-05,
      "loss": 0.5903,
      "step": 1306
    },
    {
      "epoch": 0.11767089063449548,
      "grad_norm": 1.365601417802196,
      "learning_rate": 1.9600141452739935e-05,
      "loss": 0.6817,
      "step": 1307
    },
    {
      "epoch": 0.11776092192036733,
      "grad_norm": 0.8704083396385653,
      "learning_rate": 1.959932465768755e-05,
      "loss": 0.6422,
      "step": 1308
    },
    {
      "epoch": 0.11785095320623917,
      "grad_norm": 0.8871588627179422,
      "learning_rate": 1.959850704630264e-05,
      "loss": 0.6488,
      "step": 1309
    },
    {
      "epoch": 0.117940984492111,
      "grad_norm": 0.692412653880714,
      "learning_rate": 1.9597688618654726e-05,
      "loss": 0.4487,
      "step": 1310
    },
    {
      "epoch": 0.11803101577798285,
      "grad_norm": 0.7125289107372861,
      "learning_rate": 1.9596869374813415e-05,
      "loss": 0.557,
      "step": 1311
    },
    {
      "epoch": 0.11812104706385469,
      "grad_norm": 0.7421199004996483,
      "learning_rate": 1.959604931484837e-05,
      "loss": 0.6557,
      "step": 1312
    },
    {
      "epoch": 0.11821107834972654,
      "grad_norm": 1.0134393300038886,
      "learning_rate": 1.9595228438829337e-05,
      "loss": 0.6713,
      "step": 1313
    },
    {
      "epoch": 0.11830110963559837,
      "grad_norm": 1.2064393295147953,
      "learning_rate": 1.9594406746826117e-05,
      "loss": 0.6627,
      "step": 1314
    },
    {
      "epoch": 0.11839114092147021,
      "grad_norm": 0.6899053993013884,
      "learning_rate": 1.9593584238908588e-05,
      "loss": 0.4989,
      "step": 1315
    },
    {
      "epoch": 0.11848117220734206,
      "grad_norm": 0.7067627806874134,
      "learning_rate": 1.9592760915146702e-05,
      "loss": 0.5807,
      "step": 1316
    },
    {
      "epoch": 0.11857120349321389,
      "grad_norm": 0.9768058624198335,
      "learning_rate": 1.9591936775610468e-05,
      "loss": 0.6632,
      "step": 1317
    },
    {
      "epoch": 0.11866123477908573,
      "grad_norm": 0.8267709768990938,
      "learning_rate": 1.9591111820369976e-05,
      "loss": 0.6185,
      "step": 1318
    },
    {
      "epoch": 0.11875126606495758,
      "grad_norm": 0.8254773604164025,
      "learning_rate": 1.959028604949538e-05,
      "loss": 0.6573,
      "step": 1319
    },
    {
      "epoch": 0.11884129735082942,
      "grad_norm": 0.9041526789827339,
      "learning_rate": 1.9589459463056904e-05,
      "loss": 0.6681,
      "step": 1320
    },
    {
      "epoch": 0.11893132863670125,
      "grad_norm": 0.8594892475693786,
      "learning_rate": 1.9588632061124837e-05,
      "loss": 0.6884,
      "step": 1321
    },
    {
      "epoch": 0.1190213599225731,
      "grad_norm": 0.9951831443500271,
      "learning_rate": 1.9587803843769547e-05,
      "loss": 0.5425,
      "step": 1322
    },
    {
      "epoch": 0.11911139120844494,
      "grad_norm": 0.6983345491645047,
      "learning_rate": 1.9586974811061467e-05,
      "loss": 0.5674,
      "step": 1323
    },
    {
      "epoch": 0.11920142249431677,
      "grad_norm": 0.9435795123364819,
      "learning_rate": 1.9586144963071097e-05,
      "loss": 0.6256,
      "step": 1324
    },
    {
      "epoch": 0.11929145378018861,
      "grad_norm": 0.8746724567314453,
      "learning_rate": 1.9585314299869e-05,
      "loss": 0.5483,
      "step": 1325
    },
    {
      "epoch": 0.11938148506606046,
      "grad_norm": 0.9000854226640255,
      "learning_rate": 1.958448282152583e-05,
      "loss": 0.7025,
      "step": 1326
    },
    {
      "epoch": 0.1194715163519323,
      "grad_norm": 0.8574405480815926,
      "learning_rate": 1.9583650528112286e-05,
      "loss": 0.6203,
      "step": 1327
    },
    {
      "epoch": 0.11956154763780413,
      "grad_norm": 0.8301505972056716,
      "learning_rate": 1.9582817419699155e-05,
      "loss": 0.5766,
      "step": 1328
    },
    {
      "epoch": 0.11965157892367598,
      "grad_norm": 0.6986833244204451,
      "learning_rate": 1.9581983496357275e-05,
      "loss": 0.6462,
      "step": 1329
    },
    {
      "epoch": 0.11974161020954782,
      "grad_norm": 0.815959172786655,
      "learning_rate": 1.958114875815757e-05,
      "loss": 0.4705,
      "step": 1330
    },
    {
      "epoch": 0.11983164149541965,
      "grad_norm": 0.8116195039933811,
      "learning_rate": 1.9580313205171023e-05,
      "loss": 0.5608,
      "step": 1331
    },
    {
      "epoch": 0.1199216727812915,
      "grad_norm": 0.802191906019404,
      "learning_rate": 1.9579476837468697e-05,
      "loss": 0.599,
      "step": 1332
    },
    {
      "epoch": 0.12001170406716334,
      "grad_norm": 0.7271723615875406,
      "learning_rate": 1.9578639655121708e-05,
      "loss": 0.6579,
      "step": 1333
    },
    {
      "epoch": 0.12010173535303519,
      "grad_norm": 0.7892048436661947,
      "learning_rate": 1.957780165820126e-05,
      "loss": 0.4832,
      "step": 1334
    },
    {
      "epoch": 0.12019176663890702,
      "grad_norm": 0.6331177342912535,
      "learning_rate": 1.957696284677861e-05,
      "loss": 0.5757,
      "step": 1335
    },
    {
      "epoch": 0.12028179792477886,
      "grad_norm": 0.7368729111282889,
      "learning_rate": 1.957612322092509e-05,
      "loss": 0.5833,
      "step": 1336
    },
    {
      "epoch": 0.1203718292106507,
      "grad_norm": 1.0648219567688864,
      "learning_rate": 1.9575282780712107e-05,
      "loss": 0.6399,
      "step": 1337
    },
    {
      "epoch": 0.12046186049652254,
      "grad_norm": 0.8517703973043577,
      "learning_rate": 1.957444152621113e-05,
      "loss": 0.5667,
      "step": 1338
    },
    {
      "epoch": 0.12055189178239438,
      "grad_norm": 0.7860457914828045,
      "learning_rate": 1.95735994574937e-05,
      "loss": 0.5442,
      "step": 1339
    },
    {
      "epoch": 0.12064192306826622,
      "grad_norm": 0.8279362265561221,
      "learning_rate": 1.9572756574631427e-05,
      "loss": 0.5622,
      "step": 1340
    },
    {
      "epoch": 0.12073195435413807,
      "grad_norm": 0.9815271451388312,
      "learning_rate": 1.9571912877695995e-05,
      "loss": 0.7061,
      "step": 1341
    },
    {
      "epoch": 0.1208219856400099,
      "grad_norm": 0.7677849110445581,
      "learning_rate": 1.9571068366759143e-05,
      "loss": 0.5399,
      "step": 1342
    },
    {
      "epoch": 0.12091201692588174,
      "grad_norm": 0.8965976768502127,
      "learning_rate": 1.95702230418927e-05,
      "loss": 0.6461,
      "step": 1343
    },
    {
      "epoch": 0.12100204821175359,
      "grad_norm": 0.956227638226685,
      "learning_rate": 1.956937690316854e-05,
      "loss": 0.5408,
      "step": 1344
    },
    {
      "epoch": 0.12109207949762542,
      "grad_norm": 0.949786232238998,
      "learning_rate": 1.956852995065863e-05,
      "loss": 0.6138,
      "step": 1345
    },
    {
      "epoch": 0.12118211078349726,
      "grad_norm": 0.8309132197690919,
      "learning_rate": 1.956768218443499e-05,
      "loss": 0.6513,
      "step": 1346
    },
    {
      "epoch": 0.12127214206936911,
      "grad_norm": 1.1342196554298702,
      "learning_rate": 1.9566833604569716e-05,
      "loss": 0.7863,
      "step": 1347
    },
    {
      "epoch": 0.12136217335524095,
      "grad_norm": 0.9234460417016278,
      "learning_rate": 1.9565984211134975e-05,
      "loss": 0.6938,
      "step": 1348
    },
    {
      "epoch": 0.12145220464111278,
      "grad_norm": 0.8425515215882289,
      "learning_rate": 1.956513400420299e-05,
      "loss": 0.4955,
      "step": 1349
    },
    {
      "epoch": 0.12154223592698463,
      "grad_norm": 0.7435091210961031,
      "learning_rate": 1.9564282983846077e-05,
      "loss": 0.6413,
      "step": 1350
    },
    {
      "epoch": 0.12163226721285647,
      "grad_norm": 0.7331459111700489,
      "learning_rate": 1.9563431150136598e-05,
      "loss": 0.5504,
      "step": 1351
    },
    {
      "epoch": 0.1217222984987283,
      "grad_norm": 0.8858672828190314,
      "learning_rate": 1.9562578503146998e-05,
      "loss": 0.5274,
      "step": 1352
    },
    {
      "epoch": 0.12181232978460015,
      "grad_norm": 0.829165734552607,
      "learning_rate": 1.956172504294978e-05,
      "loss": 0.686,
      "step": 1353
    },
    {
      "epoch": 0.12190236107047199,
      "grad_norm": 0.7480060838946541,
      "learning_rate": 1.956087076961753e-05,
      "loss": 0.5921,
      "step": 1354
    },
    {
      "epoch": 0.12199239235634383,
      "grad_norm": 1.1728219085661988,
      "learning_rate": 1.9560015683222888e-05,
      "loss": 0.6765,
      "step": 1355
    },
    {
      "epoch": 0.12208242364221567,
      "grad_norm": 0.911482741413674,
      "learning_rate": 1.9559159783838582e-05,
      "loss": 0.5867,
      "step": 1356
    },
    {
      "epoch": 0.12217245492808751,
      "grad_norm": 0.7919490766058709,
      "learning_rate": 1.9558303071537388e-05,
      "loss": 0.6115,
      "step": 1357
    },
    {
      "epoch": 0.12226248621395935,
      "grad_norm": 0.719514111214212,
      "learning_rate": 1.9557445546392165e-05,
      "loss": 0.5315,
      "step": 1358
    },
    {
      "epoch": 0.1223525174998312,
      "grad_norm": 0.6845761635615486,
      "learning_rate": 1.9556587208475845e-05,
      "loss": 0.573,
      "step": 1359
    },
    {
      "epoch": 0.12244254878570303,
      "grad_norm": 0.8409570612690925,
      "learning_rate": 1.955572805786141e-05,
      "loss": 0.5672,
      "step": 1360
    },
    {
      "epoch": 0.12253258007157487,
      "grad_norm": 0.6699715217821977,
      "learning_rate": 1.9554868094621926e-05,
      "loss": 0.492,
      "step": 1361
    },
    {
      "epoch": 0.12262261135744672,
      "grad_norm": 0.9701253231551943,
      "learning_rate": 1.955400731883053e-05,
      "loss": 0.6581,
      "step": 1362
    },
    {
      "epoch": 0.12271264264331855,
      "grad_norm": 0.9583197635678747,
      "learning_rate": 1.9553145730560415e-05,
      "loss": 0.6667,
      "step": 1363
    },
    {
      "epoch": 0.12280267392919039,
      "grad_norm": 0.7147184551402046,
      "learning_rate": 1.9552283329884858e-05,
      "loss": 0.5693,
      "step": 1364
    },
    {
      "epoch": 0.12289270521506224,
      "grad_norm": 0.7790794085332324,
      "learning_rate": 1.9551420116877193e-05,
      "loss": 0.6096,
      "step": 1365
    },
    {
      "epoch": 0.12298273650093408,
      "grad_norm": 0.770181761324857,
      "learning_rate": 1.9550556091610833e-05,
      "loss": 0.61,
      "step": 1366
    },
    {
      "epoch": 0.12307276778680591,
      "grad_norm": 0.7748053457392652,
      "learning_rate": 1.954969125415925e-05,
      "loss": 0.5321,
      "step": 1367
    },
    {
      "epoch": 0.12316279907267776,
      "grad_norm": 0.7315851354024411,
      "learning_rate": 1.9548825604595997e-05,
      "loss": 0.5964,
      "step": 1368
    },
    {
      "epoch": 0.1232528303585496,
      "grad_norm": 0.8508296244155202,
      "learning_rate": 1.954795914299468e-05,
      "loss": 0.6273,
      "step": 1369
    },
    {
      "epoch": 0.12334286164442143,
      "grad_norm": 0.8700465914887686,
      "learning_rate": 1.9547091869428993e-05,
      "loss": 0.5808,
      "step": 1370
    },
    {
      "epoch": 0.12343289293029328,
      "grad_norm": 0.7056067799303453,
      "learning_rate": 1.9546223783972685e-05,
      "loss": 0.5385,
      "step": 1371
    },
    {
      "epoch": 0.12352292421616512,
      "grad_norm": 0.7830423718599895,
      "learning_rate": 1.9545354886699577e-05,
      "loss": 0.6104,
      "step": 1372
    },
    {
      "epoch": 0.12361295550203696,
      "grad_norm": 0.9687149990861803,
      "learning_rate": 1.9544485177683563e-05,
      "loss": 0.6664,
      "step": 1373
    },
    {
      "epoch": 0.1237029867879088,
      "grad_norm": 0.9431673322482026,
      "learning_rate": 1.95436146569986e-05,
      "loss": 0.5502,
      "step": 1374
    },
    {
      "epoch": 0.12379301807378064,
      "grad_norm": 0.7564150931190479,
      "learning_rate": 1.9542743324718726e-05,
      "loss": 0.5789,
      "step": 1375
    },
    {
      "epoch": 0.12388304935965248,
      "grad_norm": 0.8671532726829393,
      "learning_rate": 1.9541871180918032e-05,
      "loss": 0.5782,
      "step": 1376
    },
    {
      "epoch": 0.12397308064552431,
      "grad_norm": 0.656070387371326,
      "learning_rate": 1.9540998225670688e-05,
      "loss": 0.576,
      "step": 1377
    },
    {
      "epoch": 0.12406311193139616,
      "grad_norm": 0.7155417608245558,
      "learning_rate": 1.9540124459050927e-05,
      "loss": 0.5593,
      "step": 1378
    },
    {
      "epoch": 0.124153143217268,
      "grad_norm": 0.782025633801959,
      "learning_rate": 1.9539249881133062e-05,
      "loss": 0.5689,
      "step": 1379
    },
    {
      "epoch": 0.12424317450313985,
      "grad_norm": 0.7062781695468068,
      "learning_rate": 1.9538374491991462e-05,
      "loss": 0.5926,
      "step": 1380
    },
    {
      "epoch": 0.12433320578901168,
      "grad_norm": 0.7816338347608197,
      "learning_rate": 1.9537498291700575e-05,
      "loss": 0.6204,
      "step": 1381
    },
    {
      "epoch": 0.12442323707488352,
      "grad_norm": 0.8949090202369031,
      "learning_rate": 1.9536621280334908e-05,
      "loss": 0.5809,
      "step": 1382
    },
    {
      "epoch": 0.12451326836075537,
      "grad_norm": 0.8901283176146839,
      "learning_rate": 1.953574345796905e-05,
      "loss": 0.536,
      "step": 1383
    },
    {
      "epoch": 0.1246032996466272,
      "grad_norm": 0.8062479579647368,
      "learning_rate": 1.953486482467764e-05,
      "loss": 0.6447,
      "step": 1384
    },
    {
      "epoch": 0.12469333093249904,
      "grad_norm": 0.7502884511754296,
      "learning_rate": 1.9533985380535408e-05,
      "loss": 0.5483,
      "step": 1385
    },
    {
      "epoch": 0.12478336221837089,
      "grad_norm": 0.850589573692059,
      "learning_rate": 1.9533105125617142e-05,
      "loss": 0.5858,
      "step": 1386
    },
    {
      "epoch": 0.12487339350424273,
      "grad_norm": 0.7788550019030448,
      "learning_rate": 1.9532224059997693e-05,
      "loss": 0.55,
      "step": 1387
    },
    {
      "epoch": 0.12496342479011456,
      "grad_norm": 0.962707113658035,
      "learning_rate": 1.9531342183751992e-05,
      "loss": 0.6371,
      "step": 1388
    },
    {
      "epoch": 0.12505345607598642,
      "grad_norm": 0.9431789404774422,
      "learning_rate": 1.9530459496955036e-05,
      "loss": 0.6096,
      "step": 1389
    },
    {
      "epoch": 0.12514348736185824,
      "grad_norm": 0.938377916729875,
      "learning_rate": 1.9529575999681886e-05,
      "loss": 0.6354,
      "step": 1390
    },
    {
      "epoch": 0.12523351864773008,
      "grad_norm": 0.8379529951359886,
      "learning_rate": 1.952869169200767e-05,
      "loss": 0.6015,
      "step": 1391
    },
    {
      "epoch": 0.12532354993360192,
      "grad_norm": 1.0043510419830812,
      "learning_rate": 1.95278065740076e-05,
      "loss": 0.6748,
      "step": 1392
    },
    {
      "epoch": 0.12541358121947377,
      "grad_norm": 0.7259894483305518,
      "learning_rate": 1.9526920645756942e-05,
      "loss": 0.5961,
      "step": 1393
    },
    {
      "epoch": 0.1255036125053456,
      "grad_norm": 0.8305882783217953,
      "learning_rate": 1.952603390733104e-05,
      "loss": 0.6027,
      "step": 1394
    },
    {
      "epoch": 0.12559364379121746,
      "grad_norm": 0.9488879647396539,
      "learning_rate": 1.9525146358805297e-05,
      "loss": 0.7992,
      "step": 1395
    },
    {
      "epoch": 0.1256836750770893,
      "grad_norm": 0.8512819824018809,
      "learning_rate": 1.952425800025519e-05,
      "loss": 0.5953,
      "step": 1396
    },
    {
      "epoch": 0.12577370636296112,
      "grad_norm": 1.0890165683637114,
      "learning_rate": 1.9523368831756274e-05,
      "loss": 0.5387,
      "step": 1397
    },
    {
      "epoch": 0.12586373764883296,
      "grad_norm": 0.8018808014189969,
      "learning_rate": 1.9522478853384154e-05,
      "loss": 0.5243,
      "step": 1398
    },
    {
      "epoch": 0.1259537689347048,
      "grad_norm": 0.8410310024412905,
      "learning_rate": 1.9521588065214524e-05,
      "loss": 0.6217,
      "step": 1399
    },
    {
      "epoch": 0.12604380022057665,
      "grad_norm": 0.927830902029876,
      "learning_rate": 1.952069646732313e-05,
      "loss": 0.5926,
      "step": 1400
    },
    {
      "epoch": 0.1261338315064485,
      "grad_norm": 0.58879194176261,
      "learning_rate": 1.9519804059785798e-05,
      "loss": 0.5528,
      "step": 1401
    },
    {
      "epoch": 0.12622386279232034,
      "grad_norm": 0.7517518975506116,
      "learning_rate": 1.9518910842678416e-05,
      "loss": 0.5688,
      "step": 1402
    },
    {
      "epoch": 0.12631389407819218,
      "grad_norm": 0.8206259826051397,
      "learning_rate": 1.9518016816076945e-05,
      "loss": 0.5965,
      "step": 1403
    },
    {
      "epoch": 0.126403925364064,
      "grad_norm": 1.0243055349324606,
      "learning_rate": 1.9517121980057416e-05,
      "loss": 0.6096,
      "step": 1404
    },
    {
      "epoch": 0.12649395664993585,
      "grad_norm": 0.8990644974480241,
      "learning_rate": 1.951622633469592e-05,
      "loss": 0.5979,
      "step": 1405
    },
    {
      "epoch": 0.1265839879358077,
      "grad_norm": 0.7985012266818079,
      "learning_rate": 1.951532988006863e-05,
      "loss": 0.5583,
      "step": 1406
    },
    {
      "epoch": 0.12667401922167953,
      "grad_norm": 0.9239005168405303,
      "learning_rate": 1.951443261625178e-05,
      "loss": 0.7097,
      "step": 1407
    },
    {
      "epoch": 0.12676405050755138,
      "grad_norm": 0.7118542437006198,
      "learning_rate": 1.951353454332167e-05,
      "loss": 0.5483,
      "step": 1408
    },
    {
      "epoch": 0.12685408179342322,
      "grad_norm": 0.8876881984456712,
      "learning_rate": 1.9512635661354675e-05,
      "loss": 0.5433,
      "step": 1409
    },
    {
      "epoch": 0.12694411307929507,
      "grad_norm": 0.8605315609627004,
      "learning_rate": 1.951173597042724e-05,
      "loss": 0.5674,
      "step": 1410
    },
    {
      "epoch": 0.12703414436516688,
      "grad_norm": 0.7485291821290155,
      "learning_rate": 1.951083547061587e-05,
      "loss": 0.6436,
      "step": 1411
    },
    {
      "epoch": 0.12712417565103873,
      "grad_norm": 1.0336388387057278,
      "learning_rate": 1.9509934161997142e-05,
      "loss": 0.5483,
      "step": 1412
    },
    {
      "epoch": 0.12721420693691057,
      "grad_norm": 0.6557924715124949,
      "learning_rate": 1.9509032044647713e-05,
      "loss": 0.5546,
      "step": 1413
    },
    {
      "epoch": 0.12730423822278242,
      "grad_norm": 0.8495275765160594,
      "learning_rate": 1.9508129118644294e-05,
      "loss": 0.4934,
      "step": 1414
    },
    {
      "epoch": 0.12739426950865426,
      "grad_norm": 0.8691887980601307,
      "learning_rate": 1.9507225384063668e-05,
      "loss": 0.5861,
      "step": 1415
    },
    {
      "epoch": 0.1274843007945261,
      "grad_norm": 0.9654668179627871,
      "learning_rate": 1.9506320840982694e-05,
      "loss": 0.6538,
      "step": 1416
    },
    {
      "epoch": 0.12757433208039795,
      "grad_norm": 0.916308108927816,
      "learning_rate": 1.9505415489478293e-05,
      "loss": 0.5946,
      "step": 1417
    },
    {
      "epoch": 0.12766436336626977,
      "grad_norm": 0.9375902775281889,
      "learning_rate": 1.9504509329627456e-05,
      "loss": 0.6607,
      "step": 1418
    },
    {
      "epoch": 0.1277543946521416,
      "grad_norm": 0.8314837407417865,
      "learning_rate": 1.9503602361507244e-05,
      "loss": 0.6333,
      "step": 1419
    },
    {
      "epoch": 0.12784442593801346,
      "grad_norm": 0.7943353709189583,
      "learning_rate": 1.9502694585194784e-05,
      "loss": 0.6022,
      "step": 1420
    },
    {
      "epoch": 0.1279344572238853,
      "grad_norm": 0.6846661366702901,
      "learning_rate": 1.9501786000767278e-05,
      "loss": 0.6891,
      "step": 1421
    },
    {
      "epoch": 0.12802448850975715,
      "grad_norm": 0.8080615153957442,
      "learning_rate": 1.950087660830199e-05,
      "loss": 0.5544,
      "step": 1422
    },
    {
      "epoch": 0.128114519795629,
      "grad_norm": 1.389148762330223,
      "learning_rate": 1.9499966407876254e-05,
      "loss": 0.4923,
      "step": 1423
    },
    {
      "epoch": 0.12820455108150083,
      "grad_norm": 1.2128176364724295,
      "learning_rate": 1.9499055399567476e-05,
      "loss": 0.5872,
      "step": 1424
    },
    {
      "epoch": 0.12829458236737265,
      "grad_norm": 0.7444252514774818,
      "learning_rate": 1.949814358345313e-05,
      "loss": 0.5804,
      "step": 1425
    },
    {
      "epoch": 0.1283846136532445,
      "grad_norm": 0.8311996477555597,
      "learning_rate": 1.9497230959610757e-05,
      "loss": 0.6102,
      "step": 1426
    },
    {
      "epoch": 0.12847464493911634,
      "grad_norm": 0.7445662920111744,
      "learning_rate": 1.9496317528117963e-05,
      "loss": 0.5598,
      "step": 1427
    },
    {
      "epoch": 0.12856467622498818,
      "grad_norm": 0.9236715965106277,
      "learning_rate": 1.949540328905243e-05,
      "loss": 0.5958,
      "step": 1428
    },
    {
      "epoch": 0.12865470751086003,
      "grad_norm": 0.9007824666848055,
      "learning_rate": 1.9494488242491907e-05,
      "loss": 0.5626,
      "step": 1429
    },
    {
      "epoch": 0.12874473879673187,
      "grad_norm": 0.8230807012160996,
      "learning_rate": 1.9493572388514212e-05,
      "loss": 0.5448,
      "step": 1430
    },
    {
      "epoch": 0.12883477008260372,
      "grad_norm": 0.6847858613115623,
      "learning_rate": 1.9492655727197217e-05,
      "loss": 0.5595,
      "step": 1431
    },
    {
      "epoch": 0.12892480136847553,
      "grad_norm": 0.9162207076777502,
      "learning_rate": 1.949173825861889e-05,
      "loss": 0.5242,
      "step": 1432
    },
    {
      "epoch": 0.12901483265434738,
      "grad_norm": 1.3139606752722888,
      "learning_rate": 1.949081998285725e-05,
      "loss": 0.7066,
      "step": 1433
    },
    {
      "epoch": 0.12910486394021922,
      "grad_norm": 0.9724401337343077,
      "learning_rate": 1.948990089999038e-05,
      "loss": 0.5889,
      "step": 1434
    },
    {
      "epoch": 0.12919489522609107,
      "grad_norm": 0.9540596770245732,
      "learning_rate": 1.9488981010096447e-05,
      "loss": 0.6675,
      "step": 1435
    },
    {
      "epoch": 0.1292849265119629,
      "grad_norm": 0.838068598541306,
      "learning_rate": 1.948806031325368e-05,
      "loss": 0.6132,
      "step": 1436
    },
    {
      "epoch": 0.12937495779783476,
      "grad_norm": 0.812436270602766,
      "learning_rate": 1.9487138809540368e-05,
      "loss": 0.6169,
      "step": 1437
    },
    {
      "epoch": 0.1294649890837066,
      "grad_norm": 0.9350194112905742,
      "learning_rate": 1.9486216499034882e-05,
      "loss": 0.6666,
      "step": 1438
    },
    {
      "epoch": 0.12955502036957842,
      "grad_norm": 0.8875628423743881,
      "learning_rate": 1.9485293381815655e-05,
      "loss": 0.5333,
      "step": 1439
    },
    {
      "epoch": 0.12964505165545026,
      "grad_norm": 0.7289704842028892,
      "learning_rate": 1.9484369457961188e-05,
      "loss": 0.5655,
      "step": 1440
    },
    {
      "epoch": 0.1297350829413221,
      "grad_norm": 0.9830629364859957,
      "learning_rate": 1.9483444727550057e-05,
      "loss": 0.6257,
      "step": 1441
    },
    {
      "epoch": 0.12982511422719395,
      "grad_norm": 1.5760863898717512,
      "learning_rate": 1.948251919066089e-05,
      "loss": 0.5569,
      "step": 1442
    },
    {
      "epoch": 0.1299151455130658,
      "grad_norm": 0.9699627010896688,
      "learning_rate": 1.948159284737241e-05,
      "loss": 0.6773,
      "step": 1443
    },
    {
      "epoch": 0.13000517679893764,
      "grad_norm": 0.9028437092161132,
      "learning_rate": 1.9480665697763383e-05,
      "loss": 0.5313,
      "step": 1444
    },
    {
      "epoch": 0.13009520808480948,
      "grad_norm": 0.7598319225114113,
      "learning_rate": 1.9479737741912662e-05,
      "loss": 0.6331,
      "step": 1445
    },
    {
      "epoch": 0.1301852393706813,
      "grad_norm": 1.0890280555158034,
      "learning_rate": 1.947880897989915e-05,
      "loss": 0.6247,
      "step": 1446
    },
    {
      "epoch": 0.13027527065655314,
      "grad_norm": 0.7857979524477933,
      "learning_rate": 1.9477879411801843e-05,
      "loss": 0.6147,
      "step": 1447
    },
    {
      "epoch": 0.130365301942425,
      "grad_norm": 0.8057591641850393,
      "learning_rate": 1.9476949037699784e-05,
      "loss": 0.6263,
      "step": 1448
    },
    {
      "epoch": 0.13045533322829683,
      "grad_norm": 0.7636208452522723,
      "learning_rate": 1.9476017857672092e-05,
      "loss": 0.6231,
      "step": 1449
    },
    {
      "epoch": 0.13054536451416868,
      "grad_norm": 1.1524079436456447,
      "learning_rate": 1.947508587179796e-05,
      "loss": 0.5727,
      "step": 1450
    },
    {
      "epoch": 0.13063539580004052,
      "grad_norm": 0.8728134870367006,
      "learning_rate": 1.947415308015664e-05,
      "loss": 0.6837,
      "step": 1451
    },
    {
      "epoch": 0.13072542708591237,
      "grad_norm": 0.770020425186866,
      "learning_rate": 1.9473219482827463e-05,
      "loss": 0.5654,
      "step": 1452
    },
    {
      "epoch": 0.13081545837178418,
      "grad_norm": 0.9584668347785298,
      "learning_rate": 1.9472285079889815e-05,
      "loss": 0.6039,
      "step": 1453
    },
    {
      "epoch": 0.13090548965765603,
      "grad_norm": 0.6988945407614336,
      "learning_rate": 1.947134987142316e-05,
      "loss": 0.5271,
      "step": 1454
    },
    {
      "epoch": 0.13099552094352787,
      "grad_norm": 0.8286081019782975,
      "learning_rate": 1.9470413857507036e-05,
      "loss": 0.54,
      "step": 1455
    },
    {
      "epoch": 0.13108555222939972,
      "grad_norm": 0.8741618720683558,
      "learning_rate": 1.9469477038221038e-05,
      "loss": 0.6627,
      "step": 1456
    },
    {
      "epoch": 0.13117558351527156,
      "grad_norm": 0.7441149199063058,
      "learning_rate": 1.9468539413644828e-05,
      "loss": 0.5778,
      "step": 1457
    },
    {
      "epoch": 0.1312656148011434,
      "grad_norm": 0.7372282422231483,
      "learning_rate": 1.9467600983858146e-05,
      "loss": 0.5182,
      "step": 1458
    },
    {
      "epoch": 0.13135564608701525,
      "grad_norm": 0.8634629060512705,
      "learning_rate": 1.9466661748940804e-05,
      "loss": 0.5457,
      "step": 1459
    },
    {
      "epoch": 0.13144567737288707,
      "grad_norm": 0.9187129263251983,
      "learning_rate": 1.9465721708972662e-05,
      "loss": 0.6183,
      "step": 1460
    },
    {
      "epoch": 0.1315357086587589,
      "grad_norm": 0.7702171046409035,
      "learning_rate": 1.9464780864033673e-05,
      "loss": 0.5763,
      "step": 1461
    },
    {
      "epoch": 0.13162573994463075,
      "grad_norm": 0.7682851860058948,
      "learning_rate": 1.9463839214203836e-05,
      "loss": 0.6211,
      "step": 1462
    },
    {
      "epoch": 0.1317157712305026,
      "grad_norm": 1.3224777458906858,
      "learning_rate": 1.946289675956324e-05,
      "loss": 0.5471,
      "step": 1463
    },
    {
      "epoch": 0.13180580251637444,
      "grad_norm": 0.8512669019692954,
      "learning_rate": 1.946195350019203e-05,
      "loss": 0.6288,
      "step": 1464
    },
    {
      "epoch": 0.1318958338022463,
      "grad_norm": 0.8873221781185756,
      "learning_rate": 1.9461009436170414e-05,
      "loss": 0.71,
      "step": 1465
    },
    {
      "epoch": 0.13198586508811813,
      "grad_norm": 0.8068163422590675,
      "learning_rate": 1.9460064567578684e-05,
      "loss": 0.5129,
      "step": 1466
    },
    {
      "epoch": 0.13207589637398995,
      "grad_norm": 0.8132413989094536,
      "learning_rate": 1.9459118894497188e-05,
      "loss": 0.6073,
      "step": 1467
    },
    {
      "epoch": 0.1321659276598618,
      "grad_norm": 0.8651680489240396,
      "learning_rate": 1.9458172417006347e-05,
      "loss": 0.5677,
      "step": 1468
    },
    {
      "epoch": 0.13225595894573364,
      "grad_norm": 0.8691964999549264,
      "learning_rate": 1.9457225135186654e-05,
      "loss": 0.5992,
      "step": 1469
    },
    {
      "epoch": 0.13234599023160548,
      "grad_norm": 0.7049155406126678,
      "learning_rate": 1.945627704911866e-05,
      "loss": 0.5719,
      "step": 1470
    },
    {
      "epoch": 0.13243602151747733,
      "grad_norm": 0.9888219438307241,
      "learning_rate": 1.9455328158882994e-05,
      "loss": 0.6226,
      "step": 1471
    },
    {
      "epoch": 0.13252605280334917,
      "grad_norm": 0.893268222779034,
      "learning_rate": 1.945437846456035e-05,
      "loss": 0.6976,
      "step": 1472
    },
    {
      "epoch": 0.13261608408922101,
      "grad_norm": 0.8977438339888659,
      "learning_rate": 1.9453427966231492e-05,
      "loss": 0.6123,
      "step": 1473
    },
    {
      "epoch": 0.13270611537509283,
      "grad_norm": 0.7167370865558101,
      "learning_rate": 1.945247666397725e-05,
      "loss": 0.5572,
      "step": 1474
    },
    {
      "epoch": 0.13279614666096468,
      "grad_norm": 0.7242044439486635,
      "learning_rate": 1.9451524557878522e-05,
      "loss": 0.6735,
      "step": 1475
    },
    {
      "epoch": 0.13288617794683652,
      "grad_norm": 0.8792164870265098,
      "learning_rate": 1.945057164801628e-05,
      "loss": 0.6479,
      "step": 1476
    },
    {
      "epoch": 0.13297620923270836,
      "grad_norm": 1.184619270426569,
      "learning_rate": 1.944961793447155e-05,
      "loss": 0.621,
      "step": 1477
    },
    {
      "epoch": 0.1330662405185802,
      "grad_norm": 0.8800882021184441,
      "learning_rate": 1.944866341732545e-05,
      "loss": 0.5844,
      "step": 1478
    },
    {
      "epoch": 0.13315627180445205,
      "grad_norm": 0.8814904456438415,
      "learning_rate": 1.944770809665914e-05,
      "loss": 0.6781,
      "step": 1479
    },
    {
      "epoch": 0.1332463030903239,
      "grad_norm": 0.825484444225216,
      "learning_rate": 1.944675197255387e-05,
      "loss": 0.5828,
      "step": 1480
    },
    {
      "epoch": 0.13333633437619571,
      "grad_norm": 1.01863279397776,
      "learning_rate": 1.9445795045090948e-05,
      "loss": 0.5669,
      "step": 1481
    },
    {
      "epoch": 0.13342636566206756,
      "grad_norm": 1.0740871822458828,
      "learning_rate": 1.9444837314351747e-05,
      "loss": 0.626,
      "step": 1482
    },
    {
      "epoch": 0.1335163969479394,
      "grad_norm": 0.9289816949692978,
      "learning_rate": 1.9443878780417718e-05,
      "loss": 0.6684,
      "step": 1483
    },
    {
      "epoch": 0.13360642823381125,
      "grad_norm": 0.7795613614040514,
      "learning_rate": 1.9442919443370375e-05,
      "loss": 0.6011,
      "step": 1484
    },
    {
      "epoch": 0.1336964595196831,
      "grad_norm": 0.9875635327245244,
      "learning_rate": 1.9441959303291296e-05,
      "loss": 0.5841,
      "step": 1485
    },
    {
      "epoch": 0.13378649080555494,
      "grad_norm": 0.8073430503298474,
      "learning_rate": 1.9440998360262134e-05,
      "loss": 0.6046,
      "step": 1486
    },
    {
      "epoch": 0.13387652209142678,
      "grad_norm": 0.6689244280606217,
      "learning_rate": 1.9440036614364613e-05,
      "loss": 0.6573,
      "step": 1487
    },
    {
      "epoch": 0.13396655337729863,
      "grad_norm": 0.6902930796512856,
      "learning_rate": 1.943907406568051e-05,
      "loss": 0.5887,
      "step": 1488
    },
    {
      "epoch": 0.13405658466317044,
      "grad_norm": 0.7485698054690803,
      "learning_rate": 1.9438110714291697e-05,
      "loss": 0.6964,
      "step": 1489
    },
    {
      "epoch": 0.13414661594904229,
      "grad_norm": 0.8383740117813878,
      "learning_rate": 1.943714656028008e-05,
      "loss": 0.5369,
      "step": 1490
    },
    {
      "epoch": 0.13423664723491413,
      "grad_norm": 0.6189028275698952,
      "learning_rate": 1.9436181603727663e-05,
      "loss": 0.6136,
      "step": 1491
    },
    {
      "epoch": 0.13432667852078597,
      "grad_norm": 0.7520070933274732,
      "learning_rate": 1.9435215844716503e-05,
      "loss": 0.5993,
      "step": 1492
    },
    {
      "epoch": 0.13441670980665782,
      "grad_norm": 1.0980024975585485,
      "learning_rate": 1.943424928332873e-05,
      "loss": 0.4999,
      "step": 1493
    },
    {
      "epoch": 0.13450674109252966,
      "grad_norm": 0.7840053895065442,
      "learning_rate": 1.9433281919646538e-05,
      "loss": 0.5561,
      "step": 1494
    },
    {
      "epoch": 0.1345967723784015,
      "grad_norm": 0.6312957955465462,
      "learning_rate": 1.9432313753752193e-05,
      "loss": 0.5484,
      "step": 1495
    },
    {
      "epoch": 0.13468680366427332,
      "grad_norm": 0.727173075617696,
      "learning_rate": 1.9431344785728035e-05,
      "loss": 0.6282,
      "step": 1496
    },
    {
      "epoch": 0.13477683495014517,
      "grad_norm": 0.8133141256062553,
      "learning_rate": 1.9430375015656456e-05,
      "loss": 0.5499,
      "step": 1497
    },
    {
      "epoch": 0.134866866236017,
      "grad_norm": 0.9902175178517871,
      "learning_rate": 1.9429404443619928e-05,
      "loss": 0.6218,
      "step": 1498
    },
    {
      "epoch": 0.13495689752188886,
      "grad_norm": 0.9895714300834703,
      "learning_rate": 1.9428433069700995e-05,
      "loss": 0.6284,
      "step": 1499
    },
    {
      "epoch": 0.1350469288077607,
      "grad_norm": 0.687484745576609,
      "learning_rate": 1.9427460893982256e-05,
      "loss": 0.6653,
      "step": 1500
    },
    {
      "epoch": 0.13513696009363255,
      "grad_norm": 1.0395831590442932,
      "learning_rate": 1.942648791654639e-05,
      "loss": 0.5506,
      "step": 1501
    },
    {
      "epoch": 0.1352269913795044,
      "grad_norm": 0.8698423777873462,
      "learning_rate": 1.942551413747614e-05,
      "loss": 0.6078,
      "step": 1502
    },
    {
      "epoch": 0.1353170226653762,
      "grad_norm": 0.8442035104453938,
      "learning_rate": 1.9424539556854313e-05,
      "loss": 0.531,
      "step": 1503
    },
    {
      "epoch": 0.13540705395124805,
      "grad_norm": 0.9465459128653848,
      "learning_rate": 1.9423564174763788e-05,
      "loss": 0.6564,
      "step": 1504
    },
    {
      "epoch": 0.1354970852371199,
      "grad_norm": 1.0820020333775726,
      "learning_rate": 1.9422587991287517e-05,
      "loss": 0.6324,
      "step": 1505
    },
    {
      "epoch": 0.13558711652299174,
      "grad_norm": 0.7357294713914452,
      "learning_rate": 1.9421611006508514e-05,
      "loss": 0.6512,
      "step": 1506
    },
    {
      "epoch": 0.13567714780886359,
      "grad_norm": 0.6594984534801566,
      "learning_rate": 1.9420633220509858e-05,
      "loss": 0.5902,
      "step": 1507
    },
    {
      "epoch": 0.13576717909473543,
      "grad_norm": 0.9419194358363056,
      "learning_rate": 1.9419654633374704e-05,
      "loss": 0.5793,
      "step": 1508
    },
    {
      "epoch": 0.13585721038060727,
      "grad_norm": 0.8626937276674462,
      "learning_rate": 1.9418675245186273e-05,
      "loss": 0.695,
      "step": 1509
    },
    {
      "epoch": 0.1359472416664791,
      "grad_norm": 0.8935657381083817,
      "learning_rate": 1.9417695056027847e-05,
      "loss": 0.5854,
      "step": 1510
    },
    {
      "epoch": 0.13603727295235093,
      "grad_norm": 0.9852444056500704,
      "learning_rate": 1.9416714065982787e-05,
      "loss": 0.5825,
      "step": 1511
    },
    {
      "epoch": 0.13612730423822278,
      "grad_norm": 0.9527399468590144,
      "learning_rate": 1.9415732275134515e-05,
      "loss": 0.6461,
      "step": 1512
    },
    {
      "epoch": 0.13621733552409462,
      "grad_norm": 0.8189175146719999,
      "learning_rate": 1.941474968356652e-05,
      "loss": 0.5448,
      "step": 1513
    },
    {
      "epoch": 0.13630736680996647,
      "grad_norm": 0.6686564524029746,
      "learning_rate": 1.9413766291362374e-05,
      "loss": 0.6102,
      "step": 1514
    },
    {
      "epoch": 0.1363973980958383,
      "grad_norm": 0.8333574239068202,
      "learning_rate": 1.9412782098605693e-05,
      "loss": 0.5545,
      "step": 1515
    },
    {
      "epoch": 0.13648742938171016,
      "grad_norm": 0.8753341181104544,
      "learning_rate": 1.941179710538018e-05,
      "loss": 0.572,
      "step": 1516
    },
    {
      "epoch": 0.13657746066758197,
      "grad_norm": 0.7467089739851471,
      "learning_rate": 1.9410811311769593e-05,
      "loss": 0.6854,
      "step": 1517
    },
    {
      "epoch": 0.13666749195345382,
      "grad_norm": 1.1403075762646426,
      "learning_rate": 1.940982471785777e-05,
      "loss": 0.5746,
      "step": 1518
    },
    {
      "epoch": 0.13675752323932566,
      "grad_norm": 0.8034730161727105,
      "learning_rate": 1.940883732372861e-05,
      "loss": 0.5105,
      "step": 1519
    },
    {
      "epoch": 0.1368475545251975,
      "grad_norm": 0.8119478922070604,
      "learning_rate": 1.9407849129466084e-05,
      "loss": 0.6264,
      "step": 1520
    },
    {
      "epoch": 0.13693758581106935,
      "grad_norm": 0.7404000817220735,
      "learning_rate": 1.9406860135154223e-05,
      "loss": 0.4932,
      "step": 1521
    },
    {
      "epoch": 0.1370276170969412,
      "grad_norm": 0.7680670604751147,
      "learning_rate": 1.940587034087714e-05,
      "loss": 0.5785,
      "step": 1522
    },
    {
      "epoch": 0.13711764838281304,
      "grad_norm": 0.7755445683843897,
      "learning_rate": 1.9404879746718995e-05,
      "loss": 0.5881,
      "step": 1523
    },
    {
      "epoch": 0.13720767966868486,
      "grad_norm": 0.7710905759691362,
      "learning_rate": 1.940388835276404e-05,
      "loss": 0.5668,
      "step": 1524
    },
    {
      "epoch": 0.1372977109545567,
      "grad_norm": 0.6227501348458134,
      "learning_rate": 1.940289615909658e-05,
      "loss": 0.5889,
      "step": 1525
    },
    {
      "epoch": 0.13738774224042855,
      "grad_norm": 0.7392005297889066,
      "learning_rate": 1.9401903165800993e-05,
      "loss": 0.6553,
      "step": 1526
    },
    {
      "epoch": 0.1374777735263004,
      "grad_norm": 0.8831155431278628,
      "learning_rate": 1.9400909372961726e-05,
      "loss": 0.6043,
      "step": 1527
    },
    {
      "epoch": 0.13756780481217223,
      "grad_norm": 0.8250560099158425,
      "learning_rate": 1.9399914780663286e-05,
      "loss": 0.6328,
      "step": 1528
    },
    {
      "epoch": 0.13765783609804408,
      "grad_norm": 0.6591336945431554,
      "learning_rate": 1.9398919388990252e-05,
      "loss": 0.6094,
      "step": 1529
    },
    {
      "epoch": 0.13774786738391592,
      "grad_norm": 0.7980432292695997,
      "learning_rate": 1.9397923198027283e-05,
      "loss": 0.6064,
      "step": 1530
    },
    {
      "epoch": 0.13783789866978774,
      "grad_norm": 0.7837852135790965,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 0.6383,
      "step": 1531
    },
    {
      "epoch": 0.13792792995565958,
      "grad_norm": 0.7138999995564231,
      "learning_rate": 1.939592841857045e-05,
      "loss": 0.5609,
      "step": 1532
    },
    {
      "epoch": 0.13801796124153143,
      "grad_norm": 0.776888120647714,
      "learning_rate": 1.9394929830246227e-05,
      "loss": 0.6425,
      "step": 1533
    },
    {
      "epoch": 0.13810799252740327,
      "grad_norm": 0.7705868143687594,
      "learning_rate": 1.939393044297134e-05,
      "loss": 0.6264,
      "step": 1534
    },
    {
      "epoch": 0.13819802381327512,
      "grad_norm": 0.6639296918935015,
      "learning_rate": 1.939293025683077e-05,
      "loss": 0.5532,
      "step": 1535
    },
    {
      "epoch": 0.13828805509914696,
      "grad_norm": 0.9605606429428362,
      "learning_rate": 1.9391929271909583e-05,
      "loss": 0.5569,
      "step": 1536
    },
    {
      "epoch": 0.1383780863850188,
      "grad_norm": 0.8312366947265133,
      "learning_rate": 1.9390927488292893e-05,
      "loss": 0.679,
      "step": 1537
    },
    {
      "epoch": 0.13846811767089062,
      "grad_norm": 0.6755858718724225,
      "learning_rate": 1.9389924906065903e-05,
      "loss": 0.6936,
      "step": 1538
    },
    {
      "epoch": 0.13855814895676247,
      "grad_norm": 0.7245523425725463,
      "learning_rate": 1.938892152531387e-05,
      "loss": 0.6361,
      "step": 1539
    },
    {
      "epoch": 0.1386481802426343,
      "grad_norm": 0.8504277090538606,
      "learning_rate": 1.938791734612212e-05,
      "loss": 0.5672,
      "step": 1540
    },
    {
      "epoch": 0.13873821152850616,
      "grad_norm": 0.6562889670870447,
      "learning_rate": 1.9386912368576048e-05,
      "loss": 0.5823,
      "step": 1541
    },
    {
      "epoch": 0.138828242814378,
      "grad_norm": 0.6932061984886577,
      "learning_rate": 1.9385906592761117e-05,
      "loss": 0.5696,
      "step": 1542
    },
    {
      "epoch": 0.13891827410024984,
      "grad_norm": 0.8118253782227384,
      "learning_rate": 1.9384900018762862e-05,
      "loss": 0.5704,
      "step": 1543
    },
    {
      "epoch": 0.1390083053861217,
      "grad_norm": 0.7698518831004307,
      "learning_rate": 1.9383892646666887e-05,
      "loss": 0.6661,
      "step": 1544
    },
    {
      "epoch": 0.1390983366719935,
      "grad_norm": 0.7220514106318393,
      "learning_rate": 1.938288447655885e-05,
      "loss": 0.6108,
      "step": 1545
    },
    {
      "epoch": 0.13918836795786535,
      "grad_norm": 0.7590239836261572,
      "learning_rate": 1.938187550852449e-05,
      "loss": 0.6423,
      "step": 1546
    },
    {
      "epoch": 0.1392783992437372,
      "grad_norm": 0.9004588791414605,
      "learning_rate": 1.938086574264961e-05,
      "loss": 0.6287,
      "step": 1547
    },
    {
      "epoch": 0.13936843052960904,
      "grad_norm": 0.7489316530844407,
      "learning_rate": 1.937985517902009e-05,
      "loss": 0.6074,
      "step": 1548
    },
    {
      "epoch": 0.13945846181548088,
      "grad_norm": 0.8161599128882868,
      "learning_rate": 1.9378843817721856e-05,
      "loss": 0.5828,
      "step": 1549
    },
    {
      "epoch": 0.13954849310135273,
      "grad_norm": 0.7103100745130452,
      "learning_rate": 1.937783165884092e-05,
      "loss": 0.6087,
      "step": 1550
    },
    {
      "epoch": 0.13963852438722457,
      "grad_norm": 0.7493949736018205,
      "learning_rate": 1.9376818702463356e-05,
      "loss": 0.5406,
      "step": 1551
    },
    {
      "epoch": 0.1397285556730964,
      "grad_norm": 0.9891644764021668,
      "learning_rate": 1.9375804948675308e-05,
      "loss": 0.6094,
      "step": 1552
    },
    {
      "epoch": 0.13981858695896823,
      "grad_norm": 0.6563798968035365,
      "learning_rate": 1.9374790397562987e-05,
      "loss": 0.5436,
      "step": 1553
    },
    {
      "epoch": 0.13990861824484008,
      "grad_norm": 0.6418313209514191,
      "learning_rate": 1.9373775049212667e-05,
      "loss": 0.5802,
      "step": 1554
    },
    {
      "epoch": 0.13999864953071192,
      "grad_norm": 0.8446304480479141,
      "learning_rate": 1.9372758903710697e-05,
      "loss": 0.6177,
      "step": 1555
    },
    {
      "epoch": 0.14008868081658377,
      "grad_norm": 0.8087475607284356,
      "learning_rate": 1.937174196114349e-05,
      "loss": 0.6907,
      "step": 1556
    },
    {
      "epoch": 0.1401787121024556,
      "grad_norm": 0.9119175799017841,
      "learning_rate": 1.9370724221597527e-05,
      "loss": 0.6034,
      "step": 1557
    },
    {
      "epoch": 0.14026874338832745,
      "grad_norm": 0.624494319584433,
      "learning_rate": 1.936970568515936e-05,
      "loss": 0.5506,
      "step": 1558
    },
    {
      "epoch": 0.14035877467419927,
      "grad_norm": 0.6759836752081056,
      "learning_rate": 1.93686863519156e-05,
      "loss": 0.6189,
      "step": 1559
    },
    {
      "epoch": 0.14044880596007112,
      "grad_norm": 0.8915525500014502,
      "learning_rate": 1.9367666221952937e-05,
      "loss": 0.5906,
      "step": 1560
    },
    {
      "epoch": 0.14053883724594296,
      "grad_norm": 0.6948833063263337,
      "learning_rate": 1.936664529535812e-05,
      "loss": 0.5803,
      "step": 1561
    },
    {
      "epoch": 0.1406288685318148,
      "grad_norm": 0.6853081023075153,
      "learning_rate": 1.9365623572217972e-05,
      "loss": 0.6208,
      "step": 1562
    },
    {
      "epoch": 0.14071889981768665,
      "grad_norm": 0.908756690832381,
      "learning_rate": 1.9364601052619384e-05,
      "loss": 0.7706,
      "step": 1563
    },
    {
      "epoch": 0.1408089311035585,
      "grad_norm": 0.9462805742207503,
      "learning_rate": 1.93635777366493e-05,
      "loss": 0.5871,
      "step": 1564
    },
    {
      "epoch": 0.14089896238943034,
      "grad_norm": 0.9460040421036678,
      "learning_rate": 1.9362553624394757e-05,
      "loss": 0.6005,
      "step": 1565
    },
    {
      "epoch": 0.14098899367530215,
      "grad_norm": 1.055385886370862,
      "learning_rate": 1.9361528715942836e-05,
      "loss": 0.8019,
      "step": 1566
    },
    {
      "epoch": 0.141079024961174,
      "grad_norm": 0.9036812614986973,
      "learning_rate": 1.93605030113807e-05,
      "loss": 0.5601,
      "step": 1567
    },
    {
      "epoch": 0.14116905624704584,
      "grad_norm": 0.7653074080080062,
      "learning_rate": 1.935947651079558e-05,
      "loss": 0.6356,
      "step": 1568
    },
    {
      "epoch": 0.1412590875329177,
      "grad_norm": 0.9239129824832847,
      "learning_rate": 1.9358449214274763e-05,
      "loss": 0.659,
      "step": 1569
    },
    {
      "epoch": 0.14134911881878953,
      "grad_norm": 0.6845671020681156,
      "learning_rate": 1.9357421121905615e-05,
      "loss": 0.6327,
      "step": 1570
    },
    {
      "epoch": 0.14143915010466138,
      "grad_norm": 0.7041921039617433,
      "learning_rate": 1.9356392233775562e-05,
      "loss": 0.5857,
      "step": 1571
    },
    {
      "epoch": 0.14152918139053322,
      "grad_norm": 0.6053909412279749,
      "learning_rate": 1.9355362549972106e-05,
      "loss": 0.5521,
      "step": 1572
    },
    {
      "epoch": 0.14161921267640504,
      "grad_norm": 0.916759578340873,
      "learning_rate": 1.935433207058281e-05,
      "loss": 0.6137,
      "step": 1573
    },
    {
      "epoch": 0.14170924396227688,
      "grad_norm": 0.7004597470751024,
      "learning_rate": 1.9353300795695303e-05,
      "loss": 0.5163,
      "step": 1574
    },
    {
      "epoch": 0.14179927524814873,
      "grad_norm": 0.7874270636053544,
      "learning_rate": 1.935226872539729e-05,
      "loss": 0.6499,
      "step": 1575
    },
    {
      "epoch": 0.14188930653402057,
      "grad_norm": 0.6471857334589666,
      "learning_rate": 1.9351235859776542e-05,
      "loss": 0.5838,
      "step": 1576
    },
    {
      "epoch": 0.14197933781989241,
      "grad_norm": 0.7360618846821514,
      "learning_rate": 1.9350202198920885e-05,
      "loss": 0.5392,
      "step": 1577
    },
    {
      "epoch": 0.14206936910576426,
      "grad_norm": 0.7049571101894981,
      "learning_rate": 1.9349167742918226e-05,
      "loss": 0.6167,
      "step": 1578
    },
    {
      "epoch": 0.1421594003916361,
      "grad_norm": 0.7465872677764567,
      "learning_rate": 1.9348132491856538e-05,
      "loss": 0.5162,
      "step": 1579
    },
    {
      "epoch": 0.14224943167750792,
      "grad_norm": 0.7364398150652643,
      "learning_rate": 1.934709644582386e-05,
      "loss": 0.5907,
      "step": 1580
    },
    {
      "epoch": 0.14233946296337976,
      "grad_norm": 0.6635766717134384,
      "learning_rate": 1.9346059604908297e-05,
      "loss": 0.6447,
      "step": 1581
    },
    {
      "epoch": 0.1424294942492516,
      "grad_norm": 0.8308715020621554,
      "learning_rate": 1.934502196919802e-05,
      "loss": 0.6354,
      "step": 1582
    },
    {
      "epoch": 0.14251952553512345,
      "grad_norm": 0.7176042659480995,
      "learning_rate": 1.9343983538781273e-05,
      "loss": 0.582,
      "step": 1583
    },
    {
      "epoch": 0.1426095568209953,
      "grad_norm": 0.829267589401724,
      "learning_rate": 1.934294431374636e-05,
      "loss": 0.6995,
      "step": 1584
    },
    {
      "epoch": 0.14269958810686714,
      "grad_norm": 0.6427747056975933,
      "learning_rate": 1.9341904294181667e-05,
      "loss": 0.5526,
      "step": 1585
    },
    {
      "epoch": 0.142789619392739,
      "grad_norm": 0.6690018401573983,
      "learning_rate": 1.9340863480175627e-05,
      "loss": 0.581,
      "step": 1586
    },
    {
      "epoch": 0.14287965067861083,
      "grad_norm": 0.6449516897455195,
      "learning_rate": 1.933982187181676e-05,
      "loss": 0.5884,
      "step": 1587
    },
    {
      "epoch": 0.14296968196448265,
      "grad_norm": 0.6425118230352623,
      "learning_rate": 1.9338779469193638e-05,
      "loss": 0.5785,
      "step": 1588
    },
    {
      "epoch": 0.1430597132503545,
      "grad_norm": 0.7822332938148128,
      "learning_rate": 1.9337736272394915e-05,
      "loss": 0.5842,
      "step": 1589
    },
    {
      "epoch": 0.14314974453622634,
      "grad_norm": 0.9747837367853046,
      "learning_rate": 1.9336692281509298e-05,
      "loss": 0.5953,
      "step": 1590
    },
    {
      "epoch": 0.14323977582209818,
      "grad_norm": 0.8090149072461494,
      "learning_rate": 1.9335647496625575e-05,
      "loss": 0.6099,
      "step": 1591
    },
    {
      "epoch": 0.14332980710797003,
      "grad_norm": 0.8665496172620925,
      "learning_rate": 1.9334601917832588e-05,
      "loss": 0.6705,
      "step": 1592
    },
    {
      "epoch": 0.14341983839384187,
      "grad_norm": 0.7844467653098606,
      "learning_rate": 1.933355554521926e-05,
      "loss": 0.6712,
      "step": 1593
    },
    {
      "epoch": 0.14350986967971371,
      "grad_norm": 0.8959613627334022,
      "learning_rate": 1.933250837887457e-05,
      "loss": 0.6168,
      "step": 1594
    },
    {
      "epoch": 0.14359990096558553,
      "grad_norm": 0.8063281502403086,
      "learning_rate": 1.9331460418887578e-05,
      "loss": 0.6297,
      "step": 1595
    },
    {
      "epoch": 0.14368993225145738,
      "grad_norm": 0.7876885446293526,
      "learning_rate": 1.9330411665347396e-05,
      "loss": 0.6459,
      "step": 1596
    },
    {
      "epoch": 0.14377996353732922,
      "grad_norm": 0.9255769921909762,
      "learning_rate": 1.9329362118343213e-05,
      "loss": 0.5832,
      "step": 1597
    },
    {
      "epoch": 0.14386999482320106,
      "grad_norm": 0.9585630384580007,
      "learning_rate": 1.9328311777964277e-05,
      "loss": 0.6342,
      "step": 1598
    },
    {
      "epoch": 0.1439600261090729,
      "grad_norm": 0.8307721897438202,
      "learning_rate": 1.932726064429992e-05,
      "loss": 0.6685,
      "step": 1599
    },
    {
      "epoch": 0.14405005739494475,
      "grad_norm": 0.9224941361111139,
      "learning_rate": 1.9326208717439525e-05,
      "loss": 0.5919,
      "step": 1600
    },
    {
      "epoch": 0.1441400886808166,
      "grad_norm": 0.9075295674774109,
      "learning_rate": 1.9325155997472547e-05,
      "loss": 0.6014,
      "step": 1601
    },
    {
      "epoch": 0.1442301199666884,
      "grad_norm": 0.7420516658286348,
      "learning_rate": 1.932410248448852e-05,
      "loss": 0.5904,
      "step": 1602
    },
    {
      "epoch": 0.14432015125256026,
      "grad_norm": 0.7144184470303039,
      "learning_rate": 1.932304817857702e-05,
      "loss": 0.588,
      "step": 1603
    },
    {
      "epoch": 0.1444101825384321,
      "grad_norm": 0.8123210604055879,
      "learning_rate": 1.9321993079827715e-05,
      "loss": 0.5909,
      "step": 1604
    },
    {
      "epoch": 0.14450021382430395,
      "grad_norm": 0.7444759793690283,
      "learning_rate": 1.9320937188330333e-05,
      "loss": 0.5938,
      "step": 1605
    },
    {
      "epoch": 0.1445902451101758,
      "grad_norm": 0.8201606451341769,
      "learning_rate": 1.9319880504174664e-05,
      "loss": 0.5709,
      "step": 1606
    },
    {
      "epoch": 0.14468027639604764,
      "grad_norm": 0.7852180593027553,
      "learning_rate": 1.931882302745057e-05,
      "loss": 0.6074,
      "step": 1607
    },
    {
      "epoch": 0.14477030768191948,
      "grad_norm": 0.8359565449467291,
      "learning_rate": 1.9317764758247982e-05,
      "loss": 0.516,
      "step": 1608
    },
    {
      "epoch": 0.1448603389677913,
      "grad_norm": 0.790530402946031,
      "learning_rate": 1.931670569665689e-05,
      "loss": 0.6003,
      "step": 1609
    },
    {
      "epoch": 0.14495037025366314,
      "grad_norm": 0.9848288510091678,
      "learning_rate": 1.931564584276736e-05,
      "loss": 0.7061,
      "step": 1610
    },
    {
      "epoch": 0.14504040153953499,
      "grad_norm": 0.6732788457905919,
      "learning_rate": 1.9314585196669523e-05,
      "loss": 0.5649,
      "step": 1611
    },
    {
      "epoch": 0.14513043282540683,
      "grad_norm": 0.750933230945118,
      "learning_rate": 1.9313523758453576e-05,
      "loss": 0.558,
      "step": 1612
    },
    {
      "epoch": 0.14522046411127867,
      "grad_norm": 1.138372144317408,
      "learning_rate": 1.931246152820979e-05,
      "loss": 0.6677,
      "step": 1613
    },
    {
      "epoch": 0.14531049539715052,
      "grad_norm": 0.6254408034237496,
      "learning_rate": 1.9311398506028488e-05,
      "loss": 0.5643,
      "step": 1614
    },
    {
      "epoch": 0.14540052668302236,
      "grad_norm": 0.7147169578235016,
      "learning_rate": 1.9310334692000077e-05,
      "loss": 0.6066,
      "step": 1615
    },
    {
      "epoch": 0.14549055796889418,
      "grad_norm": 0.7913576411753415,
      "learning_rate": 1.9309270086215024e-05,
      "loss": 0.6147,
      "step": 1616
    },
    {
      "epoch": 0.14558058925476602,
      "grad_norm": 0.7591737846950577,
      "learning_rate": 1.9308204688763862e-05,
      "loss": 0.6519,
      "step": 1617
    },
    {
      "epoch": 0.14567062054063787,
      "grad_norm": 0.6797161703695219,
      "learning_rate": 1.930713849973719e-05,
      "loss": 0.535,
      "step": 1618
    },
    {
      "epoch": 0.1457606518265097,
      "grad_norm": 0.6543593596994649,
      "learning_rate": 1.9306071519225683e-05,
      "loss": 0.5938,
      "step": 1619
    },
    {
      "epoch": 0.14585068311238156,
      "grad_norm": 0.723965870903659,
      "learning_rate": 1.9305003747320077e-05,
      "loss": 0.5673,
      "step": 1620
    },
    {
      "epoch": 0.1459407143982534,
      "grad_norm": 0.8057956504830354,
      "learning_rate": 1.9303935184111172e-05,
      "loss": 0.5786,
      "step": 1621
    },
    {
      "epoch": 0.14603074568412525,
      "grad_norm": 0.6991026655522543,
      "learning_rate": 1.9302865829689845e-05,
      "loss": 0.5898,
      "step": 1622
    },
    {
      "epoch": 0.14612077696999706,
      "grad_norm": 0.7122025035481211,
      "learning_rate": 1.930179568414703e-05,
      "loss": 0.5695,
      "step": 1623
    },
    {
      "epoch": 0.1462108082558689,
      "grad_norm": 0.6804216727190876,
      "learning_rate": 1.9300724747573734e-05,
      "loss": 0.55,
      "step": 1624
    },
    {
      "epoch": 0.14630083954174075,
      "grad_norm": 0.7334959785287608,
      "learning_rate": 1.9299653020061032e-05,
      "loss": 0.5632,
      "step": 1625
    },
    {
      "epoch": 0.1463908708276126,
      "grad_norm": 0.7353240409749666,
      "learning_rate": 1.9298580501700058e-05,
      "loss": 0.565,
      "step": 1626
    },
    {
      "epoch": 0.14648090211348444,
      "grad_norm": 0.8554316089935483,
      "learning_rate": 1.929750719258203e-05,
      "loss": 0.6616,
      "step": 1627
    },
    {
      "epoch": 0.14657093339935628,
      "grad_norm": 0.7313625366703818,
      "learning_rate": 1.929643309279821e-05,
      "loss": 0.6273,
      "step": 1628
    },
    {
      "epoch": 0.14666096468522813,
      "grad_norm": 0.7134549429457139,
      "learning_rate": 1.929535820243995e-05,
      "loss": 0.6287,
      "step": 1629
    },
    {
      "epoch": 0.14675099597109995,
      "grad_norm": 0.6411973743562863,
      "learning_rate": 1.929428252159866e-05,
      "loss": 0.6254,
      "step": 1630
    },
    {
      "epoch": 0.1468410272569718,
      "grad_norm": 0.7142069840045417,
      "learning_rate": 1.9293206050365812e-05,
      "loss": 0.5828,
      "step": 1631
    },
    {
      "epoch": 0.14693105854284363,
      "grad_norm": 0.6014728466454242,
      "learning_rate": 1.929212878883295e-05,
      "loss": 0.5027,
      "step": 1632
    },
    {
      "epoch": 0.14702108982871548,
      "grad_norm": 0.8885064393299894,
      "learning_rate": 1.9291050737091685e-05,
      "loss": 0.5745,
      "step": 1633
    },
    {
      "epoch": 0.14711112111458732,
      "grad_norm": 0.6879594331498639,
      "learning_rate": 1.9289971895233697e-05,
      "loss": 0.5775,
      "step": 1634
    },
    {
      "epoch": 0.14720115240045917,
      "grad_norm": 0.9106629870979212,
      "learning_rate": 1.9288892263350728e-05,
      "loss": 0.5588,
      "step": 1635
    },
    {
      "epoch": 0.147291183686331,
      "grad_norm": 1.1374504997329626,
      "learning_rate": 1.9287811841534598e-05,
      "loss": 0.7402,
      "step": 1636
    },
    {
      "epoch": 0.14738121497220283,
      "grad_norm": 0.7176517852918085,
      "learning_rate": 1.9286730629877178e-05,
      "loss": 0.6394,
      "step": 1637
    },
    {
      "epoch": 0.14747124625807467,
      "grad_norm": 0.7046571744296436,
      "learning_rate": 1.9285648628470422e-05,
      "loss": 0.6242,
      "step": 1638
    },
    {
      "epoch": 0.14756127754394652,
      "grad_norm": 0.7240108869056912,
      "learning_rate": 1.9284565837406336e-05,
      "loss": 0.6032,
      "step": 1639
    },
    {
      "epoch": 0.14765130882981836,
      "grad_norm": 2.1725997406770245,
      "learning_rate": 1.9283482256777008e-05,
      "loss": 0.7122,
      "step": 1640
    },
    {
      "epoch": 0.1477413401156902,
      "grad_norm": 0.7968301538578235,
      "learning_rate": 1.9282397886674586e-05,
      "loss": 0.5778,
      "step": 1641
    },
    {
      "epoch": 0.14783137140156205,
      "grad_norm": 0.7756501801016182,
      "learning_rate": 1.9281312727191284e-05,
      "loss": 0.6401,
      "step": 1642
    },
    {
      "epoch": 0.1479214026874339,
      "grad_norm": 0.9599238775474821,
      "learning_rate": 1.9280226778419382e-05,
      "loss": 0.6881,
      "step": 1643
    },
    {
      "epoch": 0.1480114339733057,
      "grad_norm": 1.0571962961624548,
      "learning_rate": 1.9279140040451233e-05,
      "loss": 0.6278,
      "step": 1644
    },
    {
      "epoch": 0.14810146525917756,
      "grad_norm": 0.8308551239368895,
      "learning_rate": 1.9278052513379256e-05,
      "loss": 0.5657,
      "step": 1645
    },
    {
      "epoch": 0.1481914965450494,
      "grad_norm": 1.034132152090276,
      "learning_rate": 1.9276964197295932e-05,
      "loss": 0.6478,
      "step": 1646
    },
    {
      "epoch": 0.14828152783092124,
      "grad_norm": 0.7051557959553308,
      "learning_rate": 1.927587509229381e-05,
      "loss": 0.5453,
      "step": 1647
    },
    {
      "epoch": 0.1483715591167931,
      "grad_norm": 0.6689651016345943,
      "learning_rate": 1.927478519846551e-05,
      "loss": 0.5924,
      "step": 1648
    },
    {
      "epoch": 0.14846159040266493,
      "grad_norm": 0.7275557001812906,
      "learning_rate": 1.9273694515903718e-05,
      "loss": 0.5063,
      "step": 1649
    },
    {
      "epoch": 0.14855162168853678,
      "grad_norm": 0.9461585580196188,
      "learning_rate": 1.9272603044701185e-05,
      "loss": 0.5816,
      "step": 1650
    },
    {
      "epoch": 0.1486416529744086,
      "grad_norm": 0.7624131491087484,
      "learning_rate": 1.9271510784950735e-05,
      "loss": 0.6101,
      "step": 1651
    },
    {
      "epoch": 0.14873168426028044,
      "grad_norm": 0.8335608433536433,
      "learning_rate": 1.9270417736745247e-05,
      "loss": 0.6262,
      "step": 1652
    },
    {
      "epoch": 0.14882171554615228,
      "grad_norm": 0.6718541917439377,
      "learning_rate": 1.926932390017768e-05,
      "loss": 0.4826,
      "step": 1653
    },
    {
      "epoch": 0.14891174683202413,
      "grad_norm": 0.6755802879412789,
      "learning_rate": 1.9268229275341052e-05,
      "loss": 0.6055,
      "step": 1654
    },
    {
      "epoch": 0.14900177811789597,
      "grad_norm": 0.6900051834728095,
      "learning_rate": 1.9267133862328452e-05,
      "loss": 0.5931,
      "step": 1655
    },
    {
      "epoch": 0.14909180940376782,
      "grad_norm": 0.9412286049285746,
      "learning_rate": 1.9266037661233034e-05,
      "loss": 0.6366,
      "step": 1656
    },
    {
      "epoch": 0.14918184068963966,
      "grad_norm": 0.720439908384039,
      "learning_rate": 1.9264940672148018e-05,
      "loss": 0.5668,
      "step": 1657
    },
    {
      "epoch": 0.14927187197551148,
      "grad_norm": 0.78476995476595,
      "learning_rate": 1.9263842895166697e-05,
      "loss": 0.5764,
      "step": 1658
    },
    {
      "epoch": 0.14936190326138332,
      "grad_norm": 0.7231459607086295,
      "learning_rate": 1.926274433038242e-05,
      "loss": 0.5746,
      "step": 1659
    },
    {
      "epoch": 0.14945193454725517,
      "grad_norm": 0.8340622764644909,
      "learning_rate": 1.9261644977888617e-05,
      "loss": 0.6256,
      "step": 1660
    },
    {
      "epoch": 0.149541965833127,
      "grad_norm": 0.8232082892853171,
      "learning_rate": 1.9260544837778777e-05,
      "loss": 0.5463,
      "step": 1661
    },
    {
      "epoch": 0.14963199711899886,
      "grad_norm": 1.1990181738259567,
      "learning_rate": 1.9259443910146448e-05,
      "loss": 0.6655,
      "step": 1662
    },
    {
      "epoch": 0.1497220284048707,
      "grad_norm": 0.7423029944580339,
      "learning_rate": 1.9258342195085264e-05,
      "loss": 0.6051,
      "step": 1663
    },
    {
      "epoch": 0.14981205969074254,
      "grad_norm": 1.041377847421232,
      "learning_rate": 1.9257239692688907e-05,
      "loss": 0.5953,
      "step": 1664
    },
    {
      "epoch": 0.14990209097661436,
      "grad_norm": 0.8423271511672906,
      "learning_rate": 1.9256136403051136e-05,
      "loss": 0.5282,
      "step": 1665
    },
    {
      "epoch": 0.1499921222624862,
      "grad_norm": 0.8574199238918364,
      "learning_rate": 1.9255032326265784e-05,
      "loss": 0.6385,
      "step": 1666
    },
    {
      "epoch": 0.15008215354835805,
      "grad_norm": 0.8117897117257529,
      "learning_rate": 1.925392746242673e-05,
      "loss": 0.5881,
      "step": 1667
    },
    {
      "epoch": 0.1501721848342299,
      "grad_norm": 0.7882065650270744,
      "learning_rate": 1.9252821811627943e-05,
      "loss": 0.5637,
      "step": 1668
    },
    {
      "epoch": 0.15026221612010174,
      "grad_norm": 0.6742926840689012,
      "learning_rate": 1.9251715373963445e-05,
      "loss": 0.538,
      "step": 1669
    },
    {
      "epoch": 0.15035224740597358,
      "grad_norm": 1.0423777568642372,
      "learning_rate": 1.9250608149527323e-05,
      "loss": 0.5906,
      "step": 1670
    },
    {
      "epoch": 0.15044227869184543,
      "grad_norm": 0.9810155452448334,
      "learning_rate": 1.924950013841374e-05,
      "loss": 0.5991,
      "step": 1671
    },
    {
      "epoch": 0.15053230997771724,
      "grad_norm": 0.9728652803952269,
      "learning_rate": 1.9248391340716925e-05,
      "loss": 0.6143,
      "step": 1672
    },
    {
      "epoch": 0.1506223412635891,
      "grad_norm": 0.889720013297216,
      "learning_rate": 1.9247281756531166e-05,
      "loss": 0.6491,
      "step": 1673
    },
    {
      "epoch": 0.15071237254946093,
      "grad_norm": 0.9200847727421169,
      "learning_rate": 1.9246171385950823e-05,
      "loss": 0.4945,
      "step": 1674
    },
    {
      "epoch": 0.15080240383533278,
      "grad_norm": 0.7221093495048592,
      "learning_rate": 1.9245060229070323e-05,
      "loss": 0.5823,
      "step": 1675
    },
    {
      "epoch": 0.15089243512120462,
      "grad_norm": 0.9011052909188603,
      "learning_rate": 1.9243948285984166e-05,
      "loss": 0.6083,
      "step": 1676
    },
    {
      "epoch": 0.15098246640707647,
      "grad_norm": 0.6947212569148045,
      "learning_rate": 1.9242835556786903e-05,
      "loss": 0.6625,
      "step": 1677
    },
    {
      "epoch": 0.1510724976929483,
      "grad_norm": 0.7917751587220889,
      "learning_rate": 1.9241722041573166e-05,
      "loss": 0.6543,
      "step": 1678
    },
    {
      "epoch": 0.15116252897882013,
      "grad_norm": 1.1366792067638685,
      "learning_rate": 1.9240607740437648e-05,
      "loss": 0.586,
      "step": 1679
    },
    {
      "epoch": 0.15125256026469197,
      "grad_norm": 1.0116375695318574,
      "learning_rate": 1.9239492653475106e-05,
      "loss": 0.7195,
      "step": 1680
    },
    {
      "epoch": 0.15134259155056382,
      "grad_norm": 0.7729303032405718,
      "learning_rate": 1.9238376780780378e-05,
      "loss": 0.6323,
      "step": 1681
    },
    {
      "epoch": 0.15143262283643566,
      "grad_norm": 0.8599607288612122,
      "learning_rate": 1.9237260122448347e-05,
      "loss": 0.5697,
      "step": 1682
    },
    {
      "epoch": 0.1515226541223075,
      "grad_norm": 0.7068835833366295,
      "learning_rate": 1.9236142678573983e-05,
      "loss": 0.6385,
      "step": 1683
    },
    {
      "epoch": 0.15161268540817935,
      "grad_norm": 0.7714341753393649,
      "learning_rate": 1.9235024449252306e-05,
      "loss": 0.6344,
      "step": 1684
    },
    {
      "epoch": 0.1517027166940512,
      "grad_norm": 0.8312829844992767,
      "learning_rate": 1.9233905434578418e-05,
      "loss": 0.6068,
      "step": 1685
    },
    {
      "epoch": 0.151792747979923,
      "grad_norm": 0.6892164095788745,
      "learning_rate": 1.9232785634647478e-05,
      "loss": 0.5869,
      "step": 1686
    },
    {
      "epoch": 0.15188277926579485,
      "grad_norm": 0.673862727501685,
      "learning_rate": 1.9231665049554716e-05,
      "loss": 0.5837,
      "step": 1687
    },
    {
      "epoch": 0.1519728105516667,
      "grad_norm": 1.0168206284082897,
      "learning_rate": 1.9230543679395426e-05,
      "loss": 0.5788,
      "step": 1688
    },
    {
      "epoch": 0.15206284183753854,
      "grad_norm": 0.7493865999872296,
      "learning_rate": 1.9229421524264968e-05,
      "loss": 0.6853,
      "step": 1689
    },
    {
      "epoch": 0.1521528731234104,
      "grad_norm": 0.8328864316678524,
      "learning_rate": 1.9228298584258775e-05,
      "loss": 0.6078,
      "step": 1690
    },
    {
      "epoch": 0.15224290440928223,
      "grad_norm": 0.965554592974123,
      "learning_rate": 1.9227174859472338e-05,
      "loss": 0.5525,
      "step": 1691
    },
    {
      "epoch": 0.15233293569515408,
      "grad_norm": 0.7610441198428264,
      "learning_rate": 1.922605035000122e-05,
      "loss": 0.5468,
      "step": 1692
    },
    {
      "epoch": 0.15242296698102592,
      "grad_norm": 0.6158997803154627,
      "learning_rate": 1.9224925055941056e-05,
      "loss": 0.4925,
      "step": 1693
    },
    {
      "epoch": 0.15251299826689774,
      "grad_norm": 0.7072327508268159,
      "learning_rate": 1.922379897738753e-05,
      "loss": 0.616,
      "step": 1694
    },
    {
      "epoch": 0.15260302955276958,
      "grad_norm": 0.9401524403153474,
      "learning_rate": 1.9222672114436416e-05,
      "loss": 0.5756,
      "step": 1695
    },
    {
      "epoch": 0.15269306083864143,
      "grad_norm": 1.0328303683091218,
      "learning_rate": 1.922154446718354e-05,
      "loss": 0.6225,
      "step": 1696
    },
    {
      "epoch": 0.15278309212451327,
      "grad_norm": 0.6790385634429236,
      "learning_rate": 1.9220416035724794e-05,
      "loss": 0.5656,
      "step": 1697
    },
    {
      "epoch": 0.15287312341038511,
      "grad_norm": 0.6931046290235446,
      "learning_rate": 1.9219286820156144e-05,
      "loss": 0.598,
      "step": 1698
    },
    {
      "epoch": 0.15296315469625696,
      "grad_norm": 1.0093937337072911,
      "learning_rate": 1.9218156820573618e-05,
      "loss": 0.6754,
      "step": 1699
    },
    {
      "epoch": 0.1530531859821288,
      "grad_norm": 0.9279773818168667,
      "learning_rate": 1.921702603707331e-05,
      "loss": 0.6011,
      "step": 1700
    },
    {
      "epoch": 0.15314321726800062,
      "grad_norm": 0.8717900557738346,
      "learning_rate": 1.9215894469751388e-05,
      "loss": 0.541,
      "step": 1701
    },
    {
      "epoch": 0.15323324855387246,
      "grad_norm": 1.1219970404755484,
      "learning_rate": 1.921476211870408e-05,
      "loss": 0.6551,
      "step": 1702
    },
    {
      "epoch": 0.1533232798397443,
      "grad_norm": 0.740171614496899,
      "learning_rate": 1.9213628984027674e-05,
      "loss": 0.6001,
      "step": 1703
    },
    {
      "epoch": 0.15341331112561615,
      "grad_norm": 0.8428867091585294,
      "learning_rate": 1.921249506581854e-05,
      "loss": 0.5789,
      "step": 1704
    },
    {
      "epoch": 0.153503342411488,
      "grad_norm": 0.7195706904286743,
      "learning_rate": 1.9211360364173106e-05,
      "loss": 0.5833,
      "step": 1705
    },
    {
      "epoch": 0.15359337369735984,
      "grad_norm": 0.8141516303814925,
      "learning_rate": 1.9210224879187866e-05,
      "loss": 0.5926,
      "step": 1706
    },
    {
      "epoch": 0.1536834049832317,
      "grad_norm": 0.668663466255268,
      "learning_rate": 1.9209088610959382e-05,
      "loss": 0.5331,
      "step": 1707
    },
    {
      "epoch": 0.1537734362691035,
      "grad_norm": 1.0172258391157343,
      "learning_rate": 1.9207951559584286e-05,
      "loss": 0.6273,
      "step": 1708
    },
    {
      "epoch": 0.15386346755497535,
      "grad_norm": 0.894042238853228,
      "learning_rate": 1.920681372515927e-05,
      "loss": 0.6225,
      "step": 1709
    },
    {
      "epoch": 0.1539534988408472,
      "grad_norm": 0.7501409457311534,
      "learning_rate": 1.9205675107781104e-05,
      "loss": 0.6117,
      "step": 1710
    },
    {
      "epoch": 0.15404353012671904,
      "grad_norm": 0.8591951730514454,
      "learning_rate": 1.9204535707546602e-05,
      "loss": 0.5456,
      "step": 1711
    },
    {
      "epoch": 0.15413356141259088,
      "grad_norm": 0.8169492753956564,
      "learning_rate": 1.9203395524552674e-05,
      "loss": 0.6038,
      "step": 1712
    },
    {
      "epoch": 0.15422359269846272,
      "grad_norm": 0.719384962443491,
      "learning_rate": 1.9202254558896274e-05,
      "loss": 0.5324,
      "step": 1713
    },
    {
      "epoch": 0.15431362398433457,
      "grad_norm": 0.8755526385247611,
      "learning_rate": 1.9201112810674432e-05,
      "loss": 0.5652,
      "step": 1714
    },
    {
      "epoch": 0.15440365527020639,
      "grad_norm": 1.0760197350662746,
      "learning_rate": 1.9199970279984244e-05,
      "loss": 0.5727,
      "step": 1715
    },
    {
      "epoch": 0.15449368655607823,
      "grad_norm": 0.8359066283496245,
      "learning_rate": 1.919882696692287e-05,
      "loss": 0.5771,
      "step": 1716
    },
    {
      "epoch": 0.15458371784195007,
      "grad_norm": 0.8802028638480629,
      "learning_rate": 1.9197682871587536e-05,
      "loss": 0.6532,
      "step": 1717
    },
    {
      "epoch": 0.15467374912782192,
      "grad_norm": 0.7280970810174612,
      "learning_rate": 1.9196537994075542e-05,
      "loss": 0.5706,
      "step": 1718
    },
    {
      "epoch": 0.15476378041369376,
      "grad_norm": 0.9436778998064556,
      "learning_rate": 1.9195392334484248e-05,
      "loss": 0.5711,
      "step": 1719
    },
    {
      "epoch": 0.1548538116995656,
      "grad_norm": 0.8544748989042219,
      "learning_rate": 1.9194245892911077e-05,
      "loss": 0.5773,
      "step": 1720
    },
    {
      "epoch": 0.15494384298543745,
      "grad_norm": 0.8909391263106278,
      "learning_rate": 1.9193098669453532e-05,
      "loss": 0.6909,
      "step": 1721
    },
    {
      "epoch": 0.15503387427130927,
      "grad_norm": 0.8410639192955969,
      "learning_rate": 1.919195066420916e-05,
      "loss": 0.6207,
      "step": 1722
    },
    {
      "epoch": 0.1551239055571811,
      "grad_norm": 1.0907185535635682,
      "learning_rate": 1.9190801877275603e-05,
      "loss": 0.5701,
      "step": 1723
    },
    {
      "epoch": 0.15521393684305296,
      "grad_norm": 0.5661347782314613,
      "learning_rate": 1.9189652308750545e-05,
      "loss": 0.4462,
      "step": 1724
    },
    {
      "epoch": 0.1553039681289248,
      "grad_norm": 0.7341794557258593,
      "learning_rate": 1.9188501958731745e-05,
      "loss": 0.5675,
      "step": 1725
    },
    {
      "epoch": 0.15539399941479665,
      "grad_norm": 0.7557173186324996,
      "learning_rate": 1.9187350827317034e-05,
      "loss": 0.598,
      "step": 1726
    },
    {
      "epoch": 0.1554840307006685,
      "grad_norm": 0.6288202460358219,
      "learning_rate": 1.9186198914604307e-05,
      "loss": 0.5798,
      "step": 1727
    },
    {
      "epoch": 0.15557406198654034,
      "grad_norm": 0.7827325494943163,
      "learning_rate": 1.918504622069152e-05,
      "loss": 0.6239,
      "step": 1728
    },
    {
      "epoch": 0.15566409327241215,
      "grad_norm": 0.82018016712106,
      "learning_rate": 1.9183892745676696e-05,
      "loss": 0.5879,
      "step": 1729
    },
    {
      "epoch": 0.155754124558284,
      "grad_norm": 0.8682346496073353,
      "learning_rate": 1.918273848965793e-05,
      "loss": 0.5864,
      "step": 1730
    },
    {
      "epoch": 0.15584415584415584,
      "grad_norm": 0.8979836700842687,
      "learning_rate": 1.918158345273338e-05,
      "loss": 0.6659,
      "step": 1731
    },
    {
      "epoch": 0.15593418713002768,
      "grad_norm": 0.826428162830268,
      "learning_rate": 1.918042763500128e-05,
      "loss": 0.6367,
      "step": 1732
    },
    {
      "epoch": 0.15602421841589953,
      "grad_norm": 0.8533890581527486,
      "learning_rate": 1.9179271036559902e-05,
      "loss": 0.6441,
      "step": 1733
    },
    {
      "epoch": 0.15611424970177137,
      "grad_norm": 0.8044957056456122,
      "learning_rate": 1.9178113657507625e-05,
      "loss": 0.5304,
      "step": 1734
    },
    {
      "epoch": 0.15620428098764322,
      "grad_norm": 0.8860291022286431,
      "learning_rate": 1.917695549794286e-05,
      "loss": 0.5962,
      "step": 1735
    },
    {
      "epoch": 0.15629431227351503,
      "grad_norm": 0.9984327249464562,
      "learning_rate": 1.91757965579641e-05,
      "loss": 0.6015,
      "step": 1736
    },
    {
      "epoch": 0.15638434355938688,
      "grad_norm": 1.0101910525652409,
      "learning_rate": 1.9174636837669906e-05,
      "loss": 0.6069,
      "step": 1737
    },
    {
      "epoch": 0.15647437484525872,
      "grad_norm": 0.8918607380475997,
      "learning_rate": 1.9173476337158893e-05,
      "loss": 0.5615,
      "step": 1738
    },
    {
      "epoch": 0.15656440613113057,
      "grad_norm": 0.6749403628209448,
      "learning_rate": 1.9172315056529756e-05,
      "loss": 0.5394,
      "step": 1739
    },
    {
      "epoch": 0.1566544374170024,
      "grad_norm": 1.3493739867500258,
      "learning_rate": 1.9171152995881257e-05,
      "loss": 0.6496,
      "step": 1740
    },
    {
      "epoch": 0.15674446870287426,
      "grad_norm": 0.7533665972203205,
      "learning_rate": 1.916999015531221e-05,
      "loss": 0.5752,
      "step": 1741
    },
    {
      "epoch": 0.1568344999887461,
      "grad_norm": 0.6660119685792991,
      "learning_rate": 1.9168826534921506e-05,
      "loss": 0.5568,
      "step": 1742
    },
    {
      "epoch": 0.15692453127461792,
      "grad_norm": 0.8428760134412835,
      "learning_rate": 1.91676621348081e-05,
      "loss": 0.5509,
      "step": 1743
    },
    {
      "epoch": 0.15701456256048976,
      "grad_norm": 0.730172793464012,
      "learning_rate": 1.9166496955071013e-05,
      "loss": 0.5995,
      "step": 1744
    },
    {
      "epoch": 0.1571045938463616,
      "grad_norm": 0.7420652532071367,
      "learning_rate": 1.9165330995809334e-05,
      "loss": 0.5824,
      "step": 1745
    },
    {
      "epoch": 0.15719462513223345,
      "grad_norm": 0.6684273008511209,
      "learning_rate": 1.9164164257122215e-05,
      "loss": 0.5987,
      "step": 1746
    },
    {
      "epoch": 0.1572846564181053,
      "grad_norm": 0.660481149495991,
      "learning_rate": 1.916299673910888e-05,
      "loss": 0.554,
      "step": 1747
    },
    {
      "epoch": 0.15737468770397714,
      "grad_norm": 0.7965673309220948,
      "learning_rate": 1.916182844186861e-05,
      "loss": 0.5064,
      "step": 1748
    },
    {
      "epoch": 0.15746471898984898,
      "grad_norm": 0.881612257479363,
      "learning_rate": 1.916065936550076e-05,
      "loss": 0.681,
      "step": 1749
    },
    {
      "epoch": 0.1575547502757208,
      "grad_norm": 1.0707608552802022,
      "learning_rate": 1.915948951010475e-05,
      "loss": 0.7093,
      "step": 1750
    },
    {
      "epoch": 0.15764478156159265,
      "grad_norm": 0.8146373005158887,
      "learning_rate": 1.9158318875780065e-05,
      "loss": 0.5974,
      "step": 1751
    },
    {
      "epoch": 0.1577348128474645,
      "grad_norm": 0.704277384061667,
      "learning_rate": 1.9157147462626257e-05,
      "loss": 0.5951,
      "step": 1752
    },
    {
      "epoch": 0.15782484413333633,
      "grad_norm": 0.7880047453517981,
      "learning_rate": 1.915597527074294e-05,
      "loss": 0.5261,
      "step": 1753
    },
    {
      "epoch": 0.15791487541920818,
      "grad_norm": 0.7509562441285211,
      "learning_rate": 1.9154802300229802e-05,
      "loss": 0.6139,
      "step": 1754
    },
    {
      "epoch": 0.15800490670508002,
      "grad_norm": 0.979447518267408,
      "learning_rate": 1.915362855118659e-05,
      "loss": 0.5466,
      "step": 1755
    },
    {
      "epoch": 0.15809493799095187,
      "grad_norm": 0.7101643437798169,
      "learning_rate": 1.9152454023713125e-05,
      "loss": 0.599,
      "step": 1756
    },
    {
      "epoch": 0.15818496927682368,
      "grad_norm": 0.924823298013378,
      "learning_rate": 1.9151278717909284e-05,
      "loss": 0.6662,
      "step": 1757
    },
    {
      "epoch": 0.15827500056269553,
      "grad_norm": 0.8864727113155026,
      "learning_rate": 1.9150102633875023e-05,
      "loss": 0.5502,
      "step": 1758
    },
    {
      "epoch": 0.15836503184856737,
      "grad_norm": 0.676129968305623,
      "learning_rate": 1.9148925771710347e-05,
      "loss": 0.631,
      "step": 1759
    },
    {
      "epoch": 0.15845506313443922,
      "grad_norm": 0.8211695575666951,
      "learning_rate": 1.9147748131515347e-05,
      "loss": 0.6274,
      "step": 1760
    },
    {
      "epoch": 0.15854509442031106,
      "grad_norm": 0.9965095479423508,
      "learning_rate": 1.9146569713390165e-05,
      "loss": 0.5181,
      "step": 1761
    },
    {
      "epoch": 0.1586351257061829,
      "grad_norm": 1.288483433620899,
      "learning_rate": 1.9145390517435013e-05,
      "loss": 0.5105,
      "step": 1762
    },
    {
      "epoch": 0.15872515699205475,
      "grad_norm": 0.6583201700791016,
      "learning_rate": 1.914421054375017e-05,
      "loss": 0.5669,
      "step": 1763
    },
    {
      "epoch": 0.15881518827792657,
      "grad_norm": 0.7860523581059179,
      "learning_rate": 1.914302979243599e-05,
      "loss": 0.5218,
      "step": 1764
    },
    {
      "epoch": 0.1589052195637984,
      "grad_norm": 0.8652026012642696,
      "learning_rate": 1.9141848263592874e-05,
      "loss": 0.6579,
      "step": 1765
    },
    {
      "epoch": 0.15899525084967026,
      "grad_norm": 1.0730518582874273,
      "learning_rate": 1.9140665957321308e-05,
      "loss": 0.6373,
      "step": 1766
    },
    {
      "epoch": 0.1590852821355421,
      "grad_norm": 0.7566598569808213,
      "learning_rate": 1.9139482873721837e-05,
      "loss": 0.6495,
      "step": 1767
    },
    {
      "epoch": 0.15917531342141394,
      "grad_norm": 0.8075493723421306,
      "learning_rate": 1.913829901289506e-05,
      "loss": 0.6177,
      "step": 1768
    },
    {
      "epoch": 0.1592653447072858,
      "grad_norm": 1.0101334359914087,
      "learning_rate": 1.9137114374941667e-05,
      "loss": 0.6799,
      "step": 1769
    },
    {
      "epoch": 0.15935537599315763,
      "grad_norm": 0.7133762568114802,
      "learning_rate": 1.913592895996239e-05,
      "loss": 0.6632,
      "step": 1770
    },
    {
      "epoch": 0.15944540727902945,
      "grad_norm": 0.9631795822590085,
      "learning_rate": 1.913474276805804e-05,
      "loss": 0.6922,
      "step": 1771
    },
    {
      "epoch": 0.1595354385649013,
      "grad_norm": 1.0443062484653693,
      "learning_rate": 1.9133555799329494e-05,
      "loss": 0.6174,
      "step": 1772
    },
    {
      "epoch": 0.15962546985077314,
      "grad_norm": 0.8550615513439757,
      "learning_rate": 1.9132368053877695e-05,
      "loss": 0.683,
      "step": 1773
    },
    {
      "epoch": 0.15971550113664498,
      "grad_norm": 0.9262226326679037,
      "learning_rate": 1.9131179531803642e-05,
      "loss": 0.6232,
      "step": 1774
    },
    {
      "epoch": 0.15980553242251683,
      "grad_norm": 0.8489655549540323,
      "learning_rate": 1.9129990233208415e-05,
      "loss": 0.666,
      "step": 1775
    },
    {
      "epoch": 0.15989556370838867,
      "grad_norm": 0.6964381388791603,
      "learning_rate": 1.912880015819315e-05,
      "loss": 0.5097,
      "step": 1776
    },
    {
      "epoch": 0.15998559499426052,
      "grad_norm": 0.6761366296031667,
      "learning_rate": 1.9127609306859045e-05,
      "loss": 0.6281,
      "step": 1777
    },
    {
      "epoch": 0.16007562628013233,
      "grad_norm": 0.7912512809383256,
      "learning_rate": 1.912641767930738e-05,
      "loss": 0.6015,
      "step": 1778
    },
    {
      "epoch": 0.16016565756600418,
      "grad_norm": 0.6843205315735208,
      "learning_rate": 1.912522527563949e-05,
      "loss": 0.6607,
      "step": 1779
    },
    {
      "epoch": 0.16025568885187602,
      "grad_norm": 0.7774076465951011,
      "learning_rate": 1.9124032095956774e-05,
      "loss": 0.6636,
      "step": 1780
    },
    {
      "epoch": 0.16034572013774787,
      "grad_norm": 0.5758224967329829,
      "learning_rate": 1.9122838140360708e-05,
      "loss": 0.551,
      "step": 1781
    },
    {
      "epoch": 0.1604357514236197,
      "grad_norm": 0.7857043651538882,
      "learning_rate": 1.9121643408952817e-05,
      "loss": 0.6309,
      "step": 1782
    },
    {
      "epoch": 0.16052578270949155,
      "grad_norm": 0.8101073540555382,
      "learning_rate": 1.9120447901834708e-05,
      "loss": 0.5148,
      "step": 1783
    },
    {
      "epoch": 0.1606158139953634,
      "grad_norm": 0.718417609101539,
      "learning_rate": 1.9119251619108044e-05,
      "loss": 0.6652,
      "step": 1784
    },
    {
      "epoch": 0.16070584528123522,
      "grad_norm": 0.8498717670013687,
      "learning_rate": 1.9118054560874562e-05,
      "loss": 0.5967,
      "step": 1785
    },
    {
      "epoch": 0.16079587656710706,
      "grad_norm": 0.8367279373933662,
      "learning_rate": 1.9116856727236057e-05,
      "loss": 0.5771,
      "step": 1786
    },
    {
      "epoch": 0.1608859078529789,
      "grad_norm": 0.7678599954947272,
      "learning_rate": 1.9115658118294398e-05,
      "loss": 0.5485,
      "step": 1787
    },
    {
      "epoch": 0.16097593913885075,
      "grad_norm": 0.8386575016342137,
      "learning_rate": 1.911445873415151e-05,
      "loss": 0.5987,
      "step": 1788
    },
    {
      "epoch": 0.1610659704247226,
      "grad_norm": 1.6849611842782903,
      "learning_rate": 1.911325857490939e-05,
      "loss": 0.5907,
      "step": 1789
    },
    {
      "epoch": 0.16115600171059444,
      "grad_norm": 0.86319437834064,
      "learning_rate": 1.9112057640670106e-05,
      "loss": 0.6045,
      "step": 1790
    },
    {
      "epoch": 0.16124603299646628,
      "grad_norm": 0.6363611072496815,
      "learning_rate": 1.911085593153578e-05,
      "loss": 0.5912,
      "step": 1791
    },
    {
      "epoch": 0.16133606428233813,
      "grad_norm": 0.7602287584784507,
      "learning_rate": 1.9109653447608607e-05,
      "loss": 0.6153,
      "step": 1792
    },
    {
      "epoch": 0.16142609556820994,
      "grad_norm": 0.8471649920896693,
      "learning_rate": 1.910845018899085e-05,
      "loss": 0.5444,
      "step": 1793
    },
    {
      "epoch": 0.1615161268540818,
      "grad_norm": 0.7615419984592409,
      "learning_rate": 1.9107246155784834e-05,
      "loss": 0.6404,
      "step": 1794
    },
    {
      "epoch": 0.16160615813995363,
      "grad_norm": 0.8795073986068345,
      "learning_rate": 1.910604134809295e-05,
      "loss": 0.5154,
      "step": 1795
    },
    {
      "epoch": 0.16169618942582548,
      "grad_norm": 0.7090918504791852,
      "learning_rate": 1.910483576601765e-05,
      "loss": 0.5481,
      "step": 1796
    },
    {
      "epoch": 0.16178622071169732,
      "grad_norm": 0.8709093710151978,
      "learning_rate": 1.9103629409661468e-05,
      "loss": 0.6229,
      "step": 1797
    },
    {
      "epoch": 0.16187625199756916,
      "grad_norm": 0.7473423091023698,
      "learning_rate": 1.9102422279126987e-05,
      "loss": 0.5855,
      "step": 1798
    },
    {
      "epoch": 0.161966283283441,
      "grad_norm": 0.8888004071578123,
      "learning_rate": 1.9101214374516862e-05,
      "loss": 0.6416,
      "step": 1799
    },
    {
      "epoch": 0.16205631456931283,
      "grad_norm": 0.834679693684427,
      "learning_rate": 1.910000569593382e-05,
      "loss": 0.4806,
      "step": 1800
    },
    {
      "epoch": 0.16214634585518467,
      "grad_norm": 0.7954360518375191,
      "learning_rate": 1.9098796243480637e-05,
      "loss": 0.5258,
      "step": 1801
    },
    {
      "epoch": 0.16223637714105651,
      "grad_norm": 0.9040101619260934,
      "learning_rate": 1.9097586017260177e-05,
      "loss": 0.6266,
      "step": 1802
    },
    {
      "epoch": 0.16232640842692836,
      "grad_norm": 0.886666273425468,
      "learning_rate": 1.9096375017375347e-05,
      "loss": 0.5794,
      "step": 1803
    },
    {
      "epoch": 0.1624164397128002,
      "grad_norm": 0.6892496826468456,
      "learning_rate": 1.9095163243929143e-05,
      "loss": 0.6097,
      "step": 1804
    },
    {
      "epoch": 0.16250647099867205,
      "grad_norm": 0.9727493116105289,
      "learning_rate": 1.9093950697024605e-05,
      "loss": 0.5785,
      "step": 1805
    },
    {
      "epoch": 0.1625965022845439,
      "grad_norm": 0.7859179659379802,
      "learning_rate": 1.9092737376764855e-05,
      "loss": 0.6051,
      "step": 1806
    },
    {
      "epoch": 0.1626865335704157,
      "grad_norm": 0.776702514495994,
      "learning_rate": 1.9091523283253074e-05,
      "loss": 0.6576,
      "step": 1807
    },
    {
      "epoch": 0.16277656485628755,
      "grad_norm": 0.7471390690058628,
      "learning_rate": 1.9090308416592506e-05,
      "loss": 0.608,
      "step": 1808
    },
    {
      "epoch": 0.1628665961421594,
      "grad_norm": 0.9736419964667954,
      "learning_rate": 1.9089092776886464e-05,
      "loss": 0.5382,
      "step": 1809
    },
    {
      "epoch": 0.16295662742803124,
      "grad_norm": 0.717351524224595,
      "learning_rate": 1.9087876364238328e-05,
      "loss": 0.6008,
      "step": 1810
    },
    {
      "epoch": 0.1630466587139031,
      "grad_norm": 0.7538083137768391,
      "learning_rate": 1.9086659178751547e-05,
      "loss": 0.5473,
      "step": 1811
    },
    {
      "epoch": 0.16313668999977493,
      "grad_norm": 0.9118263338704529,
      "learning_rate": 1.9085441220529626e-05,
      "loss": 0.5229,
      "step": 1812
    },
    {
      "epoch": 0.16322672128564678,
      "grad_norm": 1.166756041565357,
      "learning_rate": 1.908422248967614e-05,
      "loss": 0.5895,
      "step": 1813
    },
    {
      "epoch": 0.1633167525715186,
      "grad_norm": 0.7293600445215455,
      "learning_rate": 1.9083002986294734e-05,
      "loss": 0.5507,
      "step": 1814
    },
    {
      "epoch": 0.16340678385739044,
      "grad_norm": 0.8822140983661874,
      "learning_rate": 1.9081782710489113e-05,
      "loss": 0.5561,
      "step": 1815
    },
    {
      "epoch": 0.16349681514326228,
      "grad_norm": 0.6375562520468473,
      "learning_rate": 1.908056166236305e-05,
      "loss": 0.6515,
      "step": 1816
    },
    {
      "epoch": 0.16358684642913413,
      "grad_norm": 0.7230922455921214,
      "learning_rate": 1.9079339842020388e-05,
      "loss": 0.6,
      "step": 1817
    },
    {
      "epoch": 0.16367687771500597,
      "grad_norm": 0.8165267070068047,
      "learning_rate": 1.9078117249565027e-05,
      "loss": 0.6787,
      "step": 1818
    },
    {
      "epoch": 0.16376690900087781,
      "grad_norm": 0.685910323709071,
      "learning_rate": 1.9076893885100937e-05,
      "loss": 0.5679,
      "step": 1819
    },
    {
      "epoch": 0.16385694028674966,
      "grad_norm": 1.0341122534383422,
      "learning_rate": 1.9075669748732158e-05,
      "loss": 0.6319,
      "step": 1820
    },
    {
      "epoch": 0.16394697157262147,
      "grad_norm": 0.9264528574196562,
      "learning_rate": 1.9074444840562783e-05,
      "loss": 0.641,
      "step": 1821
    },
    {
      "epoch": 0.16403700285849332,
      "grad_norm": 0.9331349586644964,
      "learning_rate": 1.9073219160696988e-05,
      "loss": 0.6349,
      "step": 1822
    },
    {
      "epoch": 0.16412703414436516,
      "grad_norm": 0.6404663934717719,
      "learning_rate": 1.9071992709239e-05,
      "loss": 0.5658,
      "step": 1823
    },
    {
      "epoch": 0.164217065430237,
      "grad_norm": 0.8185485861287217,
      "learning_rate": 1.907076548629312e-05,
      "loss": 0.6086,
      "step": 1824
    },
    {
      "epoch": 0.16430709671610885,
      "grad_norm": 0.662018547390503,
      "learning_rate": 1.906953749196371e-05,
      "loss": 0.6253,
      "step": 1825
    },
    {
      "epoch": 0.1643971280019807,
      "grad_norm": 0.6649013486402562,
      "learning_rate": 1.9068308726355198e-05,
      "loss": 0.5482,
      "step": 1826
    },
    {
      "epoch": 0.16448715928785254,
      "grad_norm": 1.0477852311936275,
      "learning_rate": 1.906707918957208e-05,
      "loss": 0.6043,
      "step": 1827
    },
    {
      "epoch": 0.16457719057372436,
      "grad_norm": 0.8333495924084383,
      "learning_rate": 1.9065848881718924e-05,
      "loss": 0.5389,
      "step": 1828
    },
    {
      "epoch": 0.1646672218595962,
      "grad_norm": 0.8033998201148793,
      "learning_rate": 1.9064617802900346e-05,
      "loss": 0.6411,
      "step": 1829
    },
    {
      "epoch": 0.16475725314546805,
      "grad_norm": 0.8996575117351173,
      "learning_rate": 1.906338595322104e-05,
      "loss": 0.67,
      "step": 1830
    },
    {
      "epoch": 0.1648472844313399,
      "grad_norm": 0.6888735014445723,
      "learning_rate": 1.906215333278577e-05,
      "loss": 0.5944,
      "step": 1831
    },
    {
      "epoch": 0.16493731571721174,
      "grad_norm": 0.6945081144950307,
      "learning_rate": 1.9060919941699346e-05,
      "loss": 0.5081,
      "step": 1832
    },
    {
      "epoch": 0.16502734700308358,
      "grad_norm": 0.8000036122452304,
      "learning_rate": 1.9059685780066667e-05,
      "loss": 0.6508,
      "step": 1833
    },
    {
      "epoch": 0.16511737828895542,
      "grad_norm": 0.772165623970046,
      "learning_rate": 1.9058450847992687e-05,
      "loss": 0.5216,
      "step": 1834
    },
    {
      "epoch": 0.16520740957482724,
      "grad_norm": 0.6906486970358867,
      "learning_rate": 1.9057215145582418e-05,
      "loss": 0.524,
      "step": 1835
    },
    {
      "epoch": 0.16529744086069909,
      "grad_norm": 0.8047202539483419,
      "learning_rate": 1.9055978672940953e-05,
      "loss": 0.6654,
      "step": 1836
    },
    {
      "epoch": 0.16538747214657093,
      "grad_norm": 1.0808374365425182,
      "learning_rate": 1.9054741430173432e-05,
      "loss": 0.6432,
      "step": 1837
    },
    {
      "epoch": 0.16547750343244277,
      "grad_norm": 0.6465946399911574,
      "learning_rate": 1.905350341738508e-05,
      "loss": 0.6184,
      "step": 1838
    },
    {
      "epoch": 0.16556753471831462,
      "grad_norm": 0.8135022171545743,
      "learning_rate": 1.905226463468118e-05,
      "loss": 0.5411,
      "step": 1839
    },
    {
      "epoch": 0.16565756600418646,
      "grad_norm": 0.8088453407867248,
      "learning_rate": 1.905102508216707e-05,
      "loss": 0.5969,
      "step": 1840
    },
    {
      "epoch": 0.1657475972900583,
      "grad_norm": 0.6813788498718641,
      "learning_rate": 1.904978475994817e-05,
      "loss": 0.5627,
      "step": 1841
    },
    {
      "epoch": 0.16583762857593012,
      "grad_norm": 0.6536271895410891,
      "learning_rate": 1.9048543668129953e-05,
      "loss": 0.5296,
      "step": 1842
    },
    {
      "epoch": 0.16592765986180197,
      "grad_norm": 0.735407676934781,
      "learning_rate": 1.9047301806817963e-05,
      "loss": 0.5133,
      "step": 1843
    },
    {
      "epoch": 0.1660176911476738,
      "grad_norm": 0.813366691466237,
      "learning_rate": 1.904605917611781e-05,
      "loss": 0.4693,
      "step": 1844
    },
    {
      "epoch": 0.16610772243354566,
      "grad_norm": 0.6202909715779837,
      "learning_rate": 1.9044815776135167e-05,
      "loss": 0.6686,
      "step": 1845
    },
    {
      "epoch": 0.1661977537194175,
      "grad_norm": 0.9282635376417808,
      "learning_rate": 1.9043571606975776e-05,
      "loss": 0.6017,
      "step": 1846
    },
    {
      "epoch": 0.16628778500528935,
      "grad_norm": 0.7927169326744471,
      "learning_rate": 1.904232666874544e-05,
      "loss": 0.5539,
      "step": 1847
    },
    {
      "epoch": 0.1663778162911612,
      "grad_norm": 0.7065476310165226,
      "learning_rate": 1.9041080961550028e-05,
      "loss": 0.5451,
      "step": 1848
    },
    {
      "epoch": 0.166467847577033,
      "grad_norm": 0.6980794378489503,
      "learning_rate": 1.9039834485495477e-05,
      "loss": 0.5372,
      "step": 1849
    },
    {
      "epoch": 0.16655787886290485,
      "grad_norm": 0.8688286924333933,
      "learning_rate": 1.903858724068779e-05,
      "loss": 0.6055,
      "step": 1850
    },
    {
      "epoch": 0.1666479101487767,
      "grad_norm": 0.7162542634271462,
      "learning_rate": 1.9037339227233025e-05,
      "loss": 0.558,
      "step": 1851
    },
    {
      "epoch": 0.16673794143464854,
      "grad_norm": 0.7177362428071382,
      "learning_rate": 1.903609044523733e-05,
      "loss": 0.6334,
      "step": 1852
    },
    {
      "epoch": 0.16682797272052038,
      "grad_norm": 1.0768850584358627,
      "learning_rate": 1.903484089480689e-05,
      "loss": 0.6021,
      "step": 1853
    },
    {
      "epoch": 0.16691800400639223,
      "grad_norm": 0.785971440268195,
      "learning_rate": 1.9033590576047967e-05,
      "loss": 0.5728,
      "step": 1854
    },
    {
      "epoch": 0.16700803529226407,
      "grad_norm": 0.6606283480713423,
      "learning_rate": 1.903233948906689e-05,
      "loss": 0.6225,
      "step": 1855
    },
    {
      "epoch": 0.1670980665781359,
      "grad_norm": 0.7281043802915866,
      "learning_rate": 1.903108763397006e-05,
      "loss": 0.5564,
      "step": 1856
    },
    {
      "epoch": 0.16718809786400773,
      "grad_norm": 0.9483647614136902,
      "learning_rate": 1.9029835010863927e-05,
      "loss": 0.5448,
      "step": 1857
    },
    {
      "epoch": 0.16727812914987958,
      "grad_norm": 0.8983304123834884,
      "learning_rate": 1.902858161985502e-05,
      "loss": 0.5786,
      "step": 1858
    },
    {
      "epoch": 0.16736816043575142,
      "grad_norm": 0.8712258398816913,
      "learning_rate": 1.9027327461049924e-05,
      "loss": 0.5941,
      "step": 1859
    },
    {
      "epoch": 0.16745819172162327,
      "grad_norm": 0.9391962543045765,
      "learning_rate": 1.9026072534555294e-05,
      "loss": 0.6149,
      "step": 1860
    },
    {
      "epoch": 0.1675482230074951,
      "grad_norm": 1.0819793635111377,
      "learning_rate": 1.9024816840477852e-05,
      "loss": 0.6265,
      "step": 1861
    },
    {
      "epoch": 0.16763825429336696,
      "grad_norm": 0.7936827568237859,
      "learning_rate": 1.9023560378924383e-05,
      "loss": 0.6755,
      "step": 1862
    },
    {
      "epoch": 0.16772828557923877,
      "grad_norm": 0.6949533532449229,
      "learning_rate": 1.9022303150001736e-05,
      "loss": 0.618,
      "step": 1863
    },
    {
      "epoch": 0.16781831686511062,
      "grad_norm": 0.8021006962023884,
      "learning_rate": 1.9021045153816826e-05,
      "loss": 0.6381,
      "step": 1864
    },
    {
      "epoch": 0.16790834815098246,
      "grad_norm": 1.119972056975201,
      "learning_rate": 1.9019786390476634e-05,
      "loss": 0.5435,
      "step": 1865
    },
    {
      "epoch": 0.1679983794368543,
      "grad_norm": 1.0119914923400404,
      "learning_rate": 1.901852686008821e-05,
      "loss": 0.6797,
      "step": 1866
    },
    {
      "epoch": 0.16808841072272615,
      "grad_norm": 0.8658945743752046,
      "learning_rate": 1.901726656275866e-05,
      "loss": 0.654,
      "step": 1867
    },
    {
      "epoch": 0.168178442008598,
      "grad_norm": 0.8845578269394905,
      "learning_rate": 1.901600549859516e-05,
      "loss": 0.6995,
      "step": 1868
    },
    {
      "epoch": 0.16826847329446984,
      "grad_norm": 0.7117389304125248,
      "learning_rate": 1.9014743667704953e-05,
      "loss": 0.6266,
      "step": 1869
    },
    {
      "epoch": 0.16835850458034166,
      "grad_norm": 0.7375614995275939,
      "learning_rate": 1.9013481070195353e-05,
      "loss": 0.5911,
      "step": 1870
    },
    {
      "epoch": 0.1684485358662135,
      "grad_norm": 0.7854442064595222,
      "learning_rate": 1.901221770617372e-05,
      "loss": 0.6079,
      "step": 1871
    },
    {
      "epoch": 0.16853856715208534,
      "grad_norm": 0.7357265692335492,
      "learning_rate": 1.90109535757475e-05,
      "loss": 0.5318,
      "step": 1872
    },
    {
      "epoch": 0.1686285984379572,
      "grad_norm": 1.0315453142671291,
      "learning_rate": 1.900968867902419e-05,
      "loss": 0.6323,
      "step": 1873
    },
    {
      "epoch": 0.16871862972382903,
      "grad_norm": 0.716470349795427,
      "learning_rate": 1.9008423016111365e-05,
      "loss": 0.5912,
      "step": 1874
    },
    {
      "epoch": 0.16880866100970088,
      "grad_norm": 0.6745261952373609,
      "learning_rate": 1.9007156587116652e-05,
      "loss": 0.5794,
      "step": 1875
    },
    {
      "epoch": 0.16889869229557272,
      "grad_norm": 0.7218086085592372,
      "learning_rate": 1.9005889392147747e-05,
      "loss": 0.5434,
      "step": 1876
    },
    {
      "epoch": 0.16898872358144454,
      "grad_norm": 0.7436009737594637,
      "learning_rate": 1.9004621431312418e-05,
      "loss": 0.461,
      "step": 1877
    },
    {
      "epoch": 0.16907875486731638,
      "grad_norm": 0.7728214582253866,
      "learning_rate": 1.900335270471849e-05,
      "loss": 0.5419,
      "step": 1878
    },
    {
      "epoch": 0.16916878615318823,
      "grad_norm": 0.7856877633301572,
      "learning_rate": 1.900208321247386e-05,
      "loss": 0.5569,
      "step": 1879
    },
    {
      "epoch": 0.16925881743906007,
      "grad_norm": 0.7894264769813778,
      "learning_rate": 1.900081295468648e-05,
      "loss": 0.639,
      "step": 1880
    },
    {
      "epoch": 0.16934884872493192,
      "grad_norm": 0.7418337675189975,
      "learning_rate": 1.8999541931464377e-05,
      "loss": 0.4944,
      "step": 1881
    },
    {
      "epoch": 0.16943888001080376,
      "grad_norm": 0.6695621836813456,
      "learning_rate": 1.8998270142915646e-05,
      "loss": 0.5083,
      "step": 1882
    },
    {
      "epoch": 0.1695289112966756,
      "grad_norm": 0.7165880288429539,
      "learning_rate": 1.899699758914843e-05,
      "loss": 0.565,
      "step": 1883
    },
    {
      "epoch": 0.16961894258254742,
      "grad_norm": 0.8382754546090372,
      "learning_rate": 1.899572427027095e-05,
      "loss": 0.5938,
      "step": 1884
    },
    {
      "epoch": 0.16970897386841927,
      "grad_norm": 0.764261716168135,
      "learning_rate": 1.8994450186391496e-05,
      "loss": 0.6159,
      "step": 1885
    },
    {
      "epoch": 0.1697990051542911,
      "grad_norm": 1.0111608467035635,
      "learning_rate": 1.8993175337618413e-05,
      "loss": 0.6602,
      "step": 1886
    },
    {
      "epoch": 0.16988903644016295,
      "grad_norm": 0.7788672178406887,
      "learning_rate": 1.8991899724060115e-05,
      "loss": 0.7109,
      "step": 1887
    },
    {
      "epoch": 0.1699790677260348,
      "grad_norm": 0.7611762997396954,
      "learning_rate": 1.8990623345825084e-05,
      "loss": 0.637,
      "step": 1888
    },
    {
      "epoch": 0.17006909901190664,
      "grad_norm": 0.794307442775971,
      "learning_rate": 1.8989346203021855e-05,
      "loss": 0.547,
      "step": 1889
    },
    {
      "epoch": 0.1701591302977785,
      "grad_norm": 0.9907033706915868,
      "learning_rate": 1.8988068295759044e-05,
      "loss": 0.6154,
      "step": 1890
    },
    {
      "epoch": 0.1702491615836503,
      "grad_norm": 0.6498287867279894,
      "learning_rate": 1.8986789624145327e-05,
      "loss": 0.5486,
      "step": 1891
    },
    {
      "epoch": 0.17033919286952215,
      "grad_norm": 0.7707458494587102,
      "learning_rate": 1.898551018828944e-05,
      "loss": 0.641,
      "step": 1892
    },
    {
      "epoch": 0.170429224155394,
      "grad_norm": 0.6883992364918599,
      "learning_rate": 1.8984229988300184e-05,
      "loss": 0.579,
      "step": 1893
    },
    {
      "epoch": 0.17051925544126584,
      "grad_norm": 1.0187379391681377,
      "learning_rate": 1.8982949024286435e-05,
      "loss": 0.5513,
      "step": 1894
    },
    {
      "epoch": 0.17060928672713768,
      "grad_norm": 0.8353202104210672,
      "learning_rate": 1.898166729635712e-05,
      "loss": 0.6811,
      "step": 1895
    },
    {
      "epoch": 0.17069931801300953,
      "grad_norm": 1.0092670948638462,
      "learning_rate": 1.8980384804621244e-05,
      "loss": 0.6334,
      "step": 1896
    },
    {
      "epoch": 0.17078934929888137,
      "grad_norm": 1.0430655582118902,
      "learning_rate": 1.8979101549187864e-05,
      "loss": 0.5817,
      "step": 1897
    },
    {
      "epoch": 0.17087938058475322,
      "grad_norm": 0.8516943529829034,
      "learning_rate": 1.8977817530166117e-05,
      "loss": 0.6496,
      "step": 1898
    },
    {
      "epoch": 0.17096941187062503,
      "grad_norm": 0.8236680266932584,
      "learning_rate": 1.897653274766519e-05,
      "loss": 0.6183,
      "step": 1899
    },
    {
      "epoch": 0.17105944315649688,
      "grad_norm": 0.813879012786623,
      "learning_rate": 1.8975247201794344e-05,
      "loss": 0.664,
      "step": 1900
    },
    {
      "epoch": 0.17114947444236872,
      "grad_norm": 0.6720613367786663,
      "learning_rate": 1.8973960892662902e-05,
      "loss": 0.5366,
      "step": 1901
    },
    {
      "epoch": 0.17123950572824057,
      "grad_norm": 0.7907680826796321,
      "learning_rate": 1.8972673820380258e-05,
      "loss": 0.5855,
      "step": 1902
    },
    {
      "epoch": 0.1713295370141124,
      "grad_norm": 0.9540045235904634,
      "learning_rate": 1.8971385985055856e-05,
      "loss": 0.6164,
      "step": 1903
    },
    {
      "epoch": 0.17141956829998425,
      "grad_norm": 0.8911109400254679,
      "learning_rate": 1.897009738679922e-05,
      "loss": 0.6863,
      "step": 1904
    },
    {
      "epoch": 0.1715095995858561,
      "grad_norm": 0.8153364986603228,
      "learning_rate": 1.8968808025719934e-05,
      "loss": 0.6684,
      "step": 1905
    },
    {
      "epoch": 0.17159963087172792,
      "grad_norm": 0.6806406071176406,
      "learning_rate": 1.8967517901927642e-05,
      "loss": 0.5184,
      "step": 1906
    },
    {
      "epoch": 0.17168966215759976,
      "grad_norm": 0.8092005473411458,
      "learning_rate": 1.8966227015532062e-05,
      "loss": 0.5692,
      "step": 1907
    },
    {
      "epoch": 0.1717796934434716,
      "grad_norm": 0.9272076571853675,
      "learning_rate": 1.8964935366642966e-05,
      "loss": 0.6369,
      "step": 1908
    },
    {
      "epoch": 0.17186972472934345,
      "grad_norm": 0.6606772749583277,
      "learning_rate": 1.8963642955370203e-05,
      "loss": 0.5753,
      "step": 1909
    },
    {
      "epoch": 0.1719597560152153,
      "grad_norm": 0.6541871550865289,
      "learning_rate": 1.8962349781823676e-05,
      "loss": 0.6077,
      "step": 1910
    },
    {
      "epoch": 0.17204978730108714,
      "grad_norm": 0.8364563316825508,
      "learning_rate": 1.8961055846113358e-05,
      "loss": 0.6062,
      "step": 1911
    },
    {
      "epoch": 0.17213981858695898,
      "grad_norm": 0.6983022160294023,
      "learning_rate": 1.8959761148349287e-05,
      "loss": 0.5511,
      "step": 1912
    },
    {
      "epoch": 0.1722298498728308,
      "grad_norm": 1.097261419697126,
      "learning_rate": 1.8958465688641566e-05,
      "loss": 0.5478,
      "step": 1913
    },
    {
      "epoch": 0.17231988115870264,
      "grad_norm": 0.9221289230808856,
      "learning_rate": 1.8957169467100355e-05,
      "loss": 0.6592,
      "step": 1914
    },
    {
      "epoch": 0.1724099124445745,
      "grad_norm": 0.7180166364530756,
      "learning_rate": 1.8955872483835895e-05,
      "loss": 0.584,
      "step": 1915
    },
    {
      "epoch": 0.17249994373044633,
      "grad_norm": 0.7205254628360789,
      "learning_rate": 1.8954574738958474e-05,
      "loss": 0.5838,
      "step": 1916
    },
    {
      "epoch": 0.17258997501631818,
      "grad_norm": 0.7155182253085464,
      "learning_rate": 1.8953276232578464e-05,
      "loss": 0.5969,
      "step": 1917
    },
    {
      "epoch": 0.17268000630219002,
      "grad_norm": 0.7493235047131998,
      "learning_rate": 1.8951976964806277e-05,
      "loss": 0.6,
      "step": 1918
    },
    {
      "epoch": 0.17277003758806186,
      "grad_norm": 0.6927852294772437,
      "learning_rate": 1.8950676935752414e-05,
      "loss": 0.5787,
      "step": 1919
    },
    {
      "epoch": 0.17286006887393368,
      "grad_norm": 0.830149573299786,
      "learning_rate": 1.8949376145527426e-05,
      "loss": 0.6149,
      "step": 1920
    },
    {
      "epoch": 0.17295010015980553,
      "grad_norm": 0.8757049173890525,
      "learning_rate": 1.8948074594241932e-05,
      "loss": 0.631,
      "step": 1921
    },
    {
      "epoch": 0.17304013144567737,
      "grad_norm": 0.7520028150732303,
      "learning_rate": 1.894677228200662e-05,
      "loss": 0.5193,
      "step": 1922
    },
    {
      "epoch": 0.17313016273154921,
      "grad_norm": 0.5957454858279013,
      "learning_rate": 1.8945469208932235e-05,
      "loss": 0.5248,
      "step": 1923
    },
    {
      "epoch": 0.17322019401742106,
      "grad_norm": 0.9927103151788228,
      "learning_rate": 1.8944165375129596e-05,
      "loss": 0.5796,
      "step": 1924
    },
    {
      "epoch": 0.1733102253032929,
      "grad_norm": 0.8777221865361069,
      "learning_rate": 1.894286078070958e-05,
      "loss": 0.5689,
      "step": 1925
    },
    {
      "epoch": 0.17340025658916475,
      "grad_norm": 0.6578366628524883,
      "learning_rate": 1.8941555425783132e-05,
      "loss": 0.6057,
      "step": 1926
    },
    {
      "epoch": 0.17349028787503656,
      "grad_norm": 0.8072654867876298,
      "learning_rate": 1.8940249310461254e-05,
      "loss": 0.5511,
      "step": 1927
    },
    {
      "epoch": 0.1735803191609084,
      "grad_norm": 0.9516547788422929,
      "learning_rate": 1.8938942434855027e-05,
      "loss": 0.6603,
      "step": 1928
    },
    {
      "epoch": 0.17367035044678025,
      "grad_norm": 0.9168367224561398,
      "learning_rate": 1.893763479907558e-05,
      "loss": 0.6039,
      "step": 1929
    },
    {
      "epoch": 0.1737603817326521,
      "grad_norm": 0.7757498987887146,
      "learning_rate": 1.8936326403234125e-05,
      "loss": 0.5565,
      "step": 1930
    },
    {
      "epoch": 0.17385041301852394,
      "grad_norm": 0.9748580917000337,
      "learning_rate": 1.893501724744192e-05,
      "loss": 0.6707,
      "step": 1931
    },
    {
      "epoch": 0.17394044430439579,
      "grad_norm": 0.876267553689142,
      "learning_rate": 1.8933707331810304e-05,
      "loss": 0.5765,
      "step": 1932
    },
    {
      "epoch": 0.17403047559026763,
      "grad_norm": 0.7320312289904757,
      "learning_rate": 1.8932396656450667e-05,
      "loss": 0.6626,
      "step": 1933
    },
    {
      "epoch": 0.17412050687613945,
      "grad_norm": 0.736763068363099,
      "learning_rate": 1.893108522147447e-05,
      "loss": 0.6702,
      "step": 1934
    },
    {
      "epoch": 0.1742105381620113,
      "grad_norm": 0.7378568725855978,
      "learning_rate": 1.892977302699324e-05,
      "loss": 0.5743,
      "step": 1935
    },
    {
      "epoch": 0.17430056944788314,
      "grad_norm": 0.6611478086043292,
      "learning_rate": 1.892846007311857e-05,
      "loss": 0.6221,
      "step": 1936
    },
    {
      "epoch": 0.17439060073375498,
      "grad_norm": 1.0218115214985652,
      "learning_rate": 1.892714635996211e-05,
      "loss": 0.5947,
      "step": 1937
    },
    {
      "epoch": 0.17448063201962682,
      "grad_norm": 0.6376704395409032,
      "learning_rate": 1.8925831887635582e-05,
      "loss": 0.5977,
      "step": 1938
    },
    {
      "epoch": 0.17457066330549867,
      "grad_norm": 1.6130835044328127,
      "learning_rate": 1.8924516656250765e-05,
      "loss": 0.5615,
      "step": 1939
    },
    {
      "epoch": 0.1746606945913705,
      "grad_norm": 0.6282525044506931,
      "learning_rate": 1.8923200665919514e-05,
      "loss": 0.594,
      "step": 1940
    },
    {
      "epoch": 0.17475072587724233,
      "grad_norm": 0.6513925903791485,
      "learning_rate": 1.8921883916753733e-05,
      "loss": 0.6065,
      "step": 1941
    },
    {
      "epoch": 0.17484075716311417,
      "grad_norm": 0.7549771803204316,
      "learning_rate": 1.8920566408865406e-05,
      "loss": 0.6393,
      "step": 1942
    },
    {
      "epoch": 0.17493078844898602,
      "grad_norm": 0.8165171219183941,
      "learning_rate": 1.8919248142366572e-05,
      "loss": 0.5974,
      "step": 1943
    },
    {
      "epoch": 0.17502081973485786,
      "grad_norm": 0.6526350146012139,
      "learning_rate": 1.8917929117369337e-05,
      "loss": 0.5612,
      "step": 1944
    },
    {
      "epoch": 0.1751108510207297,
      "grad_norm": 0.7563564036812699,
      "learning_rate": 1.8916609333985873e-05,
      "loss": 0.5275,
      "step": 1945
    },
    {
      "epoch": 0.17520088230660155,
      "grad_norm": 0.815441651420713,
      "learning_rate": 1.8915288792328414e-05,
      "loss": 0.5896,
      "step": 1946
    },
    {
      "epoch": 0.1752909135924734,
      "grad_norm": 0.9796849721790234,
      "learning_rate": 1.891396749250926e-05,
      "loss": 0.6023,
      "step": 1947
    },
    {
      "epoch": 0.1753809448783452,
      "grad_norm": 0.7411927792064911,
      "learning_rate": 1.891264543464078e-05,
      "loss": 0.5788,
      "step": 1948
    },
    {
      "epoch": 0.17547097616421706,
      "grad_norm": 0.7997307963878423,
      "learning_rate": 1.8911322618835393e-05,
      "loss": 0.7108,
      "step": 1949
    },
    {
      "epoch": 0.1755610074500889,
      "grad_norm": 1.128632402412017,
      "learning_rate": 1.8909999045205597e-05,
      "loss": 0.6115,
      "step": 1950
    },
    {
      "epoch": 0.17565103873596075,
      "grad_norm": 0.7825666750839759,
      "learning_rate": 1.890867471386395e-05,
      "loss": 0.5682,
      "step": 1951
    },
    {
      "epoch": 0.1757410700218326,
      "grad_norm": 0.9496272085422609,
      "learning_rate": 1.890734962492308e-05,
      "loss": 0.6511,
      "step": 1952
    },
    {
      "epoch": 0.17583110130770443,
      "grad_norm": 1.0215982564532884,
      "learning_rate": 1.890602377849566e-05,
      "loss": 0.5812,
      "step": 1953
    },
    {
      "epoch": 0.17592113259357628,
      "grad_norm": 0.931538279546628,
      "learning_rate": 1.8904697174694447e-05,
      "loss": 0.6134,
      "step": 1954
    },
    {
      "epoch": 0.1760111638794481,
      "grad_norm": 0.7237748040060428,
      "learning_rate": 1.890336981363226e-05,
      "loss": 0.6166,
      "step": 1955
    },
    {
      "epoch": 0.17610119516531994,
      "grad_norm": 0.9195639570504558,
      "learning_rate": 1.8902041695421976e-05,
      "loss": 0.6496,
      "step": 1956
    },
    {
      "epoch": 0.17619122645119178,
      "grad_norm": 0.5760160617959121,
      "learning_rate": 1.8900712820176537e-05,
      "loss": 0.5705,
      "step": 1957
    },
    {
      "epoch": 0.17628125773706363,
      "grad_norm": 0.8199816055536139,
      "learning_rate": 1.8899383188008952e-05,
      "loss": 0.5147,
      "step": 1958
    },
    {
      "epoch": 0.17637128902293547,
      "grad_norm": 0.8049460244332651,
      "learning_rate": 1.8898052799032295e-05,
      "loss": 0.5154,
      "step": 1959
    },
    {
      "epoch": 0.17646132030880732,
      "grad_norm": 0.7966313044277612,
      "learning_rate": 1.8896721653359703e-05,
      "loss": 0.568,
      "step": 1960
    },
    {
      "epoch": 0.17655135159467916,
      "grad_norm": 0.9526890163881213,
      "learning_rate": 1.8895389751104377e-05,
      "loss": 0.5629,
      "step": 1961
    },
    {
      "epoch": 0.17664138288055098,
      "grad_norm": 0.9202540739135734,
      "learning_rate": 1.8894057092379587e-05,
      "loss": 0.6158,
      "step": 1962
    },
    {
      "epoch": 0.17673141416642282,
      "grad_norm": 0.8464835822997995,
      "learning_rate": 1.8892723677298653e-05,
      "loss": 0.7389,
      "step": 1963
    },
    {
      "epoch": 0.17682144545229467,
      "grad_norm": 0.6587655622553947,
      "learning_rate": 1.8891389505974982e-05,
      "loss": 0.4862,
      "step": 1964
    },
    {
      "epoch": 0.1769114767381665,
      "grad_norm": 0.7670844423446047,
      "learning_rate": 1.889005457852202e-05,
      "loss": 0.5639,
      "step": 1965
    },
    {
      "epoch": 0.17700150802403836,
      "grad_norm": 0.7426937311227302,
      "learning_rate": 1.8888718895053302e-05,
      "loss": 0.6336,
      "step": 1966
    },
    {
      "epoch": 0.1770915393099102,
      "grad_norm": 0.7045732756428514,
      "learning_rate": 1.8887382455682406e-05,
      "loss": 0.6086,
      "step": 1967
    },
    {
      "epoch": 0.17718157059578205,
      "grad_norm": 0.5777170564488001,
      "learning_rate": 1.888604526052299e-05,
      "loss": 0.5011,
      "step": 1968
    },
    {
      "epoch": 0.17727160188165386,
      "grad_norm": 0.8110045740588675,
      "learning_rate": 1.8884707309688766e-05,
      "loss": 0.5978,
      "step": 1969
    },
    {
      "epoch": 0.1773616331675257,
      "grad_norm": 0.8712327404177747,
      "learning_rate": 1.8883368603293516e-05,
      "loss": 0.5932,
      "step": 1970
    },
    {
      "epoch": 0.17745166445339755,
      "grad_norm": 0.7587048954982233,
      "learning_rate": 1.8882029141451083e-05,
      "loss": 0.6389,
      "step": 1971
    },
    {
      "epoch": 0.1775416957392694,
      "grad_norm": 0.7050677451864517,
      "learning_rate": 1.888068892427538e-05,
      "loss": 0.5928,
      "step": 1972
    },
    {
      "epoch": 0.17763172702514124,
      "grad_norm": 0.5921344345903647,
      "learning_rate": 1.8879347951880372e-05,
      "loss": 0.5706,
      "step": 1973
    },
    {
      "epoch": 0.17772175831101308,
      "grad_norm": 0.8110576836489933,
      "learning_rate": 1.88780062243801e-05,
      "loss": 0.6104,
      "step": 1974
    },
    {
      "epoch": 0.17781178959688493,
      "grad_norm": 0.678166589912071,
      "learning_rate": 1.887666374188867e-05,
      "loss": 0.5418,
      "step": 1975
    },
    {
      "epoch": 0.17790182088275674,
      "grad_norm": 0.8911045730423339,
      "learning_rate": 1.887532050452024e-05,
      "loss": 0.4379,
      "step": 1976
    },
    {
      "epoch": 0.1779918521686286,
      "grad_norm": 1.4115310834236585,
      "learning_rate": 1.8873976512389044e-05,
      "loss": 0.6351,
      "step": 1977
    },
    {
      "epoch": 0.17808188345450043,
      "grad_norm": 0.7029128667183135,
      "learning_rate": 1.887263176560938e-05,
      "loss": 0.6519,
      "step": 1978
    },
    {
      "epoch": 0.17817191474037228,
      "grad_norm": 0.6743942211106871,
      "learning_rate": 1.8871286264295596e-05,
      "loss": 0.6055,
      "step": 1979
    },
    {
      "epoch": 0.17826194602624412,
      "grad_norm": 0.712511230617474,
      "learning_rate": 1.886994000856212e-05,
      "loss": 0.5798,
      "step": 1980
    },
    {
      "epoch": 0.17835197731211597,
      "grad_norm": 0.7622676425826976,
      "learning_rate": 1.8868592998523437e-05,
      "loss": 0.5242,
      "step": 1981
    },
    {
      "epoch": 0.1784420085979878,
      "grad_norm": 0.8254925433250739,
      "learning_rate": 1.88672452342941e-05,
      "loss": 0.6031,
      "step": 1982
    },
    {
      "epoch": 0.17853203988385963,
      "grad_norm": 0.742210716302854,
      "learning_rate": 1.8865896715988725e-05,
      "loss": 0.5458,
      "step": 1983
    },
    {
      "epoch": 0.17862207116973147,
      "grad_norm": 0.8987381215822087,
      "learning_rate": 1.8864547443721984e-05,
      "loss": 0.5327,
      "step": 1984
    },
    {
      "epoch": 0.17871210245560332,
      "grad_norm": 0.8097064931479102,
      "learning_rate": 1.8863197417608627e-05,
      "loss": 0.548,
      "step": 1985
    },
    {
      "epoch": 0.17880213374147516,
      "grad_norm": 0.9432881280839341,
      "learning_rate": 1.8861846637763458e-05,
      "loss": 0.6974,
      "step": 1986
    },
    {
      "epoch": 0.178892165027347,
      "grad_norm": 0.8171837362181796,
      "learning_rate": 1.8860495104301346e-05,
      "loss": 0.5743,
      "step": 1987
    },
    {
      "epoch": 0.17898219631321885,
      "grad_norm": 0.7246371139696803,
      "learning_rate": 1.8859142817337228e-05,
      "loss": 0.6503,
      "step": 1988
    },
    {
      "epoch": 0.1790722275990907,
      "grad_norm": 0.866908196411554,
      "learning_rate": 1.8857789776986105e-05,
      "loss": 0.6739,
      "step": 1989
    },
    {
      "epoch": 0.1791622588849625,
      "grad_norm": 1.0116606867071951,
      "learning_rate": 1.8856435983363043e-05,
      "loss": 0.6171,
      "step": 1990
    },
    {
      "epoch": 0.17925229017083436,
      "grad_norm": 0.7308807539943363,
      "learning_rate": 1.885508143658316e-05,
      "loss": 0.581,
      "step": 1991
    },
    {
      "epoch": 0.1793423214567062,
      "grad_norm": 0.8882389721359037,
      "learning_rate": 1.8853726136761655e-05,
      "loss": 0.6601,
      "step": 1992
    },
    {
      "epoch": 0.17943235274257804,
      "grad_norm": 0.7290360582745269,
      "learning_rate": 1.8852370084013783e-05,
      "loss": 0.5844,
      "step": 1993
    },
    {
      "epoch": 0.1795223840284499,
      "grad_norm": 0.7564843936258203,
      "learning_rate": 1.8851013278454863e-05,
      "loss": 0.5917,
      "step": 1994
    },
    {
      "epoch": 0.17961241531432173,
      "grad_norm": 0.8735736681643171,
      "learning_rate": 1.8849655720200274e-05,
      "loss": 0.6762,
      "step": 1995
    },
    {
      "epoch": 0.17970244660019358,
      "grad_norm": 0.7826961805714123,
      "learning_rate": 1.884829740936547e-05,
      "loss": 0.5367,
      "step": 1996
    },
    {
      "epoch": 0.17979247788606542,
      "grad_norm": 1.1123775381048124,
      "learning_rate": 1.884693834606596e-05,
      "loss": 0.6316,
      "step": 1997
    },
    {
      "epoch": 0.17988250917193724,
      "grad_norm": 0.6145521526958899,
      "learning_rate": 1.8845578530417318e-05,
      "loss": 0.5751,
      "step": 1998
    },
    {
      "epoch": 0.17997254045780908,
      "grad_norm": 0.6068770392303432,
      "learning_rate": 1.8844217962535185e-05,
      "loss": 0.5573,
      "step": 1999
    },
    {
      "epoch": 0.18006257174368093,
      "grad_norm": 0.5676568970646063,
      "learning_rate": 1.884285664253527e-05,
      "loss": 0.4845,
      "step": 2000
    },
    {
      "epoch": 0.18015260302955277,
      "grad_norm": 0.7939691204733337,
      "learning_rate": 1.884149457053333e-05,
      "loss": 0.6042,
      "step": 2001
    },
    {
      "epoch": 0.18024263431542462,
      "grad_norm": 0.6645244113702894,
      "learning_rate": 1.8840131746645202e-05,
      "loss": 0.5897,
      "step": 2002
    },
    {
      "epoch": 0.18033266560129646,
      "grad_norm": 1.0110323004834023,
      "learning_rate": 1.883876817098678e-05,
      "loss": 0.6127,
      "step": 2003
    },
    {
      "epoch": 0.1804226968871683,
      "grad_norm": 0.8299444415455001,
      "learning_rate": 1.8837403843674026e-05,
      "loss": 0.5814,
      "step": 2004
    },
    {
      "epoch": 0.18051272817304012,
      "grad_norm": 1.057176562847274,
      "learning_rate": 1.8836038764822963e-05,
      "loss": 0.5645,
      "step": 2005
    },
    {
      "epoch": 0.18060275945891197,
      "grad_norm": 0.6745102034376309,
      "learning_rate": 1.8834672934549677e-05,
      "loss": 0.5106,
      "step": 2006
    },
    {
      "epoch": 0.1806927907447838,
      "grad_norm": 0.7554111887800264,
      "learning_rate": 1.8833306352970316e-05,
      "loss": 0.5564,
      "step": 2007
    },
    {
      "epoch": 0.18078282203065565,
      "grad_norm": 0.9773900850097842,
      "learning_rate": 1.8831939020201097e-05,
      "loss": 0.6053,
      "step": 2008
    },
    {
      "epoch": 0.1808728533165275,
      "grad_norm": 0.6292281204628801,
      "learning_rate": 1.8830570936358304e-05,
      "loss": 0.6197,
      "step": 2009
    },
    {
      "epoch": 0.18096288460239934,
      "grad_norm": 0.8083898646404597,
      "learning_rate": 1.8829202101558273e-05,
      "loss": 0.6019,
      "step": 2010
    },
    {
      "epoch": 0.1810529158882712,
      "grad_norm": 0.7723106605430243,
      "learning_rate": 1.882783251591741e-05,
      "loss": 0.5732,
      "step": 2011
    },
    {
      "epoch": 0.181142947174143,
      "grad_norm": 0.690051584779848,
      "learning_rate": 1.8826462179552194e-05,
      "loss": 0.5865,
      "step": 2012
    },
    {
      "epoch": 0.18123297846001485,
      "grad_norm": 0.6543602425278213,
      "learning_rate": 1.8825091092579152e-05,
      "loss": 0.6143,
      "step": 2013
    },
    {
      "epoch": 0.1813230097458867,
      "grad_norm": 0.681066602065779,
      "learning_rate": 1.882371925511488e-05,
      "loss": 0.5884,
      "step": 2014
    },
    {
      "epoch": 0.18141304103175854,
      "grad_norm": 1.0367319618378064,
      "learning_rate": 1.882234666727605e-05,
      "loss": 0.6306,
      "step": 2015
    },
    {
      "epoch": 0.18150307231763038,
      "grad_norm": 0.7598888369440595,
      "learning_rate": 1.8820973329179376e-05,
      "loss": 0.5266,
      "step": 2016
    },
    {
      "epoch": 0.18159310360350223,
      "grad_norm": 0.6993370517833344,
      "learning_rate": 1.881959924094165e-05,
      "loss": 0.5848,
      "step": 2017
    },
    {
      "epoch": 0.18168313488937407,
      "grad_norm": 0.7655607141193684,
      "learning_rate": 1.8818224402679735e-05,
      "loss": 0.5821,
      "step": 2018
    },
    {
      "epoch": 0.1817731661752459,
      "grad_norm": 0.8153919558875381,
      "learning_rate": 1.881684881451054e-05,
      "loss": 0.604,
      "step": 2019
    },
    {
      "epoch": 0.18186319746111773,
      "grad_norm": 0.6790480026675546,
      "learning_rate": 1.8815472476551046e-05,
      "loss": 0.5749,
      "step": 2020
    },
    {
      "epoch": 0.18195322874698958,
      "grad_norm": 0.8725223341520039,
      "learning_rate": 1.8814095388918298e-05,
      "loss": 0.6475,
      "step": 2021
    },
    {
      "epoch": 0.18204326003286142,
      "grad_norm": 0.729221818773242,
      "learning_rate": 1.88127175517294e-05,
      "loss": 0.5634,
      "step": 2022
    },
    {
      "epoch": 0.18213329131873326,
      "grad_norm": 0.7853024564871491,
      "learning_rate": 1.8811338965101535e-05,
      "loss": 0.6078,
      "step": 2023
    },
    {
      "epoch": 0.1822233226046051,
      "grad_norm": 0.7623831763857198,
      "learning_rate": 1.8809959629151935e-05,
      "loss": 0.638,
      "step": 2024
    },
    {
      "epoch": 0.18231335389047695,
      "grad_norm": 0.7039964958219896,
      "learning_rate": 1.8808579543997892e-05,
      "loss": 0.6518,
      "step": 2025
    },
    {
      "epoch": 0.18240338517634877,
      "grad_norm": 0.8427665164174741,
      "learning_rate": 1.880719870975678e-05,
      "loss": 0.6248,
      "step": 2026
    },
    {
      "epoch": 0.18249341646222061,
      "grad_norm": 0.629923941463633,
      "learning_rate": 1.880581712654602e-05,
      "loss": 0.5746,
      "step": 2027
    },
    {
      "epoch": 0.18258344774809246,
      "grad_norm": 0.7443321199206293,
      "learning_rate": 1.8804434794483103e-05,
      "loss": 0.5813,
      "step": 2028
    },
    {
      "epoch": 0.1826734790339643,
      "grad_norm": 0.7192188465095912,
      "learning_rate": 1.8803051713685583e-05,
      "loss": 0.5326,
      "step": 2029
    },
    {
      "epoch": 0.18276351031983615,
      "grad_norm": 0.7707240390865779,
      "learning_rate": 1.880166788427108e-05,
      "loss": 0.62,
      "step": 2030
    },
    {
      "epoch": 0.182853541605708,
      "grad_norm": 0.9984236575803918,
      "learning_rate": 1.8800283306357276e-05,
      "loss": 0.5578,
      "step": 2031
    },
    {
      "epoch": 0.18294357289157984,
      "grad_norm": 0.7776575722193876,
      "learning_rate": 1.8798897980061913e-05,
      "loss": 0.4723,
      "step": 2032
    },
    {
      "epoch": 0.18303360417745165,
      "grad_norm": 0.8098376853058284,
      "learning_rate": 1.8797511905502802e-05,
      "loss": 0.5437,
      "step": 2033
    },
    {
      "epoch": 0.1831236354633235,
      "grad_norm": 0.6933906540302575,
      "learning_rate": 1.879612508279782e-05,
      "loss": 0.4981,
      "step": 2034
    },
    {
      "epoch": 0.18321366674919534,
      "grad_norm": 0.7375692540688228,
      "learning_rate": 1.879473751206489e-05,
      "loss": 0.603,
      "step": 2035
    },
    {
      "epoch": 0.1833036980350672,
      "grad_norm": 0.8549864748755248,
      "learning_rate": 1.879334919342203e-05,
      "loss": 0.7064,
      "step": 2036
    },
    {
      "epoch": 0.18339372932093903,
      "grad_norm": 0.6077469513310801,
      "learning_rate": 1.879196012698729e-05,
      "loss": 0.5533,
      "step": 2037
    },
    {
      "epoch": 0.18348376060681088,
      "grad_norm": 0.7371510010027886,
      "learning_rate": 1.8790570312878803e-05,
      "loss": 0.5607,
      "step": 2038
    },
    {
      "epoch": 0.18357379189268272,
      "grad_norm": 0.664412609762603,
      "learning_rate": 1.8789179751214756e-05,
      "loss": 0.5941,
      "step": 2039
    },
    {
      "epoch": 0.18366382317855454,
      "grad_norm": 0.6995885962965664,
      "learning_rate": 1.878778844211341e-05,
      "loss": 0.5845,
      "step": 2040
    },
    {
      "epoch": 0.18375385446442638,
      "grad_norm": 0.846314675609711,
      "learning_rate": 1.8786396385693074e-05,
      "loss": 0.6252,
      "step": 2041
    },
    {
      "epoch": 0.18384388575029822,
      "grad_norm": 0.6844973530423768,
      "learning_rate": 1.8785003582072136e-05,
      "loss": 0.5549,
      "step": 2042
    },
    {
      "epoch": 0.18393391703617007,
      "grad_norm": 1.0264078352267796,
      "learning_rate": 1.8783610031369037e-05,
      "loss": 0.5138,
      "step": 2043
    },
    {
      "epoch": 0.1840239483220419,
      "grad_norm": 0.5956607497447219,
      "learning_rate": 1.8782215733702286e-05,
      "loss": 0.4966,
      "step": 2044
    },
    {
      "epoch": 0.18411397960791376,
      "grad_norm": 0.7431367933936164,
      "learning_rate": 1.8780820689190458e-05,
      "loss": 0.5688,
      "step": 2045
    },
    {
      "epoch": 0.1842040108937856,
      "grad_norm": 0.8452435701267039,
      "learning_rate": 1.8779424897952185e-05,
      "loss": 0.6643,
      "step": 2046
    },
    {
      "epoch": 0.18429404217965742,
      "grad_norm": 0.9539057311239743,
      "learning_rate": 1.877802836010617e-05,
      "loss": 0.6312,
      "step": 2047
    },
    {
      "epoch": 0.18438407346552926,
      "grad_norm": 0.774035246012776,
      "learning_rate": 1.8776631075771167e-05,
      "loss": 0.5547,
      "step": 2048
    },
    {
      "epoch": 0.1844741047514011,
      "grad_norm": 0.6699814299590281,
      "learning_rate": 1.877523304506601e-05,
      "loss": 0.5217,
      "step": 2049
    },
    {
      "epoch": 0.18456413603727295,
      "grad_norm": 0.6921513953155866,
      "learning_rate": 1.8773834268109587e-05,
      "loss": 0.5009,
      "step": 2050
    },
    {
      "epoch": 0.1846541673231448,
      "grad_norm": 0.8084121196694846,
      "learning_rate": 1.877243474502085e-05,
      "loss": 0.5908,
      "step": 2051
    },
    {
      "epoch": 0.18474419860901664,
      "grad_norm": 0.751032876023506,
      "learning_rate": 1.8771034475918814e-05,
      "loss": 0.5674,
      "step": 2052
    },
    {
      "epoch": 0.18483422989488849,
      "grad_norm": 0.7353637317798928,
      "learning_rate": 1.8769633460922557e-05,
      "loss": 0.6336,
      "step": 2053
    },
    {
      "epoch": 0.1849242611807603,
      "grad_norm": 0.8679955696065998,
      "learning_rate": 1.8768231700151233e-05,
      "loss": 0.6601,
      "step": 2054
    },
    {
      "epoch": 0.18501429246663215,
      "grad_norm": 0.7769515935280903,
      "learning_rate": 1.8766829193724035e-05,
      "loss": 0.6004,
      "step": 2055
    },
    {
      "epoch": 0.185104323752504,
      "grad_norm": 0.7390115148216994,
      "learning_rate": 1.8765425941760237e-05,
      "loss": 0.5651,
      "step": 2056
    },
    {
      "epoch": 0.18519435503837584,
      "grad_norm": 0.6204160626776866,
      "learning_rate": 1.8764021944379177e-05,
      "loss": 0.5784,
      "step": 2057
    },
    {
      "epoch": 0.18528438632424768,
      "grad_norm": 0.9459940230113371,
      "learning_rate": 1.876261720170025e-05,
      "loss": 0.5797,
      "step": 2058
    },
    {
      "epoch": 0.18537441761011952,
      "grad_norm": 0.7043147591949466,
      "learning_rate": 1.8761211713842914e-05,
      "loss": 0.5342,
      "step": 2059
    },
    {
      "epoch": 0.18546444889599137,
      "grad_norm": 0.7353511765143089,
      "learning_rate": 1.8759805480926693e-05,
      "loss": 0.5581,
      "step": 2060
    },
    {
      "epoch": 0.18555448018186319,
      "grad_norm": 0.7610233624321903,
      "learning_rate": 1.8758398503071175e-05,
      "loss": 0.5655,
      "step": 2061
    },
    {
      "epoch": 0.18564451146773503,
      "grad_norm": 0.7962803000768701,
      "learning_rate": 1.875699078039601e-05,
      "loss": 0.5548,
      "step": 2062
    },
    {
      "epoch": 0.18573454275360687,
      "grad_norm": 0.999148969565624,
      "learning_rate": 1.8755582313020912e-05,
      "loss": 0.5729,
      "step": 2063
    },
    {
      "epoch": 0.18582457403947872,
      "grad_norm": 0.6124501177483913,
      "learning_rate": 1.8754173101065657e-05,
      "loss": 0.4929,
      "step": 2064
    },
    {
      "epoch": 0.18591460532535056,
      "grad_norm": 0.9298812711139702,
      "learning_rate": 1.8752763144650083e-05,
      "loss": 0.628,
      "step": 2065
    },
    {
      "epoch": 0.1860046366112224,
      "grad_norm": 0.7742745566694125,
      "learning_rate": 1.87513524438941e-05,
      "loss": 0.639,
      "step": 2066
    },
    {
      "epoch": 0.18609466789709425,
      "grad_norm": 0.6589451906714285,
      "learning_rate": 1.8749940998917665e-05,
      "loss": 0.5182,
      "step": 2067
    },
    {
      "epoch": 0.18618469918296607,
      "grad_norm": 0.8826519766579667,
      "learning_rate": 1.874852880984082e-05,
      "loss": 0.4987,
      "step": 2068
    },
    {
      "epoch": 0.1862747304688379,
      "grad_norm": 1.0727459183297,
      "learning_rate": 1.874711587678365e-05,
      "loss": 0.5861,
      "step": 2069
    },
    {
      "epoch": 0.18636476175470976,
      "grad_norm": 0.666191494168916,
      "learning_rate": 1.874570219986632e-05,
      "loss": 0.5215,
      "step": 2070
    },
    {
      "epoch": 0.1864547930405816,
      "grad_norm": 0.824769721325589,
      "learning_rate": 1.8744287779209038e-05,
      "loss": 0.5868,
      "step": 2071
    },
    {
      "epoch": 0.18654482432645345,
      "grad_norm": 0.8823437040337427,
      "learning_rate": 1.8742872614932096e-05,
      "loss": 0.5517,
      "step": 2072
    },
    {
      "epoch": 0.1866348556123253,
      "grad_norm": 0.8656985634097286,
      "learning_rate": 1.8741456707155837e-05,
      "loss": 0.6043,
      "step": 2073
    },
    {
      "epoch": 0.18672488689819713,
      "grad_norm": 0.9203068418156332,
      "learning_rate": 1.8740040056000675e-05,
      "loss": 0.6086,
      "step": 2074
    },
    {
      "epoch": 0.18681491818406895,
      "grad_norm": 0.6671882261843144,
      "learning_rate": 1.8738622661587075e-05,
      "loss": 0.5524,
      "step": 2075
    },
    {
      "epoch": 0.1869049494699408,
      "grad_norm": 0.762954896727658,
      "learning_rate": 1.873720452403558e-05,
      "loss": 0.5384,
      "step": 2076
    },
    {
      "epoch": 0.18699498075581264,
      "grad_norm": 0.8368386459074716,
      "learning_rate": 1.8735785643466786e-05,
      "loss": 0.529,
      "step": 2077
    },
    {
      "epoch": 0.18708501204168448,
      "grad_norm": 0.7443885096435188,
      "learning_rate": 1.8734366020001357e-05,
      "loss": 0.6238,
      "step": 2078
    },
    {
      "epoch": 0.18717504332755633,
      "grad_norm": 0.7777984205197863,
      "learning_rate": 1.873294565376002e-05,
      "loss": 0.5434,
      "step": 2079
    },
    {
      "epoch": 0.18726507461342817,
      "grad_norm": 0.8558042508765009,
      "learning_rate": 1.873152454486356e-05,
      "loss": 0.4977,
      "step": 2080
    },
    {
      "epoch": 0.18735510589930002,
      "grad_norm": 0.8090483193188591,
      "learning_rate": 1.8730102693432834e-05,
      "loss": 0.5427,
      "step": 2081
    },
    {
      "epoch": 0.18744513718517183,
      "grad_norm": 0.9197446948047983,
      "learning_rate": 1.8728680099588748e-05,
      "loss": 0.5545,
      "step": 2082
    },
    {
      "epoch": 0.18753516847104368,
      "grad_norm": 0.8454823801480443,
      "learning_rate": 1.872725676345229e-05,
      "loss": 0.5975,
      "step": 2083
    },
    {
      "epoch": 0.18762519975691552,
      "grad_norm": 0.6552800324798067,
      "learning_rate": 1.8725832685144497e-05,
      "loss": 0.5888,
      "step": 2084
    },
    {
      "epoch": 0.18771523104278737,
      "grad_norm": 0.8151180755610357,
      "learning_rate": 1.8724407864786472e-05,
      "loss": 0.6074,
      "step": 2085
    },
    {
      "epoch": 0.1878052623286592,
      "grad_norm": 0.7657729970151491,
      "learning_rate": 1.872298230249939e-05,
      "loss": 0.5117,
      "step": 2086
    },
    {
      "epoch": 0.18789529361453106,
      "grad_norm": 0.7613480862422982,
      "learning_rate": 1.8721555998404475e-05,
      "loss": 0.5434,
      "step": 2087
    },
    {
      "epoch": 0.1879853249004029,
      "grad_norm": 1.01250077484327,
      "learning_rate": 1.872012895262302e-05,
      "loss": 0.5421,
      "step": 2088
    },
    {
      "epoch": 0.18807535618627472,
      "grad_norm": 1.019468771541752,
      "learning_rate": 1.8718701165276386e-05,
      "loss": 0.6517,
      "step": 2089
    },
    {
      "epoch": 0.18816538747214656,
      "grad_norm": 0.7274091899886784,
      "learning_rate": 1.871727263648599e-05,
      "loss": 0.5603,
      "step": 2090
    },
    {
      "epoch": 0.1882554187580184,
      "grad_norm": 0.653613157250197,
      "learning_rate": 1.8715843366373316e-05,
      "loss": 0.5377,
      "step": 2091
    },
    {
      "epoch": 0.18834545004389025,
      "grad_norm": 0.7632341916111545,
      "learning_rate": 1.871441335505991e-05,
      "loss": 0.6491,
      "step": 2092
    },
    {
      "epoch": 0.1884354813297621,
      "grad_norm": 0.7624916745315109,
      "learning_rate": 1.8712982602667384e-05,
      "loss": 0.5881,
      "step": 2093
    },
    {
      "epoch": 0.18852551261563394,
      "grad_norm": 1.1893934505399024,
      "learning_rate": 1.8711551109317403e-05,
      "loss": 0.6262,
      "step": 2094
    },
    {
      "epoch": 0.18861554390150578,
      "grad_norm": 0.75651777926851,
      "learning_rate": 1.871011887513171e-05,
      "loss": 0.5439,
      "step": 2095
    },
    {
      "epoch": 0.1887055751873776,
      "grad_norm": 0.6718390137607755,
      "learning_rate": 1.8708685900232097e-05,
      "loss": 0.5278,
      "step": 2096
    },
    {
      "epoch": 0.18879560647324944,
      "grad_norm": 0.8802645124910464,
      "learning_rate": 1.870725218474043e-05,
      "loss": 0.6137,
      "step": 2097
    },
    {
      "epoch": 0.1888856377591213,
      "grad_norm": 0.7835888780780584,
      "learning_rate": 1.8705817728778626e-05,
      "loss": 0.5794,
      "step": 2098
    },
    {
      "epoch": 0.18897566904499313,
      "grad_norm": 0.7700704933490928,
      "learning_rate": 1.870438253246868e-05,
      "loss": 0.5726,
      "step": 2099
    },
    {
      "epoch": 0.18906570033086498,
      "grad_norm": 0.6512128243611423,
      "learning_rate": 1.8702946595932635e-05,
      "loss": 0.5695,
      "step": 2100
    },
    {
      "epoch": 0.18915573161673682,
      "grad_norm": 0.6924285294240988,
      "learning_rate": 1.870150991929261e-05,
      "loss": 0.4926,
      "step": 2101
    },
    {
      "epoch": 0.18924576290260867,
      "grad_norm": 0.8332255354098496,
      "learning_rate": 1.8700072502670782e-05,
      "loss": 0.5608,
      "step": 2102
    },
    {
      "epoch": 0.1893357941884805,
      "grad_norm": 0.7053311769995199,
      "learning_rate": 1.869863434618938e-05,
      "loss": 0.776,
      "step": 2103
    },
    {
      "epoch": 0.18942582547435233,
      "grad_norm": 0.6777717323946594,
      "learning_rate": 1.8697195449970716e-05,
      "loss": 0.551,
      "step": 2104
    },
    {
      "epoch": 0.18951585676022417,
      "grad_norm": 0.8120796706721493,
      "learning_rate": 1.8695755814137147e-05,
      "loss": 0.5832,
      "step": 2105
    },
    {
      "epoch": 0.18960588804609602,
      "grad_norm": 0.7487709028828724,
      "learning_rate": 1.8694315438811106e-05,
      "loss": 0.527,
      "step": 2106
    },
    {
      "epoch": 0.18969591933196786,
      "grad_norm": 0.751993001627129,
      "learning_rate": 1.869287432411508e-05,
      "loss": 0.614,
      "step": 2107
    },
    {
      "epoch": 0.1897859506178397,
      "grad_norm": 1.0296443101336161,
      "learning_rate": 1.8691432470171627e-05,
      "loss": 0.7009,
      "step": 2108
    },
    {
      "epoch": 0.18987598190371155,
      "grad_norm": 0.7803186039995061,
      "learning_rate": 1.8689989877103358e-05,
      "loss": 0.622,
      "step": 2109
    },
    {
      "epoch": 0.1899660131895834,
      "grad_norm": 0.6810536341046061,
      "learning_rate": 1.8688546545032953e-05,
      "loss": 0.5627,
      "step": 2110
    },
    {
      "epoch": 0.1900560444754552,
      "grad_norm": 0.7668109634189962,
      "learning_rate": 1.8687102474083156e-05,
      "loss": 0.5693,
      "step": 2111
    },
    {
      "epoch": 0.19014607576132705,
      "grad_norm": 0.8437162369408924,
      "learning_rate": 1.868565766437677e-05,
      "loss": 0.6238,
      "step": 2112
    },
    {
      "epoch": 0.1902361070471989,
      "grad_norm": 0.6781926333663755,
      "learning_rate": 1.8684212116036666e-05,
      "loss": 0.5913,
      "step": 2113
    },
    {
      "epoch": 0.19032613833307074,
      "grad_norm": 0.7198025910628891,
      "learning_rate": 1.8682765829185764e-05,
      "loss": 0.5719,
      "step": 2114
    },
    {
      "epoch": 0.1904161696189426,
      "grad_norm": 0.6553264162926117,
      "learning_rate": 1.868131880394707e-05,
      "loss": 0.5328,
      "step": 2115
    },
    {
      "epoch": 0.19050620090481443,
      "grad_norm": 0.6644533113492441,
      "learning_rate": 1.8679871040443632e-05,
      "loss": 0.5057,
      "step": 2116
    },
    {
      "epoch": 0.19059623219068628,
      "grad_norm": 0.7587606069957347,
      "learning_rate": 1.8678422538798573e-05,
      "loss": 0.6428,
      "step": 2117
    },
    {
      "epoch": 0.1906862634765581,
      "grad_norm": 0.579684632861639,
      "learning_rate": 1.867697329913507e-05,
      "loss": 0.5241,
      "step": 2118
    },
    {
      "epoch": 0.19077629476242994,
      "grad_norm": 0.6902923040890241,
      "learning_rate": 1.867552332157637e-05,
      "loss": 0.6377,
      "step": 2119
    },
    {
      "epoch": 0.19086632604830178,
      "grad_norm": 0.6233472203584672,
      "learning_rate": 1.867407260624578e-05,
      "loss": 0.4896,
      "step": 2120
    },
    {
      "epoch": 0.19095635733417363,
      "grad_norm": 0.6888230697117541,
      "learning_rate": 1.8672621153266674e-05,
      "loss": 0.5523,
      "step": 2121
    },
    {
      "epoch": 0.19104638862004547,
      "grad_norm": 0.6566643544448376,
      "learning_rate": 1.8671168962762474e-05,
      "loss": 0.5219,
      "step": 2122
    },
    {
      "epoch": 0.19113641990591732,
      "grad_norm": 0.7074774634351381,
      "learning_rate": 1.8669716034856685e-05,
      "loss": 0.5703,
      "step": 2123
    },
    {
      "epoch": 0.19122645119178916,
      "grad_norm": 1.1435745688117913,
      "learning_rate": 1.8668262369672857e-05,
      "loss": 0.5394,
      "step": 2124
    },
    {
      "epoch": 0.19131648247766098,
      "grad_norm": 0.9040781391216519,
      "learning_rate": 1.8666807967334616e-05,
      "loss": 0.6007,
      "step": 2125
    },
    {
      "epoch": 0.19140651376353282,
      "grad_norm": 0.7194047377862574,
      "learning_rate": 1.8665352827965647e-05,
      "loss": 0.5086,
      "step": 2126
    },
    {
      "epoch": 0.19149654504940467,
      "grad_norm": 0.7654042372144885,
      "learning_rate": 1.866389695168969e-05,
      "loss": 0.5714,
      "step": 2127
    },
    {
      "epoch": 0.1915865763352765,
      "grad_norm": 0.6914778597990613,
      "learning_rate": 1.8662440338630557e-05,
      "loss": 0.6905,
      "step": 2128
    },
    {
      "epoch": 0.19167660762114835,
      "grad_norm": 0.7334850071005108,
      "learning_rate": 1.866098298891212e-05,
      "loss": 0.5918,
      "step": 2129
    },
    {
      "epoch": 0.1917666389070202,
      "grad_norm": 0.8368710554353409,
      "learning_rate": 1.865952490265831e-05,
      "loss": 0.57,
      "step": 2130
    },
    {
      "epoch": 0.19185667019289204,
      "grad_norm": 1.8891492472667275,
      "learning_rate": 1.8658066079993125e-05,
      "loss": 0.627,
      "step": 2131
    },
    {
      "epoch": 0.19194670147876386,
      "grad_norm": 0.7000390651360197,
      "learning_rate": 1.8656606521040625e-05,
      "loss": 0.4202,
      "step": 2132
    },
    {
      "epoch": 0.1920367327646357,
      "grad_norm": 0.7938063658186412,
      "learning_rate": 1.8655146225924933e-05,
      "loss": 0.5688,
      "step": 2133
    },
    {
      "epoch": 0.19212676405050755,
      "grad_norm": 0.9150475493478225,
      "learning_rate": 1.8653685194770232e-05,
      "loss": 0.5953,
      "step": 2134
    },
    {
      "epoch": 0.1922167953363794,
      "grad_norm": 0.8399631447691231,
      "learning_rate": 1.8652223427700763e-05,
      "loss": 0.5466,
      "step": 2135
    },
    {
      "epoch": 0.19230682662225124,
      "grad_norm": 0.760860196786983,
      "learning_rate": 1.8650760924840847e-05,
      "loss": 0.6383,
      "step": 2136
    },
    {
      "epoch": 0.19239685790812308,
      "grad_norm": 0.8311098226596746,
      "learning_rate": 1.8649297686314845e-05,
      "loss": 0.6668,
      "step": 2137
    },
    {
      "epoch": 0.19248688919399493,
      "grad_norm": 0.809333173646386,
      "learning_rate": 1.86478337122472e-05,
      "loss": 0.5933,
      "step": 2138
    },
    {
      "epoch": 0.19257692047986674,
      "grad_norm": 0.7673554468957576,
      "learning_rate": 1.864636900276241e-05,
      "loss": 0.5613,
      "step": 2139
    },
    {
      "epoch": 0.1926669517657386,
      "grad_norm": 0.8012517658178524,
      "learning_rate": 1.8644903557985027e-05,
      "loss": 0.6048,
      "step": 2140
    },
    {
      "epoch": 0.19275698305161043,
      "grad_norm": 0.7620028400826642,
      "learning_rate": 1.8643437378039678e-05,
      "loss": 0.5112,
      "step": 2141
    },
    {
      "epoch": 0.19284701433748228,
      "grad_norm": 0.8821456227097884,
      "learning_rate": 1.8641970463051045e-05,
      "loss": 0.5466,
      "step": 2142
    },
    {
      "epoch": 0.19293704562335412,
      "grad_norm": 0.8251501246553101,
      "learning_rate": 1.864050281314388e-05,
      "loss": 0.6075,
      "step": 2143
    },
    {
      "epoch": 0.19302707690922596,
      "grad_norm": 1.0274383917930723,
      "learning_rate": 1.863903442844299e-05,
      "loss": 0.5594,
      "step": 2144
    },
    {
      "epoch": 0.1931171081950978,
      "grad_norm": 0.9270558112169478,
      "learning_rate": 1.863756530907325e-05,
      "loss": 0.5711,
      "step": 2145
    },
    {
      "epoch": 0.19320713948096963,
      "grad_norm": 0.8499247808200302,
      "learning_rate": 1.863609545515959e-05,
      "loss": 0.654,
      "step": 2146
    },
    {
      "epoch": 0.19329717076684147,
      "grad_norm": 0.5624902220110434,
      "learning_rate": 1.863462486682701e-05,
      "loss": 0.5214,
      "step": 2147
    },
    {
      "epoch": 0.19338720205271331,
      "grad_norm": 0.8740082014460108,
      "learning_rate": 1.863315354420057e-05,
      "loss": 0.5208,
      "step": 2148
    },
    {
      "epoch": 0.19347723333858516,
      "grad_norm": 1.0571456345535164,
      "learning_rate": 1.8631681487405395e-05,
      "loss": 0.6472,
      "step": 2149
    },
    {
      "epoch": 0.193567264624457,
      "grad_norm": 0.7826478655283886,
      "learning_rate": 1.8630208696566663e-05,
      "loss": 0.6858,
      "step": 2150
    },
    {
      "epoch": 0.19365729591032885,
      "grad_norm": 0.8192264774295825,
      "learning_rate": 1.8628735171809628e-05,
      "loss": 0.6495,
      "step": 2151
    },
    {
      "epoch": 0.1937473271962007,
      "grad_norm": 0.6283699299641047,
      "learning_rate": 1.862726091325959e-05,
      "loss": 0.5094,
      "step": 2152
    },
    {
      "epoch": 0.1938373584820725,
      "grad_norm": 0.6647824366039695,
      "learning_rate": 1.8625785921041932e-05,
      "loss": 0.4938,
      "step": 2153
    },
    {
      "epoch": 0.19392738976794435,
      "grad_norm": 0.8258007347568814,
      "learning_rate": 1.8624310195282082e-05,
      "loss": 0.5505,
      "step": 2154
    },
    {
      "epoch": 0.1940174210538162,
      "grad_norm": 0.6834459719251241,
      "learning_rate": 1.862283373610554e-05,
      "loss": 0.5924,
      "step": 2155
    },
    {
      "epoch": 0.19410745233968804,
      "grad_norm": 0.7337754893674139,
      "learning_rate": 1.862135654363786e-05,
      "loss": 0.6468,
      "step": 2156
    },
    {
      "epoch": 0.19419748362555989,
      "grad_norm": 0.6599485515482896,
      "learning_rate": 1.8619878618004665e-05,
      "loss": 0.57,
      "step": 2157
    },
    {
      "epoch": 0.19428751491143173,
      "grad_norm": 0.748999238293572,
      "learning_rate": 1.8618399959331642e-05,
      "loss": 0.6799,
      "step": 2158
    },
    {
      "epoch": 0.19437754619730357,
      "grad_norm": 0.8731894881412319,
      "learning_rate": 1.8616920567744534e-05,
      "loss": 0.6435,
      "step": 2159
    },
    {
      "epoch": 0.1944675774831754,
      "grad_norm": 1.0224466748793302,
      "learning_rate": 1.861544044336915e-05,
      "loss": 0.618,
      "step": 2160
    },
    {
      "epoch": 0.19455760876904724,
      "grad_norm": 0.6851521976446276,
      "learning_rate": 1.8613959586331364e-05,
      "loss": 0.5301,
      "step": 2161
    },
    {
      "epoch": 0.19464764005491908,
      "grad_norm": 0.6169979113997998,
      "learning_rate": 1.8612477996757102e-05,
      "loss": 0.6385,
      "step": 2162
    },
    {
      "epoch": 0.19473767134079092,
      "grad_norm": 0.8362667702235149,
      "learning_rate": 1.8610995674772367e-05,
      "loss": 0.6111,
      "step": 2163
    },
    {
      "epoch": 0.19482770262666277,
      "grad_norm": 0.8038801978956714,
      "learning_rate": 1.860951262050321e-05,
      "loss": 0.6017,
      "step": 2164
    },
    {
      "epoch": 0.1949177339125346,
      "grad_norm": 0.6466774318925347,
      "learning_rate": 1.8608028834075754e-05,
      "loss": 0.6596,
      "step": 2165
    },
    {
      "epoch": 0.19500776519840646,
      "grad_norm": 0.9301571207415,
      "learning_rate": 1.8606544315616178e-05,
      "loss": 0.6064,
      "step": 2166
    },
    {
      "epoch": 0.19509779648427827,
      "grad_norm": 0.810031508078924,
      "learning_rate": 1.8605059065250736e-05,
      "loss": 0.6413,
      "step": 2167
    },
    {
      "epoch": 0.19518782777015012,
      "grad_norm": 0.7479899444257346,
      "learning_rate": 1.8603573083105725e-05,
      "loss": 0.5632,
      "step": 2168
    },
    {
      "epoch": 0.19527785905602196,
      "grad_norm": 0.6688410863225229,
      "learning_rate": 1.8602086369307515e-05,
      "loss": 0.498,
      "step": 2169
    },
    {
      "epoch": 0.1953678903418938,
      "grad_norm": 0.6884436711526981,
      "learning_rate": 1.860059892398254e-05,
      "loss": 0.5852,
      "step": 2170
    },
    {
      "epoch": 0.19545792162776565,
      "grad_norm": 0.7235776415723609,
      "learning_rate": 1.859911074725729e-05,
      "loss": 0.5162,
      "step": 2171
    },
    {
      "epoch": 0.1955479529136375,
      "grad_norm": 0.7172231474085331,
      "learning_rate": 1.8597621839258327e-05,
      "loss": 0.5041,
      "step": 2172
    },
    {
      "epoch": 0.19563798419950934,
      "grad_norm": 0.813769847029739,
      "learning_rate": 1.8596132200112263e-05,
      "loss": 0.6215,
      "step": 2173
    },
    {
      "epoch": 0.19572801548538116,
      "grad_norm": 0.6714942760360668,
      "learning_rate": 1.8594641829945777e-05,
      "loss": 0.5209,
      "step": 2174
    },
    {
      "epoch": 0.195818046771253,
      "grad_norm": 0.8431710843093633,
      "learning_rate": 1.8593150728885614e-05,
      "loss": 0.5915,
      "step": 2175
    },
    {
      "epoch": 0.19590807805712485,
      "grad_norm": 1.2533871732500041,
      "learning_rate": 1.8591658897058576e-05,
      "loss": 0.6118,
      "step": 2176
    },
    {
      "epoch": 0.1959981093429967,
      "grad_norm": 0.9306013496241443,
      "learning_rate": 1.8590166334591533e-05,
      "loss": 0.6385,
      "step": 2177
    },
    {
      "epoch": 0.19608814062886853,
      "grad_norm": 0.7015052606157852,
      "learning_rate": 1.8588673041611408e-05,
      "loss": 0.5552,
      "step": 2178
    },
    {
      "epoch": 0.19617817191474038,
      "grad_norm": 0.7386941889099395,
      "learning_rate": 1.8587179018245194e-05,
      "loss": 0.5398,
      "step": 2179
    },
    {
      "epoch": 0.19626820320061222,
      "grad_norm": 0.6397653093588092,
      "learning_rate": 1.8585684264619947e-05,
      "loss": 0.5821,
      "step": 2180
    },
    {
      "epoch": 0.19635823448648404,
      "grad_norm": 1.1668725638748538,
      "learning_rate": 1.858418878086278e-05,
      "loss": 0.6077,
      "step": 2181
    },
    {
      "epoch": 0.19644826577235588,
      "grad_norm": 0.79511681831056,
      "learning_rate": 1.8582692567100866e-05,
      "loss": 0.5993,
      "step": 2182
    },
    {
      "epoch": 0.19653829705822773,
      "grad_norm": 0.668994409741592,
      "learning_rate": 1.8581195623461447e-05,
      "loss": 0.5442,
      "step": 2183
    },
    {
      "epoch": 0.19662832834409957,
      "grad_norm": 0.7575256525141344,
      "learning_rate": 1.8579697950071825e-05,
      "loss": 0.5151,
      "step": 2184
    },
    {
      "epoch": 0.19671835962997142,
      "grad_norm": 1.1501795252816565,
      "learning_rate": 1.8578199547059358e-05,
      "loss": 0.6469,
      "step": 2185
    },
    {
      "epoch": 0.19680839091584326,
      "grad_norm": 0.6115919694907674,
      "learning_rate": 1.857670041455148e-05,
      "loss": 0.5117,
      "step": 2186
    },
    {
      "epoch": 0.1968984222017151,
      "grad_norm": 0.765478610083357,
      "learning_rate": 1.857520055267567e-05,
      "loss": 0.5192,
      "step": 2187
    },
    {
      "epoch": 0.19698845348758692,
      "grad_norm": 0.8093734507322673,
      "learning_rate": 1.8573699961559485e-05,
      "loss": 0.5784,
      "step": 2188
    },
    {
      "epoch": 0.19707848477345877,
      "grad_norm": 1.822129735748228,
      "learning_rate": 1.8572198641330524e-05,
      "loss": 0.5624,
      "step": 2189
    },
    {
      "epoch": 0.1971685160593306,
      "grad_norm": 0.7593470121642889,
      "learning_rate": 1.857069659211647e-05,
      "loss": 0.6641,
      "step": 2190
    },
    {
      "epoch": 0.19725854734520246,
      "grad_norm": 0.8530214652344479,
      "learning_rate": 1.856919381404506e-05,
      "loss": 0.5476,
      "step": 2191
    },
    {
      "epoch": 0.1973485786310743,
      "grad_norm": 0.6165677921365624,
      "learning_rate": 1.8567690307244083e-05,
      "loss": 0.5002,
      "step": 2192
    },
    {
      "epoch": 0.19743860991694615,
      "grad_norm": 0.7440020962939422,
      "learning_rate": 1.8566186071841405e-05,
      "loss": 0.5617,
      "step": 2193
    },
    {
      "epoch": 0.197528641202818,
      "grad_norm": 0.8425451145660428,
      "learning_rate": 1.8564681107964944e-05,
      "loss": 0.6841,
      "step": 2194
    },
    {
      "epoch": 0.1976186724886898,
      "grad_norm": 0.7363020429407643,
      "learning_rate": 1.8563175415742683e-05,
      "loss": 0.6285,
      "step": 2195
    },
    {
      "epoch": 0.19770870377456165,
      "grad_norm": 0.7123451266208091,
      "learning_rate": 1.8561668995302668e-05,
      "loss": 0.5824,
      "step": 2196
    },
    {
      "epoch": 0.1977987350604335,
      "grad_norm": 0.829290280156761,
      "learning_rate": 1.8560161846773002e-05,
      "loss": 0.5746,
      "step": 2197
    },
    {
      "epoch": 0.19788876634630534,
      "grad_norm": 0.9809657633731836,
      "learning_rate": 1.8558653970281862e-05,
      "loss": 0.5575,
      "step": 2198
    },
    {
      "epoch": 0.19797879763217718,
      "grad_norm": 0.794014405499892,
      "learning_rate": 1.855714536595747e-05,
      "loss": 0.7262,
      "step": 2199
    },
    {
      "epoch": 0.19806882891804903,
      "grad_norm": 0.8781770095192235,
      "learning_rate": 1.8555636033928122e-05,
      "loss": 0.5959,
      "step": 2200
    },
    {
      "epoch": 0.19815886020392087,
      "grad_norm": 0.7609809843021976,
      "learning_rate": 1.8554125974322176e-05,
      "loss": 0.5204,
      "step": 2201
    },
    {
      "epoch": 0.19824889148979272,
      "grad_norm": 0.7055078769749974,
      "learning_rate": 1.8552615187268045e-05,
      "loss": 0.6154,
      "step": 2202
    },
    {
      "epoch": 0.19833892277566453,
      "grad_norm": 0.8921484929067421,
      "learning_rate": 1.855110367289421e-05,
      "loss": 0.5906,
      "step": 2203
    },
    {
      "epoch": 0.19842895406153638,
      "grad_norm": 0.7780533113437621,
      "learning_rate": 1.8549591431329202e-05,
      "loss": 0.5383,
      "step": 2204
    },
    {
      "epoch": 0.19851898534740822,
      "grad_norm": 0.7959733234650108,
      "learning_rate": 1.8548078462701637e-05,
      "loss": 0.602,
      "step": 2205
    },
    {
      "epoch": 0.19860901663328007,
      "grad_norm": 0.824797663601872,
      "learning_rate": 1.8546564767140167e-05,
      "loss": 0.6346,
      "step": 2206
    },
    {
      "epoch": 0.1986990479191519,
      "grad_norm": 0.7589858700541556,
      "learning_rate": 1.8545050344773526e-05,
      "loss": 0.587,
      "step": 2207
    },
    {
      "epoch": 0.19878907920502376,
      "grad_norm": 1.4297393118032602,
      "learning_rate": 1.8543535195730495e-05,
      "loss": 0.6176,
      "step": 2208
    },
    {
      "epoch": 0.1988791104908956,
      "grad_norm": 0.7357438814651796,
      "learning_rate": 1.8542019320139926e-05,
      "loss": 0.5673,
      "step": 2209
    },
    {
      "epoch": 0.19896914177676742,
      "grad_norm": 0.7766197232842252,
      "learning_rate": 1.854050271813073e-05,
      "loss": 0.6214,
      "step": 2210
    },
    {
      "epoch": 0.19905917306263926,
      "grad_norm": 0.9520442376677124,
      "learning_rate": 1.8538985389831883e-05,
      "loss": 0.6272,
      "step": 2211
    },
    {
      "epoch": 0.1991492043485111,
      "grad_norm": 0.7942493437503233,
      "learning_rate": 1.8537467335372416e-05,
      "loss": 0.594,
      "step": 2212
    },
    {
      "epoch": 0.19923923563438295,
      "grad_norm": 0.8666814572065724,
      "learning_rate": 1.853594855488142e-05,
      "loss": 0.6864,
      "step": 2213
    },
    {
      "epoch": 0.1993292669202548,
      "grad_norm": 0.7210167068187442,
      "learning_rate": 1.853442904848806e-05,
      "loss": 0.4725,
      "step": 2214
    },
    {
      "epoch": 0.19941929820612664,
      "grad_norm": 0.7923788998652288,
      "learning_rate": 1.8532908816321557e-05,
      "loss": 0.5362,
      "step": 2215
    },
    {
      "epoch": 0.19950932949199848,
      "grad_norm": 0.6670420154818043,
      "learning_rate": 1.853138785851119e-05,
      "loss": 0.5201,
      "step": 2216
    },
    {
      "epoch": 0.1995993607778703,
      "grad_norm": 1.4490732038733458,
      "learning_rate": 1.8529866175186302e-05,
      "loss": 0.5984,
      "step": 2217
    },
    {
      "epoch": 0.19968939206374214,
      "grad_norm": 0.6967284717722043,
      "learning_rate": 1.8528343766476297e-05,
      "loss": 0.5925,
      "step": 2218
    },
    {
      "epoch": 0.199779423349614,
      "grad_norm": 1.2447895046026793,
      "learning_rate": 1.8526820632510644e-05,
      "loss": 0.5024,
      "step": 2219
    },
    {
      "epoch": 0.19986945463548583,
      "grad_norm": 0.8593146200754678,
      "learning_rate": 1.8525296773418865e-05,
      "loss": 0.6874,
      "step": 2220
    },
    {
      "epoch": 0.19995948592135768,
      "grad_norm": 0.7699404891711661,
      "learning_rate": 1.8523772189330557e-05,
      "loss": 0.5708,
      "step": 2221
    },
    {
      "epoch": 0.20004951720722952,
      "grad_norm": 0.7923725218484597,
      "learning_rate": 1.8522246880375372e-05,
      "loss": 0.5588,
      "step": 2222
    },
    {
      "epoch": 0.20013954849310137,
      "grad_norm": 0.8708255846704932,
      "learning_rate": 1.852072084668302e-05,
      "loss": 0.6131,
      "step": 2223
    },
    {
      "epoch": 0.20022957977897318,
      "grad_norm": 0.766144197634259,
      "learning_rate": 1.851919408838327e-05,
      "loss": 0.5224,
      "step": 2224
    },
    {
      "epoch": 0.20031961106484503,
      "grad_norm": 0.6730260835370645,
      "learning_rate": 1.8517666605605973e-05,
      "loss": 0.5797,
      "step": 2225
    },
    {
      "epoch": 0.20040964235071687,
      "grad_norm": 0.906749047271036,
      "learning_rate": 1.8516138398481013e-05,
      "loss": 0.5691,
      "step": 2226
    },
    {
      "epoch": 0.20049967363658872,
      "grad_norm": 0.6933328859275237,
      "learning_rate": 1.851460946713836e-05,
      "loss": 0.5028,
      "step": 2227
    },
    {
      "epoch": 0.20058970492246056,
      "grad_norm": 0.8347746860750002,
      "learning_rate": 1.8513079811708028e-05,
      "loss": 0.6571,
      "step": 2228
    },
    {
      "epoch": 0.2006797362083324,
      "grad_norm": 0.6402324714686077,
      "learning_rate": 1.8511549432320105e-05,
      "loss": 0.5528,
      "step": 2229
    },
    {
      "epoch": 0.20076976749420425,
      "grad_norm": 1.1862307058152204,
      "learning_rate": 1.851001832910473e-05,
      "loss": 0.4876,
      "step": 2230
    },
    {
      "epoch": 0.20085979878007607,
      "grad_norm": 0.7055500739809712,
      "learning_rate": 1.8508486502192115e-05,
      "loss": 0.5502,
      "step": 2231
    },
    {
      "epoch": 0.2009498300659479,
      "grad_norm": 0.6281168319798681,
      "learning_rate": 1.8506953951712524e-05,
      "loss": 0.5482,
      "step": 2232
    },
    {
      "epoch": 0.20103986135181975,
      "grad_norm": 0.7575586465937255,
      "learning_rate": 1.8505420677796285e-05,
      "loss": 0.5357,
      "step": 2233
    },
    {
      "epoch": 0.2011298926376916,
      "grad_norm": 0.9373725441878444,
      "learning_rate": 1.850388668057379e-05,
      "loss": 0.548,
      "step": 2234
    },
    {
      "epoch": 0.20121992392356344,
      "grad_norm": 0.775322277262947,
      "learning_rate": 1.8502351960175492e-05,
      "loss": 0.6238,
      "step": 2235
    },
    {
      "epoch": 0.2013099552094353,
      "grad_norm": 0.919781970251157,
      "learning_rate": 1.8500816516731905e-05,
      "loss": 0.5942,
      "step": 2236
    },
    {
      "epoch": 0.20139998649530713,
      "grad_norm": 0.7148647115194903,
      "learning_rate": 1.8499280350373603e-05,
      "loss": 0.5773,
      "step": 2237
    },
    {
      "epoch": 0.20149001778117895,
      "grad_norm": 0.9498407671103203,
      "learning_rate": 1.849774346123122e-05,
      "loss": 0.533,
      "step": 2238
    },
    {
      "epoch": 0.2015800490670508,
      "grad_norm": 0.7983674983406374,
      "learning_rate": 1.849620584943546e-05,
      "loss": 0.5783,
      "step": 2239
    },
    {
      "epoch": 0.20167008035292264,
      "grad_norm": 0.7694508238709511,
      "learning_rate": 1.8494667515117077e-05,
      "loss": 0.6004,
      "step": 2240
    },
    {
      "epoch": 0.20176011163879448,
      "grad_norm": 0.6934922634921298,
      "learning_rate": 1.8493128458406893e-05,
      "loss": 0.5346,
      "step": 2241
    },
    {
      "epoch": 0.20185014292466633,
      "grad_norm": 0.7306171260476281,
      "learning_rate": 1.849158867943579e-05,
      "loss": 0.6075,
      "step": 2242
    },
    {
      "epoch": 0.20194017421053817,
      "grad_norm": 0.6927086316109144,
      "learning_rate": 1.8490048178334715e-05,
      "loss": 0.6764,
      "step": 2243
    },
    {
      "epoch": 0.20203020549641001,
      "grad_norm": 0.771374520373596,
      "learning_rate": 1.848850695523467e-05,
      "loss": 0.5694,
      "step": 2244
    },
    {
      "epoch": 0.20212023678228183,
      "grad_norm": 0.7995916433420703,
      "learning_rate": 1.8486965010266726e-05,
      "loss": 0.6072,
      "step": 2245
    },
    {
      "epoch": 0.20221026806815368,
      "grad_norm": 0.7011661864067543,
      "learning_rate": 1.8485422343562006e-05,
      "loss": 0.5162,
      "step": 2246
    },
    {
      "epoch": 0.20230029935402552,
      "grad_norm": 0.796758534139028,
      "learning_rate": 1.84838789552517e-05,
      "loss": 0.5653,
      "step": 2247
    },
    {
      "epoch": 0.20239033063989736,
      "grad_norm": 0.6332080845903155,
      "learning_rate": 1.8482334845467062e-05,
      "loss": 0.498,
      "step": 2248
    },
    {
      "epoch": 0.2024803619257692,
      "grad_norm": 0.8653567747607367,
      "learning_rate": 1.8480790014339397e-05,
      "loss": 0.6597,
      "step": 2249
    },
    {
      "epoch": 0.20257039321164105,
      "grad_norm": 0.904687995474251,
      "learning_rate": 1.8479244462000085e-05,
      "loss": 0.6541,
      "step": 2250
    },
    {
      "epoch": 0.2026604244975129,
      "grad_norm": 0.9216838763868813,
      "learning_rate": 1.8477698188580558e-05,
      "loss": 0.606,
      "step": 2251
    },
    {
      "epoch": 0.20275045578338471,
      "grad_norm": 0.7080162236468368,
      "learning_rate": 1.8476151194212316e-05,
      "loss": 0.5162,
      "step": 2252
    },
    {
      "epoch": 0.20284048706925656,
      "grad_norm": 0.8591359786263638,
      "learning_rate": 1.8474603479026912e-05,
      "loss": 0.6556,
      "step": 2253
    },
    {
      "epoch": 0.2029305183551284,
      "grad_norm": 0.858365380909015,
      "learning_rate": 1.8473055043155964e-05,
      "loss": 0.5976,
      "step": 2254
    },
    {
      "epoch": 0.20302054964100025,
      "grad_norm": 0.7022389112628116,
      "learning_rate": 1.8471505886731157e-05,
      "loss": 0.4712,
      "step": 2255
    },
    {
      "epoch": 0.2031105809268721,
      "grad_norm": 0.8170804243965607,
      "learning_rate": 1.8469956009884228e-05,
      "loss": 0.6012,
      "step": 2256
    },
    {
      "epoch": 0.20320061221274394,
      "grad_norm": 0.7434135527058128,
      "learning_rate": 1.846840541274698e-05,
      "loss": 0.6332,
      "step": 2257
    },
    {
      "epoch": 0.20329064349861578,
      "grad_norm": 0.7984647398516526,
      "learning_rate": 1.8466854095451277e-05,
      "loss": 0.6308,
      "step": 2258
    },
    {
      "epoch": 0.2033806747844876,
      "grad_norm": 0.932831787121094,
      "learning_rate": 1.8465302058129045e-05,
      "loss": 0.6114,
      "step": 2259
    },
    {
      "epoch": 0.20347070607035944,
      "grad_norm": 0.7443676903014389,
      "learning_rate": 1.8463749300912268e-05,
      "loss": 0.5324,
      "step": 2260
    },
    {
      "epoch": 0.20356073735623129,
      "grad_norm": 0.6861679476958835,
      "learning_rate": 1.8462195823932997e-05,
      "loss": 0.5702,
      "step": 2261
    },
    {
      "epoch": 0.20365076864210313,
      "grad_norm": 0.7124278206186334,
      "learning_rate": 1.8460641627323337e-05,
      "loss": 0.4741,
      "step": 2262
    },
    {
      "epoch": 0.20374079992797497,
      "grad_norm": 0.6508127067441601,
      "learning_rate": 1.845908671121546e-05,
      "loss": 0.5385,
      "step": 2263
    },
    {
      "epoch": 0.20383083121384682,
      "grad_norm": 0.7404243647609334,
      "learning_rate": 1.8457531075741597e-05,
      "loss": 0.5636,
      "step": 2264
    },
    {
      "epoch": 0.20392086249971866,
      "grad_norm": 0.9820174827630505,
      "learning_rate": 1.8455974721034038e-05,
      "loss": 0.6265,
      "step": 2265
    },
    {
      "epoch": 0.20401089378559048,
      "grad_norm": 0.8812698288947877,
      "learning_rate": 1.845441764722514e-05,
      "loss": 0.7308,
      "step": 2266
    },
    {
      "epoch": 0.20410092507146232,
      "grad_norm": 0.6497043139018899,
      "learning_rate": 1.8452859854447313e-05,
      "loss": 0.5599,
      "step": 2267
    },
    {
      "epoch": 0.20419095635733417,
      "grad_norm": 0.9071923399744448,
      "learning_rate": 1.8451301342833038e-05,
      "loss": 0.5733,
      "step": 2268
    },
    {
      "epoch": 0.204280987643206,
      "grad_norm": 0.8929226331296257,
      "learning_rate": 1.844974211251485e-05,
      "loss": 0.5971,
      "step": 2269
    },
    {
      "epoch": 0.20437101892907786,
      "grad_norm": 0.6846060740923835,
      "learning_rate": 1.8448182163625344e-05,
      "loss": 0.6196,
      "step": 2270
    },
    {
      "epoch": 0.2044610502149497,
      "grad_norm": 0.6998532854719375,
      "learning_rate": 1.844662149629718e-05,
      "loss": 0.5073,
      "step": 2271
    },
    {
      "epoch": 0.20455108150082155,
      "grad_norm": 0.8141734099641271,
      "learning_rate": 1.844506011066308e-05,
      "loss": 0.5514,
      "step": 2272
    },
    {
      "epoch": 0.20464111278669336,
      "grad_norm": 0.7807262525191764,
      "learning_rate": 1.8443498006855825e-05,
      "loss": 0.6089,
      "step": 2273
    },
    {
      "epoch": 0.2047311440725652,
      "grad_norm": 0.8812033006275298,
      "learning_rate": 1.8441935185008258e-05,
      "loss": 0.6445,
      "step": 2274
    },
    {
      "epoch": 0.20482117535843705,
      "grad_norm": 0.7714005940949002,
      "learning_rate": 1.8440371645253278e-05,
      "loss": 0.5488,
      "step": 2275
    },
    {
      "epoch": 0.2049112066443089,
      "grad_norm": 0.8216074511224206,
      "learning_rate": 1.8438807387723853e-05,
      "loss": 0.6018,
      "step": 2276
    },
    {
      "epoch": 0.20500123793018074,
      "grad_norm": 0.9070102940306431,
      "learning_rate": 1.8437242412553006e-05,
      "loss": 0.5768,
      "step": 2277
    },
    {
      "epoch": 0.20509126921605259,
      "grad_norm": 0.8405388236283337,
      "learning_rate": 1.8435676719873828e-05,
      "loss": 0.675,
      "step": 2278
    },
    {
      "epoch": 0.20518130050192443,
      "grad_norm": 0.6104766694655387,
      "learning_rate": 1.8434110309819462e-05,
      "loss": 0.4637,
      "step": 2279
    },
    {
      "epoch": 0.20527133178779625,
      "grad_norm": 0.7714991017855066,
      "learning_rate": 1.8432543182523123e-05,
      "loss": 0.5885,
      "step": 2280
    },
    {
      "epoch": 0.2053613630736681,
      "grad_norm": 0.6541421791898713,
      "learning_rate": 1.843097533811807e-05,
      "loss": 0.6286,
      "step": 2281
    },
    {
      "epoch": 0.20545139435953994,
      "grad_norm": 0.8076417252405519,
      "learning_rate": 1.842940677673764e-05,
      "loss": 0.5642,
      "step": 2282
    },
    {
      "epoch": 0.20554142564541178,
      "grad_norm": 1.1355033410520374,
      "learning_rate": 1.8427837498515225e-05,
      "loss": 0.549,
      "step": 2283
    },
    {
      "epoch": 0.20563145693128362,
      "grad_norm": 0.8570192668903825,
      "learning_rate": 1.8426267503584275e-05,
      "loss": 0.5513,
      "step": 2284
    },
    {
      "epoch": 0.20572148821715547,
      "grad_norm": 0.6868890916629468,
      "learning_rate": 1.8424696792078306e-05,
      "loss": 0.5663,
      "step": 2285
    },
    {
      "epoch": 0.2058115195030273,
      "grad_norm": 0.9287845724215957,
      "learning_rate": 1.8423125364130887e-05,
      "loss": 0.5613,
      "step": 2286
    },
    {
      "epoch": 0.20590155078889913,
      "grad_norm": 1.4246918636197567,
      "learning_rate": 1.842155321987566e-05,
      "loss": 0.6311,
      "step": 2287
    },
    {
      "epoch": 0.20599158207477097,
      "grad_norm": 1.0418049236517777,
      "learning_rate": 1.8419980359446317e-05,
      "loss": 0.6022,
      "step": 2288
    },
    {
      "epoch": 0.20608161336064282,
      "grad_norm": 0.7671426128133243,
      "learning_rate": 1.8418406782976615e-05,
      "loss": 0.5504,
      "step": 2289
    },
    {
      "epoch": 0.20617164464651466,
      "grad_norm": 0.8584438939499852,
      "learning_rate": 1.8416832490600372e-05,
      "loss": 0.5638,
      "step": 2290
    },
    {
      "epoch": 0.2062616759323865,
      "grad_norm": 0.7386946708977487,
      "learning_rate": 1.841525748245147e-05,
      "loss": 0.5537,
      "step": 2291
    },
    {
      "epoch": 0.20635170721825835,
      "grad_norm": 0.6825125974569406,
      "learning_rate": 1.8413681758663847e-05,
      "loss": 0.5537,
      "step": 2292
    },
    {
      "epoch": 0.2064417385041302,
      "grad_norm": 0.7919125118570034,
      "learning_rate": 1.84121053193715e-05,
      "loss": 0.6043,
      "step": 2293
    },
    {
      "epoch": 0.206531769790002,
      "grad_norm": 0.8374970156674049,
      "learning_rate": 1.8410528164708493e-05,
      "loss": 0.5823,
      "step": 2294
    },
    {
      "epoch": 0.20662180107587386,
      "grad_norm": 0.8906118768166462,
      "learning_rate": 1.8408950294808953e-05,
      "loss": 0.6154,
      "step": 2295
    },
    {
      "epoch": 0.2067118323617457,
      "grad_norm": 0.5883039890551965,
      "learning_rate": 1.8407371709807054e-05,
      "loss": 0.4839,
      "step": 2296
    },
    {
      "epoch": 0.20680186364761755,
      "grad_norm": 0.6106926349083974,
      "learning_rate": 1.8405792409837048e-05,
      "loss": 0.5023,
      "step": 2297
    },
    {
      "epoch": 0.2068918949334894,
      "grad_norm": 0.8231457739221097,
      "learning_rate": 1.840421239503323e-05,
      "loss": 0.5426,
      "step": 2298
    },
    {
      "epoch": 0.20698192621936123,
      "grad_norm": 0.754101890913648,
      "learning_rate": 1.8402631665529976e-05,
      "loss": 0.5485,
      "step": 2299
    },
    {
      "epoch": 0.20707195750523308,
      "grad_norm": 0.8483675323183637,
      "learning_rate": 1.8401050221461706e-05,
      "loss": 0.6536,
      "step": 2300
    },
    {
      "epoch": 0.2071619887911049,
      "grad_norm": 0.7575732789944419,
      "learning_rate": 1.839946806296291e-05,
      "loss": 0.5744,
      "step": 2301
    },
    {
      "epoch": 0.20725202007697674,
      "grad_norm": 0.8448082158574879,
      "learning_rate": 1.839788519016813e-05,
      "loss": 0.6444,
      "step": 2302
    },
    {
      "epoch": 0.20734205136284858,
      "grad_norm": 0.6211915244500942,
      "learning_rate": 1.8396301603211987e-05,
      "loss": 0.4905,
      "step": 2303
    },
    {
      "epoch": 0.20743208264872043,
      "grad_norm": 0.6593143208753263,
      "learning_rate": 1.8394717302229133e-05,
      "loss": 0.6426,
      "step": 2304
    },
    {
      "epoch": 0.20752211393459227,
      "grad_norm": 0.8718472264691394,
      "learning_rate": 1.8393132287354314e-05,
      "loss": 0.5444,
      "step": 2305
    },
    {
      "epoch": 0.20761214522046412,
      "grad_norm": 0.7137044747863042,
      "learning_rate": 1.839154655872231e-05,
      "loss": 0.5395,
      "step": 2306
    },
    {
      "epoch": 0.20770217650633596,
      "grad_norm": 0.8213591613479915,
      "learning_rate": 1.8389960116467978e-05,
      "loss": 0.6036,
      "step": 2307
    },
    {
      "epoch": 0.2077922077922078,
      "grad_norm": 0.7346939397646667,
      "learning_rate": 1.8388372960726228e-05,
      "loss": 0.523,
      "step": 2308
    },
    {
      "epoch": 0.20788223907807962,
      "grad_norm": 0.8092007971799869,
      "learning_rate": 1.838678509163203e-05,
      "loss": 0.5683,
      "step": 2309
    },
    {
      "epoch": 0.20797227036395147,
      "grad_norm": 0.833330511129145,
      "learning_rate": 1.8385196509320424e-05,
      "loss": 0.6695,
      "step": 2310
    },
    {
      "epoch": 0.2080623016498233,
      "grad_norm": 0.6752781015534408,
      "learning_rate": 1.8383607213926496e-05,
      "loss": 0.5727,
      "step": 2311
    },
    {
      "epoch": 0.20815233293569516,
      "grad_norm": 1.0670200512345536,
      "learning_rate": 1.8382017205585407e-05,
      "loss": 0.6927,
      "step": 2312
    },
    {
      "epoch": 0.208242364221567,
      "grad_norm": 0.6338858181868595,
      "learning_rate": 1.838042648443237e-05,
      "loss": 0.5765,
      "step": 2313
    },
    {
      "epoch": 0.20833239550743884,
      "grad_norm": 1.0118078114854716,
      "learning_rate": 1.8378835050602663e-05,
      "loss": 0.5952,
      "step": 2314
    },
    {
      "epoch": 0.2084224267933107,
      "grad_norm": 0.8839050812040615,
      "learning_rate": 1.8377242904231618e-05,
      "loss": 0.639,
      "step": 2315
    },
    {
      "epoch": 0.2085124580791825,
      "grad_norm": 0.7298103591166287,
      "learning_rate": 1.8375650045454635e-05,
      "loss": 0.544,
      "step": 2316
    },
    {
      "epoch": 0.20860248936505435,
      "grad_norm": 0.7440342908220792,
      "learning_rate": 1.8374056474407172e-05,
      "loss": 0.6473,
      "step": 2317
    },
    {
      "epoch": 0.2086925206509262,
      "grad_norm": 0.7346760296839292,
      "learning_rate": 1.8372462191224745e-05,
      "loss": 0.5878,
      "step": 2318
    },
    {
      "epoch": 0.20878255193679804,
      "grad_norm": 0.6555063500177539,
      "learning_rate": 1.8370867196042933e-05,
      "loss": 0.5933,
      "step": 2319
    },
    {
      "epoch": 0.20887258322266988,
      "grad_norm": 0.9090103582479898,
      "learning_rate": 1.8369271488997383e-05,
      "loss": 0.5329,
      "step": 2320
    },
    {
      "epoch": 0.20896261450854173,
      "grad_norm": 0.722044111661361,
      "learning_rate": 1.8367675070223783e-05,
      "loss": 0.5893,
      "step": 2321
    },
    {
      "epoch": 0.20905264579441357,
      "grad_norm": 0.9553444798944487,
      "learning_rate": 1.8366077939857903e-05,
      "loss": 0.5253,
      "step": 2322
    },
    {
      "epoch": 0.2091426770802854,
      "grad_norm": 0.7589287707695334,
      "learning_rate": 1.8364480098035558e-05,
      "loss": 0.5906,
      "step": 2323
    },
    {
      "epoch": 0.20923270836615723,
      "grad_norm": 0.833783094589414,
      "learning_rate": 1.8362881544892634e-05,
      "loss": 0.5673,
      "step": 2324
    },
    {
      "epoch": 0.20932273965202908,
      "grad_norm": 0.9584797730402403,
      "learning_rate": 1.836128228056507e-05,
      "loss": 0.5146,
      "step": 2325
    },
    {
      "epoch": 0.20941277093790092,
      "grad_norm": 0.5851134963546163,
      "learning_rate": 1.8359682305188865e-05,
      "loss": 0.5458,
      "step": 2326
    },
    {
      "epoch": 0.20950280222377277,
      "grad_norm": 0.6959956168223557,
      "learning_rate": 1.8358081618900087e-05,
      "loss": 0.4768,
      "step": 2327
    },
    {
      "epoch": 0.2095928335096446,
      "grad_norm": 0.886694867580772,
      "learning_rate": 1.835648022183486e-05,
      "loss": 0.5831,
      "step": 2328
    },
    {
      "epoch": 0.20968286479551645,
      "grad_norm": 0.6718997893353698,
      "learning_rate": 1.8354878114129368e-05,
      "loss": 0.6018,
      "step": 2329
    },
    {
      "epoch": 0.20977289608138827,
      "grad_norm": 0.6590874381827162,
      "learning_rate": 1.835327529591985e-05,
      "loss": 0.5066,
      "step": 2330
    },
    {
      "epoch": 0.20986292736726012,
      "grad_norm": 0.7103661433807698,
      "learning_rate": 1.8351671767342614e-05,
      "loss": 0.5792,
      "step": 2331
    },
    {
      "epoch": 0.20995295865313196,
      "grad_norm": 0.8486692625210378,
      "learning_rate": 1.8350067528534026e-05,
      "loss": 0.6154,
      "step": 2332
    },
    {
      "epoch": 0.2100429899390038,
      "grad_norm": 0.733565449657154,
      "learning_rate": 1.834846257963051e-05,
      "loss": 0.6401,
      "step": 2333
    },
    {
      "epoch": 0.21013302122487565,
      "grad_norm": 0.7558847487488433,
      "learning_rate": 1.8346856920768553e-05,
      "loss": 0.6524,
      "step": 2334
    },
    {
      "epoch": 0.2102230525107475,
      "grad_norm": 0.7075479202411249,
      "learning_rate": 1.83452505520847e-05,
      "loss": 0.5382,
      "step": 2335
    },
    {
      "epoch": 0.21031308379661934,
      "grad_norm": 0.766054599516681,
      "learning_rate": 1.834364347371556e-05,
      "loss": 0.6059,
      "step": 2336
    },
    {
      "epoch": 0.21040311508249115,
      "grad_norm": 0.795311250609109,
      "learning_rate": 1.8342035685797794e-05,
      "loss": 0.5818,
      "step": 2337
    },
    {
      "epoch": 0.210493146368363,
      "grad_norm": 0.8489667185366188,
      "learning_rate": 1.8340427188468136e-05,
      "loss": 0.6544,
      "step": 2338
    },
    {
      "epoch": 0.21058317765423484,
      "grad_norm": 0.7735161125040824,
      "learning_rate": 1.8338817981863372e-05,
      "loss": 0.6487,
      "step": 2339
    },
    {
      "epoch": 0.2106732089401067,
      "grad_norm": 0.7957801272451488,
      "learning_rate": 1.8337208066120347e-05,
      "loss": 0.5613,
      "step": 2340
    },
    {
      "epoch": 0.21076324022597853,
      "grad_norm": 0.703666326546703,
      "learning_rate": 1.8335597441375973e-05,
      "loss": 0.5643,
      "step": 2341
    },
    {
      "epoch": 0.21085327151185038,
      "grad_norm": 0.7176146348825684,
      "learning_rate": 1.8333986107767215e-05,
      "loss": 0.6912,
      "step": 2342
    },
    {
      "epoch": 0.21094330279772222,
      "grad_norm": 0.7280651924401637,
      "learning_rate": 1.8332374065431103e-05,
      "loss": 0.5609,
      "step": 2343
    },
    {
      "epoch": 0.21103333408359404,
      "grad_norm": 0.7673080871848241,
      "learning_rate": 1.833076131450473e-05,
      "loss": 0.5961,
      "step": 2344
    },
    {
      "epoch": 0.21112336536946588,
      "grad_norm": 0.825624556287688,
      "learning_rate": 1.8329147855125238e-05,
      "loss": 0.6677,
      "step": 2345
    },
    {
      "epoch": 0.21121339665533773,
      "grad_norm": 0.6910771967740907,
      "learning_rate": 1.8327533687429843e-05,
      "loss": 0.5757,
      "step": 2346
    },
    {
      "epoch": 0.21130342794120957,
      "grad_norm": 0.5772332951207616,
      "learning_rate": 1.832591881155581e-05,
      "loss": 0.5408,
      "step": 2347
    },
    {
      "epoch": 0.21139345922708142,
      "grad_norm": 0.7383545640544228,
      "learning_rate": 1.8324303227640472e-05,
      "loss": 0.61,
      "step": 2348
    },
    {
      "epoch": 0.21148349051295326,
      "grad_norm": 0.9707084228655405,
      "learning_rate": 1.8322686935821223e-05,
      "loss": 0.6351,
      "step": 2349
    },
    {
      "epoch": 0.2115735217988251,
      "grad_norm": 0.7069943988847904,
      "learning_rate": 1.8321069936235503e-05,
      "loss": 0.574,
      "step": 2350
    },
    {
      "epoch": 0.21166355308469692,
      "grad_norm": 1.039353200176711,
      "learning_rate": 1.831945222902083e-05,
      "loss": 0.5137,
      "step": 2351
    },
    {
      "epoch": 0.21175358437056876,
      "grad_norm": 0.7365973517249702,
      "learning_rate": 1.8317833814314775e-05,
      "loss": 0.5734,
      "step": 2352
    },
    {
      "epoch": 0.2118436156564406,
      "grad_norm": 0.9601032784523456,
      "learning_rate": 1.8316214692254967e-05,
      "loss": 0.6276,
      "step": 2353
    },
    {
      "epoch": 0.21193364694231245,
      "grad_norm": 0.8226823511869825,
      "learning_rate": 1.8314594862979097e-05,
      "loss": 0.6047,
      "step": 2354
    },
    {
      "epoch": 0.2120236782281843,
      "grad_norm": 0.6754747686538728,
      "learning_rate": 1.8312974326624918e-05,
      "loss": 0.5278,
      "step": 2355
    },
    {
      "epoch": 0.21211370951405614,
      "grad_norm": 0.8894969988512956,
      "learning_rate": 1.8311353083330238e-05,
      "loss": 0.6097,
      "step": 2356
    },
    {
      "epoch": 0.212203740799928,
      "grad_norm": 0.7549880198052249,
      "learning_rate": 1.8309731133232936e-05,
      "loss": 0.623,
      "step": 2357
    },
    {
      "epoch": 0.2122937720857998,
      "grad_norm": 0.6810279613150241,
      "learning_rate": 1.8308108476470938e-05,
      "loss": 0.5945,
      "step": 2358
    },
    {
      "epoch": 0.21238380337167165,
      "grad_norm": 0.7009026016498213,
      "learning_rate": 1.830648511318223e-05,
      "loss": 0.5606,
      "step": 2359
    },
    {
      "epoch": 0.2124738346575435,
      "grad_norm": 0.7560584258206311,
      "learning_rate": 1.8304861043504874e-05,
      "loss": 0.5652,
      "step": 2360
    },
    {
      "epoch": 0.21256386594341534,
      "grad_norm": 0.6359298473788743,
      "learning_rate": 1.830323626757698e-05,
      "loss": 0.5738,
      "step": 2361
    },
    {
      "epoch": 0.21265389722928718,
      "grad_norm": 0.7097257870981712,
      "learning_rate": 1.8301610785536717e-05,
      "loss": 0.5393,
      "step": 2362
    },
    {
      "epoch": 0.21274392851515903,
      "grad_norm": 0.6545561397509311,
      "learning_rate": 1.8299984597522317e-05,
      "loss": 0.4835,
      "step": 2363
    },
    {
      "epoch": 0.21283395980103087,
      "grad_norm": 0.6568424955250186,
      "learning_rate": 1.8298357703672072e-05,
      "loss": 0.5843,
      "step": 2364
    },
    {
      "epoch": 0.2129239910869027,
      "grad_norm": 0.6690708563971015,
      "learning_rate": 1.8296730104124336e-05,
      "loss": 0.5787,
      "step": 2365
    },
    {
      "epoch": 0.21301402237277453,
      "grad_norm": 0.7915328249871249,
      "learning_rate": 1.829510179901752e-05,
      "loss": 0.5795,
      "step": 2366
    },
    {
      "epoch": 0.21310405365864638,
      "grad_norm": 0.8515766395295327,
      "learning_rate": 1.8293472788490096e-05,
      "loss": 0.688,
      "step": 2367
    },
    {
      "epoch": 0.21319408494451822,
      "grad_norm": 0.7828467375582705,
      "learning_rate": 1.8291843072680598e-05,
      "loss": 0.5985,
      "step": 2368
    },
    {
      "epoch": 0.21328411623039006,
      "grad_norm": 0.8067312327764167,
      "learning_rate": 1.8290212651727613e-05,
      "loss": 0.6649,
      "step": 2369
    },
    {
      "epoch": 0.2133741475162619,
      "grad_norm": 0.7003723223375247,
      "learning_rate": 1.82885815257698e-05,
      "loss": 0.5734,
      "step": 2370
    },
    {
      "epoch": 0.21346417880213375,
      "grad_norm": 0.8989354538646442,
      "learning_rate": 1.8286949694945864e-05,
      "loss": 0.5482,
      "step": 2371
    },
    {
      "epoch": 0.21355421008800557,
      "grad_norm": 0.64583118516915,
      "learning_rate": 1.8285317159394585e-05,
      "loss": 0.6507,
      "step": 2372
    },
    {
      "epoch": 0.2136442413738774,
      "grad_norm": 0.7893361622553172,
      "learning_rate": 1.8283683919254787e-05,
      "loss": 0.5615,
      "step": 2373
    },
    {
      "epoch": 0.21373427265974926,
      "grad_norm": 0.8890652043953396,
      "learning_rate": 1.8282049974665365e-05,
      "loss": 0.4603,
      "step": 2374
    },
    {
      "epoch": 0.2138243039456211,
      "grad_norm": 0.8288552607561163,
      "learning_rate": 1.828041532576527e-05,
      "loss": 0.6313,
      "step": 2375
    },
    {
      "epoch": 0.21391433523149295,
      "grad_norm": 0.77499094949088,
      "learning_rate": 1.8278779972693515e-05,
      "loss": 0.4546,
      "step": 2376
    },
    {
      "epoch": 0.2140043665173648,
      "grad_norm": 0.9023511404376932,
      "learning_rate": 1.8277143915589167e-05,
      "loss": 0.556,
      "step": 2377
    },
    {
      "epoch": 0.21409439780323664,
      "grad_norm": 0.8677026935708169,
      "learning_rate": 1.8275507154591364e-05,
      "loss": 0.5973,
      "step": 2378
    },
    {
      "epoch": 0.21418442908910845,
      "grad_norm": 0.883918557572471,
      "learning_rate": 1.8273869689839295e-05,
      "loss": 0.4983,
      "step": 2379
    },
    {
      "epoch": 0.2142744603749803,
      "grad_norm": 0.6920243912642979,
      "learning_rate": 1.8272231521472208e-05,
      "loss": 0.6235,
      "step": 2380
    },
    {
      "epoch": 0.21436449166085214,
      "grad_norm": 0.7594389033309775,
      "learning_rate": 1.8270592649629417e-05,
      "loss": 0.567,
      "step": 2381
    },
    {
      "epoch": 0.21445452294672399,
      "grad_norm": 0.9641671440630181,
      "learning_rate": 1.8268953074450292e-05,
      "loss": 0.5824,
      "step": 2382
    },
    {
      "epoch": 0.21454455423259583,
      "grad_norm": 1.3393273886162258,
      "learning_rate": 1.8267312796074263e-05,
      "loss": 0.7689,
      "step": 2383
    },
    {
      "epoch": 0.21463458551846767,
      "grad_norm": 0.726512314876189,
      "learning_rate": 1.826567181464082e-05,
      "loss": 0.5182,
      "step": 2384
    },
    {
      "epoch": 0.21472461680433952,
      "grad_norm": 0.8384616320269511,
      "learning_rate": 1.8264030130289512e-05,
      "loss": 0.6162,
      "step": 2385
    },
    {
      "epoch": 0.21481464809021134,
      "grad_norm": 0.6744311863086635,
      "learning_rate": 1.826238774315995e-05,
      "loss": 0.5135,
      "step": 2386
    },
    {
      "epoch": 0.21490467937608318,
      "grad_norm": 0.8506757216242722,
      "learning_rate": 1.8260744653391803e-05,
      "loss": 0.642,
      "step": 2387
    },
    {
      "epoch": 0.21499471066195502,
      "grad_norm": 0.6444752428448183,
      "learning_rate": 1.8259100861124807e-05,
      "loss": 0.5491,
      "step": 2388
    },
    {
      "epoch": 0.21508474194782687,
      "grad_norm": 0.6749655802821171,
      "learning_rate": 1.8257456366498738e-05,
      "loss": 0.5872,
      "step": 2389
    },
    {
      "epoch": 0.2151747732336987,
      "grad_norm": 0.6578731533021948,
      "learning_rate": 1.8255811169653458e-05,
      "loss": 0.4946,
      "step": 2390
    },
    {
      "epoch": 0.21526480451957056,
      "grad_norm": 0.7008530071459711,
      "learning_rate": 1.8254165270728867e-05,
      "loss": 0.5775,
      "step": 2391
    },
    {
      "epoch": 0.2153548358054424,
      "grad_norm": 0.6041838677416805,
      "learning_rate": 1.8252518669864935e-05,
      "loss": 0.5366,
      "step": 2392
    },
    {
      "epoch": 0.21544486709131422,
      "grad_norm": 0.7071667667527844,
      "learning_rate": 1.8250871367201694e-05,
      "loss": 0.6819,
      "step": 2393
    },
    {
      "epoch": 0.21553489837718606,
      "grad_norm": 0.6652776635009343,
      "learning_rate": 1.8249223362879226e-05,
      "loss": 0.5082,
      "step": 2394
    },
    {
      "epoch": 0.2156249296630579,
      "grad_norm": 0.9544882066499921,
      "learning_rate": 1.8247574657037684e-05,
      "loss": 0.6285,
      "step": 2395
    },
    {
      "epoch": 0.21571496094892975,
      "grad_norm": 0.7533863188397103,
      "learning_rate": 1.8245925249817266e-05,
      "loss": 0.5752,
      "step": 2396
    },
    {
      "epoch": 0.2158049922348016,
      "grad_norm": 0.6533096467954458,
      "learning_rate": 1.824427514135825e-05,
      "loss": 0.5438,
      "step": 2397
    },
    {
      "epoch": 0.21589502352067344,
      "grad_norm": 0.7336114078932049,
      "learning_rate": 1.8242624331800955e-05,
      "loss": 0.6565,
      "step": 2398
    },
    {
      "epoch": 0.21598505480654528,
      "grad_norm": 0.753240363222896,
      "learning_rate": 1.824097282128577e-05,
      "loss": 0.4741,
      "step": 2399
    },
    {
      "epoch": 0.2160750860924171,
      "grad_norm": 0.6260279099814875,
      "learning_rate": 1.823932060995314e-05,
      "loss": 0.5762,
      "step": 2400
    },
    {
      "epoch": 0.21616511737828895,
      "grad_norm": 0.6435648768514503,
      "learning_rate": 1.8237667697943564e-05,
      "loss": 0.6177,
      "step": 2401
    },
    {
      "epoch": 0.2162551486641608,
      "grad_norm": 0.6615239314405328,
      "learning_rate": 1.8236014085397616e-05,
      "loss": 0.5709,
      "step": 2402
    },
    {
      "epoch": 0.21634517995003263,
      "grad_norm": 0.9090520217212207,
      "learning_rate": 1.8234359772455915e-05,
      "loss": 0.615,
      "step": 2403
    },
    {
      "epoch": 0.21643521123590448,
      "grad_norm": 0.7918345973885632,
      "learning_rate": 1.8232704759259145e-05,
      "loss": 0.5534,
      "step": 2404
    },
    {
      "epoch": 0.21652524252177632,
      "grad_norm": 0.794029371503039,
      "learning_rate": 1.8231049045948054e-05,
      "loss": 0.6041,
      "step": 2405
    },
    {
      "epoch": 0.21661527380764817,
      "grad_norm": 0.7683119817603089,
      "learning_rate": 1.8229392632663437e-05,
      "loss": 0.5628,
      "step": 2406
    },
    {
      "epoch": 0.21670530509352,
      "grad_norm": 0.7888826908157073,
      "learning_rate": 1.822773551954616e-05,
      "loss": 0.4977,
      "step": 2407
    },
    {
      "epoch": 0.21679533637939183,
      "grad_norm": 0.9170268212427922,
      "learning_rate": 1.8226077706737147e-05,
      "loss": 0.6461,
      "step": 2408
    },
    {
      "epoch": 0.21688536766526367,
      "grad_norm": 0.63857919804555,
      "learning_rate": 1.822441919437738e-05,
      "loss": 0.4884,
      "step": 2409
    },
    {
      "epoch": 0.21697539895113552,
      "grad_norm": 0.673043157125798,
      "learning_rate": 1.8222759982607895e-05,
      "loss": 0.519,
      "step": 2410
    },
    {
      "epoch": 0.21706543023700736,
      "grad_norm": 0.64069628125301,
      "learning_rate": 1.8221100071569796e-05,
      "loss": 0.5856,
      "step": 2411
    },
    {
      "epoch": 0.2171554615228792,
      "grad_norm": 0.6889176312458895,
      "learning_rate": 1.8219439461404244e-05,
      "loss": 0.637,
      "step": 2412
    },
    {
      "epoch": 0.21724549280875105,
      "grad_norm": 0.5745777891129655,
      "learning_rate": 1.821777815225245e-05,
      "loss": 0.5325,
      "step": 2413
    },
    {
      "epoch": 0.2173355240946229,
      "grad_norm": 0.7742183651715899,
      "learning_rate": 1.821611614425571e-05,
      "loss": 0.563,
      "step": 2414
    },
    {
      "epoch": 0.2174255553804947,
      "grad_norm": 1.277456726263067,
      "learning_rate": 1.821445343755534e-05,
      "loss": 0.5851,
      "step": 2415
    },
    {
      "epoch": 0.21751558666636656,
      "grad_norm": 0.9419510824562946,
      "learning_rate": 1.8212790032292757e-05,
      "loss": 0.5714,
      "step": 2416
    },
    {
      "epoch": 0.2176056179522384,
      "grad_norm": 0.8046699912887111,
      "learning_rate": 1.8211125928609407e-05,
      "loss": 0.6364,
      "step": 2417
    },
    {
      "epoch": 0.21769564923811024,
      "grad_norm": 0.6995617105379209,
      "learning_rate": 1.820946112664681e-05,
      "loss": 0.5397,
      "step": 2418
    },
    {
      "epoch": 0.2177856805239821,
      "grad_norm": 0.8163309306294322,
      "learning_rate": 1.820779562654654e-05,
      "loss": 0.6732,
      "step": 2419
    },
    {
      "epoch": 0.21787571180985393,
      "grad_norm": 0.707950793798462,
      "learning_rate": 1.8206129428450235e-05,
      "loss": 0.5355,
      "step": 2420
    },
    {
      "epoch": 0.21796574309572578,
      "grad_norm": 0.6900710063604825,
      "learning_rate": 1.8204462532499586e-05,
      "loss": 0.6281,
      "step": 2421
    },
    {
      "epoch": 0.2180557743815976,
      "grad_norm": 0.6978537191221676,
      "learning_rate": 1.820279493883635e-05,
      "loss": 0.6251,
      "step": 2422
    },
    {
      "epoch": 0.21814580566746944,
      "grad_norm": 0.7138816173858902,
      "learning_rate": 1.8201126647602342e-05,
      "loss": 0.5832,
      "step": 2423
    },
    {
      "epoch": 0.21823583695334128,
      "grad_norm": 0.7325337339058856,
      "learning_rate": 1.8199457658939425e-05,
      "loss": 0.6028,
      "step": 2424
    },
    {
      "epoch": 0.21832586823921313,
      "grad_norm": 0.8449565815627322,
      "learning_rate": 1.819778797298954e-05,
      "loss": 0.566,
      "step": 2425
    },
    {
      "epoch": 0.21841589952508497,
      "grad_norm": 0.7299321167927514,
      "learning_rate": 1.8196117589894676e-05,
      "loss": 0.62,
      "step": 2426
    },
    {
      "epoch": 0.21850593081095682,
      "grad_norm": 0.696489793707839,
      "learning_rate": 1.8194446509796885e-05,
      "loss": 0.5858,
      "step": 2427
    },
    {
      "epoch": 0.21859596209682866,
      "grad_norm": 0.8721431960814695,
      "learning_rate": 1.819277473283827e-05,
      "loss": 0.5974,
      "step": 2428
    },
    {
      "epoch": 0.21868599338270048,
      "grad_norm": 0.7424169835296065,
      "learning_rate": 1.8191102259161007e-05,
      "loss": 0.6196,
      "step": 2429
    },
    {
      "epoch": 0.21877602466857232,
      "grad_norm": 0.8609979120307996,
      "learning_rate": 1.818942908890732e-05,
      "loss": 0.518,
      "step": 2430
    },
    {
      "epoch": 0.21886605595444417,
      "grad_norm": 0.7381260381252471,
      "learning_rate": 1.81877552222195e-05,
      "loss": 0.593,
      "step": 2431
    },
    {
      "epoch": 0.218956087240316,
      "grad_norm": 0.669515116995217,
      "learning_rate": 1.8186080659239892e-05,
      "loss": 0.6201,
      "step": 2432
    },
    {
      "epoch": 0.21904611852618786,
      "grad_norm": 0.8278657424022443,
      "learning_rate": 1.8184405400110902e-05,
      "loss": 0.6009,
      "step": 2433
    },
    {
      "epoch": 0.2191361498120597,
      "grad_norm": 0.6469462181652154,
      "learning_rate": 1.8182729444974993e-05,
      "loss": 0.5623,
      "step": 2434
    },
    {
      "epoch": 0.21922618109793154,
      "grad_norm": 0.770537527344853,
      "learning_rate": 1.8181052793974695e-05,
      "loss": 0.575,
      "step": 2435
    },
    {
      "epoch": 0.21931621238380336,
      "grad_norm": 0.7652538828970912,
      "learning_rate": 1.8179375447252582e-05,
      "loss": 0.5816,
      "step": 2436
    },
    {
      "epoch": 0.2194062436696752,
      "grad_norm": 0.5873543873101988,
      "learning_rate": 1.8177697404951306e-05,
      "loss": 0.4757,
      "step": 2437
    },
    {
      "epoch": 0.21949627495554705,
      "grad_norm": 0.9290146571270066,
      "learning_rate": 1.817601866721356e-05,
      "loss": 0.6194,
      "step": 2438
    },
    {
      "epoch": 0.2195863062414189,
      "grad_norm": 0.5919927674351128,
      "learning_rate": 1.8174339234182113e-05,
      "loss": 0.502,
      "step": 2439
    },
    {
      "epoch": 0.21967633752729074,
      "grad_norm": 0.7130361828140112,
      "learning_rate": 1.817265910599978e-05,
      "loss": 0.6015,
      "step": 2440
    },
    {
      "epoch": 0.21976636881316258,
      "grad_norm": 0.6015593417740508,
      "learning_rate": 1.8170978282809443e-05,
      "loss": 0.6375,
      "step": 2441
    },
    {
      "epoch": 0.21985640009903443,
      "grad_norm": 0.7511922123684255,
      "learning_rate": 1.8169296764754042e-05,
      "loss": 0.6585,
      "step": 2442
    },
    {
      "epoch": 0.21994643138490624,
      "grad_norm": 0.7427828892158468,
      "learning_rate": 1.816761455197657e-05,
      "loss": 0.6151,
      "step": 2443
    },
    {
      "epoch": 0.2200364626707781,
      "grad_norm": 0.6083224520030768,
      "learning_rate": 1.8165931644620082e-05,
      "loss": 0.4866,
      "step": 2444
    },
    {
      "epoch": 0.22012649395664993,
      "grad_norm": 0.7053291531927907,
      "learning_rate": 1.81642480428277e-05,
      "loss": 0.5501,
      "step": 2445
    },
    {
      "epoch": 0.22021652524252178,
      "grad_norm": 0.7104392748795115,
      "learning_rate": 1.8162563746742593e-05,
      "loss": 0.6045,
      "step": 2446
    },
    {
      "epoch": 0.22030655652839362,
      "grad_norm": 0.7206308136845155,
      "learning_rate": 1.8160878756507996e-05,
      "loss": 0.5497,
      "step": 2447
    },
    {
      "epoch": 0.22039658781426547,
      "grad_norm": 0.7434070860753726,
      "learning_rate": 1.8159193072267202e-05,
      "loss": 0.5937,
      "step": 2448
    },
    {
      "epoch": 0.2204866191001373,
      "grad_norm": 0.6632042824623697,
      "learning_rate": 1.815750669416356e-05,
      "loss": 0.5506,
      "step": 2449
    },
    {
      "epoch": 0.22057665038600913,
      "grad_norm": 0.6271990855013503,
      "learning_rate": 1.815581962234049e-05,
      "loss": 0.4873,
      "step": 2450
    },
    {
      "epoch": 0.22066668167188097,
      "grad_norm": 0.6674793524700526,
      "learning_rate": 1.815413185694145e-05,
      "loss": 0.5603,
      "step": 2451
    },
    {
      "epoch": 0.22075671295775282,
      "grad_norm": 0.630307207263742,
      "learning_rate": 1.8152443398109978e-05,
      "loss": 0.6167,
      "step": 2452
    },
    {
      "epoch": 0.22084674424362466,
      "grad_norm": 0.6772917474782731,
      "learning_rate": 1.8150754245989654e-05,
      "loss": 0.5836,
      "step": 2453
    },
    {
      "epoch": 0.2209367755294965,
      "grad_norm": 0.814829040422227,
      "learning_rate": 1.814906440072413e-05,
      "loss": 0.601,
      "step": 2454
    },
    {
      "epoch": 0.22102680681536835,
      "grad_norm": 0.8172919794767604,
      "learning_rate": 1.8147373862457107e-05,
      "loss": 0.6204,
      "step": 2455
    },
    {
      "epoch": 0.2211168381012402,
      "grad_norm": 0.6905330364531183,
      "learning_rate": 1.8145682631332354e-05,
      "loss": 0.5731,
      "step": 2456
    },
    {
      "epoch": 0.221206869387112,
      "grad_norm": 0.9444110314663876,
      "learning_rate": 1.8143990707493692e-05,
      "loss": 0.6331,
      "step": 2457
    },
    {
      "epoch": 0.22129690067298385,
      "grad_norm": 0.6989279500283022,
      "learning_rate": 1.8142298091085e-05,
      "loss": 0.578,
      "step": 2458
    },
    {
      "epoch": 0.2213869319588557,
      "grad_norm": 0.8998892197698842,
      "learning_rate": 1.8140604782250226e-05,
      "loss": 0.6236,
      "step": 2459
    },
    {
      "epoch": 0.22147696324472754,
      "grad_norm": 0.6689130027990552,
      "learning_rate": 1.8138910781133365e-05,
      "loss": 0.5378,
      "step": 2460
    },
    {
      "epoch": 0.2215669945305994,
      "grad_norm": 0.8583732589404145,
      "learning_rate": 1.8137216087878476e-05,
      "loss": 0.5728,
      "step": 2461
    },
    {
      "epoch": 0.22165702581647123,
      "grad_norm": 0.7387053355781554,
      "learning_rate": 1.8135520702629677e-05,
      "loss": 0.5831,
      "step": 2462
    },
    {
      "epoch": 0.22174705710234308,
      "grad_norm": 0.7172586413498501,
      "learning_rate": 1.813382462553115e-05,
      "loss": 0.6617,
      "step": 2463
    },
    {
      "epoch": 0.2218370883882149,
      "grad_norm": 0.8841554338754469,
      "learning_rate": 1.8132127856727123e-05,
      "loss": 0.6398,
      "step": 2464
    },
    {
      "epoch": 0.22192711967408674,
      "grad_norm": 0.7539180234709096,
      "learning_rate": 1.8130430396361895e-05,
      "loss": 0.5437,
      "step": 2465
    },
    {
      "epoch": 0.22201715095995858,
      "grad_norm": 0.5726204756509179,
      "learning_rate": 1.8128732244579817e-05,
      "loss": 0.5711,
      "step": 2466
    },
    {
      "epoch": 0.22210718224583043,
      "grad_norm": 0.6741697264756036,
      "learning_rate": 1.81270334015253e-05,
      "loss": 0.5411,
      "step": 2467
    },
    {
      "epoch": 0.22219721353170227,
      "grad_norm": 0.6747894902577881,
      "learning_rate": 1.812533386734282e-05,
      "loss": 0.5588,
      "step": 2468
    },
    {
      "epoch": 0.22228724481757411,
      "grad_norm": 0.5608441911639236,
      "learning_rate": 1.8123633642176898e-05,
      "loss": 0.5255,
      "step": 2469
    },
    {
      "epoch": 0.22237727610344596,
      "grad_norm": 0.6885574955823592,
      "learning_rate": 1.812193272617213e-05,
      "loss": 0.5677,
      "step": 2470
    },
    {
      "epoch": 0.22246730738931778,
      "grad_norm": 1.1478358713092622,
      "learning_rate": 1.8120231119473158e-05,
      "loss": 0.6018,
      "step": 2471
    },
    {
      "epoch": 0.22255733867518962,
      "grad_norm": 0.8706988079673035,
      "learning_rate": 1.8118528822224693e-05,
      "loss": 0.6756,
      "step": 2472
    },
    {
      "epoch": 0.22264736996106146,
      "grad_norm": 1.103894748121047,
      "learning_rate": 1.811682583457149e-05,
      "loss": 0.6687,
      "step": 2473
    },
    {
      "epoch": 0.2227374012469333,
      "grad_norm": 0.7235804876240063,
      "learning_rate": 1.8115122156658383e-05,
      "loss": 0.5771,
      "step": 2474
    },
    {
      "epoch": 0.22282743253280515,
      "grad_norm": 0.7130715373277018,
      "learning_rate": 1.8113417788630247e-05,
      "loss": 0.638,
      "step": 2475
    },
    {
      "epoch": 0.222917463818677,
      "grad_norm": 0.8662749543028156,
      "learning_rate": 1.8111712730632024e-05,
      "loss": 0.6363,
      "step": 2476
    },
    {
      "epoch": 0.22300749510454884,
      "grad_norm": 0.5140612518732318,
      "learning_rate": 1.8110006982808714e-05,
      "loss": 0.5423,
      "step": 2477
    },
    {
      "epoch": 0.22309752639042066,
      "grad_norm": 0.7055332040781044,
      "learning_rate": 1.8108300545305372e-05,
      "loss": 0.6315,
      "step": 2478
    },
    {
      "epoch": 0.2231875576762925,
      "grad_norm": 0.7547586443189543,
      "learning_rate": 1.8106593418267124e-05,
      "loss": 0.5778,
      "step": 2479
    },
    {
      "epoch": 0.22327758896216435,
      "grad_norm": 0.626206852293113,
      "learning_rate": 1.810488560183913e-05,
      "loss": 0.5508,
      "step": 2480
    },
    {
      "epoch": 0.2233676202480362,
      "grad_norm": 0.929715638069227,
      "learning_rate": 1.8103177096166632e-05,
      "loss": 0.6889,
      "step": 2481
    },
    {
      "epoch": 0.22345765153390804,
      "grad_norm": 0.6751877960458561,
      "learning_rate": 1.8101467901394928e-05,
      "loss": 0.5325,
      "step": 2482
    },
    {
      "epoch": 0.22354768281977988,
      "grad_norm": 0.6527227882504765,
      "learning_rate": 1.809975801766936e-05,
      "loss": 0.5304,
      "step": 2483
    },
    {
      "epoch": 0.22363771410565172,
      "grad_norm": 0.7293968333155677,
      "learning_rate": 1.8098047445135337e-05,
      "loss": 0.629,
      "step": 2484
    },
    {
      "epoch": 0.22372774539152354,
      "grad_norm": 0.7100129972354764,
      "learning_rate": 1.8096336183938338e-05,
      "loss": 0.5098,
      "step": 2485
    },
    {
      "epoch": 0.22381777667739539,
      "grad_norm": 0.7452676451823529,
      "learning_rate": 1.809462423422388e-05,
      "loss": 0.5528,
      "step": 2486
    },
    {
      "epoch": 0.22390780796326723,
      "grad_norm": 0.6776842435139804,
      "learning_rate": 1.8092911596137547e-05,
      "loss": 0.656,
      "step": 2487
    },
    {
      "epoch": 0.22399783924913907,
      "grad_norm": 0.8562925878366829,
      "learning_rate": 1.809119826982499e-05,
      "loss": 0.681,
      "step": 2488
    },
    {
      "epoch": 0.22408787053501092,
      "grad_norm": 0.8432742373408942,
      "learning_rate": 1.8089484255431906e-05,
      "loss": 0.5138,
      "step": 2489
    },
    {
      "epoch": 0.22417790182088276,
      "grad_norm": 0.7618893823295594,
      "learning_rate": 1.808776955310406e-05,
      "loss": 0.5844,
      "step": 2490
    },
    {
      "epoch": 0.2242679331067546,
      "grad_norm": 0.8357530902841438,
      "learning_rate": 1.808605416298727e-05,
      "loss": 0.5762,
      "step": 2491
    },
    {
      "epoch": 0.22435796439262642,
      "grad_norm": 1.5782652457243127,
      "learning_rate": 1.8084338085227408e-05,
      "loss": 0.6221,
      "step": 2492
    },
    {
      "epoch": 0.22444799567849827,
      "grad_norm": 0.7964855860293848,
      "learning_rate": 1.8082621319970422e-05,
      "loss": 0.5807,
      "step": 2493
    },
    {
      "epoch": 0.2245380269643701,
      "grad_norm": 0.692803900310681,
      "learning_rate": 1.8080903867362297e-05,
      "loss": 0.5359,
      "step": 2494
    },
    {
      "epoch": 0.22462805825024196,
      "grad_norm": 0.6107674752553689,
      "learning_rate": 1.8079185727549087e-05,
      "loss": 0.5544,
      "step": 2495
    },
    {
      "epoch": 0.2247180895361138,
      "grad_norm": 0.6933966764879442,
      "learning_rate": 1.807746690067691e-05,
      "loss": 0.5916,
      "step": 2496
    },
    {
      "epoch": 0.22480812082198565,
      "grad_norm": 0.770861077369841,
      "learning_rate": 1.807574738689193e-05,
      "loss": 0.5197,
      "step": 2497
    },
    {
      "epoch": 0.2248981521078575,
      "grad_norm": 0.8080392305899037,
      "learning_rate": 1.8074027186340378e-05,
      "loss": 0.7005,
      "step": 2498
    },
    {
      "epoch": 0.2249881833937293,
      "grad_norm": 0.7778680613765163,
      "learning_rate": 1.807230629916854e-05,
      "loss": 0.5687,
      "step": 2499
    },
    {
      "epoch": 0.22507821467960115,
      "grad_norm": 0.7226037528981313,
      "learning_rate": 1.8070584725522763e-05,
      "loss": 0.5194,
      "step": 2500
    },
    {
      "epoch": 0.225168245965473,
      "grad_norm": 0.906125347171864,
      "learning_rate": 1.806886246554945e-05,
      "loss": 0.6124,
      "step": 2501
    },
    {
      "epoch": 0.22525827725134484,
      "grad_norm": 0.7346314864477732,
      "learning_rate": 1.8067139519395062e-05,
      "loss": 0.7117,
      "step": 2502
    },
    {
      "epoch": 0.22534830853721668,
      "grad_norm": 0.6605174669504534,
      "learning_rate": 1.8065415887206117e-05,
      "loss": 0.5684,
      "step": 2503
    },
    {
      "epoch": 0.22543833982308853,
      "grad_norm": 0.7151526700287109,
      "learning_rate": 1.80636915691292e-05,
      "loss": 0.5473,
      "step": 2504
    },
    {
      "epoch": 0.22552837110896037,
      "grad_norm": 0.7916398725120899,
      "learning_rate": 1.8061966565310946e-05,
      "loss": 0.5646,
      "step": 2505
    },
    {
      "epoch": 0.2256184023948322,
      "grad_norm": 0.6992840852927348,
      "learning_rate": 1.8060240875898048e-05,
      "loss": 0.5274,
      "step": 2506
    },
    {
      "epoch": 0.22570843368070403,
      "grad_norm": 0.6545321107449922,
      "learning_rate": 1.805851450103726e-05,
      "loss": 0.5794,
      "step": 2507
    },
    {
      "epoch": 0.22579846496657588,
      "grad_norm": 0.8215888585381814,
      "learning_rate": 1.8056787440875395e-05,
      "loss": 0.4986,
      "step": 2508
    },
    {
      "epoch": 0.22588849625244772,
      "grad_norm": 0.845088785762848,
      "learning_rate": 1.8055059695559323e-05,
      "loss": 0.5813,
      "step": 2509
    },
    {
      "epoch": 0.22597852753831957,
      "grad_norm": 0.6743374315793488,
      "learning_rate": 1.8053331265235974e-05,
      "loss": 0.5379,
      "step": 2510
    },
    {
      "epoch": 0.2260685588241914,
      "grad_norm": 0.8723912952831646,
      "learning_rate": 1.805160215005233e-05,
      "loss": 0.5689,
      "step": 2511
    },
    {
      "epoch": 0.22615859011006326,
      "grad_norm": 0.5795064114002374,
      "learning_rate": 1.8049872350155446e-05,
      "loss": 0.5577,
      "step": 2512
    },
    {
      "epoch": 0.2262486213959351,
      "grad_norm": 0.8905212690144513,
      "learning_rate": 1.8048141865692415e-05,
      "loss": 0.5557,
      "step": 2513
    },
    {
      "epoch": 0.22633865268180692,
      "grad_norm": 0.7024028395428032,
      "learning_rate": 1.8046410696810405e-05,
      "loss": 0.5622,
      "step": 2514
    },
    {
      "epoch": 0.22642868396767876,
      "grad_norm": 0.7581420013800756,
      "learning_rate": 1.804467884365663e-05,
      "loss": 0.499,
      "step": 2515
    },
    {
      "epoch": 0.2265187152535506,
      "grad_norm": 0.761368373071944,
      "learning_rate": 1.8042946306378372e-05,
      "loss": 0.5745,
      "step": 2516
    },
    {
      "epoch": 0.22660874653942245,
      "grad_norm": 0.7373575409026472,
      "learning_rate": 1.8041213085122964e-05,
      "loss": 0.4969,
      "step": 2517
    },
    {
      "epoch": 0.2266987778252943,
      "grad_norm": 0.7878633080583237,
      "learning_rate": 1.8039479180037803e-05,
      "loss": 0.6027,
      "step": 2518
    },
    {
      "epoch": 0.22678880911116614,
      "grad_norm": 0.7769529429686971,
      "learning_rate": 1.803774459127034e-05,
      "loss": 0.5035,
      "step": 2519
    },
    {
      "epoch": 0.22687884039703798,
      "grad_norm": 0.7732245194794957,
      "learning_rate": 1.8036009318968087e-05,
      "loss": 0.6098,
      "step": 2520
    },
    {
      "epoch": 0.2269688716829098,
      "grad_norm": 0.7243862429900028,
      "learning_rate": 1.8034273363278615e-05,
      "loss": 0.5221,
      "step": 2521
    },
    {
      "epoch": 0.22705890296878165,
      "grad_norm": 0.8224060535682154,
      "learning_rate": 1.8032536724349545e-05,
      "loss": 0.5408,
      "step": 2522
    },
    {
      "epoch": 0.2271489342546535,
      "grad_norm": 0.7246485325167087,
      "learning_rate": 1.8030799402328562e-05,
      "loss": 0.5248,
      "step": 2523
    },
    {
      "epoch": 0.22723896554052533,
      "grad_norm": 0.6725601061518999,
      "learning_rate": 1.8029061397363413e-05,
      "loss": 0.5791,
      "step": 2524
    },
    {
      "epoch": 0.22732899682639718,
      "grad_norm": 0.8394170436633417,
      "learning_rate": 1.8027322709601895e-05,
      "loss": 0.5967,
      "step": 2525
    },
    {
      "epoch": 0.22741902811226902,
      "grad_norm": 0.8376146937881921,
      "learning_rate": 1.8025583339191873e-05,
      "loss": 0.5757,
      "step": 2526
    },
    {
      "epoch": 0.22750905939814087,
      "grad_norm": 0.7584193673162031,
      "learning_rate": 1.802384328628126e-05,
      "loss": 0.5469,
      "step": 2527
    },
    {
      "epoch": 0.22759909068401268,
      "grad_norm": 0.7077970956907919,
      "learning_rate": 1.8022102551018026e-05,
      "loss": 0.6092,
      "step": 2528
    },
    {
      "epoch": 0.22768912196988453,
      "grad_norm": 1.1252873063369158,
      "learning_rate": 1.8020361133550215e-05,
      "loss": 0.6587,
      "step": 2529
    },
    {
      "epoch": 0.22777915325575637,
      "grad_norm": 0.7210346776877133,
      "learning_rate": 1.8018619034025912e-05,
      "loss": 0.6665,
      "step": 2530
    },
    {
      "epoch": 0.22786918454162822,
      "grad_norm": 0.8107009215999658,
      "learning_rate": 1.8016876252593266e-05,
      "loss": 0.4839,
      "step": 2531
    },
    {
      "epoch": 0.22795921582750006,
      "grad_norm": 0.6256890216478066,
      "learning_rate": 1.801513278940049e-05,
      "loss": 0.6014,
      "step": 2532
    },
    {
      "epoch": 0.2280492471133719,
      "grad_norm": 0.7365846530729613,
      "learning_rate": 1.801338864459584e-05,
      "loss": 0.5148,
      "step": 2533
    },
    {
      "epoch": 0.22813927839924375,
      "grad_norm": 0.6811154139381542,
      "learning_rate": 1.8011643818327644e-05,
      "loss": 0.5179,
      "step": 2534
    },
    {
      "epoch": 0.22822930968511557,
      "grad_norm": 0.6253420744184028,
      "learning_rate": 1.8009898310744285e-05,
      "loss": 0.5509,
      "step": 2535
    },
    {
      "epoch": 0.2283193409709874,
      "grad_norm": 0.6242438249132928,
      "learning_rate": 1.8008152121994198e-05,
      "loss": 0.4899,
      "step": 2536
    },
    {
      "epoch": 0.22840937225685926,
      "grad_norm": 0.5518608732639507,
      "learning_rate": 1.800640525222588e-05,
      "loss": 0.5619,
      "step": 2537
    },
    {
      "epoch": 0.2284994035427311,
      "grad_norm": 0.707498448816231,
      "learning_rate": 1.8004657701587893e-05,
      "loss": 0.647,
      "step": 2538
    },
    {
      "epoch": 0.22858943482860294,
      "grad_norm": 0.6371328461850465,
      "learning_rate": 1.800290947022884e-05,
      "loss": 0.4884,
      "step": 2539
    },
    {
      "epoch": 0.2286794661144748,
      "grad_norm": 0.9330781681835023,
      "learning_rate": 1.80011605582974e-05,
      "loss": 0.6037,
      "step": 2540
    },
    {
      "epoch": 0.22876949740034663,
      "grad_norm": 0.7308152722436404,
      "learning_rate": 1.7999410965942295e-05,
      "loss": 0.6758,
      "step": 2541
    },
    {
      "epoch": 0.22885952868621845,
      "grad_norm": 1.020096636106317,
      "learning_rate": 1.799766069331232e-05,
      "loss": 0.6513,
      "step": 2542
    },
    {
      "epoch": 0.2289495599720903,
      "grad_norm": 0.6745150935067952,
      "learning_rate": 1.7995909740556308e-05,
      "loss": 0.6608,
      "step": 2543
    },
    {
      "epoch": 0.22903959125796214,
      "grad_norm": 0.688958436981269,
      "learning_rate": 1.7994158107823167e-05,
      "loss": 0.5139,
      "step": 2544
    },
    {
      "epoch": 0.22912962254383398,
      "grad_norm": 0.8357079956479537,
      "learning_rate": 1.799240579526186e-05,
      "loss": 0.5154,
      "step": 2545
    },
    {
      "epoch": 0.22921965382970583,
      "grad_norm": 0.5373882093301959,
      "learning_rate": 1.79906528030214e-05,
      "loss": 0.5255,
      "step": 2546
    },
    {
      "epoch": 0.22930968511557767,
      "grad_norm": 0.809969350993809,
      "learning_rate": 1.7988899131250863e-05,
      "loss": 0.6467,
      "step": 2547
    },
    {
      "epoch": 0.22939971640144952,
      "grad_norm": 0.7575234994048095,
      "learning_rate": 1.798714478009938e-05,
      "loss": 0.6616,
      "step": 2548
    },
    {
      "epoch": 0.22948974768732133,
      "grad_norm": 0.7358653864381702,
      "learning_rate": 1.798538974971615e-05,
      "loss": 0.5793,
      "step": 2549
    },
    {
      "epoch": 0.22957977897319318,
      "grad_norm": 0.6528231451712637,
      "learning_rate": 1.7983634040250414e-05,
      "loss": 0.6385,
      "step": 2550
    },
    {
      "epoch": 0.22966981025906502,
      "grad_norm": 0.7049241410087237,
      "learning_rate": 1.7981877651851486e-05,
      "loss": 0.5831,
      "step": 2551
    },
    {
      "epoch": 0.22975984154493687,
      "grad_norm": 0.7198677656593148,
      "learning_rate": 1.7980120584668722e-05,
      "loss": 0.5883,
      "step": 2552
    },
    {
      "epoch": 0.2298498728308087,
      "grad_norm": 0.7773545944690556,
      "learning_rate": 1.797836283885155e-05,
      "loss": 0.5515,
      "step": 2553
    },
    {
      "epoch": 0.22993990411668055,
      "grad_norm": 0.92530949252763,
      "learning_rate": 1.797660441454945e-05,
      "loss": 0.6566,
      "step": 2554
    },
    {
      "epoch": 0.2300299354025524,
      "grad_norm": 0.7086888586833973,
      "learning_rate": 1.7974845311911954e-05,
      "loss": 0.6075,
      "step": 2555
    },
    {
      "epoch": 0.23011996668842422,
      "grad_norm": 0.8506770418619248,
      "learning_rate": 1.7973085531088663e-05,
      "loss": 0.6339,
      "step": 2556
    },
    {
      "epoch": 0.23020999797429606,
      "grad_norm": 0.7502014396353829,
      "learning_rate": 1.7971325072229227e-05,
      "loss": 0.5475,
      "step": 2557
    },
    {
      "epoch": 0.2303000292601679,
      "grad_norm": 0.68802938297178,
      "learning_rate": 1.7969563935483357e-05,
      "loss": 0.6054,
      "step": 2558
    },
    {
      "epoch": 0.23039006054603975,
      "grad_norm": 0.7595772005560104,
      "learning_rate": 1.796780212100082e-05,
      "loss": 0.6242,
      "step": 2559
    },
    {
      "epoch": 0.2304800918319116,
      "grad_norm": 0.7593664909182577,
      "learning_rate": 1.7966039628931447e-05,
      "loss": 0.6944,
      "step": 2560
    },
    {
      "epoch": 0.23057012311778344,
      "grad_norm": 0.8640139795824502,
      "learning_rate": 1.7964276459425115e-05,
      "loss": 0.5739,
      "step": 2561
    },
    {
      "epoch": 0.23066015440365528,
      "grad_norm": 0.6768693638653698,
      "learning_rate": 1.7962512612631767e-05,
      "loss": 0.5862,
      "step": 2562
    },
    {
      "epoch": 0.2307501856895271,
      "grad_norm": 0.7306122880066628,
      "learning_rate": 1.7960748088701402e-05,
      "loss": 0.5474,
      "step": 2563
    },
    {
      "epoch": 0.23084021697539894,
      "grad_norm": 0.7915231813245754,
      "learning_rate": 1.7958982887784078e-05,
      "loss": 0.5915,
      "step": 2564
    },
    {
      "epoch": 0.2309302482612708,
      "grad_norm": 0.6886860311325993,
      "learning_rate": 1.7957217010029905e-05,
      "loss": 0.5571,
      "step": 2565
    },
    {
      "epoch": 0.23102027954714263,
      "grad_norm": 0.8191345089827973,
      "learning_rate": 1.795545045558906e-05,
      "loss": 0.5844,
      "step": 2566
    },
    {
      "epoch": 0.23111031083301448,
      "grad_norm": 0.7528067501409365,
      "learning_rate": 1.7953683224611763e-05,
      "loss": 0.5833,
      "step": 2567
    },
    {
      "epoch": 0.23120034211888632,
      "grad_norm": 0.7650316841245915,
      "learning_rate": 1.795191531724831e-05,
      "loss": 0.5703,
      "step": 2568
    },
    {
      "epoch": 0.23129037340475816,
      "grad_norm": 0.7689543928972205,
      "learning_rate": 1.795014673364904e-05,
      "loss": 0.5676,
      "step": 2569
    },
    {
      "epoch": 0.23138040469062998,
      "grad_norm": 0.7168080112881899,
      "learning_rate": 1.7948377473964356e-05,
      "loss": 0.5644,
      "step": 2570
    },
    {
      "epoch": 0.23147043597650183,
      "grad_norm": 0.7283466008863134,
      "learning_rate": 1.7946607538344713e-05,
      "loss": 0.5512,
      "step": 2571
    },
    {
      "epoch": 0.23156046726237367,
      "grad_norm": 0.9527665087605863,
      "learning_rate": 1.7944836926940633e-05,
      "loss": 0.5948,
      "step": 2572
    },
    {
      "epoch": 0.23165049854824551,
      "grad_norm": 0.7840048236792376,
      "learning_rate": 1.7943065639902687e-05,
      "loss": 0.6104,
      "step": 2573
    },
    {
      "epoch": 0.23174052983411736,
      "grad_norm": 0.727714241692623,
      "learning_rate": 1.794129367738151e-05,
      "loss": 0.6223,
      "step": 2574
    },
    {
      "epoch": 0.2318305611199892,
      "grad_norm": 0.7748669227112773,
      "learning_rate": 1.7939521039527783e-05,
      "loss": 0.6385,
      "step": 2575
    },
    {
      "epoch": 0.23192059240586105,
      "grad_norm": 0.7003198635228767,
      "learning_rate": 1.7937747726492256e-05,
      "loss": 0.5257,
      "step": 2576
    },
    {
      "epoch": 0.23201062369173286,
      "grad_norm": 1.1492268428883836,
      "learning_rate": 1.7935973738425738e-05,
      "loss": 0.5982,
      "step": 2577
    },
    {
      "epoch": 0.2321006549776047,
      "grad_norm": 0.7792639671820858,
      "learning_rate": 1.793419907547908e-05,
      "loss": 0.5902,
      "step": 2578
    },
    {
      "epoch": 0.23219068626347655,
      "grad_norm": 0.6224627913129716,
      "learning_rate": 1.793242373780321e-05,
      "loss": 0.4936,
      "step": 2579
    },
    {
      "epoch": 0.2322807175493484,
      "grad_norm": 0.631386671718279,
      "learning_rate": 1.7930647725549103e-05,
      "loss": 0.5977,
      "step": 2580
    },
    {
      "epoch": 0.23237074883522024,
      "grad_norm": 0.8468903360870963,
      "learning_rate": 1.7928871038867785e-05,
      "loss": 0.6395,
      "step": 2581
    },
    {
      "epoch": 0.2324607801210921,
      "grad_norm": 0.6433864585701626,
      "learning_rate": 1.792709367791035e-05,
      "loss": 0.5356,
      "step": 2582
    },
    {
      "epoch": 0.23255081140696393,
      "grad_norm": 0.7592694753978175,
      "learning_rate": 1.7925315642827944e-05,
      "loss": 0.5693,
      "step": 2583
    },
    {
      "epoch": 0.23264084269283575,
      "grad_norm": 0.7559388428255261,
      "learning_rate": 1.7923536933771778e-05,
      "loss": 0.5452,
      "step": 2584
    },
    {
      "epoch": 0.2327308739787076,
      "grad_norm": 0.7084114478027274,
      "learning_rate": 1.792175755089311e-05,
      "loss": 0.5263,
      "step": 2585
    },
    {
      "epoch": 0.23282090526457944,
      "grad_norm": 0.7395426430181864,
      "learning_rate": 1.7919977494343263e-05,
      "loss": 0.6935,
      "step": 2586
    },
    {
      "epoch": 0.23291093655045128,
      "grad_norm": 0.6901482890025769,
      "learning_rate": 1.791819676427361e-05,
      "loss": 0.5972,
      "step": 2587
    },
    {
      "epoch": 0.23300096783632313,
      "grad_norm": 0.68825973949053,
      "learning_rate": 1.791641536083559e-05,
      "loss": 0.6219,
      "step": 2588
    },
    {
      "epoch": 0.23309099912219497,
      "grad_norm": 0.708173996653663,
      "learning_rate": 1.7914633284180685e-05,
      "loss": 0.5307,
      "step": 2589
    },
    {
      "epoch": 0.23318103040806681,
      "grad_norm": 0.6625936836346368,
      "learning_rate": 1.791285053446046e-05,
      "loss": 0.6063,
      "step": 2590
    },
    {
      "epoch": 0.23327106169393863,
      "grad_norm": 0.6578413829744137,
      "learning_rate": 1.7911067111826504e-05,
      "loss": 0.5964,
      "step": 2591
    },
    {
      "epoch": 0.23336109297981047,
      "grad_norm": 0.7467193928084875,
      "learning_rate": 1.7909283016430493e-05,
      "loss": 0.5582,
      "step": 2592
    },
    {
      "epoch": 0.23345112426568232,
      "grad_norm": 0.6333912799160631,
      "learning_rate": 1.7907498248424144e-05,
      "loss": 0.5575,
      "step": 2593
    },
    {
      "epoch": 0.23354115555155416,
      "grad_norm": 0.8246501458477035,
      "learning_rate": 1.7905712807959233e-05,
      "loss": 0.6314,
      "step": 2594
    },
    {
      "epoch": 0.233631186837426,
      "grad_norm": 0.6028374223547318,
      "learning_rate": 1.7903926695187595e-05,
      "loss": 0.6128,
      "step": 2595
    },
    {
      "epoch": 0.23372121812329785,
      "grad_norm": 0.7604149586754938,
      "learning_rate": 1.7902139910261124e-05,
      "loss": 0.564,
      "step": 2596
    },
    {
      "epoch": 0.2338112494091697,
      "grad_norm": 0.5391336532348553,
      "learning_rate": 1.7900352453331767e-05,
      "loss": 0.5382,
      "step": 2597
    },
    {
      "epoch": 0.2339012806950415,
      "grad_norm": 0.8509309837970485,
      "learning_rate": 1.789856432455153e-05,
      "loss": 0.5697,
      "step": 2598
    },
    {
      "epoch": 0.23399131198091336,
      "grad_norm": 0.7167331258096598,
      "learning_rate": 1.7896775524072484e-05,
      "loss": 0.6566,
      "step": 2599
    },
    {
      "epoch": 0.2340813432667852,
      "grad_norm": 0.7130773256610496,
      "learning_rate": 1.7894986052046738e-05,
      "loss": 0.5922,
      "step": 2600
    },
    {
      "epoch": 0.23417137455265705,
      "grad_norm": 0.8173956052693141,
      "learning_rate": 1.789319590862648e-05,
      "loss": 0.5199,
      "step": 2601
    },
    {
      "epoch": 0.2342614058385289,
      "grad_norm": 0.9380743902237657,
      "learning_rate": 1.789140509396394e-05,
      "loss": 0.6721,
      "step": 2602
    },
    {
      "epoch": 0.23435143712440074,
      "grad_norm": 0.7391099246122661,
      "learning_rate": 1.7889613608211406e-05,
      "loss": 0.5384,
      "step": 2603
    },
    {
      "epoch": 0.23444146841027258,
      "grad_norm": 0.8596629590156222,
      "learning_rate": 1.7887821451521235e-05,
      "loss": 0.548,
      "step": 2604
    },
    {
      "epoch": 0.2345314996961444,
      "grad_norm": 0.5927638535117828,
      "learning_rate": 1.7886028624045833e-05,
      "loss": 0.5879,
      "step": 2605
    },
    {
      "epoch": 0.23462153098201624,
      "grad_norm": 0.66071226359975,
      "learning_rate": 1.7884235125937658e-05,
      "loss": 0.5415,
      "step": 2606
    },
    {
      "epoch": 0.23471156226788809,
      "grad_norm": 0.7853991705492126,
      "learning_rate": 1.788244095734923e-05,
      "loss": 0.6367,
      "step": 2607
    },
    {
      "epoch": 0.23480159355375993,
      "grad_norm": 0.5755986837997642,
      "learning_rate": 1.7880646118433128e-05,
      "loss": 0.5389,
      "step": 2608
    },
    {
      "epoch": 0.23489162483963177,
      "grad_norm": 0.7355826816105021,
      "learning_rate": 1.787885060934199e-05,
      "loss": 0.6757,
      "step": 2609
    },
    {
      "epoch": 0.23498165612550362,
      "grad_norm": 0.9935122240878059,
      "learning_rate": 1.7877054430228504e-05,
      "loss": 0.5734,
      "step": 2610
    },
    {
      "epoch": 0.23507168741137546,
      "grad_norm": 0.6990331901561996,
      "learning_rate": 1.7875257581245417e-05,
      "loss": 0.5188,
      "step": 2611
    },
    {
      "epoch": 0.2351617186972473,
      "grad_norm": 0.7721231019277321,
      "learning_rate": 1.7873460062545534e-05,
      "loss": 0.6274,
      "step": 2612
    },
    {
      "epoch": 0.23525174998311912,
      "grad_norm": 0.729647841369928,
      "learning_rate": 1.787166187428172e-05,
      "loss": 0.6101,
      "step": 2613
    },
    {
      "epoch": 0.23534178126899097,
      "grad_norm": 0.7597491415608115,
      "learning_rate": 1.7869863016606893e-05,
      "loss": 0.6481,
      "step": 2614
    },
    {
      "epoch": 0.2354318125548628,
      "grad_norm": 0.6761179032853771,
      "learning_rate": 1.7868063489674027e-05,
      "loss": 0.6063,
      "step": 2615
    },
    {
      "epoch": 0.23552184384073466,
      "grad_norm": 0.7884764269531276,
      "learning_rate": 1.7866263293636152e-05,
      "loss": 0.5971,
      "step": 2616
    },
    {
      "epoch": 0.2356118751266065,
      "grad_norm": 0.8608039662634513,
      "learning_rate": 1.7864462428646367e-05,
      "loss": 0.655,
      "step": 2617
    },
    {
      "epoch": 0.23570190641247835,
      "grad_norm": 0.7934612708849613,
      "learning_rate": 1.7862660894857815e-05,
      "loss": 0.5868,
      "step": 2618
    },
    {
      "epoch": 0.2357919376983502,
      "grad_norm": 0.6619807235732951,
      "learning_rate": 1.7860858692423694e-05,
      "loss": 0.5713,
      "step": 2619
    },
    {
      "epoch": 0.235881968984222,
      "grad_norm": 0.7919964485224341,
      "learning_rate": 1.7859055821497272e-05,
      "loss": 0.5203,
      "step": 2620
    },
    {
      "epoch": 0.23597200027009385,
      "grad_norm": 0.7453115295237582,
      "learning_rate": 1.785725228223186e-05,
      "loss": 0.553,
      "step": 2621
    },
    {
      "epoch": 0.2360620315559657,
      "grad_norm": 0.863325158694672,
      "learning_rate": 1.785544807478084e-05,
      "loss": 0.582,
      "step": 2622
    },
    {
      "epoch": 0.23615206284183754,
      "grad_norm": 0.7165008916491833,
      "learning_rate": 1.7853643199297632e-05,
      "loss": 0.5429,
      "step": 2623
    },
    {
      "epoch": 0.23624209412770938,
      "grad_norm": 0.6774247518942588,
      "learning_rate": 1.7851837655935733e-05,
      "loss": 0.5818,
      "step": 2624
    },
    {
      "epoch": 0.23633212541358123,
      "grad_norm": 0.8508863424421603,
      "learning_rate": 1.7850031444848685e-05,
      "loss": 0.6124,
      "step": 2625
    },
    {
      "epoch": 0.23642215669945307,
      "grad_norm": 0.9131840577769178,
      "learning_rate": 1.7848224566190085e-05,
      "loss": 0.5014,
      "step": 2626
    },
    {
      "epoch": 0.2365121879853249,
      "grad_norm": 0.670851797115039,
      "learning_rate": 1.7846417020113596e-05,
      "loss": 0.528,
      "step": 2627
    },
    {
      "epoch": 0.23660221927119673,
      "grad_norm": 0.8509322157900796,
      "learning_rate": 1.7844608806772933e-05,
      "loss": 0.6442,
      "step": 2628
    },
    {
      "epoch": 0.23669225055706858,
      "grad_norm": 0.800338057429908,
      "learning_rate": 1.7842799926321867e-05,
      "loss": 0.579,
      "step": 2629
    },
    {
      "epoch": 0.23678228184294042,
      "grad_norm": 0.7136879317988786,
      "learning_rate": 1.7840990378914223e-05,
      "loss": 0.5624,
      "step": 2630
    },
    {
      "epoch": 0.23687231312881227,
      "grad_norm": 0.7714471045225346,
      "learning_rate": 1.7839180164703885e-05,
      "loss": 0.4959,
      "step": 2631
    },
    {
      "epoch": 0.2369623444146841,
      "grad_norm": 0.6075185048599764,
      "learning_rate": 1.7837369283844805e-05,
      "loss": 0.5677,
      "step": 2632
    },
    {
      "epoch": 0.23705237570055596,
      "grad_norm": 1.0599612059119714,
      "learning_rate": 1.783555773649097e-05,
      "loss": 0.5704,
      "step": 2633
    },
    {
      "epoch": 0.23714240698642777,
      "grad_norm": 0.9460818069712303,
      "learning_rate": 1.783374552279644e-05,
      "loss": 0.5608,
      "step": 2634
    },
    {
      "epoch": 0.23723243827229962,
      "grad_norm": 0.708848980409419,
      "learning_rate": 1.783193264291533e-05,
      "loss": 0.6036,
      "step": 2635
    },
    {
      "epoch": 0.23732246955817146,
      "grad_norm": 0.8279655857401635,
      "learning_rate": 1.7830119097001802e-05,
      "loss": 0.6111,
      "step": 2636
    },
    {
      "epoch": 0.2374125008440433,
      "grad_norm": 0.6899064992544908,
      "learning_rate": 1.7828304885210085e-05,
      "loss": 0.5771,
      "step": 2637
    },
    {
      "epoch": 0.23750253212991515,
      "grad_norm": 0.6913087554834673,
      "learning_rate": 1.782649000769446e-05,
      "loss": 0.5673,
      "step": 2638
    },
    {
      "epoch": 0.237592563415787,
      "grad_norm": 0.9868803146415911,
      "learning_rate": 1.7824674464609262e-05,
      "loss": 0.6448,
      "step": 2639
    },
    {
      "epoch": 0.23768259470165884,
      "grad_norm": 1.070613596995799,
      "learning_rate": 1.782285825610889e-05,
      "loss": 0.6549,
      "step": 2640
    },
    {
      "epoch": 0.23777262598753066,
      "grad_norm": 0.6811808798907153,
      "learning_rate": 1.7821041382347796e-05,
      "loss": 0.6136,
      "step": 2641
    },
    {
      "epoch": 0.2378626572734025,
      "grad_norm": 0.8028230524593463,
      "learning_rate": 1.7819223843480488e-05,
      "loss": 0.6912,
      "step": 2642
    },
    {
      "epoch": 0.23795268855927434,
      "grad_norm": 1.0103911731122028,
      "learning_rate": 1.7817405639661528e-05,
      "loss": 0.5182,
      "step": 2643
    },
    {
      "epoch": 0.2380427198451462,
      "grad_norm": 0.7396909098590876,
      "learning_rate": 1.7815586771045535e-05,
      "loss": 0.6282,
      "step": 2644
    },
    {
      "epoch": 0.23813275113101803,
      "grad_norm": 0.9715452349569675,
      "learning_rate": 1.7813767237787192e-05,
      "loss": 0.5158,
      "step": 2645
    },
    {
      "epoch": 0.23822278241688988,
      "grad_norm": 0.7519451044362406,
      "learning_rate": 1.781194704004123e-05,
      "loss": 0.6979,
      "step": 2646
    },
    {
      "epoch": 0.23831281370276172,
      "grad_norm": 0.659243708438794,
      "learning_rate": 1.781012617796244e-05,
      "loss": 0.535,
      "step": 2647
    },
    {
      "epoch": 0.23840284498863354,
      "grad_norm": 0.739897477343101,
      "learning_rate": 1.7808304651705674e-05,
      "loss": 0.6178,
      "step": 2648
    },
    {
      "epoch": 0.23849287627450538,
      "grad_norm": 0.8829957026852511,
      "learning_rate": 1.780648246142583e-05,
      "loss": 0.6114,
      "step": 2649
    },
    {
      "epoch": 0.23858290756037723,
      "grad_norm": 0.837583779496531,
      "learning_rate": 1.7804659607277873e-05,
      "loss": 0.6047,
      "step": 2650
    },
    {
      "epoch": 0.23867293884624907,
      "grad_norm": 0.8416303812496736,
      "learning_rate": 1.780283608941681e-05,
      "loss": 0.5518,
      "step": 2651
    },
    {
      "epoch": 0.23876297013212092,
      "grad_norm": 0.8262567964923518,
      "learning_rate": 1.7801011907997728e-05,
      "loss": 0.6407,
      "step": 2652
    },
    {
      "epoch": 0.23885300141799276,
      "grad_norm": 1.0171566630261095,
      "learning_rate": 1.7799187063175747e-05,
      "loss": 0.5716,
      "step": 2653
    },
    {
      "epoch": 0.2389430327038646,
      "grad_norm": 0.734749894586766,
      "learning_rate": 1.779736155510605e-05,
      "loss": 0.6127,
      "step": 2654
    },
    {
      "epoch": 0.23903306398973642,
      "grad_norm": 0.796167810787631,
      "learning_rate": 1.7795535383943892e-05,
      "loss": 0.6251,
      "step": 2655
    },
    {
      "epoch": 0.23912309527560827,
      "grad_norm": 0.7786764435437896,
      "learning_rate": 1.779370854984456e-05,
      "loss": 0.5501,
      "step": 2656
    },
    {
      "epoch": 0.2392131265614801,
      "grad_norm": 0.6684465465300717,
      "learning_rate": 1.7791881052963415e-05,
      "loss": 0.5325,
      "step": 2657
    },
    {
      "epoch": 0.23930315784735195,
      "grad_norm": 0.8513043556297117,
      "learning_rate": 1.779005289345587e-05,
      "loss": 0.5377,
      "step": 2658
    },
    {
      "epoch": 0.2393931891332238,
      "grad_norm": 0.7347732523550501,
      "learning_rate": 1.7788224071477386e-05,
      "loss": 0.6014,
      "step": 2659
    },
    {
      "epoch": 0.23948322041909564,
      "grad_norm": 0.8761579979077097,
      "learning_rate": 1.778639458718349e-05,
      "loss": 0.6321,
      "step": 2660
    },
    {
      "epoch": 0.2395732517049675,
      "grad_norm": 0.9496296772752106,
      "learning_rate": 1.7784564440729766e-05,
      "loss": 0.6323,
      "step": 2661
    },
    {
      "epoch": 0.2396632829908393,
      "grad_norm": 0.6093894191147482,
      "learning_rate": 1.778273363227185e-05,
      "loss": 0.5464,
      "step": 2662
    },
    {
      "epoch": 0.23975331427671115,
      "grad_norm": 0.5717184859281241,
      "learning_rate": 1.778090216196543e-05,
      "loss": 0.5921,
      "step": 2663
    },
    {
      "epoch": 0.239843345562583,
      "grad_norm": 0.542910107795766,
      "learning_rate": 1.7779070029966256e-05,
      "loss": 0.5281,
      "step": 2664
    },
    {
      "epoch": 0.23993337684845484,
      "grad_norm": 0.8733318787602604,
      "learning_rate": 1.777723723643014e-05,
      "loss": 0.5001,
      "step": 2665
    },
    {
      "epoch": 0.24002340813432668,
      "grad_norm": 0.8487681827393977,
      "learning_rate": 1.777540378151294e-05,
      "loss": 0.5689,
      "step": 2666
    },
    {
      "epoch": 0.24011343942019853,
      "grad_norm": 0.6330048823053867,
      "learning_rate": 1.7773569665370574e-05,
      "loss": 0.5345,
      "step": 2667
    },
    {
      "epoch": 0.24020347070607037,
      "grad_norm": 0.7955380785518895,
      "learning_rate": 1.7771734888159017e-05,
      "loss": 0.621,
      "step": 2668
    },
    {
      "epoch": 0.2402935019919422,
      "grad_norm": 0.7923693820805203,
      "learning_rate": 1.77698994500343e-05,
      "loss": 0.5825,
      "step": 2669
    },
    {
      "epoch": 0.24038353327781403,
      "grad_norm": 0.8185564529831233,
      "learning_rate": 1.776806335115251e-05,
      "loss": 0.5434,
      "step": 2670
    },
    {
      "epoch": 0.24047356456368588,
      "grad_norm": 0.7247012189730473,
      "learning_rate": 1.7766226591669787e-05,
      "loss": 0.6716,
      "step": 2671
    },
    {
      "epoch": 0.24056359584955772,
      "grad_norm": 0.724180991488985,
      "learning_rate": 1.7764389171742332e-05,
      "loss": 0.5238,
      "step": 2672
    },
    {
      "epoch": 0.24065362713542957,
      "grad_norm": 0.6670331453838486,
      "learning_rate": 1.77625510915264e-05,
      "loss": 0.7228,
      "step": 2673
    },
    {
      "epoch": 0.2407436584213014,
      "grad_norm": 0.7357468387168227,
      "learning_rate": 1.7760712351178305e-05,
      "loss": 0.5754,
      "step": 2674
    },
    {
      "epoch": 0.24083368970717325,
      "grad_norm": 0.7455456955032818,
      "learning_rate": 1.775887295085441e-05,
      "loss": 0.6306,
      "step": 2675
    },
    {
      "epoch": 0.24092372099304507,
      "grad_norm": 0.6452789359241664,
      "learning_rate": 1.7757032890711147e-05,
      "loss": 0.5979,
      "step": 2676
    },
    {
      "epoch": 0.24101375227891692,
      "grad_norm": 0.7449416875353314,
      "learning_rate": 1.7755192170904988e-05,
      "loss": 0.6455,
      "step": 2677
    },
    {
      "epoch": 0.24110378356478876,
      "grad_norm": 0.698917827888079,
      "learning_rate": 1.7753350791592467e-05,
      "loss": 0.5324,
      "step": 2678
    },
    {
      "epoch": 0.2411938148506606,
      "grad_norm": 1.0027722668816466,
      "learning_rate": 1.7751508752930182e-05,
      "loss": 0.5539,
      "step": 2679
    },
    {
      "epoch": 0.24128384613653245,
      "grad_norm": 0.624010929050767,
      "learning_rate": 1.774966605507478e-05,
      "loss": 0.5275,
      "step": 2680
    },
    {
      "epoch": 0.2413738774224043,
      "grad_norm": 0.8275145972605055,
      "learning_rate": 1.774782269818297e-05,
      "loss": 0.5766,
      "step": 2681
    },
    {
      "epoch": 0.24146390870827614,
      "grad_norm": 0.6909706233823525,
      "learning_rate": 1.77459786824115e-05,
      "loss": 0.4874,
      "step": 2682
    },
    {
      "epoch": 0.24155393999414795,
      "grad_norm": 0.6878970370497802,
      "learning_rate": 1.7744134007917195e-05,
      "loss": 0.6857,
      "step": 2683
    },
    {
      "epoch": 0.2416439712800198,
      "grad_norm": 0.8533488453946627,
      "learning_rate": 1.7742288674856928e-05,
      "loss": 0.6307,
      "step": 2684
    },
    {
      "epoch": 0.24173400256589164,
      "grad_norm": 0.6766453985173057,
      "learning_rate": 1.774044268338762e-05,
      "loss": 0.6416,
      "step": 2685
    },
    {
      "epoch": 0.2418240338517635,
      "grad_norm": 0.650647491131353,
      "learning_rate": 1.773859603366626e-05,
      "loss": 0.5196,
      "step": 2686
    },
    {
      "epoch": 0.24191406513763533,
      "grad_norm": 0.7316352500701546,
      "learning_rate": 1.7736748725849892e-05,
      "loss": 0.5311,
      "step": 2687
    },
    {
      "epoch": 0.24200409642350718,
      "grad_norm": 0.6783865232438224,
      "learning_rate": 1.7734900760095605e-05,
      "loss": 0.5282,
      "step": 2688
    },
    {
      "epoch": 0.24209412770937902,
      "grad_norm": 0.5985477929010865,
      "learning_rate": 1.773305213656056e-05,
      "loss": 0.6566,
      "step": 2689
    },
    {
      "epoch": 0.24218415899525084,
      "grad_norm": 0.9297518592136598,
      "learning_rate": 1.773120285540195e-05,
      "loss": 0.5973,
      "step": 2690
    },
    {
      "epoch": 0.24227419028112268,
      "grad_norm": 0.6953266521291773,
      "learning_rate": 1.7729352916777056e-05,
      "loss": 0.5678,
      "step": 2691
    },
    {
      "epoch": 0.24236422156699453,
      "grad_norm": 0.7558106094346525,
      "learning_rate": 1.772750232084319e-05,
      "loss": 0.5113,
      "step": 2692
    },
    {
      "epoch": 0.24245425285286637,
      "grad_norm": 0.811529662099999,
      "learning_rate": 1.7725651067757724e-05,
      "loss": 0.6433,
      "step": 2693
    },
    {
      "epoch": 0.24254428413873821,
      "grad_norm": 0.6984494261667152,
      "learning_rate": 1.7723799157678095e-05,
      "loss": 0.4929,
      "step": 2694
    },
    {
      "epoch": 0.24263431542461006,
      "grad_norm": 0.7094697246378956,
      "learning_rate": 1.7721946590761794e-05,
      "loss": 0.6536,
      "step": 2695
    },
    {
      "epoch": 0.2427243467104819,
      "grad_norm": 0.8840181455071207,
      "learning_rate": 1.7720093367166357e-05,
      "loss": 0.529,
      "step": 2696
    },
    {
      "epoch": 0.24281437799635372,
      "grad_norm": 0.6010758232282717,
      "learning_rate": 1.7718239487049385e-05,
      "loss": 0.5618,
      "step": 2697
    },
    {
      "epoch": 0.24290440928222556,
      "grad_norm": 0.6881586749262465,
      "learning_rate": 1.7716384950568537e-05,
      "loss": 0.5426,
      "step": 2698
    },
    {
      "epoch": 0.2429944405680974,
      "grad_norm": 0.650799362020396,
      "learning_rate": 1.7714529757881524e-05,
      "loss": 0.5591,
      "step": 2699
    },
    {
      "epoch": 0.24308447185396925,
      "grad_norm": 0.6645982200639865,
      "learning_rate": 1.7712673909146104e-05,
      "loss": 0.5615,
      "step": 2700
    },
    {
      "epoch": 0.2431745031398411,
      "grad_norm": 0.6981588109159169,
      "learning_rate": 1.7710817404520106e-05,
      "loss": 0.5148,
      "step": 2701
    },
    {
      "epoch": 0.24326453442571294,
      "grad_norm": 0.6825138218090032,
      "learning_rate": 1.770896024416141e-05,
      "loss": 0.5708,
      "step": 2702
    },
    {
      "epoch": 0.24335456571158479,
      "grad_norm": 1.0362771701320217,
      "learning_rate": 1.770710242822795e-05,
      "loss": 0.5048,
      "step": 2703
    },
    {
      "epoch": 0.2434445969974566,
      "grad_norm": 0.8159642386200898,
      "learning_rate": 1.770524395687771e-05,
      "loss": 0.5858,
      "step": 2704
    },
    {
      "epoch": 0.24353462828332845,
      "grad_norm": 0.6564752130457111,
      "learning_rate": 1.7703384830268742e-05,
      "loss": 0.5077,
      "step": 2705
    },
    {
      "epoch": 0.2436246595692003,
      "grad_norm": 0.6099477553199019,
      "learning_rate": 1.7701525048559143e-05,
      "loss": 0.5443,
      "step": 2706
    },
    {
      "epoch": 0.24371469085507214,
      "grad_norm": 1.096834700144895,
      "learning_rate": 1.769966461190707e-05,
      "loss": 0.5364,
      "step": 2707
    },
    {
      "epoch": 0.24380472214094398,
      "grad_norm": 0.7542647287024462,
      "learning_rate": 1.7697803520470745e-05,
      "loss": 0.5667,
      "step": 2708
    },
    {
      "epoch": 0.24389475342681582,
      "grad_norm": 0.7136768649289806,
      "learning_rate": 1.7695941774408424e-05,
      "loss": 0.4897,
      "step": 2709
    },
    {
      "epoch": 0.24398478471268767,
      "grad_norm": 0.7171059076109463,
      "learning_rate": 1.7694079373878435e-05,
      "loss": 0.7047,
      "step": 2710
    },
    {
      "epoch": 0.24407481599855949,
      "grad_norm": 0.7119008219848435,
      "learning_rate": 1.7692216319039158e-05,
      "loss": 0.5135,
      "step": 2711
    },
    {
      "epoch": 0.24416484728443133,
      "grad_norm": 0.6457543278727782,
      "learning_rate": 1.769035261004903e-05,
      "loss": 0.5139,
      "step": 2712
    },
    {
      "epoch": 0.24425487857030317,
      "grad_norm": 0.7121723464955206,
      "learning_rate": 1.768848824706654e-05,
      "loss": 0.7093,
      "step": 2713
    },
    {
      "epoch": 0.24434490985617502,
      "grad_norm": 0.8463456189539876,
      "learning_rate": 1.7686623230250237e-05,
      "loss": 0.5855,
      "step": 2714
    },
    {
      "epoch": 0.24443494114204686,
      "grad_norm": 0.7087108303810855,
      "learning_rate": 1.768475755975872e-05,
      "loss": 0.4994,
      "step": 2715
    },
    {
      "epoch": 0.2445249724279187,
      "grad_norm": 1.232740434588212,
      "learning_rate": 1.7682891235750647e-05,
      "loss": 0.5488,
      "step": 2716
    },
    {
      "epoch": 0.24461500371379055,
      "grad_norm": 0.9418999025586958,
      "learning_rate": 1.768102425838474e-05,
      "loss": 0.5181,
      "step": 2717
    },
    {
      "epoch": 0.2447050349996624,
      "grad_norm": 0.7666055224638683,
      "learning_rate": 1.767915662781975e-05,
      "loss": 0.5646,
      "step": 2718
    },
    {
      "epoch": 0.2447950662855342,
      "grad_norm": 1.3865762786815221,
      "learning_rate": 1.767728834421452e-05,
      "loss": 0.5671,
      "step": 2719
    },
    {
      "epoch": 0.24488509757140606,
      "grad_norm": 0.5940206470618645,
      "learning_rate": 1.7675419407727917e-05,
      "loss": 0.5877,
      "step": 2720
    },
    {
      "epoch": 0.2449751288572779,
      "grad_norm": 0.7208787265929967,
      "learning_rate": 1.7673549818518884e-05,
      "loss": 0.5887,
      "step": 2721
    },
    {
      "epoch": 0.24506516014314975,
      "grad_norm": 0.7205364474746765,
      "learning_rate": 1.7671679576746412e-05,
      "loss": 0.5832,
      "step": 2722
    },
    {
      "epoch": 0.2451551914290216,
      "grad_norm": 0.7068049954885826,
      "learning_rate": 1.7669808682569544e-05,
      "loss": 0.5787,
      "step": 2723
    },
    {
      "epoch": 0.24524522271489343,
      "grad_norm": 0.8527614765200792,
      "learning_rate": 1.766793713614738e-05,
      "loss": 0.6205,
      "step": 2724
    },
    {
      "epoch": 0.24533525400076528,
      "grad_norm": 0.5894505028667534,
      "learning_rate": 1.766606493763908e-05,
      "loss": 0.5318,
      "step": 2725
    },
    {
      "epoch": 0.2454252852866371,
      "grad_norm": 0.773522490631857,
      "learning_rate": 1.766419208720386e-05,
      "loss": 0.6137,
      "step": 2726
    },
    {
      "epoch": 0.24551531657250894,
      "grad_norm": 0.7353968715725604,
      "learning_rate": 1.7662318585000985e-05,
      "loss": 0.5478,
      "step": 2727
    },
    {
      "epoch": 0.24560534785838078,
      "grad_norm": 0.6271974311908579,
      "learning_rate": 1.766044443118978e-05,
      "loss": 0.4703,
      "step": 2728
    },
    {
      "epoch": 0.24569537914425263,
      "grad_norm": 0.8971653752650793,
      "learning_rate": 1.7658569625929625e-05,
      "loss": 0.4994,
      "step": 2729
    },
    {
      "epoch": 0.24578541043012447,
      "grad_norm": 0.6427739811630443,
      "learning_rate": 1.7656694169379952e-05,
      "loss": 0.5657,
      "step": 2730
    },
    {
      "epoch": 0.24587544171599632,
      "grad_norm": 0.9515925632909873,
      "learning_rate": 1.7654818061700253e-05,
      "loss": 0.5334,
      "step": 2731
    },
    {
      "epoch": 0.24596547300186816,
      "grad_norm": 0.5577341872494546,
      "learning_rate": 1.7652941303050076e-05,
      "loss": 0.5111,
      "step": 2732
    },
    {
      "epoch": 0.24605550428773998,
      "grad_norm": 0.7299392465537118,
      "learning_rate": 1.7651063893589015e-05,
      "loss": 0.5093,
      "step": 2733
    },
    {
      "epoch": 0.24614553557361182,
      "grad_norm": 0.7327361949902572,
      "learning_rate": 1.7649185833476733e-05,
      "loss": 0.6025,
      "step": 2734
    },
    {
      "epoch": 0.24623556685948367,
      "grad_norm": 0.9431695682528505,
      "learning_rate": 1.7647307122872932e-05,
      "loss": 0.6455,
      "step": 2735
    },
    {
      "epoch": 0.2463255981453555,
      "grad_norm": 0.714274545406901,
      "learning_rate": 1.7645427761937388e-05,
      "loss": 0.5142,
      "step": 2736
    },
    {
      "epoch": 0.24641562943122736,
      "grad_norm": 0.9058127345112629,
      "learning_rate": 1.764354775082992e-05,
      "loss": 0.5868,
      "step": 2737
    },
    {
      "epoch": 0.2465056607170992,
      "grad_norm": 0.7312383443370285,
      "learning_rate": 1.7641667089710405e-05,
      "loss": 0.568,
      "step": 2738
    },
    {
      "epoch": 0.24659569200297105,
      "grad_norm": 0.9334758098351792,
      "learning_rate": 1.7639785778738777e-05,
      "loss": 0.4755,
      "step": 2739
    },
    {
      "epoch": 0.24668572328884286,
      "grad_norm": 0.6933556701469249,
      "learning_rate": 1.763790381807502e-05,
      "loss": 0.6121,
      "step": 2740
    },
    {
      "epoch": 0.2467757545747147,
      "grad_norm": 1.116566180099413,
      "learning_rate": 1.763602120787918e-05,
      "loss": 0.7482,
      "step": 2741
    },
    {
      "epoch": 0.24686578586058655,
      "grad_norm": 0.887752824643889,
      "learning_rate": 1.7634137948311358e-05,
      "loss": 0.6871,
      "step": 2742
    },
    {
      "epoch": 0.2469558171464584,
      "grad_norm": 0.635273213486222,
      "learning_rate": 1.76322540395317e-05,
      "loss": 0.5791,
      "step": 2743
    },
    {
      "epoch": 0.24704584843233024,
      "grad_norm": 0.8164274484001081,
      "learning_rate": 1.7630369481700422e-05,
      "loss": 0.5575,
      "step": 2744
    },
    {
      "epoch": 0.24713587971820208,
      "grad_norm": 0.6697242394418788,
      "learning_rate": 1.7628484274977783e-05,
      "loss": 0.5174,
      "step": 2745
    },
    {
      "epoch": 0.24722591100407393,
      "grad_norm": 0.8259124539231336,
      "learning_rate": 1.762659841952411e-05,
      "loss": 0.5658,
      "step": 2746
    },
    {
      "epoch": 0.24731594228994574,
      "grad_norm": 0.607122761352975,
      "learning_rate": 1.7624711915499767e-05,
      "loss": 0.5387,
      "step": 2747
    },
    {
      "epoch": 0.2474059735758176,
      "grad_norm": 0.6813623079636604,
      "learning_rate": 1.7622824763065188e-05,
      "loss": 0.6435,
      "step": 2748
    },
    {
      "epoch": 0.24749600486168943,
      "grad_norm": 0.7408310514867131,
      "learning_rate": 1.762093696238086e-05,
      "loss": 0.5493,
      "step": 2749
    },
    {
      "epoch": 0.24758603614756128,
      "grad_norm": 0.6809312741662243,
      "learning_rate": 1.7619048513607318e-05,
      "loss": 0.5442,
      "step": 2750
    },
    {
      "epoch": 0.24767606743343312,
      "grad_norm": 0.7102830523433519,
      "learning_rate": 1.761715941690516e-05,
      "loss": 0.5275,
      "step": 2751
    },
    {
      "epoch": 0.24776609871930497,
      "grad_norm": 0.7644135602677903,
      "learning_rate": 1.7615269672435034e-05,
      "loss": 0.5336,
      "step": 2752
    },
    {
      "epoch": 0.2478561300051768,
      "grad_norm": 0.706958656311026,
      "learning_rate": 1.761337928035765e-05,
      "loss": 0.6597,
      "step": 2753
    },
    {
      "epoch": 0.24794616129104863,
      "grad_norm": 0.661637378770217,
      "learning_rate": 1.7611488240833764e-05,
      "loss": 0.5736,
      "step": 2754
    },
    {
      "epoch": 0.24803619257692047,
      "grad_norm": 0.7795850590865079,
      "learning_rate": 1.760959655402419e-05,
      "loss": 0.6166,
      "step": 2755
    },
    {
      "epoch": 0.24812622386279232,
      "grad_norm": 0.706682364125734,
      "learning_rate": 1.76077042200898e-05,
      "loss": 0.5673,
      "step": 2756
    },
    {
      "epoch": 0.24821625514866416,
      "grad_norm": 1.1235110925209044,
      "learning_rate": 1.760581123919152e-05,
      "loss": 0.5852,
      "step": 2757
    },
    {
      "epoch": 0.248306286434536,
      "grad_norm": 0.9251172534018632,
      "learning_rate": 1.760391761149033e-05,
      "loss": 0.6249,
      "step": 2758
    },
    {
      "epoch": 0.24839631772040785,
      "grad_norm": 1.0085270520320826,
      "learning_rate": 1.7602023337147263e-05,
      "loss": 0.6274,
      "step": 2759
    },
    {
      "epoch": 0.2484863490062797,
      "grad_norm": 0.6255356361154292,
      "learning_rate": 1.7600128416323413e-05,
      "loss": 0.5055,
      "step": 2760
    },
    {
      "epoch": 0.2485763802921515,
      "grad_norm": 0.6223562685582569,
      "learning_rate": 1.7598232849179923e-05,
      "loss": 0.504,
      "step": 2761
    },
    {
      "epoch": 0.24866641157802336,
      "grad_norm": 0.772917173242115,
      "learning_rate": 1.7596336635877992e-05,
      "loss": 0.6205,
      "step": 2762
    },
    {
      "epoch": 0.2487564428638952,
      "grad_norm": 0.6697449248653417,
      "learning_rate": 1.7594439776578877e-05,
      "loss": 0.58,
      "step": 2763
    },
    {
      "epoch": 0.24884647414976704,
      "grad_norm": 0.6485183233720138,
      "learning_rate": 1.7592542271443888e-05,
      "loss": 0.4777,
      "step": 2764
    },
    {
      "epoch": 0.2489365054356389,
      "grad_norm": 0.6301386355386281,
      "learning_rate": 1.759064412063439e-05,
      "loss": 0.5926,
      "step": 2765
    },
    {
      "epoch": 0.24902653672151073,
      "grad_norm": 0.6773904984644947,
      "learning_rate": 1.75887453243118e-05,
      "loss": 0.5967,
      "step": 2766
    },
    {
      "epoch": 0.24911656800738258,
      "grad_norm": 0.7628084285379633,
      "learning_rate": 1.7586845882637603e-05,
      "loss": 0.626,
      "step": 2767
    },
    {
      "epoch": 0.2492065992932544,
      "grad_norm": 0.6738199319516309,
      "learning_rate": 1.7584945795773313e-05,
      "loss": 0.5046,
      "step": 2768
    },
    {
      "epoch": 0.24929663057912624,
      "grad_norm": 0.728237362159407,
      "learning_rate": 1.7583045063880525e-05,
      "loss": 0.5965,
      "step": 2769
    },
    {
      "epoch": 0.24938666186499808,
      "grad_norm": 0.978021693426121,
      "learning_rate": 1.7581143687120877e-05,
      "loss": 0.591,
      "step": 2770
    },
    {
      "epoch": 0.24947669315086993,
      "grad_norm": 0.9761312351443426,
      "learning_rate": 1.7579241665656063e-05,
      "loss": 0.7131,
      "step": 2771
    },
    {
      "epoch": 0.24956672443674177,
      "grad_norm": 0.8819528996507756,
      "learning_rate": 1.757733899964783e-05,
      "loss": 0.5537,
      "step": 2772
    },
    {
      "epoch": 0.24965675572261362,
      "grad_norm": 0.6502688354234141,
      "learning_rate": 1.7575435689257984e-05,
      "loss": 0.6268,
      "step": 2773
    },
    {
      "epoch": 0.24974678700848546,
      "grad_norm": 0.7400483002848884,
      "learning_rate": 1.7573531734648384e-05,
      "loss": 0.5826,
      "step": 2774
    },
    {
      "epoch": 0.24983681829435728,
      "grad_norm": 0.7006154197549251,
      "learning_rate": 1.757162713598094e-05,
      "loss": 0.6223,
      "step": 2775
    },
    {
      "epoch": 0.24992684958022912,
      "grad_norm": 0.7734761371579577,
      "learning_rate": 1.7569721893417626e-05,
      "loss": 0.6237,
      "step": 2776
    },
    {
      "epoch": 0.25001688086610097,
      "grad_norm": 0.758368747567254,
      "learning_rate": 1.7567816007120462e-05,
      "loss": 0.52,
      "step": 2777
    },
    {
      "epoch": 0.25010691215197284,
      "grad_norm": 0.9530242503213276,
      "learning_rate": 1.7565909477251524e-05,
      "loss": 0.6777,
      "step": 2778
    },
    {
      "epoch": 0.25019694343784465,
      "grad_norm": 0.6057897473138055,
      "learning_rate": 1.7564002303972944e-05,
      "loss": 0.5497,
      "step": 2779
    },
    {
      "epoch": 0.25028697472371647,
      "grad_norm": 0.7377375920879867,
      "learning_rate": 1.7562094487446915e-05,
      "loss": 0.6265,
      "step": 2780
    },
    {
      "epoch": 0.25037700600958834,
      "grad_norm": 0.7356363656528003,
      "learning_rate": 1.7560186027835675e-05,
      "loss": 0.5209,
      "step": 2781
    },
    {
      "epoch": 0.25046703729546016,
      "grad_norm": 0.6641468111482236,
      "learning_rate": 1.7558276925301518e-05,
      "loss": 0.6074,
      "step": 2782
    },
    {
      "epoch": 0.25055706858133203,
      "grad_norm": 0.8153347781049737,
      "learning_rate": 1.75563671800068e-05,
      "loss": 0.5972,
      "step": 2783
    },
    {
      "epoch": 0.25064709986720385,
      "grad_norm": 0.8341463251483091,
      "learning_rate": 1.7554456792113922e-05,
      "loss": 0.4699,
      "step": 2784
    },
    {
      "epoch": 0.2507371311530757,
      "grad_norm": 0.7515346849483726,
      "learning_rate": 1.755254576178535e-05,
      "loss": 0.4996,
      "step": 2785
    },
    {
      "epoch": 0.25082716243894754,
      "grad_norm": 0.7355082282461418,
      "learning_rate": 1.7550634089183595e-05,
      "loss": 0.6181,
      "step": 2786
    },
    {
      "epoch": 0.25091719372481935,
      "grad_norm": 0.7224472315753964,
      "learning_rate": 1.754872177447123e-05,
      "loss": 0.6178,
      "step": 2787
    },
    {
      "epoch": 0.2510072250106912,
      "grad_norm": 0.7996939361759231,
      "learning_rate": 1.7546808817810878e-05,
      "loss": 0.5889,
      "step": 2788
    },
    {
      "epoch": 0.25109725629656304,
      "grad_norm": 0.8234398270383523,
      "learning_rate": 1.754489521936522e-05,
      "loss": 0.6114,
      "step": 2789
    },
    {
      "epoch": 0.2511872875824349,
      "grad_norm": 0.7065803572045052,
      "learning_rate": 1.7542980979296986e-05,
      "loss": 0.5365,
      "step": 2790
    },
    {
      "epoch": 0.25127731886830673,
      "grad_norm": 0.6562689345348064,
      "learning_rate": 1.7541066097768965e-05,
      "loss": 0.5747,
      "step": 2791
    },
    {
      "epoch": 0.2513673501541786,
      "grad_norm": 1.2108319202376594,
      "learning_rate": 1.7539150574944002e-05,
      "loss": 0.6105,
      "step": 2792
    },
    {
      "epoch": 0.2514573814400504,
      "grad_norm": 0.6887421298216212,
      "learning_rate": 1.753723441098499e-05,
      "loss": 0.5286,
      "step": 2793
    },
    {
      "epoch": 0.25154741272592224,
      "grad_norm": 0.7491788017348227,
      "learning_rate": 1.7535317606054884e-05,
      "loss": 0.6326,
      "step": 2794
    },
    {
      "epoch": 0.2516374440117941,
      "grad_norm": 0.756919556832123,
      "learning_rate": 1.753340016031669e-05,
      "loss": 0.6394,
      "step": 2795
    },
    {
      "epoch": 0.2517274752976659,
      "grad_norm": 0.6135670361367117,
      "learning_rate": 1.753148207393347e-05,
      "loss": 0.5403,
      "step": 2796
    },
    {
      "epoch": 0.2518175065835378,
      "grad_norm": 0.6003106907586696,
      "learning_rate": 1.752956334706834e-05,
      "loss": 0.5032,
      "step": 2797
    },
    {
      "epoch": 0.2519075378694096,
      "grad_norm": 0.7393598475669508,
      "learning_rate": 1.7527643979884463e-05,
      "loss": 0.6586,
      "step": 2798
    },
    {
      "epoch": 0.2519975691552815,
      "grad_norm": 0.8566697769709548,
      "learning_rate": 1.752572397254507e-05,
      "loss": 0.6037,
      "step": 2799
    },
    {
      "epoch": 0.2520876004411533,
      "grad_norm": 0.6432280991218833,
      "learning_rate": 1.7523803325213436e-05,
      "loss": 0.5488,
      "step": 2800
    },
    {
      "epoch": 0.2521776317270251,
      "grad_norm": 0.7693964064091738,
      "learning_rate": 1.7521882038052894e-05,
      "loss": 0.5822,
      "step": 2801
    },
    {
      "epoch": 0.252267663012897,
      "grad_norm": 0.9554154035328023,
      "learning_rate": 1.751996011122684e-05,
      "loss": 0.5995,
      "step": 2802
    },
    {
      "epoch": 0.2523576942987688,
      "grad_norm": 0.7149402408833953,
      "learning_rate": 1.75180375448987e-05,
      "loss": 0.6307,
      "step": 2803
    },
    {
      "epoch": 0.2524477255846407,
      "grad_norm": 0.631825150625461,
      "learning_rate": 1.7516114339231984e-05,
      "loss": 0.5555,
      "step": 2804
    },
    {
      "epoch": 0.2525377568705125,
      "grad_norm": 1.144965767564067,
      "learning_rate": 1.7514190494390236e-05,
      "loss": 0.6598,
      "step": 2805
    },
    {
      "epoch": 0.25262778815638437,
      "grad_norm": 0.660764840585905,
      "learning_rate": 1.7512266010537065e-05,
      "loss": 0.4646,
      "step": 2806
    },
    {
      "epoch": 0.2527178194422562,
      "grad_norm": 0.8591989718095105,
      "learning_rate": 1.7510340887836124e-05,
      "loss": 0.561,
      "step": 2807
    },
    {
      "epoch": 0.252807850728128,
      "grad_norm": 0.7288860771385304,
      "learning_rate": 1.750841512645113e-05,
      "loss": 0.5519,
      "step": 2808
    },
    {
      "epoch": 0.2528978820139999,
      "grad_norm": 0.8795243003393768,
      "learning_rate": 1.7506488726545858e-05,
      "loss": 0.572,
      "step": 2809
    },
    {
      "epoch": 0.2529879132998717,
      "grad_norm": 1.175196492431648,
      "learning_rate": 1.7504561688284118e-05,
      "loss": 0.5777,
      "step": 2810
    },
    {
      "epoch": 0.25307794458574356,
      "grad_norm": 0.7835899780475833,
      "learning_rate": 1.7502634011829788e-05,
      "loss": 0.577,
      "step": 2811
    },
    {
      "epoch": 0.2531679758716154,
      "grad_norm": 1.0936653815738686,
      "learning_rate": 1.750070569734681e-05,
      "loss": 0.5418,
      "step": 2812
    },
    {
      "epoch": 0.25325800715748725,
      "grad_norm": 0.8211767614609164,
      "learning_rate": 1.749877674499916e-05,
      "loss": 0.6486,
      "step": 2813
    },
    {
      "epoch": 0.25334803844335907,
      "grad_norm": 0.7095221249499014,
      "learning_rate": 1.749684715495088e-05,
      "loss": 0.5183,
      "step": 2814
    },
    {
      "epoch": 0.2534380697292309,
      "grad_norm": 0.8416307462100729,
      "learning_rate": 1.749491692736606e-05,
      "loss": 0.6379,
      "step": 2815
    },
    {
      "epoch": 0.25352810101510276,
      "grad_norm": 0.802665363387365,
      "learning_rate": 1.7492986062408854e-05,
      "loss": 0.6547,
      "step": 2816
    },
    {
      "epoch": 0.2536181323009746,
      "grad_norm": 0.7900596921883982,
      "learning_rate": 1.7491054560243458e-05,
      "loss": 0.61,
      "step": 2817
    },
    {
      "epoch": 0.25370816358684645,
      "grad_norm": 0.7777968363243903,
      "learning_rate": 1.7489122421034134e-05,
      "loss": 0.5856,
      "step": 2818
    },
    {
      "epoch": 0.25379819487271826,
      "grad_norm": 0.739894240197453,
      "learning_rate": 1.7487189644945185e-05,
      "loss": 0.563,
      "step": 2819
    },
    {
      "epoch": 0.25388822615859014,
      "grad_norm": 0.856890975666392,
      "learning_rate": 1.748525623214098e-05,
      "loss": 0.5764,
      "step": 2820
    },
    {
      "epoch": 0.25397825744446195,
      "grad_norm": 0.6610413534831288,
      "learning_rate": 1.7483322182785942e-05,
      "loss": 0.5828,
      "step": 2821
    },
    {
      "epoch": 0.25406828873033377,
      "grad_norm": 0.7727347909682057,
      "learning_rate": 1.7481387497044533e-05,
      "loss": 0.6078,
      "step": 2822
    },
    {
      "epoch": 0.25415832001620564,
      "grad_norm": 0.7022845355991264,
      "learning_rate": 1.747945217508129e-05,
      "loss": 0.5127,
      "step": 2823
    },
    {
      "epoch": 0.25424835130207746,
      "grad_norm": 0.7725267275354006,
      "learning_rate": 1.7477516217060785e-05,
      "loss": 0.5993,
      "step": 2824
    },
    {
      "epoch": 0.25433838258794933,
      "grad_norm": 0.6720618238658895,
      "learning_rate": 1.7475579623147664e-05,
      "loss": 0.5383,
      "step": 2825
    },
    {
      "epoch": 0.25442841387382115,
      "grad_norm": 0.6278630598074484,
      "learning_rate": 1.7473642393506604e-05,
      "loss": 0.5573,
      "step": 2826
    },
    {
      "epoch": 0.254518445159693,
      "grad_norm": 0.6868512324450958,
      "learning_rate": 1.747170452830236e-05,
      "loss": 0.4897,
      "step": 2827
    },
    {
      "epoch": 0.25460847644556484,
      "grad_norm": 0.6376005922814054,
      "learning_rate": 1.7469766027699716e-05,
      "loss": 0.5512,
      "step": 2828
    },
    {
      "epoch": 0.25469850773143665,
      "grad_norm": 0.7431390376050739,
      "learning_rate": 1.7467826891863535e-05,
      "loss": 0.619,
      "step": 2829
    },
    {
      "epoch": 0.2547885390173085,
      "grad_norm": 0.6635371274786535,
      "learning_rate": 1.7465887120958717e-05,
      "loss": 0.6268,
      "step": 2830
    },
    {
      "epoch": 0.25487857030318034,
      "grad_norm": 0.73660120008902,
      "learning_rate": 1.7463946715150225e-05,
      "loss": 0.5556,
      "step": 2831
    },
    {
      "epoch": 0.2549686015890522,
      "grad_norm": 0.9898110894738893,
      "learning_rate": 1.7462005674603067e-05,
      "loss": 0.5588,
      "step": 2832
    },
    {
      "epoch": 0.25505863287492403,
      "grad_norm": 0.6937749541323475,
      "learning_rate": 1.7460063999482314e-05,
      "loss": 0.5483,
      "step": 2833
    },
    {
      "epoch": 0.2551486641607959,
      "grad_norm": 0.7784141022455662,
      "learning_rate": 1.7458121689953085e-05,
      "loss": 0.5787,
      "step": 2834
    },
    {
      "epoch": 0.2552386954466677,
      "grad_norm": 0.9456631713934478,
      "learning_rate": 1.745617874618056e-05,
      "loss": 0.6066,
      "step": 2835
    },
    {
      "epoch": 0.25532872673253953,
      "grad_norm": 0.9082620027867815,
      "learning_rate": 1.7454235168329962e-05,
      "loss": 0.5625,
      "step": 2836
    },
    {
      "epoch": 0.2554187580184114,
      "grad_norm": 0.6452738193224822,
      "learning_rate": 1.7452290956566574e-05,
      "loss": 0.6118,
      "step": 2837
    },
    {
      "epoch": 0.2555087893042832,
      "grad_norm": 0.7555511337599511,
      "learning_rate": 1.7450346111055737e-05,
      "loss": 0.5815,
      "step": 2838
    },
    {
      "epoch": 0.2555988205901551,
      "grad_norm": 0.8549059158836636,
      "learning_rate": 1.7448400631962843e-05,
      "loss": 0.5729,
      "step": 2839
    },
    {
      "epoch": 0.2556888518760269,
      "grad_norm": 0.6761877384239815,
      "learning_rate": 1.7446454519453337e-05,
      "loss": 0.6252,
      "step": 2840
    },
    {
      "epoch": 0.2557788831618988,
      "grad_norm": 0.9007770396257085,
      "learning_rate": 1.744450777369271e-05,
      "loss": 0.6574,
      "step": 2841
    },
    {
      "epoch": 0.2558689144477706,
      "grad_norm": 0.7342991284532364,
      "learning_rate": 1.7442560394846518e-05,
      "loss": 0.5731,
      "step": 2842
    },
    {
      "epoch": 0.2559589457336424,
      "grad_norm": 0.6584736568040356,
      "learning_rate": 1.744061238308037e-05,
      "loss": 0.5854,
      "step": 2843
    },
    {
      "epoch": 0.2560489770195143,
      "grad_norm": 0.7191463934597917,
      "learning_rate": 1.7438663738559928e-05,
      "loss": 0.5349,
      "step": 2844
    },
    {
      "epoch": 0.2561390083053861,
      "grad_norm": 0.6468486027330392,
      "learning_rate": 1.74367144614509e-05,
      "loss": 0.569,
      "step": 2845
    },
    {
      "epoch": 0.256229039591258,
      "grad_norm": 0.7840805544323655,
      "learning_rate": 1.7434764551919055e-05,
      "loss": 0.5809,
      "step": 2846
    },
    {
      "epoch": 0.2563190708771298,
      "grad_norm": 0.7191391404514651,
      "learning_rate": 1.7432814010130218e-05,
      "loss": 0.5687,
      "step": 2847
    },
    {
      "epoch": 0.25640910216300167,
      "grad_norm": 0.6471743874946504,
      "learning_rate": 1.743086283625026e-05,
      "loss": 0.6407,
      "step": 2848
    },
    {
      "epoch": 0.2564991334488735,
      "grad_norm": 0.7209999914331504,
      "learning_rate": 1.7428911030445114e-05,
      "loss": 0.6578,
      "step": 2849
    },
    {
      "epoch": 0.2565891647347453,
      "grad_norm": 0.7470167770694194,
      "learning_rate": 1.7426958592880762e-05,
      "loss": 0.5398,
      "step": 2850
    },
    {
      "epoch": 0.2566791960206172,
      "grad_norm": 0.6376289388973195,
      "learning_rate": 1.742500552372324e-05,
      "loss": 0.5503,
      "step": 2851
    },
    {
      "epoch": 0.256769227306489,
      "grad_norm": 0.8006292952292753,
      "learning_rate": 1.7423051823138638e-05,
      "loss": 0.688,
      "step": 2852
    },
    {
      "epoch": 0.25685925859236086,
      "grad_norm": 0.8553239017843748,
      "learning_rate": 1.7421097491293096e-05,
      "loss": 0.6272,
      "step": 2853
    },
    {
      "epoch": 0.2569492898782327,
      "grad_norm": 0.8090151415878908,
      "learning_rate": 1.7419142528352815e-05,
      "loss": 0.5985,
      "step": 2854
    },
    {
      "epoch": 0.25703932116410455,
      "grad_norm": 0.6658160745001042,
      "learning_rate": 1.7417186934484052e-05,
      "loss": 0.5728,
      "step": 2855
    },
    {
      "epoch": 0.25712935244997637,
      "grad_norm": 0.7823533183103879,
      "learning_rate": 1.7415230709853103e-05,
      "loss": 0.5808,
      "step": 2856
    },
    {
      "epoch": 0.2572193837358482,
      "grad_norm": 0.6047917845519053,
      "learning_rate": 1.741327385462633e-05,
      "loss": 0.4622,
      "step": 2857
    },
    {
      "epoch": 0.25730941502172006,
      "grad_norm": 0.8027813237416919,
      "learning_rate": 1.7411316368970145e-05,
      "loss": 0.5221,
      "step": 2858
    },
    {
      "epoch": 0.2573994463075919,
      "grad_norm": 0.7218910365712907,
      "learning_rate": 1.7409358253051017e-05,
      "loss": 0.5527,
      "step": 2859
    },
    {
      "epoch": 0.25748947759346374,
      "grad_norm": 0.5792826167702746,
      "learning_rate": 1.740739950703546e-05,
      "loss": 0.5539,
      "step": 2860
    },
    {
      "epoch": 0.25757950887933556,
      "grad_norm": 0.960804866203644,
      "learning_rate": 1.740544013109005e-05,
      "loss": 0.6546,
      "step": 2861
    },
    {
      "epoch": 0.25766954016520743,
      "grad_norm": 0.9389172260563384,
      "learning_rate": 1.740348012538141e-05,
      "loss": 0.6367,
      "step": 2862
    },
    {
      "epoch": 0.25775957145107925,
      "grad_norm": 0.7190399382319469,
      "learning_rate": 1.7401519490076226e-05,
      "loss": 0.5796,
      "step": 2863
    },
    {
      "epoch": 0.25784960273695107,
      "grad_norm": 0.8305535624491528,
      "learning_rate": 1.7399558225341227e-05,
      "loss": 0.5591,
      "step": 2864
    },
    {
      "epoch": 0.25793963402282294,
      "grad_norm": 0.7447880917655298,
      "learning_rate": 1.7397596331343202e-05,
      "loss": 0.5686,
      "step": 2865
    },
    {
      "epoch": 0.25802966530869476,
      "grad_norm": 0.6618277384350565,
      "learning_rate": 1.7395633808248993e-05,
      "loss": 0.5406,
      "step": 2866
    },
    {
      "epoch": 0.2581196965945666,
      "grad_norm": 0.640718982390539,
      "learning_rate": 1.739367065622549e-05,
      "loss": 0.5013,
      "step": 2867
    },
    {
      "epoch": 0.25820972788043844,
      "grad_norm": 0.7597109356551791,
      "learning_rate": 1.7391706875439645e-05,
      "loss": 0.571,
      "step": 2868
    },
    {
      "epoch": 0.2582997591663103,
      "grad_norm": 0.7446838403785934,
      "learning_rate": 1.7389742466058458e-05,
      "loss": 0.542,
      "step": 2869
    },
    {
      "epoch": 0.25838979045218213,
      "grad_norm": 1.914296429252561,
      "learning_rate": 1.7387777428248986e-05,
      "loss": 0.5924,
      "step": 2870
    },
    {
      "epoch": 0.25847982173805395,
      "grad_norm": 0.7584593575838687,
      "learning_rate": 1.7385811762178327e-05,
      "loss": 0.5749,
      "step": 2871
    },
    {
      "epoch": 0.2585698530239258,
      "grad_norm": 0.6904671741892608,
      "learning_rate": 1.7383845468013656e-05,
      "loss": 0.571,
      "step": 2872
    },
    {
      "epoch": 0.25865988430979764,
      "grad_norm": 0.8074565892135969,
      "learning_rate": 1.738187854592218e-05,
      "loss": 0.5051,
      "step": 2873
    },
    {
      "epoch": 0.2587499155956695,
      "grad_norm": 0.8145686145954973,
      "learning_rate": 1.7379910996071166e-05,
      "loss": 0.5383,
      "step": 2874
    },
    {
      "epoch": 0.2588399468815413,
      "grad_norm": 0.7253107170814316,
      "learning_rate": 1.737794281862794e-05,
      "loss": 0.6261,
      "step": 2875
    },
    {
      "epoch": 0.2589299781674132,
      "grad_norm": 0.743076086711331,
      "learning_rate": 1.7375974013759877e-05,
      "loss": 0.5317,
      "step": 2876
    },
    {
      "epoch": 0.259020009453285,
      "grad_norm": 0.6749501565798508,
      "learning_rate": 1.7374004581634402e-05,
      "loss": 0.5211,
      "step": 2877
    },
    {
      "epoch": 0.25911004073915683,
      "grad_norm": 0.7874869220661593,
      "learning_rate": 1.7372034522419e-05,
      "loss": 0.5993,
      "step": 2878
    },
    {
      "epoch": 0.2592000720250287,
      "grad_norm": 0.8181417919131255,
      "learning_rate": 1.7370063836281203e-05,
      "loss": 0.6266,
      "step": 2879
    },
    {
      "epoch": 0.2592901033109005,
      "grad_norm": 0.7560858181983794,
      "learning_rate": 1.73680925233886e-05,
      "loss": 0.5881,
      "step": 2880
    },
    {
      "epoch": 0.2593801345967724,
      "grad_norm": 0.6810684206665519,
      "learning_rate": 1.7366120583908834e-05,
      "loss": 0.5952,
      "step": 2881
    },
    {
      "epoch": 0.2594701658826442,
      "grad_norm": 0.7578840906539513,
      "learning_rate": 1.7364148018009604e-05,
      "loss": 0.579,
      "step": 2882
    },
    {
      "epoch": 0.2595601971685161,
      "grad_norm": 0.973069688086137,
      "learning_rate": 1.736217482585865e-05,
      "loss": 0.6951,
      "step": 2883
    },
    {
      "epoch": 0.2596502284543879,
      "grad_norm": 1.602908077263684,
      "learning_rate": 1.7360201007623777e-05,
      "loss": 0.6138,
      "step": 2884
    },
    {
      "epoch": 0.2597402597402597,
      "grad_norm": 0.8133058572972536,
      "learning_rate": 1.735822656347284e-05,
      "loss": 0.5871,
      "step": 2885
    },
    {
      "epoch": 0.2598302910261316,
      "grad_norm": 0.7290954851105241,
      "learning_rate": 1.7356251493573747e-05,
      "loss": 0.6004,
      "step": 2886
    },
    {
      "epoch": 0.2599203223120034,
      "grad_norm": 0.6615952415282899,
      "learning_rate": 1.7354275798094462e-05,
      "loss": 0.5635,
      "step": 2887
    },
    {
      "epoch": 0.2600103535978753,
      "grad_norm": 0.7418149491393553,
      "learning_rate": 1.7352299477202998e-05,
      "loss": 0.4929,
      "step": 2888
    },
    {
      "epoch": 0.2601003848837471,
      "grad_norm": 0.8635603360773247,
      "learning_rate": 1.7350322531067414e-05,
      "loss": 0.6468,
      "step": 2889
    },
    {
      "epoch": 0.26019041616961897,
      "grad_norm": 0.9113115102882223,
      "learning_rate": 1.7348344959855843e-05,
      "loss": 0.5114,
      "step": 2890
    },
    {
      "epoch": 0.2602804474554908,
      "grad_norm": 0.7839422319686893,
      "learning_rate": 1.7346366763736453e-05,
      "loss": 0.5444,
      "step": 2891
    },
    {
      "epoch": 0.2603704787413626,
      "grad_norm": 0.7296481077785119,
      "learning_rate": 1.734438794287747e-05,
      "loss": 0.5251,
      "step": 2892
    },
    {
      "epoch": 0.26046051002723447,
      "grad_norm": 1.168977286770855,
      "learning_rate": 1.7342408497447177e-05,
      "loss": 0.5769,
      "step": 2893
    },
    {
      "epoch": 0.2605505413131063,
      "grad_norm": 0.8825322975066455,
      "learning_rate": 1.7340428427613908e-05,
      "loss": 0.5593,
      "step": 2894
    },
    {
      "epoch": 0.26064057259897816,
      "grad_norm": 0.686949061890056,
      "learning_rate": 1.7338447733546045e-05,
      "loss": 0.5425,
      "step": 2895
    },
    {
      "epoch": 0.26073060388485,
      "grad_norm": 0.845487413100874,
      "learning_rate": 1.7336466415412028e-05,
      "loss": 0.5468,
      "step": 2896
    },
    {
      "epoch": 0.26082063517072185,
      "grad_norm": 0.9120236935345977,
      "learning_rate": 1.733448447338036e-05,
      "loss": 0.6193,
      "step": 2897
    },
    {
      "epoch": 0.26091066645659367,
      "grad_norm": 0.8126338002733016,
      "learning_rate": 1.7332501907619567e-05,
      "loss": 0.5354,
      "step": 2898
    },
    {
      "epoch": 0.2610006977424655,
      "grad_norm": 0.809991585684189,
      "learning_rate": 1.7330518718298263e-05,
      "loss": 0.5825,
      "step": 2899
    },
    {
      "epoch": 0.26109072902833735,
      "grad_norm": 0.7002636935044444,
      "learning_rate": 1.73285349055851e-05,
      "loss": 0.5741,
      "step": 2900
    },
    {
      "epoch": 0.26118076031420917,
      "grad_norm": 0.5917631476026881,
      "learning_rate": 1.7326550469648772e-05,
      "loss": 0.5081,
      "step": 2901
    },
    {
      "epoch": 0.26127079160008104,
      "grad_norm": 0.6548000600417968,
      "learning_rate": 1.7324565410658047e-05,
      "loss": 0.6355,
      "step": 2902
    },
    {
      "epoch": 0.26136082288595286,
      "grad_norm": 0.72284892385586,
      "learning_rate": 1.732257972878173e-05,
      "loss": 0.5814,
      "step": 2903
    },
    {
      "epoch": 0.26145085417182473,
      "grad_norm": 0.7559400697503299,
      "learning_rate": 1.7320593424188685e-05,
      "loss": 0.6298,
      "step": 2904
    },
    {
      "epoch": 0.26154088545769655,
      "grad_norm": 0.6928385189513803,
      "learning_rate": 1.7318606497047828e-05,
      "loss": 0.5994,
      "step": 2905
    },
    {
      "epoch": 0.26163091674356836,
      "grad_norm": 0.7522342600134081,
      "learning_rate": 1.7316618947528133e-05,
      "loss": 0.5428,
      "step": 2906
    },
    {
      "epoch": 0.26172094802944024,
      "grad_norm": 0.7391890870200511,
      "learning_rate": 1.731463077579862e-05,
      "loss": 0.4978,
      "step": 2907
    },
    {
      "epoch": 0.26181097931531205,
      "grad_norm": 0.7740878440898172,
      "learning_rate": 1.731264198202836e-05,
      "loss": 0.5592,
      "step": 2908
    },
    {
      "epoch": 0.2619010106011839,
      "grad_norm": 0.7352320868108133,
      "learning_rate": 1.731065256638649e-05,
      "loss": 0.5982,
      "step": 2909
    },
    {
      "epoch": 0.26199104188705574,
      "grad_norm": 0.7962547399511705,
      "learning_rate": 1.7308662529042185e-05,
      "loss": 0.547,
      "step": 2910
    },
    {
      "epoch": 0.2620810731729276,
      "grad_norm": 0.7320569263937643,
      "learning_rate": 1.7306671870164677e-05,
      "loss": 0.6755,
      "step": 2911
    },
    {
      "epoch": 0.26217110445879943,
      "grad_norm": 0.6376426247008093,
      "learning_rate": 1.7304680589923258e-05,
      "loss": 0.561,
      "step": 2912
    },
    {
      "epoch": 0.26226113574467125,
      "grad_norm": 1.0262366279819484,
      "learning_rate": 1.7302688688487265e-05,
      "loss": 0.723,
      "step": 2913
    },
    {
      "epoch": 0.2623511670305431,
      "grad_norm": 0.8782466375491692,
      "learning_rate": 1.7300696166026095e-05,
      "loss": 0.5076,
      "step": 2914
    },
    {
      "epoch": 0.26244119831641494,
      "grad_norm": 0.7604447771706646,
      "learning_rate": 1.7298703022709186e-05,
      "loss": 0.5912,
      "step": 2915
    },
    {
      "epoch": 0.2625312296022868,
      "grad_norm": 0.7747039179093672,
      "learning_rate": 1.7296709258706042e-05,
      "loss": 0.6027,
      "step": 2916
    },
    {
      "epoch": 0.2626212608881586,
      "grad_norm": 0.8231279490485248,
      "learning_rate": 1.729471487418621e-05,
      "loss": 0.5551,
      "step": 2917
    },
    {
      "epoch": 0.2627112921740305,
      "grad_norm": 0.6392700800061341,
      "learning_rate": 1.7292719869319295e-05,
      "loss": 0.4874,
      "step": 2918
    },
    {
      "epoch": 0.2628013234599023,
      "grad_norm": 0.848275713244474,
      "learning_rate": 1.7290724244274957e-05,
      "loss": 0.6362,
      "step": 2919
    },
    {
      "epoch": 0.26289135474577413,
      "grad_norm": 0.6819190844617455,
      "learning_rate": 1.7288727999222897e-05,
      "loss": 0.5259,
      "step": 2920
    },
    {
      "epoch": 0.262981386031646,
      "grad_norm": 0.662080748365884,
      "learning_rate": 1.728673113433289e-05,
      "loss": 0.6219,
      "step": 2921
    },
    {
      "epoch": 0.2630714173175178,
      "grad_norm": 0.6641346714246784,
      "learning_rate": 1.7284733649774736e-05,
      "loss": 0.5265,
      "step": 2922
    },
    {
      "epoch": 0.2631614486033897,
      "grad_norm": 0.6706706476542226,
      "learning_rate": 1.7282735545718312e-05,
      "loss": 0.56,
      "step": 2923
    },
    {
      "epoch": 0.2632514798892615,
      "grad_norm": 0.8058125567138106,
      "learning_rate": 1.7280736822333535e-05,
      "loss": 0.6212,
      "step": 2924
    },
    {
      "epoch": 0.2633415111751334,
      "grad_norm": 0.6300114470155422,
      "learning_rate": 1.7278737479790377e-05,
      "loss": 0.5519,
      "step": 2925
    },
    {
      "epoch": 0.2634315424610052,
      "grad_norm": 0.7021107962511373,
      "learning_rate": 1.7276737518258865e-05,
      "loss": 0.5372,
      "step": 2926
    },
    {
      "epoch": 0.263521573746877,
      "grad_norm": 0.6849332296541771,
      "learning_rate": 1.7274736937909073e-05,
      "loss": 0.5817,
      "step": 2927
    },
    {
      "epoch": 0.2636116050327489,
      "grad_norm": 0.6527665083473673,
      "learning_rate": 1.7272735738911137e-05,
      "loss": 0.5197,
      "step": 2928
    },
    {
      "epoch": 0.2637016363186207,
      "grad_norm": 0.864250748617762,
      "learning_rate": 1.7270733921435238e-05,
      "loss": 0.6352,
      "step": 2929
    },
    {
      "epoch": 0.2637916676044926,
      "grad_norm": 0.7374232798767292,
      "learning_rate": 1.726873148565161e-05,
      "loss": 0.6404,
      "step": 2930
    },
    {
      "epoch": 0.2638816988903644,
      "grad_norm": 0.7839312600159223,
      "learning_rate": 1.7266728431730544e-05,
      "loss": 0.5088,
      "step": 2931
    },
    {
      "epoch": 0.26397173017623626,
      "grad_norm": 0.7302642308115298,
      "learning_rate": 1.726472475984238e-05,
      "loss": 0.6463,
      "step": 2932
    },
    {
      "epoch": 0.2640617614621081,
      "grad_norm": 0.6339500106408454,
      "learning_rate": 1.7262720470157514e-05,
      "loss": 0.451,
      "step": 2933
    },
    {
      "epoch": 0.2641517927479799,
      "grad_norm": 0.6729699882699773,
      "learning_rate": 1.7260715562846387e-05,
      "loss": 0.5692,
      "step": 2934
    },
    {
      "epoch": 0.26424182403385177,
      "grad_norm": 0.8768692214915502,
      "learning_rate": 1.7258710038079496e-05,
      "loss": 0.6203,
      "step": 2935
    },
    {
      "epoch": 0.2643318553197236,
      "grad_norm": 0.7906778587057358,
      "learning_rate": 1.7256703896027405e-05,
      "loss": 0.6036,
      "step": 2936
    },
    {
      "epoch": 0.26442188660559546,
      "grad_norm": 0.640789105220136,
      "learning_rate": 1.72546971368607e-05,
      "loss": 0.5885,
      "step": 2937
    },
    {
      "epoch": 0.2645119178914673,
      "grad_norm": 0.8056428325720579,
      "learning_rate": 1.7252689760750053e-05,
      "loss": 0.5309,
      "step": 2938
    },
    {
      "epoch": 0.26460194917733915,
      "grad_norm": 0.6830775300245556,
      "learning_rate": 1.725068176786616e-05,
      "loss": 0.444,
      "step": 2939
    },
    {
      "epoch": 0.26469198046321096,
      "grad_norm": 0.732408807419393,
      "learning_rate": 1.724867315837979e-05,
      "loss": 0.6264,
      "step": 2940
    },
    {
      "epoch": 0.2647820117490828,
      "grad_norm": 0.8182358527533454,
      "learning_rate": 1.7246663932461754e-05,
      "loss": 0.5535,
      "step": 2941
    },
    {
      "epoch": 0.26487204303495465,
      "grad_norm": 0.568181665938179,
      "learning_rate": 1.7244654090282916e-05,
      "loss": 0.5013,
      "step": 2942
    },
    {
      "epoch": 0.26496207432082647,
      "grad_norm": 0.6317993444395609,
      "learning_rate": 1.7242643632014197e-05,
      "loss": 0.6051,
      "step": 2943
    },
    {
      "epoch": 0.26505210560669834,
      "grad_norm": 0.6762863674052038,
      "learning_rate": 1.724063255782656e-05,
      "loss": 0.5509,
      "step": 2944
    },
    {
      "epoch": 0.26514213689257016,
      "grad_norm": 0.6775614295422282,
      "learning_rate": 1.723862086789104e-05,
      "loss": 0.5329,
      "step": 2945
    },
    {
      "epoch": 0.26523216817844203,
      "grad_norm": 0.7501378559782602,
      "learning_rate": 1.7236608562378708e-05,
      "loss": 0.6052,
      "step": 2946
    },
    {
      "epoch": 0.26532219946431385,
      "grad_norm": 0.7276024134887629,
      "learning_rate": 1.7234595641460686e-05,
      "loss": 0.5538,
      "step": 2947
    },
    {
      "epoch": 0.26541223075018566,
      "grad_norm": 0.8917760625357066,
      "learning_rate": 1.7232582105308163e-05,
      "loss": 0.6729,
      "step": 2948
    },
    {
      "epoch": 0.26550226203605753,
      "grad_norm": 0.7501219667585161,
      "learning_rate": 1.7230567954092366e-05,
      "loss": 0.6011,
      "step": 2949
    },
    {
      "epoch": 0.26559229332192935,
      "grad_norm": 0.6579648769423105,
      "learning_rate": 1.7228553187984578e-05,
      "loss": 0.5647,
      "step": 2950
    },
    {
      "epoch": 0.2656823246078012,
      "grad_norm": 0.6558978765183688,
      "learning_rate": 1.722653780715614e-05,
      "loss": 0.5557,
      "step": 2951
    },
    {
      "epoch": 0.26577235589367304,
      "grad_norm": 1.1091345050005792,
      "learning_rate": 1.7224521811778435e-05,
      "loss": 0.4697,
      "step": 2952
    },
    {
      "epoch": 0.2658623871795449,
      "grad_norm": 1.0564620791579329,
      "learning_rate": 1.7222505202022915e-05,
      "loss": 0.5314,
      "step": 2953
    },
    {
      "epoch": 0.26595241846541673,
      "grad_norm": 0.7543090619508902,
      "learning_rate": 1.7220487978061067e-05,
      "loss": 0.5961,
      "step": 2954
    },
    {
      "epoch": 0.26604244975128855,
      "grad_norm": 0.7335048882882171,
      "learning_rate": 1.7218470140064436e-05,
      "loss": 0.6345,
      "step": 2955
    },
    {
      "epoch": 0.2661324810371604,
      "grad_norm": 0.8286639895265954,
      "learning_rate": 1.7216451688204623e-05,
      "loss": 0.7019,
      "step": 2956
    },
    {
      "epoch": 0.26622251232303223,
      "grad_norm": 0.7116754561447489,
      "learning_rate": 1.7214432622653276e-05,
      "loss": 0.6053,
      "step": 2957
    },
    {
      "epoch": 0.2663125436089041,
      "grad_norm": 0.6972976376046968,
      "learning_rate": 1.72124129435821e-05,
      "loss": 0.4848,
      "step": 2958
    },
    {
      "epoch": 0.2664025748947759,
      "grad_norm": 0.8432941042790667,
      "learning_rate": 1.721039265116285e-05,
      "loss": 0.5691,
      "step": 2959
    },
    {
      "epoch": 0.2664926061806478,
      "grad_norm": 0.704251730732853,
      "learning_rate": 1.720837174556733e-05,
      "loss": 0.5239,
      "step": 2960
    },
    {
      "epoch": 0.2665826374665196,
      "grad_norm": 0.6710031316028734,
      "learning_rate": 1.7206350226967403e-05,
      "loss": 0.5259,
      "step": 2961
    },
    {
      "epoch": 0.26667266875239143,
      "grad_norm": 0.7837545939567369,
      "learning_rate": 1.7204328095534973e-05,
      "loss": 0.5403,
      "step": 2962
    },
    {
      "epoch": 0.2667627000382633,
      "grad_norm": 0.6020424634682969,
      "learning_rate": 1.7202305351442013e-05,
      "loss": 0.4724,
      "step": 2963
    },
    {
      "epoch": 0.2668527313241351,
      "grad_norm": 0.7170638578331028,
      "learning_rate": 1.7200281994860532e-05,
      "loss": 0.4997,
      "step": 2964
    },
    {
      "epoch": 0.266942762610007,
      "grad_norm": 0.7115780271161717,
      "learning_rate": 1.7198258025962603e-05,
      "loss": 0.612,
      "step": 2965
    },
    {
      "epoch": 0.2670327938958788,
      "grad_norm": 1.33514426080157,
      "learning_rate": 1.7196233444920336e-05,
      "loss": 0.621,
      "step": 2966
    },
    {
      "epoch": 0.2671228251817507,
      "grad_norm": 0.77879164197651,
      "learning_rate": 1.7194208251905912e-05,
      "loss": 0.5562,
      "step": 2967
    },
    {
      "epoch": 0.2672128564676225,
      "grad_norm": 0.7298233831635121,
      "learning_rate": 1.719218244709155e-05,
      "loss": 0.6076,
      "step": 2968
    },
    {
      "epoch": 0.26730288775349437,
      "grad_norm": 0.7991859425953153,
      "learning_rate": 1.7190156030649524e-05,
      "loss": 0.5887,
      "step": 2969
    },
    {
      "epoch": 0.2673929190393662,
      "grad_norm": 0.8773356407953851,
      "learning_rate": 1.718812900275217e-05,
      "loss": 0.6175,
      "step": 2970
    },
    {
      "epoch": 0.267482950325238,
      "grad_norm": 0.701998540045376,
      "learning_rate": 1.7186101363571858e-05,
      "loss": 0.4801,
      "step": 2971
    },
    {
      "epoch": 0.2675729816111099,
      "grad_norm": 0.8011966891380317,
      "learning_rate": 1.7184073113281024e-05,
      "loss": 0.5507,
      "step": 2972
    },
    {
      "epoch": 0.2676630128969817,
      "grad_norm": 0.813492898225872,
      "learning_rate": 1.7182044252052156e-05,
      "loss": 0.5245,
      "step": 2973
    },
    {
      "epoch": 0.26775304418285356,
      "grad_norm": 0.7943144572868323,
      "learning_rate": 1.718001478005778e-05,
      "loss": 0.5955,
      "step": 2974
    },
    {
      "epoch": 0.2678430754687254,
      "grad_norm": 0.682789012013217,
      "learning_rate": 1.717798469747049e-05,
      "loss": 0.5487,
      "step": 2975
    },
    {
      "epoch": 0.26793310675459725,
      "grad_norm": 0.7844602988454447,
      "learning_rate": 1.7175954004462926e-05,
      "loss": 0.618,
      "step": 2976
    },
    {
      "epoch": 0.26802313804046907,
      "grad_norm": 0.7850586242552102,
      "learning_rate": 1.7173922701207777e-05,
      "loss": 0.4533,
      "step": 2977
    },
    {
      "epoch": 0.2681131693263409,
      "grad_norm": 0.760307079045474,
      "learning_rate": 1.7171890787877786e-05,
      "loss": 0.6115,
      "step": 2978
    },
    {
      "epoch": 0.26820320061221276,
      "grad_norm": 0.8211701092200835,
      "learning_rate": 1.716985826464575e-05,
      "loss": 0.5334,
      "step": 2979
    },
    {
      "epoch": 0.26829323189808457,
      "grad_norm": 1.0053152291413512,
      "learning_rate": 1.7167825131684516e-05,
      "loss": 0.6158,
      "step": 2980
    },
    {
      "epoch": 0.26838326318395644,
      "grad_norm": 0.6888340548325315,
      "learning_rate": 1.716579138916698e-05,
      "loss": 0.5245,
      "step": 2981
    },
    {
      "epoch": 0.26847329446982826,
      "grad_norm": 0.6416076006001463,
      "learning_rate": 1.7163757037266094e-05,
      "loss": 0.5823,
      "step": 2982
    },
    {
      "epoch": 0.26856332575570013,
      "grad_norm": 0.6721856597921142,
      "learning_rate": 1.7161722076154864e-05,
      "loss": 0.5459,
      "step": 2983
    },
    {
      "epoch": 0.26865335704157195,
      "grad_norm": 0.8126950366239875,
      "learning_rate": 1.715968650600634e-05,
      "loss": 0.6971,
      "step": 2984
    },
    {
      "epoch": 0.26874338832744377,
      "grad_norm": 0.7508103291668927,
      "learning_rate": 1.715765032699363e-05,
      "loss": 0.4798,
      "step": 2985
    },
    {
      "epoch": 0.26883341961331564,
      "grad_norm": 0.8115681529717292,
      "learning_rate": 1.715561353928989e-05,
      "loss": 0.5101,
      "step": 2986
    },
    {
      "epoch": 0.26892345089918746,
      "grad_norm": 0.8070121346386872,
      "learning_rate": 1.7153576143068333e-05,
      "loss": 0.5852,
      "step": 2987
    },
    {
      "epoch": 0.2690134821850593,
      "grad_norm": 0.8039353810238407,
      "learning_rate": 1.715153813850222e-05,
      "loss": 0.6224,
      "step": 2988
    },
    {
      "epoch": 0.26910351347093114,
      "grad_norm": 0.9155288825975837,
      "learning_rate": 1.714949952576486e-05,
      "loss": 0.4859,
      "step": 2989
    },
    {
      "epoch": 0.269193544756803,
      "grad_norm": 0.8379420389675039,
      "learning_rate": 1.7147460305029624e-05,
      "loss": 0.5549,
      "step": 2990
    },
    {
      "epoch": 0.26928357604267483,
      "grad_norm": 0.9229362019526334,
      "learning_rate": 1.7145420476469926e-05,
      "loss": 0.5539,
      "step": 2991
    },
    {
      "epoch": 0.26937360732854665,
      "grad_norm": 0.5566320214147796,
      "learning_rate": 1.714338004025923e-05,
      "loss": 0.4876,
      "step": 2992
    },
    {
      "epoch": 0.2694636386144185,
      "grad_norm": 1.030195087485051,
      "learning_rate": 1.7141338996571063e-05,
      "loss": 0.5732,
      "step": 2993
    },
    {
      "epoch": 0.26955366990029034,
      "grad_norm": 0.7273539531680481,
      "learning_rate": 1.7139297345578992e-05,
      "loss": 0.4896,
      "step": 2994
    },
    {
      "epoch": 0.2696437011861622,
      "grad_norm": 0.7319574783411138,
      "learning_rate": 1.7137255087456643e-05,
      "loss": 0.552,
      "step": 2995
    },
    {
      "epoch": 0.269733732472034,
      "grad_norm": 0.8228700351401561,
      "learning_rate": 1.713521222237769e-05,
      "loss": 0.5637,
      "step": 2996
    },
    {
      "epoch": 0.2698237637579059,
      "grad_norm": 0.7167690122865582,
      "learning_rate": 1.7133168750515853e-05,
      "loss": 0.5093,
      "step": 2997
    },
    {
      "epoch": 0.2699137950437777,
      "grad_norm": 0.8250143794472605,
      "learning_rate": 1.7131124672044924e-05,
      "loss": 0.5587,
      "step": 2998
    },
    {
      "epoch": 0.27000382632964953,
      "grad_norm": 0.7718649861552596,
      "learning_rate": 1.712907998713872e-05,
      "loss": 0.6005,
      "step": 2999
    },
    {
      "epoch": 0.2700938576155214,
      "grad_norm": 0.6841935058695514,
      "learning_rate": 1.712703469597113e-05,
      "loss": 0.5593,
      "step": 3000
    },
    {
      "epoch": 0.2701838889013932,
      "grad_norm": 0.7459554105681168,
      "learning_rate": 1.7124988798716084e-05,
      "loss": 0.627,
      "step": 3001
    },
    {
      "epoch": 0.2702739201872651,
      "grad_norm": 0.7445813023044667,
      "learning_rate": 1.7122942295547563e-05,
      "loss": 0.5279,
      "step": 3002
    },
    {
      "epoch": 0.2703639514731369,
      "grad_norm": 0.8808907423522461,
      "learning_rate": 1.712089518663961e-05,
      "loss": 0.6476,
      "step": 3003
    },
    {
      "epoch": 0.2704539827590088,
      "grad_norm": 1.109397190260045,
      "learning_rate": 1.7118847472166304e-05,
      "loss": 0.5934,
      "step": 3004
    },
    {
      "epoch": 0.2705440140448806,
      "grad_norm": 1.0807459176607477,
      "learning_rate": 1.711679915230179e-05,
      "loss": 0.5711,
      "step": 3005
    },
    {
      "epoch": 0.2706340453307524,
      "grad_norm": 0.757556940706322,
      "learning_rate": 1.711475022722026e-05,
      "loss": 0.5574,
      "step": 3006
    },
    {
      "epoch": 0.2707240766166243,
      "grad_norm": 0.8711237198708824,
      "learning_rate": 1.7112700697095955e-05,
      "loss": 0.6547,
      "step": 3007
    },
    {
      "epoch": 0.2708141079024961,
      "grad_norm": 0.9001312836638624,
      "learning_rate": 1.7110650562103158e-05,
      "loss": 0.6736,
      "step": 3008
    },
    {
      "epoch": 0.270904139188368,
      "grad_norm": 0.7529630484894699,
      "learning_rate": 1.710859982241623e-05,
      "loss": 0.5268,
      "step": 3009
    },
    {
      "epoch": 0.2709941704742398,
      "grad_norm": 0.6969589618101082,
      "learning_rate": 1.710654847820955e-05,
      "loss": 0.5816,
      "step": 3010
    },
    {
      "epoch": 0.27108420176011166,
      "grad_norm": 0.6348238535798739,
      "learning_rate": 1.7104496529657584e-05,
      "loss": 0.5093,
      "step": 3011
    },
    {
      "epoch": 0.2711742330459835,
      "grad_norm": 0.7194356779875876,
      "learning_rate": 1.7102443976934816e-05,
      "loss": 0.6388,
      "step": 3012
    },
    {
      "epoch": 0.2712642643318553,
      "grad_norm": 0.6968897268570335,
      "learning_rate": 1.7100390820215805e-05,
      "loss": 0.5825,
      "step": 3013
    },
    {
      "epoch": 0.27135429561772717,
      "grad_norm": 0.7560910738661785,
      "learning_rate": 1.7098337059675148e-05,
      "loss": 0.5662,
      "step": 3014
    },
    {
      "epoch": 0.271444326903599,
      "grad_norm": 0.658898338845128,
      "learning_rate": 1.70962826954875e-05,
      "loss": 0.5146,
      "step": 3015
    },
    {
      "epoch": 0.27153435818947086,
      "grad_norm": 0.7743939658154398,
      "learning_rate": 1.7094227727827565e-05,
      "loss": 0.6137,
      "step": 3016
    },
    {
      "epoch": 0.2716243894753427,
      "grad_norm": 0.6598472068412794,
      "learning_rate": 1.70921721568701e-05,
      "loss": 0.5497,
      "step": 3017
    },
    {
      "epoch": 0.27171442076121455,
      "grad_norm": 0.9698282173053374,
      "learning_rate": 1.709011598278991e-05,
      "loss": 0.6397,
      "step": 3018
    },
    {
      "epoch": 0.27180445204708636,
      "grad_norm": 0.6147801671454334,
      "learning_rate": 1.7088059205761855e-05,
      "loss": 0.5064,
      "step": 3019
    },
    {
      "epoch": 0.2718944833329582,
      "grad_norm": 0.6734992347625013,
      "learning_rate": 1.7086001825960843e-05,
      "loss": 0.5685,
      "step": 3020
    },
    {
      "epoch": 0.27198451461883005,
      "grad_norm": 0.6603632202436501,
      "learning_rate": 1.7083943843561835e-05,
      "loss": 0.5697,
      "step": 3021
    },
    {
      "epoch": 0.27207454590470187,
      "grad_norm": 0.7685902387940012,
      "learning_rate": 1.7081885258739846e-05,
      "loss": 0.4904,
      "step": 3022
    },
    {
      "epoch": 0.27216457719057374,
      "grad_norm": 0.85493555061948,
      "learning_rate": 1.7079826071669938e-05,
      "loss": 0.5447,
      "step": 3023
    },
    {
      "epoch": 0.27225460847644556,
      "grad_norm": 0.756003278728157,
      "learning_rate": 1.7077766282527224e-05,
      "loss": 0.5857,
      "step": 3024
    },
    {
      "epoch": 0.27234463976231743,
      "grad_norm": 0.6640402048151419,
      "learning_rate": 1.707570589148687e-05,
      "loss": 0.6862,
      "step": 3025
    },
    {
      "epoch": 0.27243467104818925,
      "grad_norm": 0.5730005419664728,
      "learning_rate": 1.707364489872409e-05,
      "loss": 0.5035,
      "step": 3026
    },
    {
      "epoch": 0.27252470233406106,
      "grad_norm": 0.7654065405797525,
      "learning_rate": 1.707158330441416e-05,
      "loss": 0.528,
      "step": 3027
    },
    {
      "epoch": 0.27261473361993294,
      "grad_norm": 0.7086451927337952,
      "learning_rate": 1.7069521108732394e-05,
      "loss": 0.5931,
      "step": 3028
    },
    {
      "epoch": 0.27270476490580475,
      "grad_norm": 0.6251926557613914,
      "learning_rate": 1.706745831185416e-05,
      "loss": 0.5501,
      "step": 3029
    },
    {
      "epoch": 0.2727947961916766,
      "grad_norm": 1.0031452254804842,
      "learning_rate": 1.7065394913954884e-05,
      "loss": 0.6734,
      "step": 3030
    },
    {
      "epoch": 0.27288482747754844,
      "grad_norm": 0.6961671085121784,
      "learning_rate": 1.7063330915210038e-05,
      "loss": 0.5709,
      "step": 3031
    },
    {
      "epoch": 0.2729748587634203,
      "grad_norm": 0.6874267680153789,
      "learning_rate": 1.7061266315795146e-05,
      "loss": 0.5423,
      "step": 3032
    },
    {
      "epoch": 0.27306489004929213,
      "grad_norm": 0.7087692330531998,
      "learning_rate": 1.7059201115885783e-05,
      "loss": 0.6596,
      "step": 3033
    },
    {
      "epoch": 0.27315492133516395,
      "grad_norm": 0.7234676539014498,
      "learning_rate": 1.705713531565757e-05,
      "loss": 0.5561,
      "step": 3034
    },
    {
      "epoch": 0.2732449526210358,
      "grad_norm": 0.7120174455863656,
      "learning_rate": 1.705506891528619e-05,
      "loss": 0.617,
      "step": 3035
    },
    {
      "epoch": 0.27333498390690764,
      "grad_norm": 0.6564204694727936,
      "learning_rate": 1.7053001914947368e-05,
      "loss": 0.6173,
      "step": 3036
    },
    {
      "epoch": 0.2734250151927795,
      "grad_norm": 0.7539867584220816,
      "learning_rate": 1.7050934314816884e-05,
      "loss": 0.5034,
      "step": 3037
    },
    {
      "epoch": 0.2735150464786513,
      "grad_norm": 0.719851045932367,
      "learning_rate": 1.7048866115070567e-05,
      "loss": 0.5755,
      "step": 3038
    },
    {
      "epoch": 0.2736050777645232,
      "grad_norm": 0.6764430294014617,
      "learning_rate": 1.7046797315884298e-05,
      "loss": 0.5849,
      "step": 3039
    },
    {
      "epoch": 0.273695109050395,
      "grad_norm": 0.6320698435344059,
      "learning_rate": 1.7044727917434013e-05,
      "loss": 0.6022,
      "step": 3040
    },
    {
      "epoch": 0.27378514033626683,
      "grad_norm": 0.8103723982284569,
      "learning_rate": 1.7042657919895688e-05,
      "loss": 0.6225,
      "step": 3041
    },
    {
      "epoch": 0.2738751716221387,
      "grad_norm": 1.0163649349246513,
      "learning_rate": 1.704058732344536e-05,
      "loss": 0.4831,
      "step": 3042
    },
    {
      "epoch": 0.2739652029080105,
      "grad_norm": 0.6955168878773121,
      "learning_rate": 1.7038516128259118e-05,
      "loss": 0.5581,
      "step": 3043
    },
    {
      "epoch": 0.2740552341938824,
      "grad_norm": 0.6173680597765242,
      "learning_rate": 1.703644433451309e-05,
      "loss": 0.5226,
      "step": 3044
    },
    {
      "epoch": 0.2741452654797542,
      "grad_norm": 0.7209166154799561,
      "learning_rate": 1.703437194238347e-05,
      "loss": 0.6373,
      "step": 3045
    },
    {
      "epoch": 0.2742352967656261,
      "grad_norm": 0.8186654638920812,
      "learning_rate": 1.703229895204649e-05,
      "loss": 0.6077,
      "step": 3046
    },
    {
      "epoch": 0.2743253280514979,
      "grad_norm": 0.6719851704673361,
      "learning_rate": 1.703022536367844e-05,
      "loss": 0.5605,
      "step": 3047
    },
    {
      "epoch": 0.2744153593373697,
      "grad_norm": 0.6009679922764743,
      "learning_rate": 1.7028151177455662e-05,
      "loss": 0.4497,
      "step": 3048
    },
    {
      "epoch": 0.2745053906232416,
      "grad_norm": 0.8458348000875446,
      "learning_rate": 1.702607639355454e-05,
      "loss": 0.6883,
      "step": 3049
    },
    {
      "epoch": 0.2745954219091134,
      "grad_norm": 1.0240100994523873,
      "learning_rate": 1.7024001012151522e-05,
      "loss": 0.5897,
      "step": 3050
    },
    {
      "epoch": 0.2746854531949853,
      "grad_norm": 0.8460945737895881,
      "learning_rate": 1.7021925033423096e-05,
      "loss": 0.6618,
      "step": 3051
    },
    {
      "epoch": 0.2747754844808571,
      "grad_norm": 0.5680776685095094,
      "learning_rate": 1.7019848457545806e-05,
      "loss": 0.514,
      "step": 3052
    },
    {
      "epoch": 0.27486551576672896,
      "grad_norm": 0.685890612353509,
      "learning_rate": 1.701777128469624e-05,
      "loss": 0.5839,
      "step": 3053
    },
    {
      "epoch": 0.2749555470526008,
      "grad_norm": 0.6910173592124844,
      "learning_rate": 1.701569351505105e-05,
      "loss": 0.5198,
      "step": 3054
    },
    {
      "epoch": 0.2750455783384726,
      "grad_norm": 0.8113539338738388,
      "learning_rate": 1.7013615148786924e-05,
      "loss": 0.5534,
      "step": 3055
    },
    {
      "epoch": 0.27513560962434447,
      "grad_norm": 0.9366567680821648,
      "learning_rate": 1.7011536186080616e-05,
      "loss": 0.5238,
      "step": 3056
    },
    {
      "epoch": 0.2752256409102163,
      "grad_norm": 0.549961956066803,
      "learning_rate": 1.700945662710891e-05,
      "loss": 0.4883,
      "step": 3057
    },
    {
      "epoch": 0.27531567219608816,
      "grad_norm": 0.8570285877601113,
      "learning_rate": 1.7007376472048666e-05,
      "loss": 0.6649,
      "step": 3058
    },
    {
      "epoch": 0.27540570348196,
      "grad_norm": 0.8613495288414035,
      "learning_rate": 1.7005295721076774e-05,
      "loss": 0.6089,
      "step": 3059
    },
    {
      "epoch": 0.27549573476783185,
      "grad_norm": 0.8054193963819236,
      "learning_rate": 1.7003214374370184e-05,
      "loss": 0.5731,
      "step": 3060
    },
    {
      "epoch": 0.27558576605370366,
      "grad_norm": 0.6422500144792261,
      "learning_rate": 1.7001132432105896e-05,
      "loss": 0.442,
      "step": 3061
    },
    {
      "epoch": 0.2756757973395755,
      "grad_norm": 0.7927128549196445,
      "learning_rate": 1.6999049894460956e-05,
      "loss": 0.5286,
      "step": 3062
    },
    {
      "epoch": 0.27576582862544735,
      "grad_norm": 1.0658954068016233,
      "learning_rate": 1.6996966761612467e-05,
      "loss": 0.5363,
      "step": 3063
    },
    {
      "epoch": 0.27585585991131917,
      "grad_norm": 0.6860505968185561,
      "learning_rate": 1.6994883033737582e-05,
      "loss": 0.6085,
      "step": 3064
    },
    {
      "epoch": 0.27594589119719104,
      "grad_norm": 0.7101304700801879,
      "learning_rate": 1.6992798711013502e-05,
      "loss": 0.4888,
      "step": 3065
    },
    {
      "epoch": 0.27603592248306286,
      "grad_norm": 0.6977805860865581,
      "learning_rate": 1.6990713793617473e-05,
      "loss": 0.5418,
      "step": 3066
    },
    {
      "epoch": 0.27612595376893473,
      "grad_norm": 0.6989688969865695,
      "learning_rate": 1.6988628281726806e-05,
      "loss": 0.5025,
      "step": 3067
    },
    {
      "epoch": 0.27621598505480655,
      "grad_norm": 0.7408258076259726,
      "learning_rate": 1.6986542175518846e-05,
      "loss": 0.6197,
      "step": 3068
    },
    {
      "epoch": 0.27630601634067836,
      "grad_norm": 0.6141378816303577,
      "learning_rate": 1.6984455475171003e-05,
      "loss": 0.5834,
      "step": 3069
    },
    {
      "epoch": 0.27639604762655023,
      "grad_norm": 0.7218629869068265,
      "learning_rate": 1.698236818086073e-05,
      "loss": 0.5623,
      "step": 3070
    },
    {
      "epoch": 0.27648607891242205,
      "grad_norm": 0.6243045042840186,
      "learning_rate": 1.6980280292765533e-05,
      "loss": 0.5241,
      "step": 3071
    },
    {
      "epoch": 0.2765761101982939,
      "grad_norm": 0.7673971667650705,
      "learning_rate": 1.6978191811062963e-05,
      "loss": 0.6265,
      "step": 3072
    },
    {
      "epoch": 0.27666614148416574,
      "grad_norm": 0.7551409824981606,
      "learning_rate": 1.6976102735930628e-05,
      "loss": 0.5828,
      "step": 3073
    },
    {
      "epoch": 0.2767561727700376,
      "grad_norm": 0.6434934748470521,
      "learning_rate": 1.6974013067546188e-05,
      "loss": 0.5364,
      "step": 3074
    },
    {
      "epoch": 0.27684620405590943,
      "grad_norm": 0.8481735796698305,
      "learning_rate": 1.6971922806087344e-05,
      "loss": 0.5735,
      "step": 3075
    },
    {
      "epoch": 0.27693623534178125,
      "grad_norm": 0.7512000661037835,
      "learning_rate": 1.6969831951731854e-05,
      "loss": 0.5629,
      "step": 3076
    },
    {
      "epoch": 0.2770262666276531,
      "grad_norm": 0.6865865965843921,
      "learning_rate": 1.6967740504657527e-05,
      "loss": 0.4939,
      "step": 3077
    },
    {
      "epoch": 0.27711629791352493,
      "grad_norm": 0.6503890307416043,
      "learning_rate": 1.6965648465042224e-05,
      "loss": 0.5717,
      "step": 3078
    },
    {
      "epoch": 0.2772063291993968,
      "grad_norm": 0.6129054192472544,
      "learning_rate": 1.6963555833063848e-05,
      "loss": 0.508,
      "step": 3079
    },
    {
      "epoch": 0.2772963604852686,
      "grad_norm": 0.783331375459387,
      "learning_rate": 1.696146260890036e-05,
      "loss": 0.5489,
      "step": 3080
    },
    {
      "epoch": 0.2773863917711405,
      "grad_norm": 0.6902493522265306,
      "learning_rate": 1.695936879272977e-05,
      "loss": 0.5232,
      "step": 3081
    },
    {
      "epoch": 0.2774764230570123,
      "grad_norm": 0.7569743258619503,
      "learning_rate": 1.6957274384730137e-05,
      "loss": 0.5892,
      "step": 3082
    },
    {
      "epoch": 0.27756645434288413,
      "grad_norm": 0.7231686930993486,
      "learning_rate": 1.695517938507957e-05,
      "loss": 0.6206,
      "step": 3083
    },
    {
      "epoch": 0.277656485628756,
      "grad_norm": 0.9506644438889019,
      "learning_rate": 1.695308379395623e-05,
      "loss": 0.595,
      "step": 3084
    },
    {
      "epoch": 0.2777465169146278,
      "grad_norm": 0.7120886220612935,
      "learning_rate": 1.695098761153832e-05,
      "loss": 0.6055,
      "step": 3085
    },
    {
      "epoch": 0.2778365482004997,
      "grad_norm": 0.7775912699483744,
      "learning_rate": 1.6948890838004114e-05,
      "loss": 0.6746,
      "step": 3086
    },
    {
      "epoch": 0.2779265794863715,
      "grad_norm": 0.7775433748616325,
      "learning_rate": 1.694679347353192e-05,
      "loss": 0.4923,
      "step": 3087
    },
    {
      "epoch": 0.2780166107722434,
      "grad_norm": 0.7407837229740714,
      "learning_rate": 1.6944695518300087e-05,
      "loss": 0.4625,
      "step": 3088
    },
    {
      "epoch": 0.2781066420581152,
      "grad_norm": 0.5803908700830009,
      "learning_rate": 1.694259697248704e-05,
      "loss": 0.5341,
      "step": 3089
    },
    {
      "epoch": 0.278196673343987,
      "grad_norm": 0.7281098968793553,
      "learning_rate": 1.6940497836271234e-05,
      "loss": 0.6006,
      "step": 3090
    },
    {
      "epoch": 0.2782867046298589,
      "grad_norm": 0.6766283453805529,
      "learning_rate": 1.6938398109831183e-05,
      "loss": 0.5415,
      "step": 3091
    },
    {
      "epoch": 0.2783767359157307,
      "grad_norm": 1.0389869163065817,
      "learning_rate": 1.6936297793345445e-05,
      "loss": 0.5981,
      "step": 3092
    },
    {
      "epoch": 0.27846676720160257,
      "grad_norm": 0.774232349960238,
      "learning_rate": 1.6934196886992638e-05,
      "loss": 0.6563,
      "step": 3093
    },
    {
      "epoch": 0.2785567984874744,
      "grad_norm": 0.5823486575773011,
      "learning_rate": 1.6932095390951424e-05,
      "loss": 0.5887,
      "step": 3094
    },
    {
      "epoch": 0.27864682977334626,
      "grad_norm": 0.6130780131992506,
      "learning_rate": 1.6929993305400514e-05,
      "loss": 0.5369,
      "step": 3095
    },
    {
      "epoch": 0.2787368610592181,
      "grad_norm": 0.711558340092758,
      "learning_rate": 1.6927890630518665e-05,
      "loss": 0.5516,
      "step": 3096
    },
    {
      "epoch": 0.2788268923450899,
      "grad_norm": 0.7959268352972786,
      "learning_rate": 1.69257873664847e-05,
      "loss": 0.6326,
      "step": 3097
    },
    {
      "epoch": 0.27891692363096177,
      "grad_norm": 0.6989798432698661,
      "learning_rate": 1.6923683513477477e-05,
      "loss": 0.5389,
      "step": 3098
    },
    {
      "epoch": 0.2790069549168336,
      "grad_norm": 0.9341202990316249,
      "learning_rate": 1.6921579071675904e-05,
      "loss": 0.6222,
      "step": 3099
    },
    {
      "epoch": 0.27909698620270545,
      "grad_norm": 0.7770978987404107,
      "learning_rate": 1.6919474041258953e-05,
      "loss": 0.6424,
      "step": 3100
    },
    {
      "epoch": 0.27918701748857727,
      "grad_norm": 0.6890058098003023,
      "learning_rate": 1.6917368422405628e-05,
      "loss": 0.525,
      "step": 3101
    },
    {
      "epoch": 0.27927704877444914,
      "grad_norm": 0.606943061008447,
      "learning_rate": 1.6915262215295002e-05,
      "loss": 0.5083,
      "step": 3102
    },
    {
      "epoch": 0.27936708006032096,
      "grad_norm": 0.8386299288320173,
      "learning_rate": 1.6913155420106183e-05,
      "loss": 0.5812,
      "step": 3103
    },
    {
      "epoch": 0.2794571113461928,
      "grad_norm": 0.706018774793345,
      "learning_rate": 1.691104803701833e-05,
      "loss": 0.6145,
      "step": 3104
    },
    {
      "epoch": 0.27954714263206465,
      "grad_norm": 0.5895175907456389,
      "learning_rate": 1.6908940066210668e-05,
      "loss": 0.564,
      "step": 3105
    },
    {
      "epoch": 0.27963717391793647,
      "grad_norm": 0.6740304738798897,
      "learning_rate": 1.6906831507862446e-05,
      "loss": 0.5187,
      "step": 3106
    },
    {
      "epoch": 0.27972720520380834,
      "grad_norm": 0.7283128313959386,
      "learning_rate": 1.6904722362152987e-05,
      "loss": 0.607,
      "step": 3107
    },
    {
      "epoch": 0.27981723648968015,
      "grad_norm": 0.6977893614057287,
      "learning_rate": 1.690261262926165e-05,
      "loss": 0.5147,
      "step": 3108
    },
    {
      "epoch": 0.279907267775552,
      "grad_norm": 0.7523176570053777,
      "learning_rate": 1.6900502309367848e-05,
      "loss": 0.5041,
      "step": 3109
    },
    {
      "epoch": 0.27999729906142384,
      "grad_norm": 0.8818859922316993,
      "learning_rate": 1.6898391402651046e-05,
      "loss": 0.6412,
      "step": 3110
    },
    {
      "epoch": 0.28008733034729566,
      "grad_norm": 0.8821061704188415,
      "learning_rate": 1.6896279909290755e-05,
      "loss": 0.6826,
      "step": 3111
    },
    {
      "epoch": 0.28017736163316753,
      "grad_norm": 0.5979386764391282,
      "learning_rate": 1.689416782946654e-05,
      "loss": 0.532,
      "step": 3112
    },
    {
      "epoch": 0.28026739291903935,
      "grad_norm": 0.743978859164818,
      "learning_rate": 1.6892055163358013e-05,
      "loss": 0.5594,
      "step": 3113
    },
    {
      "epoch": 0.2803574242049112,
      "grad_norm": 0.8441644808551724,
      "learning_rate": 1.688994191114483e-05,
      "loss": 0.5263,
      "step": 3114
    },
    {
      "epoch": 0.28044745549078304,
      "grad_norm": 0.9107205709300613,
      "learning_rate": 1.6887828073006713e-05,
      "loss": 0.4712,
      "step": 3115
    },
    {
      "epoch": 0.2805374867766549,
      "grad_norm": 0.7522859198334246,
      "learning_rate": 1.688571364912342e-05,
      "loss": 0.5244,
      "step": 3116
    },
    {
      "epoch": 0.2806275180625267,
      "grad_norm": 0.8005814928869767,
      "learning_rate": 1.6883598639674762e-05,
      "loss": 0.602,
      "step": 3117
    },
    {
      "epoch": 0.28071754934839854,
      "grad_norm": 0.6965303812895747,
      "learning_rate": 1.6881483044840597e-05,
      "loss": 0.6022,
      "step": 3118
    },
    {
      "epoch": 0.2808075806342704,
      "grad_norm": 0.6665176154720551,
      "learning_rate": 1.6879366864800846e-05,
      "loss": 0.6425,
      "step": 3119
    },
    {
      "epoch": 0.28089761192014223,
      "grad_norm": 0.6381198941094876,
      "learning_rate": 1.687725009973546e-05,
      "loss": 0.5602,
      "step": 3120
    },
    {
      "epoch": 0.2809876432060141,
      "grad_norm": 0.6624884013558289,
      "learning_rate": 1.6875132749824463e-05,
      "loss": 0.5011,
      "step": 3121
    },
    {
      "epoch": 0.2810776744918859,
      "grad_norm": 0.6721521248647696,
      "learning_rate": 1.68730148152479e-05,
      "loss": 0.5819,
      "step": 3122
    },
    {
      "epoch": 0.2811677057777578,
      "grad_norm": 0.7039836834605738,
      "learning_rate": 1.6870896296185894e-05,
      "loss": 0.6197,
      "step": 3123
    },
    {
      "epoch": 0.2812577370636296,
      "grad_norm": 0.7706396946961442,
      "learning_rate": 1.68687771928186e-05,
      "loss": 0.6254,
      "step": 3124
    },
    {
      "epoch": 0.2813477683495014,
      "grad_norm": 0.6392403862040054,
      "learning_rate": 1.6866657505326227e-05,
      "loss": 0.598,
      "step": 3125
    },
    {
      "epoch": 0.2814377996353733,
      "grad_norm": 0.6995157160812681,
      "learning_rate": 1.6864537233889038e-05,
      "loss": 0.5161,
      "step": 3126
    },
    {
      "epoch": 0.2815278309212451,
      "grad_norm": 0.684825221904999,
      "learning_rate": 1.686241637868734e-05,
      "loss": 0.631,
      "step": 3127
    },
    {
      "epoch": 0.281617862207117,
      "grad_norm": 0.9114554221757453,
      "learning_rate": 1.686029493990149e-05,
      "loss": 0.5087,
      "step": 3128
    },
    {
      "epoch": 0.2817078934929888,
      "grad_norm": 0.6483176991716216,
      "learning_rate": 1.6858172917711898e-05,
      "loss": 0.5336,
      "step": 3129
    },
    {
      "epoch": 0.2817979247788607,
      "grad_norm": 0.8169553951759144,
      "learning_rate": 1.6856050312299024e-05,
      "loss": 0.5814,
      "step": 3130
    },
    {
      "epoch": 0.2818879560647325,
      "grad_norm": 0.7136150177038763,
      "learning_rate": 1.6853927123843375e-05,
      "loss": 0.5317,
      "step": 3131
    },
    {
      "epoch": 0.2819779873506043,
      "grad_norm": 0.8069877182042023,
      "learning_rate": 1.6851803352525506e-05,
      "loss": 0.6051,
      "step": 3132
    },
    {
      "epoch": 0.2820680186364762,
      "grad_norm": 0.6168952109244471,
      "learning_rate": 1.6849678998526025e-05,
      "loss": 0.5064,
      "step": 3133
    },
    {
      "epoch": 0.282158049922348,
      "grad_norm": 0.9474180949907296,
      "learning_rate": 1.684755406202559e-05,
      "loss": 0.5879,
      "step": 3134
    },
    {
      "epoch": 0.28224808120821987,
      "grad_norm": 0.7281077840992256,
      "learning_rate": 1.6845428543204905e-05,
      "loss": 0.5496,
      "step": 3135
    },
    {
      "epoch": 0.2823381124940917,
      "grad_norm": 0.7948784216144098,
      "learning_rate": 1.684330244224473e-05,
      "loss": 0.4941,
      "step": 3136
    },
    {
      "epoch": 0.28242814377996356,
      "grad_norm": 0.7940453833175869,
      "learning_rate": 1.684117575932586e-05,
      "loss": 0.6153,
      "step": 3137
    },
    {
      "epoch": 0.2825181750658354,
      "grad_norm": 0.7564106767053222,
      "learning_rate": 1.683904849462916e-05,
      "loss": 0.5405,
      "step": 3138
    },
    {
      "epoch": 0.2826082063517072,
      "grad_norm": 0.7433452518670848,
      "learning_rate": 1.6836920648335524e-05,
      "loss": 0.5221,
      "step": 3139
    },
    {
      "epoch": 0.28269823763757906,
      "grad_norm": 1.041715180017357,
      "learning_rate": 1.6834792220625914e-05,
      "loss": 0.5888,
      "step": 3140
    },
    {
      "epoch": 0.2827882689234509,
      "grad_norm": 0.7805070733369345,
      "learning_rate": 1.6832663211681327e-05,
      "loss": 0.5236,
      "step": 3141
    },
    {
      "epoch": 0.28287830020932275,
      "grad_norm": 0.9660771320659174,
      "learning_rate": 1.6830533621682825e-05,
      "loss": 0.563,
      "step": 3142
    },
    {
      "epoch": 0.28296833149519457,
      "grad_norm": 0.7457406976888923,
      "learning_rate": 1.68284034508115e-05,
      "loss": 0.581,
      "step": 3143
    },
    {
      "epoch": 0.28305836278106644,
      "grad_norm": 0.7792689172591784,
      "learning_rate": 1.68262726992485e-05,
      "loss": 0.5006,
      "step": 3144
    },
    {
      "epoch": 0.28314839406693826,
      "grad_norm": 0.6757738671556713,
      "learning_rate": 1.6824141367175033e-05,
      "loss": 0.5474,
      "step": 3145
    },
    {
      "epoch": 0.2832384253528101,
      "grad_norm": 0.699403127014277,
      "learning_rate": 1.682200945477235e-05,
      "loss": 0.5815,
      "step": 3146
    },
    {
      "epoch": 0.28332845663868195,
      "grad_norm": 0.8296687871878645,
      "learning_rate": 1.6819876962221743e-05,
      "loss": 0.537,
      "step": 3147
    },
    {
      "epoch": 0.28341848792455376,
      "grad_norm": 0.7832599253678597,
      "learning_rate": 1.6817743889704564e-05,
      "loss": 0.5055,
      "step": 3148
    },
    {
      "epoch": 0.28350851921042564,
      "grad_norm": 0.7500254649973334,
      "learning_rate": 1.6815610237402216e-05,
      "loss": 0.6022,
      "step": 3149
    },
    {
      "epoch": 0.28359855049629745,
      "grad_norm": 0.9718520124152895,
      "learning_rate": 1.6813476005496138e-05,
      "loss": 0.5164,
      "step": 3150
    },
    {
      "epoch": 0.2836885817821693,
      "grad_norm": 0.6551998535885903,
      "learning_rate": 1.681134119416783e-05,
      "loss": 0.6406,
      "step": 3151
    },
    {
      "epoch": 0.28377861306804114,
      "grad_norm": 0.6065965287235797,
      "learning_rate": 1.6809205803598838e-05,
      "loss": 0.5382,
      "step": 3152
    },
    {
      "epoch": 0.28386864435391296,
      "grad_norm": 0.7167896679684339,
      "learning_rate": 1.6807069833970756e-05,
      "loss": 0.5753,
      "step": 3153
    },
    {
      "epoch": 0.28395867563978483,
      "grad_norm": 0.6597648985022644,
      "learning_rate": 1.6804933285465227e-05,
      "loss": 0.5052,
      "step": 3154
    },
    {
      "epoch": 0.28404870692565665,
      "grad_norm": 0.875504566455438,
      "learning_rate": 1.680279615826395e-05,
      "loss": 0.5823,
      "step": 3155
    },
    {
      "epoch": 0.2841387382115285,
      "grad_norm": 0.8214306810786552,
      "learning_rate": 1.6800658452548662e-05,
      "loss": 0.5914,
      "step": 3156
    },
    {
      "epoch": 0.28422876949740034,
      "grad_norm": 0.8222194353679387,
      "learning_rate": 1.6798520168501155e-05,
      "loss": 0.6434,
      "step": 3157
    },
    {
      "epoch": 0.2843188007832722,
      "grad_norm": 0.915708781566435,
      "learning_rate": 1.679638130630327e-05,
      "loss": 0.5485,
      "step": 3158
    },
    {
      "epoch": 0.284408832069144,
      "grad_norm": 0.672561290327485,
      "learning_rate": 1.6794241866136903e-05,
      "loss": 0.5907,
      "step": 3159
    },
    {
      "epoch": 0.28449886335501584,
      "grad_norm": 0.6969242227153206,
      "learning_rate": 1.6792101848183985e-05,
      "loss": 0.6223,
      "step": 3160
    },
    {
      "epoch": 0.2845888946408877,
      "grad_norm": 0.6893622929460663,
      "learning_rate": 1.678996125262651e-05,
      "loss": 0.5326,
      "step": 3161
    },
    {
      "epoch": 0.28467892592675953,
      "grad_norm": 0.7461745261484817,
      "learning_rate": 1.678782007964651e-05,
      "loss": 0.5061,
      "step": 3162
    },
    {
      "epoch": 0.2847689572126314,
      "grad_norm": 0.7428431298573734,
      "learning_rate": 1.6785678329426083e-05,
      "loss": 0.5868,
      "step": 3163
    },
    {
      "epoch": 0.2848589884985032,
      "grad_norm": 0.6729202823297434,
      "learning_rate": 1.6783536002147355e-05,
      "loss": 0.6205,
      "step": 3164
    },
    {
      "epoch": 0.2849490197843751,
      "grad_norm": 0.712743564642263,
      "learning_rate": 1.6781393097992512e-05,
      "loss": 0.4973,
      "step": 3165
    },
    {
      "epoch": 0.2850390510702469,
      "grad_norm": 0.6271921204748115,
      "learning_rate": 1.6779249617143788e-05,
      "loss": 0.5929,
      "step": 3166
    },
    {
      "epoch": 0.2851290823561187,
      "grad_norm": 0.880653775936523,
      "learning_rate": 1.6777105559783468e-05,
      "loss": 0.5332,
      "step": 3167
    },
    {
      "epoch": 0.2852191136419906,
      "grad_norm": 0.7748523692024845,
      "learning_rate": 1.6774960926093883e-05,
      "loss": 0.6187,
      "step": 3168
    },
    {
      "epoch": 0.2853091449278624,
      "grad_norm": 0.805781598855641,
      "learning_rate": 1.6772815716257414e-05,
      "loss": 0.5916,
      "step": 3169
    },
    {
      "epoch": 0.2853991762137343,
      "grad_norm": 0.753998468703264,
      "learning_rate": 1.6770669930456487e-05,
      "loss": 0.5607,
      "step": 3170
    },
    {
      "epoch": 0.2854892074996061,
      "grad_norm": 0.716736963605889,
      "learning_rate": 1.6768523568873592e-05,
      "loss": 0.5502,
      "step": 3171
    },
    {
      "epoch": 0.285579238785478,
      "grad_norm": 0.7464656006560232,
      "learning_rate": 1.6766376631691246e-05,
      "loss": 0.6754,
      "step": 3172
    },
    {
      "epoch": 0.2856692700713498,
      "grad_norm": 0.8287178716685796,
      "learning_rate": 1.676422911909203e-05,
      "loss": 0.5749,
      "step": 3173
    },
    {
      "epoch": 0.28575930135722166,
      "grad_norm": 0.8846875786831807,
      "learning_rate": 1.676208103125857e-05,
      "loss": 0.586,
      "step": 3174
    },
    {
      "epoch": 0.2858493326430935,
      "grad_norm": 0.8643454143778805,
      "learning_rate": 1.675993236837354e-05,
      "loss": 0.6091,
      "step": 3175
    },
    {
      "epoch": 0.2859393639289653,
      "grad_norm": 0.9195828631040551,
      "learning_rate": 1.6757783130619664e-05,
      "loss": 0.5736,
      "step": 3176
    },
    {
      "epoch": 0.28602939521483717,
      "grad_norm": 1.3423233674004411,
      "learning_rate": 1.6755633318179713e-05,
      "loss": 0.5596,
      "step": 3177
    },
    {
      "epoch": 0.286119426500709,
      "grad_norm": 0.8701732348490238,
      "learning_rate": 1.6753482931236512e-05,
      "loss": 0.6138,
      "step": 3178
    },
    {
      "epoch": 0.28620945778658086,
      "grad_norm": 0.6969812467171141,
      "learning_rate": 1.6751331969972927e-05,
      "loss": 0.5529,
      "step": 3179
    },
    {
      "epoch": 0.2862994890724527,
      "grad_norm": 0.7939257010191586,
      "learning_rate": 1.674918043457188e-05,
      "loss": 0.5271,
      "step": 3180
    },
    {
      "epoch": 0.28638952035832455,
      "grad_norm": 0.8403102032416663,
      "learning_rate": 1.674702832521634e-05,
      "loss": 0.626,
      "step": 3181
    },
    {
      "epoch": 0.28647955164419636,
      "grad_norm": 0.7385994041020719,
      "learning_rate": 1.674487564208932e-05,
      "loss": 0.5936,
      "step": 3182
    },
    {
      "epoch": 0.2865695829300682,
      "grad_norm": 0.7539107664666629,
      "learning_rate": 1.6742722385373887e-05,
      "loss": 0.4694,
      "step": 3183
    },
    {
      "epoch": 0.28665961421594005,
      "grad_norm": 0.5554759681046633,
      "learning_rate": 1.6740568555253153e-05,
      "loss": 0.583,
      "step": 3184
    },
    {
      "epoch": 0.28674964550181187,
      "grad_norm": 0.6951900633464766,
      "learning_rate": 1.673841415191029e-05,
      "loss": 0.5963,
      "step": 3185
    },
    {
      "epoch": 0.28683967678768374,
      "grad_norm": 0.6713877715622709,
      "learning_rate": 1.67362591755285e-05,
      "loss": 0.5419,
      "step": 3186
    },
    {
      "epoch": 0.28692970807355556,
      "grad_norm": 0.6836546656702388,
      "learning_rate": 1.6734103626291046e-05,
      "loss": 0.5245,
      "step": 3187
    },
    {
      "epoch": 0.28701973935942743,
      "grad_norm": 0.7085048438508882,
      "learning_rate": 1.673194750438124e-05,
      "loss": 0.6461,
      "step": 3188
    },
    {
      "epoch": 0.28710977064529924,
      "grad_norm": 0.6644431317486518,
      "learning_rate": 1.6729790809982435e-05,
      "loss": 0.5312,
      "step": 3189
    },
    {
      "epoch": 0.28719980193117106,
      "grad_norm": 0.6134156895749724,
      "learning_rate": 1.672763354327804e-05,
      "loss": 0.5139,
      "step": 3190
    },
    {
      "epoch": 0.28728983321704293,
      "grad_norm": 0.6974613911481999,
      "learning_rate": 1.6725475704451514e-05,
      "loss": 0.5618,
      "step": 3191
    },
    {
      "epoch": 0.28737986450291475,
      "grad_norm": 0.5987124602768826,
      "learning_rate": 1.672331729368636e-05,
      "loss": 0.4584,
      "step": 3192
    },
    {
      "epoch": 0.2874698957887866,
      "grad_norm": 0.8344247751522454,
      "learning_rate": 1.6721158311166122e-05,
      "loss": 0.6476,
      "step": 3193
    },
    {
      "epoch": 0.28755992707465844,
      "grad_norm": 0.6678170826073236,
      "learning_rate": 1.6718998757074414e-05,
      "loss": 0.6061,
      "step": 3194
    },
    {
      "epoch": 0.2876499583605303,
      "grad_norm": 0.574538256216891,
      "learning_rate": 1.6716838631594877e-05,
      "loss": 0.4897,
      "step": 3195
    },
    {
      "epoch": 0.28773998964640213,
      "grad_norm": 0.6767425556461969,
      "learning_rate": 1.671467793491121e-05,
      "loss": 0.4978,
      "step": 3196
    },
    {
      "epoch": 0.28783002093227394,
      "grad_norm": 0.727137955822794,
      "learning_rate": 1.6712516667207165e-05,
      "loss": 0.6629,
      "step": 3197
    },
    {
      "epoch": 0.2879200522181458,
      "grad_norm": 0.6171790951131491,
      "learning_rate": 1.6710354828666535e-05,
      "loss": 0.5346,
      "step": 3198
    },
    {
      "epoch": 0.28801008350401763,
      "grad_norm": 0.8835010982806834,
      "learning_rate": 1.6708192419473164e-05,
      "loss": 0.5631,
      "step": 3199
    },
    {
      "epoch": 0.2881001147898895,
      "grad_norm": 0.8041167261103745,
      "learning_rate": 1.670602943981094e-05,
      "loss": 0.568,
      "step": 3200
    },
    {
      "epoch": 0.2881901460757613,
      "grad_norm": 0.7513871875886391,
      "learning_rate": 1.6703865889863814e-05,
      "loss": 0.6234,
      "step": 3201
    },
    {
      "epoch": 0.2882801773616332,
      "grad_norm": 0.6435028559293334,
      "learning_rate": 1.6701701769815763e-05,
      "loss": 0.5193,
      "step": 3202
    },
    {
      "epoch": 0.288370208647505,
      "grad_norm": 0.8096494948632055,
      "learning_rate": 1.669953707985084e-05,
      "loss": 0.5815,
      "step": 3203
    },
    {
      "epoch": 0.2884602399333768,
      "grad_norm": 0.8129540078926173,
      "learning_rate": 1.669737182015312e-05,
      "loss": 0.5082,
      "step": 3204
    },
    {
      "epoch": 0.2885502712192487,
      "grad_norm": 0.6887059408443643,
      "learning_rate": 1.669520599090674e-05,
      "loss": 0.4729,
      "step": 3205
    },
    {
      "epoch": 0.2886403025051205,
      "grad_norm": 0.7723715199414593,
      "learning_rate": 1.6693039592295887e-05,
      "loss": 0.5595,
      "step": 3206
    },
    {
      "epoch": 0.2887303337909924,
      "grad_norm": 0.8496441665807181,
      "learning_rate": 1.669087262450479e-05,
      "loss": 0.5576,
      "step": 3207
    },
    {
      "epoch": 0.2888203650768642,
      "grad_norm": 0.709317137916849,
      "learning_rate": 1.668870508771773e-05,
      "loss": 0.5572,
      "step": 3208
    },
    {
      "epoch": 0.2889103963627361,
      "grad_norm": 0.7431836621441924,
      "learning_rate": 1.6686536982119038e-05,
      "loss": 0.6094,
      "step": 3209
    },
    {
      "epoch": 0.2890004276486079,
      "grad_norm": 0.6930711917671164,
      "learning_rate": 1.668436830789309e-05,
      "loss": 0.5346,
      "step": 3210
    },
    {
      "epoch": 0.2890904589344797,
      "grad_norm": 0.726369307017401,
      "learning_rate": 1.6682199065224307e-05,
      "loss": 0.5988,
      "step": 3211
    },
    {
      "epoch": 0.2891804902203516,
      "grad_norm": 0.6926263805251902,
      "learning_rate": 1.668002925429717e-05,
      "loss": 0.5656,
      "step": 3212
    },
    {
      "epoch": 0.2892705215062234,
      "grad_norm": 0.6951134389765595,
      "learning_rate": 1.6677858875296197e-05,
      "loss": 0.5029,
      "step": 3213
    },
    {
      "epoch": 0.28936055279209527,
      "grad_norm": 0.7894091410623797,
      "learning_rate": 1.6675687928405954e-05,
      "loss": 0.4834,
      "step": 3214
    },
    {
      "epoch": 0.2894505840779671,
      "grad_norm": 0.7798550836109586,
      "learning_rate": 1.6673516413811068e-05,
      "loss": 0.6443,
      "step": 3215
    },
    {
      "epoch": 0.28954061536383896,
      "grad_norm": 0.8723687063947858,
      "learning_rate": 1.6671344331696203e-05,
      "loss": 0.6212,
      "step": 3216
    },
    {
      "epoch": 0.2896306466497108,
      "grad_norm": 0.8351333268665444,
      "learning_rate": 1.666917168224607e-05,
      "loss": 0.5191,
      "step": 3217
    },
    {
      "epoch": 0.2897206779355826,
      "grad_norm": 0.6815179045556391,
      "learning_rate": 1.666699846564544e-05,
      "loss": 0.5355,
      "step": 3218
    },
    {
      "epoch": 0.28981070922145447,
      "grad_norm": 1.1918645164294313,
      "learning_rate": 1.666482468207912e-05,
      "loss": 0.5581,
      "step": 3219
    },
    {
      "epoch": 0.2899007405073263,
      "grad_norm": 1.0378105995790037,
      "learning_rate": 1.666265033173197e-05,
      "loss": 0.5807,
      "step": 3220
    },
    {
      "epoch": 0.28999077179319815,
      "grad_norm": 0.7708416807549211,
      "learning_rate": 1.6660475414788896e-05,
      "loss": 0.652,
      "step": 3221
    },
    {
      "epoch": 0.29008080307906997,
      "grad_norm": 0.7494800335227941,
      "learning_rate": 1.6658299931434857e-05,
      "loss": 0.542,
      "step": 3222
    },
    {
      "epoch": 0.29017083436494184,
      "grad_norm": 0.6682735969294582,
      "learning_rate": 1.665612388185486e-05,
      "loss": 0.5263,
      "step": 3223
    },
    {
      "epoch": 0.29026086565081366,
      "grad_norm": 0.7452946765070032,
      "learning_rate": 1.6653947266233956e-05,
      "loss": 0.5482,
      "step": 3224
    },
    {
      "epoch": 0.2903508969366855,
      "grad_norm": 0.6736235018244175,
      "learning_rate": 1.665177008475724e-05,
      "loss": 0.5631,
      "step": 3225
    },
    {
      "epoch": 0.29044092822255735,
      "grad_norm": 0.8580396507981003,
      "learning_rate": 1.6649592337609872e-05,
      "loss": 0.5871,
      "step": 3226
    },
    {
      "epoch": 0.29053095950842917,
      "grad_norm": 0.7404372672119915,
      "learning_rate": 1.664741402497704e-05,
      "loss": 0.6421,
      "step": 3227
    },
    {
      "epoch": 0.29062099079430104,
      "grad_norm": 0.6815432492355308,
      "learning_rate": 1.664523514704399e-05,
      "loss": 0.5234,
      "step": 3228
    },
    {
      "epoch": 0.29071102208017285,
      "grad_norm": 0.8182818807138259,
      "learning_rate": 1.664305570399602e-05,
      "loss": 0.4796,
      "step": 3229
    },
    {
      "epoch": 0.2908010533660447,
      "grad_norm": 0.7741385342993122,
      "learning_rate": 1.6640875696018465e-05,
      "loss": 0.5938,
      "step": 3230
    },
    {
      "epoch": 0.29089108465191654,
      "grad_norm": 0.7755999206086029,
      "learning_rate": 1.6638695123296718e-05,
      "loss": 0.5547,
      "step": 3231
    },
    {
      "epoch": 0.29098111593778836,
      "grad_norm": 0.92086980012008,
      "learning_rate": 1.6636513986016215e-05,
      "loss": 0.4968,
      "step": 3232
    },
    {
      "epoch": 0.29107114722366023,
      "grad_norm": 0.7069764297957791,
      "learning_rate": 1.6634332284362444e-05,
      "loss": 0.6064,
      "step": 3233
    },
    {
      "epoch": 0.29116117850953205,
      "grad_norm": 0.7728775828940514,
      "learning_rate": 1.6632150018520934e-05,
      "loss": 0.5623,
      "step": 3234
    },
    {
      "epoch": 0.2912512097954039,
      "grad_norm": 0.8206992271264164,
      "learning_rate": 1.6629967188677268e-05,
      "loss": 0.6173,
      "step": 3235
    },
    {
      "epoch": 0.29134124108127574,
      "grad_norm": 0.8540884187282599,
      "learning_rate": 1.6627783795017075e-05,
      "loss": 0.5557,
      "step": 3236
    },
    {
      "epoch": 0.2914312723671476,
      "grad_norm": 0.7138677009375686,
      "learning_rate": 1.6625599837726033e-05,
      "loss": 0.5955,
      "step": 3237
    },
    {
      "epoch": 0.2915213036530194,
      "grad_norm": 0.7324909239248677,
      "learning_rate": 1.6623415316989868e-05,
      "loss": 0.5235,
      "step": 3238
    },
    {
      "epoch": 0.29161133493889124,
      "grad_norm": 0.811676507834694,
      "learning_rate": 1.662123023299435e-05,
      "loss": 0.5637,
      "step": 3239
    },
    {
      "epoch": 0.2917013662247631,
      "grad_norm": 0.8004399023216644,
      "learning_rate": 1.6619044585925305e-05,
      "loss": 0.5937,
      "step": 3240
    },
    {
      "epoch": 0.29179139751063493,
      "grad_norm": 0.7283712626137923,
      "learning_rate": 1.6616858375968596e-05,
      "loss": 0.5924,
      "step": 3241
    },
    {
      "epoch": 0.2918814287965068,
      "grad_norm": 0.7523615251088093,
      "learning_rate": 1.6614671603310143e-05,
      "loss": 0.5785,
      "step": 3242
    },
    {
      "epoch": 0.2919714600823786,
      "grad_norm": 0.7344147265259329,
      "learning_rate": 1.661248426813591e-05,
      "loss": 0.5189,
      "step": 3243
    },
    {
      "epoch": 0.2920614913682505,
      "grad_norm": 0.6280907300879381,
      "learning_rate": 1.6610296370631905e-05,
      "loss": 0.562,
      "step": 3244
    },
    {
      "epoch": 0.2921515226541223,
      "grad_norm": 0.7753915592496958,
      "learning_rate": 1.6608107910984196e-05,
      "loss": 0.5448,
      "step": 3245
    },
    {
      "epoch": 0.2922415539399941,
      "grad_norm": 0.6429573329344652,
      "learning_rate": 1.6605918889378885e-05,
      "loss": 0.5647,
      "step": 3246
    },
    {
      "epoch": 0.292331585225866,
      "grad_norm": 0.764933200208614,
      "learning_rate": 1.660372930600213e-05,
      "loss": 0.6193,
      "step": 3247
    },
    {
      "epoch": 0.2924216165117378,
      "grad_norm": 0.6940939958489251,
      "learning_rate": 1.6601539161040134e-05,
      "loss": 0.5894,
      "step": 3248
    },
    {
      "epoch": 0.2925116477976097,
      "grad_norm": 0.6857345640002147,
      "learning_rate": 1.6599348454679148e-05,
      "loss": 0.5571,
      "step": 3249
    },
    {
      "epoch": 0.2926016790834815,
      "grad_norm": 0.6897323748526573,
      "learning_rate": 1.6597157187105475e-05,
      "loss": 0.5256,
      "step": 3250
    },
    {
      "epoch": 0.2926917103693534,
      "grad_norm": 0.7612953559918257,
      "learning_rate": 1.6594965358505452e-05,
      "loss": 0.5235,
      "step": 3251
    },
    {
      "epoch": 0.2927817416552252,
      "grad_norm": 0.6695183633659109,
      "learning_rate": 1.6592772969065487e-05,
      "loss": 0.5567,
      "step": 3252
    },
    {
      "epoch": 0.292871772941097,
      "grad_norm": 0.7600043830078228,
      "learning_rate": 1.6590580018972012e-05,
      "loss": 0.5729,
      "step": 3253
    },
    {
      "epoch": 0.2929618042269689,
      "grad_norm": 0.991910796542885,
      "learning_rate": 1.658838650841152e-05,
      "loss": 0.5489,
      "step": 3254
    },
    {
      "epoch": 0.2930518355128407,
      "grad_norm": 0.9139229460457575,
      "learning_rate": 1.6586192437570548e-05,
      "loss": 0.5807,
      "step": 3255
    },
    {
      "epoch": 0.29314186679871257,
      "grad_norm": 0.7838214813390889,
      "learning_rate": 1.658399780663568e-05,
      "loss": 0.5122,
      "step": 3256
    },
    {
      "epoch": 0.2932318980845844,
      "grad_norm": 0.7136173027014896,
      "learning_rate": 1.6581802615793552e-05,
      "loss": 0.5774,
      "step": 3257
    },
    {
      "epoch": 0.29332192937045626,
      "grad_norm": 0.7954409511537506,
      "learning_rate": 1.6579606865230843e-05,
      "loss": 0.5542,
      "step": 3258
    },
    {
      "epoch": 0.2934119606563281,
      "grad_norm": 0.7205138434941544,
      "learning_rate": 1.657741055513428e-05,
      "loss": 0.547,
      "step": 3259
    },
    {
      "epoch": 0.2935019919421999,
      "grad_norm": 0.9871338409090902,
      "learning_rate": 1.657521368569064e-05,
      "loss": 0.5887,
      "step": 3260
    },
    {
      "epoch": 0.29359202322807176,
      "grad_norm": 0.9340014288597986,
      "learning_rate": 1.6573016257086743e-05,
      "loss": 0.4343,
      "step": 3261
    },
    {
      "epoch": 0.2936820545139436,
      "grad_norm": 0.6601625390811737,
      "learning_rate": 1.6570818269509465e-05,
      "loss": 0.5188,
      "step": 3262
    },
    {
      "epoch": 0.29377208579981545,
      "grad_norm": 0.7399030331135364,
      "learning_rate": 1.6568619723145716e-05,
      "loss": 0.6614,
      "step": 3263
    },
    {
      "epoch": 0.29386211708568727,
      "grad_norm": 0.8664997443905232,
      "learning_rate": 1.6566420618182472e-05,
      "loss": 0.55,
      "step": 3264
    },
    {
      "epoch": 0.29395214837155914,
      "grad_norm": 0.6686437598062392,
      "learning_rate": 1.6564220954806744e-05,
      "loss": 0.5324,
      "step": 3265
    },
    {
      "epoch": 0.29404217965743096,
      "grad_norm": 0.833054083259702,
      "learning_rate": 1.6562020733205583e-05,
      "loss": 0.5961,
      "step": 3266
    },
    {
      "epoch": 0.2941322109433028,
      "grad_norm": 1.1005881761757244,
      "learning_rate": 1.6559819953566106e-05,
      "loss": 0.5502,
      "step": 3267
    },
    {
      "epoch": 0.29422224222917465,
      "grad_norm": 0.8044170704932387,
      "learning_rate": 1.6557618616075467e-05,
      "loss": 0.5427,
      "step": 3268
    },
    {
      "epoch": 0.29431227351504646,
      "grad_norm": 0.7343731682001495,
      "learning_rate": 1.6555416720920873e-05,
      "loss": 0.6065,
      "step": 3269
    },
    {
      "epoch": 0.29440230480091834,
      "grad_norm": 0.7910650121428626,
      "learning_rate": 1.6553214268289566e-05,
      "loss": 0.5151,
      "step": 3270
    },
    {
      "epoch": 0.29449233608679015,
      "grad_norm": 0.758121050180742,
      "learning_rate": 1.6551011258368852e-05,
      "loss": 0.5193,
      "step": 3271
    },
    {
      "epoch": 0.294582367372662,
      "grad_norm": 0.7123861534390695,
      "learning_rate": 1.654880769134607e-05,
      "loss": 0.6197,
      "step": 3272
    },
    {
      "epoch": 0.29467239865853384,
      "grad_norm": 0.7612478414609894,
      "learning_rate": 1.6546603567408618e-05,
      "loss": 0.5607,
      "step": 3273
    },
    {
      "epoch": 0.29476242994440566,
      "grad_norm": 0.7163681640077405,
      "learning_rate": 1.6544398886743934e-05,
      "loss": 0.5805,
      "step": 3274
    },
    {
      "epoch": 0.29485246123027753,
      "grad_norm": 0.5627981220467213,
      "learning_rate": 1.65421936495395e-05,
      "loss": 0.5395,
      "step": 3275
    },
    {
      "epoch": 0.29494249251614935,
      "grad_norm": 0.6902854343327517,
      "learning_rate": 1.653998785598286e-05,
      "loss": 0.5568,
      "step": 3276
    },
    {
      "epoch": 0.2950325238020212,
      "grad_norm": 0.585826849448557,
      "learning_rate": 1.6537781506261588e-05,
      "loss": 0.5537,
      "step": 3277
    },
    {
      "epoch": 0.29512255508789303,
      "grad_norm": 0.6774338382153453,
      "learning_rate": 1.653557460056332e-05,
      "loss": 0.6157,
      "step": 3278
    },
    {
      "epoch": 0.2952125863737649,
      "grad_norm": 0.827105211110262,
      "learning_rate": 1.6533367139075732e-05,
      "loss": 0.5102,
      "step": 3279
    },
    {
      "epoch": 0.2953026176596367,
      "grad_norm": 0.712646986670046,
      "learning_rate": 1.653115912198654e-05,
      "loss": 0.5848,
      "step": 3280
    },
    {
      "epoch": 0.29539264894550854,
      "grad_norm": 1.5020438577164599,
      "learning_rate": 1.6528950549483528e-05,
      "loss": 0.5217,
      "step": 3281
    },
    {
      "epoch": 0.2954826802313804,
      "grad_norm": 0.8221083842743115,
      "learning_rate": 1.65267414217545e-05,
      "loss": 0.5702,
      "step": 3282
    },
    {
      "epoch": 0.29557271151725223,
      "grad_norm": 0.6600780278700655,
      "learning_rate": 1.6524531738987334e-05,
      "loss": 0.5606,
      "step": 3283
    },
    {
      "epoch": 0.2956627428031241,
      "grad_norm": 0.6456743481832449,
      "learning_rate": 1.6522321501369935e-05,
      "loss": 0.5773,
      "step": 3284
    },
    {
      "epoch": 0.2957527740889959,
      "grad_norm": 0.7842557495998285,
      "learning_rate": 1.6520110709090263e-05,
      "loss": 0.6287,
      "step": 3285
    },
    {
      "epoch": 0.2958428053748678,
      "grad_norm": 0.6394917163303433,
      "learning_rate": 1.651789936233633e-05,
      "loss": 0.4855,
      "step": 3286
    },
    {
      "epoch": 0.2959328366607396,
      "grad_norm": 0.7923046816839848,
      "learning_rate": 1.651568746129619e-05,
      "loss": 0.5352,
      "step": 3287
    },
    {
      "epoch": 0.2960228679466114,
      "grad_norm": 0.6595598210055934,
      "learning_rate": 1.6513475006157942e-05,
      "loss": 0.6347,
      "step": 3288
    },
    {
      "epoch": 0.2961128992324833,
      "grad_norm": 0.6363787458722258,
      "learning_rate": 1.6511261997109732e-05,
      "loss": 0.5273,
      "step": 3289
    },
    {
      "epoch": 0.2962029305183551,
      "grad_norm": 0.6836814905380547,
      "learning_rate": 1.6509048434339764e-05,
      "loss": 0.5132,
      "step": 3290
    },
    {
      "epoch": 0.296292961804227,
      "grad_norm": 0.7287043815153319,
      "learning_rate": 1.650683431803627e-05,
      "loss": 0.5375,
      "step": 3291
    },
    {
      "epoch": 0.2963829930900988,
      "grad_norm": 0.6356154930716426,
      "learning_rate": 1.6504619648387546e-05,
      "loss": 0.5332,
      "step": 3292
    },
    {
      "epoch": 0.2964730243759707,
      "grad_norm": 0.8085830490238107,
      "learning_rate": 1.650240442558193e-05,
      "loss": 0.6384,
      "step": 3293
    },
    {
      "epoch": 0.2965630556618425,
      "grad_norm": 0.666727406455715,
      "learning_rate": 1.6500188649807802e-05,
      "loss": 0.5065,
      "step": 3294
    },
    {
      "epoch": 0.2966530869477143,
      "grad_norm": 0.6163184541260157,
      "learning_rate": 1.64979723212536e-05,
      "loss": 0.5259,
      "step": 3295
    },
    {
      "epoch": 0.2967431182335862,
      "grad_norm": 0.6330854088956083,
      "learning_rate": 1.649575544010779e-05,
      "loss": 0.561,
      "step": 3296
    },
    {
      "epoch": 0.296833149519458,
      "grad_norm": 0.749668729381998,
      "learning_rate": 1.6493538006558906e-05,
      "loss": 0.5838,
      "step": 3297
    },
    {
      "epoch": 0.29692318080532987,
      "grad_norm": 0.6555893113869848,
      "learning_rate": 1.649132002079552e-05,
      "loss": 0.5305,
      "step": 3298
    },
    {
      "epoch": 0.2970132120912017,
      "grad_norm": 0.745751596690661,
      "learning_rate": 1.6489101483006248e-05,
      "loss": 0.5727,
      "step": 3299
    },
    {
      "epoch": 0.29710324337707356,
      "grad_norm": 0.7682504157369927,
      "learning_rate": 1.6486882393379757e-05,
      "loss": 0.6224,
      "step": 3300
    },
    {
      "epoch": 0.2971932746629454,
      "grad_norm": 0.7543898194929387,
      "learning_rate": 1.648466275210476e-05,
      "loss": 0.6548,
      "step": 3301
    },
    {
      "epoch": 0.2972833059488172,
      "grad_norm": 0.7097241226622115,
      "learning_rate": 1.6482442559370013e-05,
      "loss": 0.4483,
      "step": 3302
    },
    {
      "epoch": 0.29737333723468906,
      "grad_norm": 0.6681240963061058,
      "learning_rate": 1.648022181536433e-05,
      "loss": 0.5589,
      "step": 3303
    },
    {
      "epoch": 0.2974633685205609,
      "grad_norm": 0.9269084835732863,
      "learning_rate": 1.6478000520276554e-05,
      "loss": 0.5286,
      "step": 3304
    },
    {
      "epoch": 0.29755339980643275,
      "grad_norm": 0.7168766335347843,
      "learning_rate": 1.6475778674295596e-05,
      "loss": 0.5402,
      "step": 3305
    },
    {
      "epoch": 0.29764343109230457,
      "grad_norm": 0.7140896790536285,
      "learning_rate": 1.6473556277610394e-05,
      "loss": 0.5993,
      "step": 3306
    },
    {
      "epoch": 0.29773346237817644,
      "grad_norm": 0.642962150259489,
      "learning_rate": 1.647133333040995e-05,
      "loss": 0.5215,
      "step": 3307
    },
    {
      "epoch": 0.29782349366404826,
      "grad_norm": 0.920421572397526,
      "learning_rate": 1.6469109832883302e-05,
      "loss": 0.6092,
      "step": 3308
    },
    {
      "epoch": 0.29791352494992007,
      "grad_norm": 0.7705525393318259,
      "learning_rate": 1.6466885785219534e-05,
      "loss": 0.5342,
      "step": 3309
    },
    {
      "epoch": 0.29800355623579194,
      "grad_norm": 0.7631012144979019,
      "learning_rate": 1.6464661187607784e-05,
      "loss": 0.5746,
      "step": 3310
    },
    {
      "epoch": 0.29809358752166376,
      "grad_norm": 0.8536781011827699,
      "learning_rate": 1.646243604023723e-05,
      "loss": 0.6162,
      "step": 3311
    },
    {
      "epoch": 0.29818361880753563,
      "grad_norm": 0.6655459183731179,
      "learning_rate": 1.646021034329711e-05,
      "loss": 0.583,
      "step": 3312
    },
    {
      "epoch": 0.29827365009340745,
      "grad_norm": 0.7517834338586142,
      "learning_rate": 1.6457984096976684e-05,
      "loss": 0.5086,
      "step": 3313
    },
    {
      "epoch": 0.2983636813792793,
      "grad_norm": 0.7328208197635355,
      "learning_rate": 1.6455757301465282e-05,
      "loss": 0.5726,
      "step": 3314
    },
    {
      "epoch": 0.29845371266515114,
      "grad_norm": 0.6658693855870637,
      "learning_rate": 1.645352995695227e-05,
      "loss": 0.6145,
      "step": 3315
    },
    {
      "epoch": 0.29854374395102296,
      "grad_norm": 0.7139352861462488,
      "learning_rate": 1.6451302063627067e-05,
      "loss": 0.6109,
      "step": 3316
    },
    {
      "epoch": 0.2986337752368948,
      "grad_norm": 0.6969132656726074,
      "learning_rate": 1.6449073621679128e-05,
      "loss": 0.5336,
      "step": 3317
    },
    {
      "epoch": 0.29872380652276664,
      "grad_norm": 0.9549531422460558,
      "learning_rate": 1.6446844631297962e-05,
      "loss": 0.5493,
      "step": 3318
    },
    {
      "epoch": 0.2988138378086385,
      "grad_norm": 0.7278404686388328,
      "learning_rate": 1.644461509267313e-05,
      "loss": 0.5234,
      "step": 3319
    },
    {
      "epoch": 0.29890386909451033,
      "grad_norm": 0.7185419725733653,
      "learning_rate": 1.6442385005994224e-05,
      "loss": 0.5468,
      "step": 3320
    },
    {
      "epoch": 0.2989939003803822,
      "grad_norm": 0.6609063364343204,
      "learning_rate": 1.64401543714509e-05,
      "loss": 0.5913,
      "step": 3321
    },
    {
      "epoch": 0.299083931666254,
      "grad_norm": 0.6781187351319112,
      "learning_rate": 1.6437923189232846e-05,
      "loss": 0.4874,
      "step": 3322
    },
    {
      "epoch": 0.29917396295212584,
      "grad_norm": 0.7571238044134877,
      "learning_rate": 1.643569145952981e-05,
      "loss": 0.5882,
      "step": 3323
    },
    {
      "epoch": 0.2992639942379977,
      "grad_norm": 0.7376039169874395,
      "learning_rate": 1.6433459182531574e-05,
      "loss": 0.523,
      "step": 3324
    },
    {
      "epoch": 0.2993540255238695,
      "grad_norm": 0.7331944345424652,
      "learning_rate": 1.6431226358427976e-05,
      "loss": 0.6649,
      "step": 3325
    },
    {
      "epoch": 0.2994440568097414,
      "grad_norm": 0.7807723415515345,
      "learning_rate": 1.642899298740889e-05,
      "loss": 0.5819,
      "step": 3326
    },
    {
      "epoch": 0.2995340880956132,
      "grad_norm": 0.7130116750733212,
      "learning_rate": 1.6426759069664253e-05,
      "loss": 0.5644,
      "step": 3327
    },
    {
      "epoch": 0.2996241193814851,
      "grad_norm": 0.777501531022417,
      "learning_rate": 1.6424524605384033e-05,
      "loss": 0.5832,
      "step": 3328
    },
    {
      "epoch": 0.2997141506673569,
      "grad_norm": 1.0047769444183763,
      "learning_rate": 1.6422289594758255e-05,
      "loss": 0.6385,
      "step": 3329
    },
    {
      "epoch": 0.2998041819532287,
      "grad_norm": 0.6136966921324646,
      "learning_rate": 1.642005403797698e-05,
      "loss": 0.5436,
      "step": 3330
    },
    {
      "epoch": 0.2998942132391006,
      "grad_norm": 0.6629308425821848,
      "learning_rate": 1.6417817935230318e-05,
      "loss": 0.567,
      "step": 3331
    },
    {
      "epoch": 0.2999842445249724,
      "grad_norm": 0.7179166346978682,
      "learning_rate": 1.641558128670844e-05,
      "loss": 0.493,
      "step": 3332
    },
    {
      "epoch": 0.3000742758108443,
      "grad_norm": 0.6636470603559251,
      "learning_rate": 1.641334409260154e-05,
      "loss": 0.6107,
      "step": 3333
    },
    {
      "epoch": 0.3001643070967161,
      "grad_norm": 0.855344214772015,
      "learning_rate": 1.641110635309988e-05,
      "loss": 0.5426,
      "step": 3334
    },
    {
      "epoch": 0.30025433838258797,
      "grad_norm": 0.834164533603278,
      "learning_rate": 1.6408868068393754e-05,
      "loss": 0.6394,
      "step": 3335
    },
    {
      "epoch": 0.3003443696684598,
      "grad_norm": 0.7871006558034582,
      "learning_rate": 1.6406629238673507e-05,
      "loss": 0.5939,
      "step": 3336
    },
    {
      "epoch": 0.3004344009543316,
      "grad_norm": 0.6247349881915685,
      "learning_rate": 1.6404389864129533e-05,
      "loss": 0.5818,
      "step": 3337
    },
    {
      "epoch": 0.3005244322402035,
      "grad_norm": 1.1390445432210619,
      "learning_rate": 1.6402149944952267e-05,
      "loss": 0.5093,
      "step": 3338
    },
    {
      "epoch": 0.3006144635260753,
      "grad_norm": 0.6664956502023188,
      "learning_rate": 1.639990948133219e-05,
      "loss": 0.5525,
      "step": 3339
    },
    {
      "epoch": 0.30070449481194717,
      "grad_norm": 0.7561310750729695,
      "learning_rate": 1.6397668473459843e-05,
      "loss": 0.6414,
      "step": 3340
    },
    {
      "epoch": 0.300794526097819,
      "grad_norm": 0.6764143332152254,
      "learning_rate": 1.6395426921525795e-05,
      "loss": 0.5135,
      "step": 3341
    },
    {
      "epoch": 0.30088455738369085,
      "grad_norm": 0.6226372366674983,
      "learning_rate": 1.639318482572067e-05,
      "loss": 0.4734,
      "step": 3342
    },
    {
      "epoch": 0.30097458866956267,
      "grad_norm": 0.6840497418094978,
      "learning_rate": 1.6390942186235143e-05,
      "loss": 0.5115,
      "step": 3343
    },
    {
      "epoch": 0.3010646199554345,
      "grad_norm": 0.8358772669534501,
      "learning_rate": 1.6388699003259916e-05,
      "loss": 0.5922,
      "step": 3344
    },
    {
      "epoch": 0.30115465124130636,
      "grad_norm": 0.9056955256865684,
      "learning_rate": 1.638645527698576e-05,
      "loss": 0.6215,
      "step": 3345
    },
    {
      "epoch": 0.3012446825271782,
      "grad_norm": 0.5715066196372243,
      "learning_rate": 1.6384211007603484e-05,
      "loss": 0.5279,
      "step": 3346
    },
    {
      "epoch": 0.30133471381305005,
      "grad_norm": 0.6758749795774346,
      "learning_rate": 1.638196619530394e-05,
      "loss": 0.5816,
      "step": 3347
    },
    {
      "epoch": 0.30142474509892186,
      "grad_norm": 0.7796644517197657,
      "learning_rate": 1.6379720840278025e-05,
      "loss": 0.6118,
      "step": 3348
    },
    {
      "epoch": 0.30151477638479374,
      "grad_norm": 0.7516444942322941,
      "learning_rate": 1.6377474942716692e-05,
      "loss": 0.5428,
      "step": 3349
    },
    {
      "epoch": 0.30160480767066555,
      "grad_norm": 0.7835502393107462,
      "learning_rate": 1.637522850281093e-05,
      "loss": 0.4847,
      "step": 3350
    },
    {
      "epoch": 0.30169483895653737,
      "grad_norm": 0.7685276937901939,
      "learning_rate": 1.6372981520751775e-05,
      "loss": 0.6081,
      "step": 3351
    },
    {
      "epoch": 0.30178487024240924,
      "grad_norm": 0.7494888780622728,
      "learning_rate": 1.6370733996730313e-05,
      "loss": 0.6312,
      "step": 3352
    },
    {
      "epoch": 0.30187490152828106,
      "grad_norm": 0.7329864508603849,
      "learning_rate": 1.636848593093768e-05,
      "loss": 0.5948,
      "step": 3353
    },
    {
      "epoch": 0.30196493281415293,
      "grad_norm": 0.8483845317005432,
      "learning_rate": 1.6366237323565045e-05,
      "loss": 0.599,
      "step": 3354
    },
    {
      "epoch": 0.30205496410002475,
      "grad_norm": 0.8142556267431655,
      "learning_rate": 1.6363988174803638e-05,
      "loss": 0.5231,
      "step": 3355
    },
    {
      "epoch": 0.3021449953858966,
      "grad_norm": 1.0132646895321349,
      "learning_rate": 1.6361738484844722e-05,
      "loss": 0.5757,
      "step": 3356
    },
    {
      "epoch": 0.30223502667176844,
      "grad_norm": 0.6461668044077316,
      "learning_rate": 1.6359488253879615e-05,
      "loss": 0.5362,
      "step": 3357
    },
    {
      "epoch": 0.30232505795764025,
      "grad_norm": 0.8334074411370038,
      "learning_rate": 1.6357237482099682e-05,
      "loss": 0.6038,
      "step": 3358
    },
    {
      "epoch": 0.3024150892435121,
      "grad_norm": 0.7876843437560962,
      "learning_rate": 1.6354986169696326e-05,
      "loss": 0.5442,
      "step": 3359
    },
    {
      "epoch": 0.30250512052938394,
      "grad_norm": 0.6586379027777789,
      "learning_rate": 1.6352734316860995e-05,
      "loss": 0.515,
      "step": 3360
    },
    {
      "epoch": 0.3025951518152558,
      "grad_norm": 0.5716151121690907,
      "learning_rate": 1.6350481923785198e-05,
      "loss": 0.4513,
      "step": 3361
    },
    {
      "epoch": 0.30268518310112763,
      "grad_norm": 0.7268247513317382,
      "learning_rate": 1.6348228990660472e-05,
      "loss": 0.5391,
      "step": 3362
    },
    {
      "epoch": 0.3027752143869995,
      "grad_norm": 0.8477997578196153,
      "learning_rate": 1.634597551767841e-05,
      "loss": 0.5389,
      "step": 3363
    },
    {
      "epoch": 0.3028652456728713,
      "grad_norm": 0.7504719677574432,
      "learning_rate": 1.6343721505030654e-05,
      "loss": 0.5752,
      "step": 3364
    },
    {
      "epoch": 0.30295527695874314,
      "grad_norm": 0.6128223814016854,
      "learning_rate": 1.6341466952908877e-05,
      "loss": 0.51,
      "step": 3365
    },
    {
      "epoch": 0.303045308244615,
      "grad_norm": 1.1667338184242981,
      "learning_rate": 1.6339211861504815e-05,
      "loss": 0.5832,
      "step": 3366
    },
    {
      "epoch": 0.3031353395304868,
      "grad_norm": 0.8715684253338721,
      "learning_rate": 1.6336956231010238e-05,
      "loss": 0.5593,
      "step": 3367
    },
    {
      "epoch": 0.3032253708163587,
      "grad_norm": 0.736272142909858,
      "learning_rate": 1.633470006161697e-05,
      "loss": 0.5538,
      "step": 3368
    },
    {
      "epoch": 0.3033154021022305,
      "grad_norm": 0.925697391740745,
      "learning_rate": 1.633244335351688e-05,
      "loss": 0.629,
      "step": 3369
    },
    {
      "epoch": 0.3034054333881024,
      "grad_norm": 1.0786132962860742,
      "learning_rate": 1.6330186106901867e-05,
      "loss": 0.5351,
      "step": 3370
    },
    {
      "epoch": 0.3034954646739742,
      "grad_norm": 0.6800559633334881,
      "learning_rate": 1.63279283219639e-05,
      "loss": 0.6296,
      "step": 3371
    },
    {
      "epoch": 0.303585495959846,
      "grad_norm": 0.6454723604184632,
      "learning_rate": 1.632566999889498e-05,
      "loss": 0.5509,
      "step": 3372
    },
    {
      "epoch": 0.3036755272457179,
      "grad_norm": 0.7435754066892386,
      "learning_rate": 1.6323411137887152e-05,
      "loss": 0.5548,
      "step": 3373
    },
    {
      "epoch": 0.3037655585315897,
      "grad_norm": 1.6209250978379275,
      "learning_rate": 1.632115173913252e-05,
      "loss": 0.669,
      "step": 3374
    },
    {
      "epoch": 0.3038555898174616,
      "grad_norm": 0.7121997248150134,
      "learning_rate": 1.6318891802823217e-05,
      "loss": 0.5404,
      "step": 3375
    },
    {
      "epoch": 0.3039456211033334,
      "grad_norm": 0.8101590978235189,
      "learning_rate": 1.631663132915143e-05,
      "loss": 0.4977,
      "step": 3376
    },
    {
      "epoch": 0.30403565238920527,
      "grad_norm": 1.6550714935592081,
      "learning_rate": 1.631437031830939e-05,
      "loss": 0.5784,
      "step": 3377
    },
    {
      "epoch": 0.3041256836750771,
      "grad_norm": 0.937885604930895,
      "learning_rate": 1.6312108770489383e-05,
      "loss": 0.5656,
      "step": 3378
    },
    {
      "epoch": 0.30421571496094896,
      "grad_norm": 0.8227164785898732,
      "learning_rate": 1.6309846685883726e-05,
      "loss": 0.5412,
      "step": 3379
    },
    {
      "epoch": 0.3043057462468208,
      "grad_norm": 0.8275116280178304,
      "learning_rate": 1.6307584064684785e-05,
      "loss": 0.5516,
      "step": 3380
    },
    {
      "epoch": 0.3043957775326926,
      "grad_norm": 0.8336533208620837,
      "learning_rate": 1.630532090708498e-05,
      "loss": 0.5589,
      "step": 3381
    },
    {
      "epoch": 0.30448580881856446,
      "grad_norm": 0.8751022905923145,
      "learning_rate": 1.6303057213276773e-05,
      "loss": 0.6354,
      "step": 3382
    },
    {
      "epoch": 0.3045758401044363,
      "grad_norm": 0.8431318324175373,
      "learning_rate": 1.630079298345266e-05,
      "loss": 0.5849,
      "step": 3383
    },
    {
      "epoch": 0.30466587139030815,
      "grad_norm": 0.6586397321711626,
      "learning_rate": 1.62985282178052e-05,
      "loss": 0.567,
      "step": 3384
    },
    {
      "epoch": 0.30475590267617997,
      "grad_norm": 0.9270953045095541,
      "learning_rate": 1.6296262916526998e-05,
      "loss": 0.6026,
      "step": 3385
    },
    {
      "epoch": 0.30484593396205184,
      "grad_norm": 0.8117844128207553,
      "learning_rate": 1.6293997079810685e-05,
      "loss": 0.5776,
      "step": 3386
    },
    {
      "epoch": 0.30493596524792366,
      "grad_norm": 0.8132048681904248,
      "learning_rate": 1.6291730707848948e-05,
      "loss": 0.5517,
      "step": 3387
    },
    {
      "epoch": 0.3050259965337955,
      "grad_norm": 0.7214718301136143,
      "learning_rate": 1.6289463800834523e-05,
      "loss": 0.4785,
      "step": 3388
    },
    {
      "epoch": 0.30511602781966735,
      "grad_norm": 0.6795006408577448,
      "learning_rate": 1.6287196358960198e-05,
      "loss": 0.5768,
      "step": 3389
    },
    {
      "epoch": 0.30520605910553916,
      "grad_norm": 0.6548551364529164,
      "learning_rate": 1.6284928382418784e-05,
      "loss": 0.5471,
      "step": 3390
    },
    {
      "epoch": 0.30529609039141103,
      "grad_norm": 0.6974144658445937,
      "learning_rate": 1.6282659871403163e-05,
      "loss": 0.6342,
      "step": 3391
    },
    {
      "epoch": 0.30538612167728285,
      "grad_norm": 0.7865703791671814,
      "learning_rate": 1.6280390826106243e-05,
      "loss": 0.6455,
      "step": 3392
    },
    {
      "epoch": 0.3054761529631547,
      "grad_norm": 0.962258578705213,
      "learning_rate": 1.627812124672099e-05,
      "loss": 0.5953,
      "step": 3393
    },
    {
      "epoch": 0.30556618424902654,
      "grad_norm": 0.7062133604008173,
      "learning_rate": 1.6275851133440402e-05,
      "loss": 0.4778,
      "step": 3394
    },
    {
      "epoch": 0.30565621553489836,
      "grad_norm": 0.5977071752635885,
      "learning_rate": 1.6273580486457542e-05,
      "loss": 0.599,
      "step": 3395
    },
    {
      "epoch": 0.30574624682077023,
      "grad_norm": 0.6517382514365309,
      "learning_rate": 1.6271309305965498e-05,
      "loss": 0.5628,
      "step": 3396
    },
    {
      "epoch": 0.30583627810664205,
      "grad_norm": 0.910233204744988,
      "learning_rate": 1.626903759215742e-05,
      "loss": 0.6164,
      "step": 3397
    },
    {
      "epoch": 0.3059263093925139,
      "grad_norm": 0.5787652689704126,
      "learning_rate": 1.626676534522649e-05,
      "loss": 0.5587,
      "step": 3398
    },
    {
      "epoch": 0.30601634067838573,
      "grad_norm": 1.0744523900324934,
      "learning_rate": 1.6264492565365945e-05,
      "loss": 0.6785,
      "step": 3399
    },
    {
      "epoch": 0.3061063719642576,
      "grad_norm": 1.0630324772129776,
      "learning_rate": 1.6262219252769065e-05,
      "loss": 0.5883,
      "step": 3400
    },
    {
      "epoch": 0.3061964032501294,
      "grad_norm": 0.6621588636156462,
      "learning_rate": 1.6259945407629168e-05,
      "loss": 0.4892,
      "step": 3401
    },
    {
      "epoch": 0.30628643453600124,
      "grad_norm": 0.6737370451932307,
      "learning_rate": 1.6257671030139627e-05,
      "loss": 0.5818,
      "step": 3402
    },
    {
      "epoch": 0.3063764658218731,
      "grad_norm": 0.7709467296998184,
      "learning_rate": 1.6255396120493855e-05,
      "loss": 0.5989,
      "step": 3403
    },
    {
      "epoch": 0.30646649710774493,
      "grad_norm": 0.8541782310810695,
      "learning_rate": 1.6253120678885313e-05,
      "loss": 0.5989,
      "step": 3404
    },
    {
      "epoch": 0.3065565283936168,
      "grad_norm": 1.0454220064826534,
      "learning_rate": 1.6250844705507505e-05,
      "loss": 0.5674,
      "step": 3405
    },
    {
      "epoch": 0.3066465596794886,
      "grad_norm": 0.8333859624670307,
      "learning_rate": 1.624856820055398e-05,
      "loss": 0.5169,
      "step": 3406
    },
    {
      "epoch": 0.3067365909653605,
      "grad_norm": 0.7253326204401416,
      "learning_rate": 1.6246291164218343e-05,
      "loss": 0.5892,
      "step": 3407
    },
    {
      "epoch": 0.3068266222512323,
      "grad_norm": 0.6983162323519466,
      "learning_rate": 1.624401359669422e-05,
      "loss": 0.5957,
      "step": 3408
    },
    {
      "epoch": 0.3069166535371041,
      "grad_norm": 0.7647811756756195,
      "learning_rate": 1.6241735498175306e-05,
      "loss": 0.5625,
      "step": 3409
    },
    {
      "epoch": 0.307006684822976,
      "grad_norm": 0.6518485136545086,
      "learning_rate": 1.623945686885533e-05,
      "loss": 0.4878,
      "step": 3410
    },
    {
      "epoch": 0.3070967161088478,
      "grad_norm": 0.6078082565675802,
      "learning_rate": 1.6237177708928065e-05,
      "loss": 0.4968,
      "step": 3411
    },
    {
      "epoch": 0.3071867473947197,
      "grad_norm": 0.9573461364591653,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 0.5977,
      "step": 3412
    },
    {
      "epoch": 0.3072767786805915,
      "grad_norm": 0.7038128501757724,
      "learning_rate": 1.623261779802701e-05,
      "loss": 0.5434,
      "step": 3413
    },
    {
      "epoch": 0.3073668099664634,
      "grad_norm": 0.6715129482532803,
      "learning_rate": 1.6230337047440995e-05,
      "loss": 0.5593,
      "step": 3414
    },
    {
      "epoch": 0.3074568412523352,
      "grad_norm": 0.7008178695879309,
      "learning_rate": 1.6228055767023247e-05,
      "loss": 0.543,
      "step": 3415
    },
    {
      "epoch": 0.307546872538207,
      "grad_norm": 0.7873838862394633,
      "learning_rate": 1.622577395696777e-05,
      "loss": 0.5713,
      "step": 3416
    },
    {
      "epoch": 0.3076369038240789,
      "grad_norm": 0.794424917068765,
      "learning_rate": 1.622349161746861e-05,
      "loss": 0.6046,
      "step": 3417
    },
    {
      "epoch": 0.3077269351099507,
      "grad_norm": 0.6603563261843997,
      "learning_rate": 1.622120874871986e-05,
      "loss": 0.5648,
      "step": 3418
    },
    {
      "epoch": 0.30781696639582257,
      "grad_norm": 0.6969342165637882,
      "learning_rate": 1.6218925350915657e-05,
      "loss": 0.5686,
      "step": 3419
    },
    {
      "epoch": 0.3079069976816944,
      "grad_norm": 0.6409193609917008,
      "learning_rate": 1.6216641424250176e-05,
      "loss": 0.5617,
      "step": 3420
    },
    {
      "epoch": 0.30799702896756626,
      "grad_norm": 0.6124333668769529,
      "learning_rate": 1.621435696891765e-05,
      "loss": 0.5698,
      "step": 3421
    },
    {
      "epoch": 0.30808706025343807,
      "grad_norm": 0.6232051747529528,
      "learning_rate": 1.621207198511235e-05,
      "loss": 0.5062,
      "step": 3422
    },
    {
      "epoch": 0.3081770915393099,
      "grad_norm": 0.7134701672506958,
      "learning_rate": 1.6209786473028585e-05,
      "loss": 0.6381,
      "step": 3423
    },
    {
      "epoch": 0.30826712282518176,
      "grad_norm": 0.6708147136426535,
      "learning_rate": 1.6207500432860728e-05,
      "loss": 0.5214,
      "step": 3424
    },
    {
      "epoch": 0.3083571541110536,
      "grad_norm": 0.6076708586746664,
      "learning_rate": 1.620521386480318e-05,
      "loss": 0.5424,
      "step": 3425
    },
    {
      "epoch": 0.30844718539692545,
      "grad_norm": 0.6082373751819903,
      "learning_rate": 1.620292676905039e-05,
      "loss": 0.4634,
      "step": 3426
    },
    {
      "epoch": 0.30853721668279727,
      "grad_norm": 0.7464264338926418,
      "learning_rate": 1.6200639145796856e-05,
      "loss": 0.5054,
      "step": 3427
    },
    {
      "epoch": 0.30862724796866914,
      "grad_norm": 0.7256339974678789,
      "learning_rate": 1.619835099523712e-05,
      "loss": 0.5744,
      "step": 3428
    },
    {
      "epoch": 0.30871727925454095,
      "grad_norm": 0.6503385527542253,
      "learning_rate": 1.6196062317565762e-05,
      "loss": 0.5004,
      "step": 3429
    },
    {
      "epoch": 0.30880731054041277,
      "grad_norm": 0.6177897010122646,
      "learning_rate": 1.6193773112977424e-05,
      "loss": 0.5705,
      "step": 3430
    },
    {
      "epoch": 0.30889734182628464,
      "grad_norm": 0.8067612265738923,
      "learning_rate": 1.619148338166677e-05,
      "loss": 0.6229,
      "step": 3431
    },
    {
      "epoch": 0.30898737311215646,
      "grad_norm": 0.7938039048963454,
      "learning_rate": 1.6189193123828527e-05,
      "loss": 0.5309,
      "step": 3432
    },
    {
      "epoch": 0.30907740439802833,
      "grad_norm": 0.6892254019969906,
      "learning_rate": 1.6186902339657455e-05,
      "loss": 0.5554,
      "step": 3433
    },
    {
      "epoch": 0.30916743568390015,
      "grad_norm": 0.8021365439435308,
      "learning_rate": 1.6184611029348368e-05,
      "loss": 0.5978,
      "step": 3434
    },
    {
      "epoch": 0.309257466969772,
      "grad_norm": 0.9162392390276638,
      "learning_rate": 1.618231919309612e-05,
      "loss": 0.6606,
      "step": 3435
    },
    {
      "epoch": 0.30934749825564384,
      "grad_norm": 0.7582243628055669,
      "learning_rate": 1.6180026831095608e-05,
      "loss": 0.5925,
      "step": 3436
    },
    {
      "epoch": 0.30943752954151565,
      "grad_norm": 0.7088375370160558,
      "learning_rate": 1.6177733943541777e-05,
      "loss": 0.5508,
      "step": 3437
    },
    {
      "epoch": 0.3095275608273875,
      "grad_norm": 0.9393523570685335,
      "learning_rate": 1.6175440530629614e-05,
      "loss": 0.5354,
      "step": 3438
    },
    {
      "epoch": 0.30961759211325934,
      "grad_norm": 0.7232041103089376,
      "learning_rate": 1.6173146592554155e-05,
      "loss": 0.4823,
      "step": 3439
    },
    {
      "epoch": 0.3097076233991312,
      "grad_norm": 0.754702685059496,
      "learning_rate": 1.6170852129510474e-05,
      "loss": 0.6118,
      "step": 3440
    },
    {
      "epoch": 0.30979765468500303,
      "grad_norm": 0.7160439652492557,
      "learning_rate": 1.61685571416937e-05,
      "loss": 0.4946,
      "step": 3441
    },
    {
      "epoch": 0.3098876859708749,
      "grad_norm": 0.5945813510139404,
      "learning_rate": 1.6166261629298996e-05,
      "loss": 0.5316,
      "step": 3442
    },
    {
      "epoch": 0.3099777172567467,
      "grad_norm": 0.6102623808258594,
      "learning_rate": 1.616396559252157e-05,
      "loss": 0.4859,
      "step": 3443
    },
    {
      "epoch": 0.31006774854261854,
      "grad_norm": 0.6758471695812516,
      "learning_rate": 1.6161669031556687e-05,
      "loss": 0.5592,
      "step": 3444
    },
    {
      "epoch": 0.3101577798284904,
      "grad_norm": 0.7847113286792532,
      "learning_rate": 1.615937194659964e-05,
      "loss": 0.5963,
      "step": 3445
    },
    {
      "epoch": 0.3102478111143622,
      "grad_norm": 0.614387943864444,
      "learning_rate": 1.6157074337845783e-05,
      "loss": 0.583,
      "step": 3446
    },
    {
      "epoch": 0.3103378424002341,
      "grad_norm": 0.5940275218036221,
      "learning_rate": 1.6154776205490495e-05,
      "loss": 0.5138,
      "step": 3447
    },
    {
      "epoch": 0.3104278736861059,
      "grad_norm": 0.8364383968015263,
      "learning_rate": 1.6152477549729223e-05,
      "loss": 0.5433,
      "step": 3448
    },
    {
      "epoch": 0.3105179049719778,
      "grad_norm": 0.7392990641213267,
      "learning_rate": 1.6150178370757434e-05,
      "loss": 0.5445,
      "step": 3449
    },
    {
      "epoch": 0.3106079362578496,
      "grad_norm": 0.8605558200735641,
      "learning_rate": 1.614787866877066e-05,
      "loss": 0.5575,
      "step": 3450
    },
    {
      "epoch": 0.3106979675437214,
      "grad_norm": 0.6704692396158103,
      "learning_rate": 1.6145578443964465e-05,
      "loss": 0.4843,
      "step": 3451
    },
    {
      "epoch": 0.3107879988295933,
      "grad_norm": 0.7447465010618576,
      "learning_rate": 1.6143277696534466e-05,
      "loss": 0.4689,
      "step": 3452
    },
    {
      "epoch": 0.3108780301154651,
      "grad_norm": 1.060641211626924,
      "learning_rate": 1.6140976426676318e-05,
      "loss": 0.5908,
      "step": 3453
    },
    {
      "epoch": 0.310968061401337,
      "grad_norm": 0.6519691476766669,
      "learning_rate": 1.6138674634585718e-05,
      "loss": 0.523,
      "step": 3454
    },
    {
      "epoch": 0.3110580926872088,
      "grad_norm": 0.8304940843232221,
      "learning_rate": 1.6136372320458416e-05,
      "loss": 0.6041,
      "step": 3455
    },
    {
      "epoch": 0.31114812397308067,
      "grad_norm": 0.6840596872160917,
      "learning_rate": 1.6134069484490202e-05,
      "loss": 0.4893,
      "step": 3456
    },
    {
      "epoch": 0.3112381552589525,
      "grad_norm": 0.6295037574222437,
      "learning_rate": 1.613176612687691e-05,
      "loss": 0.5982,
      "step": 3457
    },
    {
      "epoch": 0.3113281865448243,
      "grad_norm": 0.7453594615777339,
      "learning_rate": 1.612946224781442e-05,
      "loss": 0.5832,
      "step": 3458
    },
    {
      "epoch": 0.3114182178306962,
      "grad_norm": 0.7049060525696696,
      "learning_rate": 1.6127157847498652e-05,
      "loss": 0.5261,
      "step": 3459
    },
    {
      "epoch": 0.311508249116568,
      "grad_norm": 0.7457517341554628,
      "learning_rate": 1.6124852926125578e-05,
      "loss": 0.5681,
      "step": 3460
    },
    {
      "epoch": 0.31159828040243986,
      "grad_norm": 0.6447607366306048,
      "learning_rate": 1.612254748389121e-05,
      "loss": 0.6256,
      "step": 3461
    },
    {
      "epoch": 0.3116883116883117,
      "grad_norm": 0.7824999958698373,
      "learning_rate": 1.61202415209916e-05,
      "loss": 0.5933,
      "step": 3462
    },
    {
      "epoch": 0.31177834297418355,
      "grad_norm": 0.6433895702195601,
      "learning_rate": 1.6117935037622848e-05,
      "loss": 0.536,
      "step": 3463
    },
    {
      "epoch": 0.31186837426005537,
      "grad_norm": 0.7843236331913306,
      "learning_rate": 1.6115628033981107e-05,
      "loss": 0.6236,
      "step": 3464
    },
    {
      "epoch": 0.3119584055459272,
      "grad_norm": 0.7638873326371098,
      "learning_rate": 1.611332051026256e-05,
      "loss": 0.5519,
      "step": 3465
    },
    {
      "epoch": 0.31204843683179906,
      "grad_norm": 0.7242783297834156,
      "learning_rate": 1.6111012466663436e-05,
      "loss": 0.5525,
      "step": 3466
    },
    {
      "epoch": 0.3121384681176709,
      "grad_norm": 0.7719798703952914,
      "learning_rate": 1.6108703903380017e-05,
      "loss": 0.6044,
      "step": 3467
    },
    {
      "epoch": 0.31222849940354275,
      "grad_norm": 0.773337563426212,
      "learning_rate": 1.610639482060863e-05,
      "loss": 0.592,
      "step": 3468
    },
    {
      "epoch": 0.31231853068941456,
      "grad_norm": 0.6953564433496221,
      "learning_rate": 1.6104085218545633e-05,
      "loss": 0.5106,
      "step": 3469
    },
    {
      "epoch": 0.31240856197528644,
      "grad_norm": 0.7373049300731014,
      "learning_rate": 1.610177509738744e-05,
      "loss": 0.6234,
      "step": 3470
    },
    {
      "epoch": 0.31249859326115825,
      "grad_norm": 0.6224597706851506,
      "learning_rate": 1.6099464457330502e-05,
      "loss": 0.4956,
      "step": 3471
    },
    {
      "epoch": 0.31258862454703007,
      "grad_norm": 0.663397884134729,
      "learning_rate": 1.6097153298571318e-05,
      "loss": 0.5113,
      "step": 3472
    },
    {
      "epoch": 0.31267865583290194,
      "grad_norm": 0.7090202532251045,
      "learning_rate": 1.6094841621306435e-05,
      "loss": 0.6185,
      "step": 3473
    },
    {
      "epoch": 0.31276868711877376,
      "grad_norm": 0.5732989294939577,
      "learning_rate": 1.609252942573244e-05,
      "loss": 0.5424,
      "step": 3474
    },
    {
      "epoch": 0.31285871840464563,
      "grad_norm": 0.6074330930571369,
      "learning_rate": 1.609021671204595e-05,
      "loss": 0.642,
      "step": 3475
    },
    {
      "epoch": 0.31294874969051745,
      "grad_norm": 0.8322425007388531,
      "learning_rate": 1.6087903480443654e-05,
      "loss": 0.6404,
      "step": 3476
    },
    {
      "epoch": 0.3130387809763893,
      "grad_norm": 0.6019472102922397,
      "learning_rate": 1.6085589731122266e-05,
      "loss": 0.518,
      "step": 3477
    },
    {
      "epoch": 0.31312881226226114,
      "grad_norm": 0.7406724532684646,
      "learning_rate": 1.6083275464278546e-05,
      "loss": 0.6115,
      "step": 3478
    },
    {
      "epoch": 0.31321884354813295,
      "grad_norm": 0.7933682025007629,
      "learning_rate": 1.6080960680109304e-05,
      "loss": 0.5656,
      "step": 3479
    },
    {
      "epoch": 0.3133088748340048,
      "grad_norm": 0.5987931950392674,
      "learning_rate": 1.607864537881139e-05,
      "loss": 0.5792,
      "step": 3480
    },
    {
      "epoch": 0.31339890611987664,
      "grad_norm": 0.5769128512510254,
      "learning_rate": 1.6076329560581702e-05,
      "loss": 0.4964,
      "step": 3481
    },
    {
      "epoch": 0.3134889374057485,
      "grad_norm": 0.6620154755040832,
      "learning_rate": 1.607401322561717e-05,
      "loss": 0.5491,
      "step": 3482
    },
    {
      "epoch": 0.31357896869162033,
      "grad_norm": 1.0748454042264641,
      "learning_rate": 1.6071696374114784e-05,
      "loss": 0.6859,
      "step": 3483
    },
    {
      "epoch": 0.3136689999774922,
      "grad_norm": 0.7539179571260667,
      "learning_rate": 1.606937900627157e-05,
      "loss": 0.5544,
      "step": 3484
    },
    {
      "epoch": 0.313759031263364,
      "grad_norm": 0.8583570769575165,
      "learning_rate": 1.6067061122284596e-05,
      "loss": 0.4897,
      "step": 3485
    },
    {
      "epoch": 0.31384906254923584,
      "grad_norm": 0.8017017660895158,
      "learning_rate": 1.6064742722350972e-05,
      "loss": 0.4991,
      "step": 3486
    },
    {
      "epoch": 0.3139390938351077,
      "grad_norm": 0.6080545134448005,
      "learning_rate": 1.6062423806667867e-05,
      "loss": 0.5973,
      "step": 3487
    },
    {
      "epoch": 0.3140291251209795,
      "grad_norm": 0.6087149069388252,
      "learning_rate": 1.6060104375432476e-05,
      "loss": 0.5187,
      "step": 3488
    },
    {
      "epoch": 0.3141191564068514,
      "grad_norm": 0.8228204054994483,
      "learning_rate": 1.605778442884204e-05,
      "loss": 0.5122,
      "step": 3489
    },
    {
      "epoch": 0.3142091876927232,
      "grad_norm": 0.7382365723355331,
      "learning_rate": 1.605546396709386e-05,
      "loss": 0.588,
      "step": 3490
    },
    {
      "epoch": 0.3142992189785951,
      "grad_norm": 0.6835771388883921,
      "learning_rate": 1.6053142990385263e-05,
      "loss": 0.6323,
      "step": 3491
    },
    {
      "epoch": 0.3143892502644669,
      "grad_norm": 0.6685177103999596,
      "learning_rate": 1.605082149891363e-05,
      "loss": 0.4847,
      "step": 3492
    },
    {
      "epoch": 0.3144792815503387,
      "grad_norm": 0.5817904191930003,
      "learning_rate": 1.6048499492876378e-05,
      "loss": 0.4692,
      "step": 3493
    },
    {
      "epoch": 0.3145693128362106,
      "grad_norm": 0.6565367484838006,
      "learning_rate": 1.604617697247097e-05,
      "loss": 0.5837,
      "step": 3494
    },
    {
      "epoch": 0.3146593441220824,
      "grad_norm": 0.6264650267682882,
      "learning_rate": 1.6043853937894922e-05,
      "loss": 0.5316,
      "step": 3495
    },
    {
      "epoch": 0.3147493754079543,
      "grad_norm": 0.6807201647968818,
      "learning_rate": 1.604153038934578e-05,
      "loss": 0.6538,
      "step": 3496
    },
    {
      "epoch": 0.3148394066938261,
      "grad_norm": 1.6910647871360893,
      "learning_rate": 1.6039206327021146e-05,
      "loss": 0.6157,
      "step": 3497
    },
    {
      "epoch": 0.31492943797969797,
      "grad_norm": 1.0608725747928045,
      "learning_rate": 1.603688175111865e-05,
      "loss": 0.5635,
      "step": 3498
    },
    {
      "epoch": 0.3150194692655698,
      "grad_norm": 0.6807901992275835,
      "learning_rate": 1.6034556661835983e-05,
      "loss": 0.5763,
      "step": 3499
    },
    {
      "epoch": 0.3151095005514416,
      "grad_norm": 0.6245096024378309,
      "learning_rate": 1.6032231059370874e-05,
      "loss": 0.5161,
      "step": 3500
    },
    {
      "epoch": 0.3151995318373135,
      "grad_norm": 0.8377086261575195,
      "learning_rate": 1.602990494392109e-05,
      "loss": 0.5396,
      "step": 3501
    },
    {
      "epoch": 0.3152895631231853,
      "grad_norm": 0.8381721727729399,
      "learning_rate": 1.602757831568444e-05,
      "loss": 0.5932,
      "step": 3502
    },
    {
      "epoch": 0.31537959440905716,
      "grad_norm": 0.7718584140341863,
      "learning_rate": 1.602525117485879e-05,
      "loss": 0.6382,
      "step": 3503
    },
    {
      "epoch": 0.315469625694929,
      "grad_norm": 0.5885381182455315,
      "learning_rate": 1.602292352164204e-05,
      "loss": 0.6099,
      "step": 3504
    },
    {
      "epoch": 0.31555965698080085,
      "grad_norm": 0.6963451992855338,
      "learning_rate": 1.6020595356232137e-05,
      "loss": 0.6037,
      "step": 3505
    },
    {
      "epoch": 0.31564968826667267,
      "grad_norm": 0.7426929899667045,
      "learning_rate": 1.601826667882706e-05,
      "loss": 0.624,
      "step": 3506
    },
    {
      "epoch": 0.3157397195525445,
      "grad_norm": 0.6133593850983978,
      "learning_rate": 1.601593748962485e-05,
      "loss": 0.6063,
      "step": 3507
    },
    {
      "epoch": 0.31582975083841636,
      "grad_norm": 0.7265155430009561,
      "learning_rate": 1.601360778882358e-05,
      "loss": 0.5903,
      "step": 3508
    },
    {
      "epoch": 0.3159197821242882,
      "grad_norm": 0.8046192405909239,
      "learning_rate": 1.6011277576621373e-05,
      "loss": 0.5577,
      "step": 3509
    },
    {
      "epoch": 0.31600981341016005,
      "grad_norm": 0.6510078274561574,
      "learning_rate": 1.600894685321638e-05,
      "loss": 0.5294,
      "step": 3510
    },
    {
      "epoch": 0.31609984469603186,
      "grad_norm": 0.7512790139338961,
      "learning_rate": 1.6006615618806822e-05,
      "loss": 0.4793,
      "step": 3511
    },
    {
      "epoch": 0.31618987598190373,
      "grad_norm": 0.8042815680674756,
      "learning_rate": 1.6004283873590942e-05,
      "loss": 0.5641,
      "step": 3512
    },
    {
      "epoch": 0.31627990726777555,
      "grad_norm": 1.1410442839821924,
      "learning_rate": 1.600195161776703e-05,
      "loss": 0.6729,
      "step": 3513
    },
    {
      "epoch": 0.31636993855364737,
      "grad_norm": 0.7330459389071062,
      "learning_rate": 1.599961885153343e-05,
      "loss": 0.5886,
      "step": 3514
    },
    {
      "epoch": 0.31645996983951924,
      "grad_norm": 0.7659820002879766,
      "learning_rate": 1.5997285575088512e-05,
      "loss": 0.6022,
      "step": 3515
    },
    {
      "epoch": 0.31655000112539106,
      "grad_norm": 0.7625036229407911,
      "learning_rate": 1.5994951788630708e-05,
      "loss": 0.7172,
      "step": 3516
    },
    {
      "epoch": 0.31664003241126293,
      "grad_norm": 0.8646436258643002,
      "learning_rate": 1.5992617492358483e-05,
      "loss": 0.567,
      "step": 3517
    },
    {
      "epoch": 0.31673006369713474,
      "grad_norm": 0.9085216397263111,
      "learning_rate": 1.5990282686470343e-05,
      "loss": 0.5492,
      "step": 3518
    },
    {
      "epoch": 0.3168200949830066,
      "grad_norm": 0.7995873958869948,
      "learning_rate": 1.5987947371164842e-05,
      "loss": 0.6859,
      "step": 3519
    },
    {
      "epoch": 0.31691012626887843,
      "grad_norm": 0.6564859040326766,
      "learning_rate": 1.598561154664058e-05,
      "loss": 0.6252,
      "step": 3520
    },
    {
      "epoch": 0.31700015755475025,
      "grad_norm": 0.6305753869898949,
      "learning_rate": 1.5983275213096198e-05,
      "loss": 0.5209,
      "step": 3521
    },
    {
      "epoch": 0.3170901888406221,
      "grad_norm": 0.9272885463982891,
      "learning_rate": 1.5980938370730372e-05,
      "loss": 0.5253,
      "step": 3522
    },
    {
      "epoch": 0.31718022012649394,
      "grad_norm": 0.8614026325251162,
      "learning_rate": 1.5978601019741837e-05,
      "loss": 0.5364,
      "step": 3523
    },
    {
      "epoch": 0.3172702514123658,
      "grad_norm": 0.8253683823112071,
      "learning_rate": 1.5976263160329358e-05,
      "loss": 0.5976,
      "step": 3524
    },
    {
      "epoch": 0.31736028269823763,
      "grad_norm": 0.5605908234343548,
      "learning_rate": 1.5973924792691747e-05,
      "loss": 0.4748,
      "step": 3525
    },
    {
      "epoch": 0.3174503139841095,
      "grad_norm": 0.7197348956633132,
      "learning_rate": 1.5971585917027864e-05,
      "loss": 0.6142,
      "step": 3526
    },
    {
      "epoch": 0.3175403452699813,
      "grad_norm": 0.5825416902354322,
      "learning_rate": 1.5969246533536606e-05,
      "loss": 0.4895,
      "step": 3527
    },
    {
      "epoch": 0.31763037655585313,
      "grad_norm": 1.006668993050627,
      "learning_rate": 1.5966906642416917e-05,
      "loss": 0.5756,
      "step": 3528
    },
    {
      "epoch": 0.317720407841725,
      "grad_norm": 0.7148238040289077,
      "learning_rate": 1.596456624386778e-05,
      "loss": 0.5479,
      "step": 3529
    },
    {
      "epoch": 0.3178104391275968,
      "grad_norm": 0.6902837881409465,
      "learning_rate": 1.5962225338088227e-05,
      "loss": 0.5705,
      "step": 3530
    },
    {
      "epoch": 0.3179004704134687,
      "grad_norm": 0.8846134307668447,
      "learning_rate": 1.595988392527733e-05,
      "loss": 0.5692,
      "step": 3531
    },
    {
      "epoch": 0.3179905016993405,
      "grad_norm": 0.9091313643029357,
      "learning_rate": 1.59575420056342e-05,
      "loss": 0.5912,
      "step": 3532
    },
    {
      "epoch": 0.3180805329852124,
      "grad_norm": 0.8079486942723116,
      "learning_rate": 1.5955199579358007e-05,
      "loss": 0.7088,
      "step": 3533
    },
    {
      "epoch": 0.3181705642710842,
      "grad_norm": 0.8186668040496902,
      "learning_rate": 1.5952856646647938e-05,
      "loss": 0.6201,
      "step": 3534
    },
    {
      "epoch": 0.318260595556956,
      "grad_norm": 0.7789947203926759,
      "learning_rate": 1.595051320770324e-05,
      "loss": 0.5519,
      "step": 3535
    },
    {
      "epoch": 0.3183506268428279,
      "grad_norm": 0.8027883673605285,
      "learning_rate": 1.594816926272321e-05,
      "loss": 0.6668,
      "step": 3536
    },
    {
      "epoch": 0.3184406581286997,
      "grad_norm": 0.8049277482487888,
      "learning_rate": 1.5945824811907167e-05,
      "loss": 0.6961,
      "step": 3537
    },
    {
      "epoch": 0.3185306894145716,
      "grad_norm": 0.7743693022663138,
      "learning_rate": 1.5943479855454494e-05,
      "loss": 0.6223,
      "step": 3538
    },
    {
      "epoch": 0.3186207207004434,
      "grad_norm": 0.7954476594755389,
      "learning_rate": 1.59411343935646e-05,
      "loss": 0.4905,
      "step": 3539
    },
    {
      "epoch": 0.31871075198631527,
      "grad_norm": 0.7713874526364615,
      "learning_rate": 1.593878842643695e-05,
      "loss": 0.6383,
      "step": 3540
    },
    {
      "epoch": 0.3188007832721871,
      "grad_norm": 0.6727483119259074,
      "learning_rate": 1.5936441954271046e-05,
      "loss": 0.5273,
      "step": 3541
    },
    {
      "epoch": 0.3188908145580589,
      "grad_norm": 0.6947713047642256,
      "learning_rate": 1.5934094977266432e-05,
      "loss": 0.4299,
      "step": 3542
    },
    {
      "epoch": 0.31898084584393077,
      "grad_norm": 0.8737695096092782,
      "learning_rate": 1.5931747495622695e-05,
      "loss": 0.6603,
      "step": 3543
    },
    {
      "epoch": 0.3190708771298026,
      "grad_norm": 0.7336539090292341,
      "learning_rate": 1.5929399509539467e-05,
      "loss": 0.5541,
      "step": 3544
    },
    {
      "epoch": 0.31916090841567446,
      "grad_norm": 0.7494357680273644,
      "learning_rate": 1.5927051019216428e-05,
      "loss": 0.552,
      "step": 3545
    },
    {
      "epoch": 0.3192509397015463,
      "grad_norm": 0.5849845661970837,
      "learning_rate": 1.5924702024853284e-05,
      "loss": 0.522,
      "step": 3546
    },
    {
      "epoch": 0.31934097098741815,
      "grad_norm": 1.0049540982417218,
      "learning_rate": 1.5922352526649803e-05,
      "loss": 0.456,
      "step": 3547
    },
    {
      "epoch": 0.31943100227328997,
      "grad_norm": 0.7531630313279497,
      "learning_rate": 1.5920002524805784e-05,
      "loss": 0.5663,
      "step": 3548
    },
    {
      "epoch": 0.3195210335591618,
      "grad_norm": 0.7679570039091768,
      "learning_rate": 1.5917652019521078e-05,
      "loss": 0.5816,
      "step": 3549
    },
    {
      "epoch": 0.31961106484503365,
      "grad_norm": 0.7942234919797027,
      "learning_rate": 1.5915301010995564e-05,
      "loss": 0.6089,
      "step": 3550
    },
    {
      "epoch": 0.31970109613090547,
      "grad_norm": 0.8787939669946915,
      "learning_rate": 1.5912949499429185e-05,
      "loss": 0.6179,
      "step": 3551
    },
    {
      "epoch": 0.31979112741677734,
      "grad_norm": 0.5804738653016738,
      "learning_rate": 1.5910597485021906e-05,
      "loss": 0.5051,
      "step": 3552
    },
    {
      "epoch": 0.31988115870264916,
      "grad_norm": 0.6403514092199274,
      "learning_rate": 1.5908244967973746e-05,
      "loss": 0.4798,
      "step": 3553
    },
    {
      "epoch": 0.31997118998852103,
      "grad_norm": 1.0039565109673414,
      "learning_rate": 1.590589194848477e-05,
      "loss": 0.5275,
      "step": 3554
    },
    {
      "epoch": 0.32006122127439285,
      "grad_norm": 0.6617863479778178,
      "learning_rate": 1.590353842675507e-05,
      "loss": 0.6634,
      "step": 3555
    },
    {
      "epoch": 0.32015125256026467,
      "grad_norm": 0.7090358117168027,
      "learning_rate": 1.59011844029848e-05,
      "loss": 0.6632,
      "step": 3556
    },
    {
      "epoch": 0.32024128384613654,
      "grad_norm": 0.8493547260659465,
      "learning_rate": 1.589882987737414e-05,
      "loss": 0.6504,
      "step": 3557
    },
    {
      "epoch": 0.32033131513200835,
      "grad_norm": 0.7357055983191471,
      "learning_rate": 1.5896474850123328e-05,
      "loss": 0.6548,
      "step": 3558
    },
    {
      "epoch": 0.3204213464178802,
      "grad_norm": 0.821078187948184,
      "learning_rate": 1.589411932143263e-05,
      "loss": 0.5819,
      "step": 3559
    },
    {
      "epoch": 0.32051137770375204,
      "grad_norm": 0.7983207572305404,
      "learning_rate": 1.5891763291502372e-05,
      "loss": 0.5677,
      "step": 3560
    },
    {
      "epoch": 0.3206014089896239,
      "grad_norm": 0.9931632921814627,
      "learning_rate": 1.58894067605329e-05,
      "loss": 0.5584,
      "step": 3561
    },
    {
      "epoch": 0.32069144027549573,
      "grad_norm": 0.8838608855906994,
      "learning_rate": 1.588704972872462e-05,
      "loss": 0.5896,
      "step": 3562
    },
    {
      "epoch": 0.32078147156136755,
      "grad_norm": 0.6871162178407006,
      "learning_rate": 1.5884692196277977e-05,
      "loss": 0.61,
      "step": 3563
    },
    {
      "epoch": 0.3208715028472394,
      "grad_norm": 1.0930339641870639,
      "learning_rate": 1.588233416339345e-05,
      "loss": 0.5952,
      "step": 3564
    },
    {
      "epoch": 0.32096153413311124,
      "grad_norm": 0.7034008306259503,
      "learning_rate": 1.587997563027158e-05,
      "loss": 0.6135,
      "step": 3565
    },
    {
      "epoch": 0.3210515654189831,
      "grad_norm": 0.7784038844009595,
      "learning_rate": 1.587761659711293e-05,
      "loss": 0.6457,
      "step": 3566
    },
    {
      "epoch": 0.3211415967048549,
      "grad_norm": 0.7803529643014953,
      "learning_rate": 1.5875257064118113e-05,
      "loss": 0.5802,
      "step": 3567
    },
    {
      "epoch": 0.3212316279907268,
      "grad_norm": 0.750661136976097,
      "learning_rate": 1.587289703148779e-05,
      "loss": 0.6139,
      "step": 3568
    },
    {
      "epoch": 0.3213216592765986,
      "grad_norm": 0.6270922321585183,
      "learning_rate": 1.5870536499422655e-05,
      "loss": 0.5584,
      "step": 3569
    },
    {
      "epoch": 0.32141169056247043,
      "grad_norm": 0.7026803213375686,
      "learning_rate": 1.586817546812345e-05,
      "loss": 0.5027,
      "step": 3570
    },
    {
      "epoch": 0.3215017218483423,
      "grad_norm": 0.8956589251089555,
      "learning_rate": 1.586581393779096e-05,
      "loss": 0.5823,
      "step": 3571
    },
    {
      "epoch": 0.3215917531342141,
      "grad_norm": 0.678410409556661,
      "learning_rate": 1.586345190862601e-05,
      "loss": 0.5495,
      "step": 3572
    },
    {
      "epoch": 0.321681784420086,
      "grad_norm": 0.7631421413405535,
      "learning_rate": 1.5861089380829472e-05,
      "loss": 0.486,
      "step": 3573
    },
    {
      "epoch": 0.3217718157059578,
      "grad_norm": 0.638703089492715,
      "learning_rate": 1.5858726354602248e-05,
      "loss": 0.6008,
      "step": 3574
    },
    {
      "epoch": 0.3218618469918297,
      "grad_norm": 0.589659420848659,
      "learning_rate": 1.58563628301453e-05,
      "loss": 0.4919,
      "step": 3575
    },
    {
      "epoch": 0.3219518782777015,
      "grad_norm": 0.8886901476558032,
      "learning_rate": 1.5853998807659622e-05,
      "loss": 0.6711,
      "step": 3576
    },
    {
      "epoch": 0.3220419095635733,
      "grad_norm": 0.6716501840234849,
      "learning_rate": 1.585163428734625e-05,
      "loss": 0.5989,
      "step": 3577
    },
    {
      "epoch": 0.3221319408494452,
      "grad_norm": 0.7571677893661894,
      "learning_rate": 1.5849269269406262e-05,
      "loss": 0.5776,
      "step": 3578
    },
    {
      "epoch": 0.322221972135317,
      "grad_norm": 0.6819519108567947,
      "learning_rate": 1.5846903754040785e-05,
      "loss": 0.5468,
      "step": 3579
    },
    {
      "epoch": 0.3223120034211889,
      "grad_norm": 0.6899516063463567,
      "learning_rate": 1.5844537741450983e-05,
      "loss": 0.5022,
      "step": 3580
    },
    {
      "epoch": 0.3224020347070607,
      "grad_norm": 0.7963148273059999,
      "learning_rate": 1.584217123183806e-05,
      "loss": 0.5191,
      "step": 3581
    },
    {
      "epoch": 0.32249206599293256,
      "grad_norm": 0.5767610495012465,
      "learning_rate": 1.5839804225403268e-05,
      "loss": 0.5864,
      "step": 3582
    },
    {
      "epoch": 0.3225820972788044,
      "grad_norm": 0.6648371753768704,
      "learning_rate": 1.5837436722347902e-05,
      "loss": 0.5513,
      "step": 3583
    },
    {
      "epoch": 0.32267212856467625,
      "grad_norm": 0.5947900266216077,
      "learning_rate": 1.5835068722873292e-05,
      "loss": 0.5704,
      "step": 3584
    },
    {
      "epoch": 0.32276215985054807,
      "grad_norm": 1.164348374245674,
      "learning_rate": 1.5832700227180808e-05,
      "loss": 0.5982,
      "step": 3585
    },
    {
      "epoch": 0.3228521911364199,
      "grad_norm": 0.7131961780680947,
      "learning_rate": 1.583033123547188e-05,
      "loss": 0.5969,
      "step": 3586
    },
    {
      "epoch": 0.32294222242229176,
      "grad_norm": 1.0439386393929937,
      "learning_rate": 1.5827961747947963e-05,
      "loss": 0.6171,
      "step": 3587
    },
    {
      "epoch": 0.3230322537081636,
      "grad_norm": 0.9289582656443495,
      "learning_rate": 1.582559176481056e-05,
      "loss": 0.5677,
      "step": 3588
    },
    {
      "epoch": 0.32312228499403545,
      "grad_norm": 0.6828505692420352,
      "learning_rate": 1.5823221286261217e-05,
      "loss": 0.5138,
      "step": 3589
    },
    {
      "epoch": 0.32321231627990726,
      "grad_norm": 0.6396719316554496,
      "learning_rate": 1.5820850312501516e-05,
      "loss": 0.5437,
      "step": 3590
    },
    {
      "epoch": 0.32330234756577914,
      "grad_norm": 0.7199419606923345,
      "learning_rate": 1.5818478843733098e-05,
      "loss": 0.5893,
      "step": 3591
    },
    {
      "epoch": 0.32339237885165095,
      "grad_norm": 0.7538288872217457,
      "learning_rate": 1.5816106880157616e-05,
      "loss": 0.5556,
      "step": 3592
    },
    {
      "epoch": 0.32348241013752277,
      "grad_norm": 0.9579840473974137,
      "learning_rate": 1.5813734421976802e-05,
      "loss": 0.5469,
      "step": 3593
    },
    {
      "epoch": 0.32357244142339464,
      "grad_norm": 1.0702189335652779,
      "learning_rate": 1.58113614693924e-05,
      "loss": 0.6094,
      "step": 3594
    },
    {
      "epoch": 0.32366247270926646,
      "grad_norm": 0.7400933043595709,
      "learning_rate": 1.5808988022606206e-05,
      "loss": 0.5693,
      "step": 3595
    },
    {
      "epoch": 0.32375250399513833,
      "grad_norm": 0.9096901872937718,
      "learning_rate": 1.580661408182007e-05,
      "loss": 0.4905,
      "step": 3596
    },
    {
      "epoch": 0.32384253528101015,
      "grad_norm": 0.7285336963388744,
      "learning_rate": 1.5804239647235863e-05,
      "loss": 0.4842,
      "step": 3597
    },
    {
      "epoch": 0.323932566566882,
      "grad_norm": 0.8083686511875438,
      "learning_rate": 1.5801864719055512e-05,
      "loss": 0.5162,
      "step": 3598
    },
    {
      "epoch": 0.32402259785275384,
      "grad_norm": 0.7148842566921007,
      "learning_rate": 1.5799489297480987e-05,
      "loss": 0.5817,
      "step": 3599
    },
    {
      "epoch": 0.32411262913862565,
      "grad_norm": 0.6134987291614453,
      "learning_rate": 1.5797113382714284e-05,
      "loss": 0.5826,
      "step": 3600
    },
    {
      "epoch": 0.3242026604244975,
      "grad_norm": 0.8964168870404312,
      "learning_rate": 1.5794736974957465e-05,
      "loss": 0.5968,
      "step": 3601
    },
    {
      "epoch": 0.32429269171036934,
      "grad_norm": 0.7592882845253638,
      "learning_rate": 1.5792360074412612e-05,
      "loss": 0.6232,
      "step": 3602
    },
    {
      "epoch": 0.3243827229962412,
      "grad_norm": 0.7741421246812604,
      "learning_rate": 1.5789982681281864e-05,
      "loss": 0.6215,
      "step": 3603
    },
    {
      "epoch": 0.32447275428211303,
      "grad_norm": 0.6356724659733274,
      "learning_rate": 1.578760479576739e-05,
      "loss": 0.5602,
      "step": 3604
    },
    {
      "epoch": 0.3245627855679849,
      "grad_norm": 0.8581902193762211,
      "learning_rate": 1.5785226418071418e-05,
      "loss": 0.5247,
      "step": 3605
    },
    {
      "epoch": 0.3246528168538567,
      "grad_norm": 1.4798653284310397,
      "learning_rate": 1.578284754839619e-05,
      "loss": 0.5442,
      "step": 3606
    },
    {
      "epoch": 0.32474284813972853,
      "grad_norm": 0.6696000910377384,
      "learning_rate": 1.5780468186944022e-05,
      "loss": 0.5243,
      "step": 3607
    },
    {
      "epoch": 0.3248328794256004,
      "grad_norm": 0.9756670341242839,
      "learning_rate": 1.5778088333917248e-05,
      "loss": 0.5738,
      "step": 3608
    },
    {
      "epoch": 0.3249229107114722,
      "grad_norm": 0.7586554549614357,
      "learning_rate": 1.5775707989518253e-05,
      "loss": 0.5588,
      "step": 3609
    },
    {
      "epoch": 0.3250129419973441,
      "grad_norm": 0.6551530561390723,
      "learning_rate": 1.5773327153949465e-05,
      "loss": 0.5099,
      "step": 3610
    },
    {
      "epoch": 0.3251029732832159,
      "grad_norm": 0.710990641940873,
      "learning_rate": 1.577094582741335e-05,
      "loss": 0.4943,
      "step": 3611
    },
    {
      "epoch": 0.3251930045690878,
      "grad_norm": 0.796448557136558,
      "learning_rate": 1.5768564010112423e-05,
      "loss": 0.6373,
      "step": 3612
    },
    {
      "epoch": 0.3252830358549596,
      "grad_norm": 0.6799319335009734,
      "learning_rate": 1.5766181702249226e-05,
      "loss": 0.5283,
      "step": 3613
    },
    {
      "epoch": 0.3253730671408314,
      "grad_norm": 0.5633261017733642,
      "learning_rate": 1.576379890402636e-05,
      "loss": 0.4326,
      "step": 3614
    },
    {
      "epoch": 0.3254630984267033,
      "grad_norm": 0.7960088074014656,
      "learning_rate": 1.5761415615646452e-05,
      "loss": 0.6572,
      "step": 3615
    },
    {
      "epoch": 0.3255531297125751,
      "grad_norm": 0.7653253308381485,
      "learning_rate": 1.5759031837312188e-05,
      "loss": 0.5321,
      "step": 3616
    },
    {
      "epoch": 0.325643160998447,
      "grad_norm": 0.741394979939238,
      "learning_rate": 1.5756647569226276e-05,
      "loss": 0.6197,
      "step": 3617
    },
    {
      "epoch": 0.3257331922843188,
      "grad_norm": 0.7445342721565342,
      "learning_rate": 1.5754262811591478e-05,
      "loss": 0.6557,
      "step": 3618
    },
    {
      "epoch": 0.32582322357019067,
      "grad_norm": 0.7276490879599817,
      "learning_rate": 1.5751877564610602e-05,
      "loss": 0.5249,
      "step": 3619
    },
    {
      "epoch": 0.3259132548560625,
      "grad_norm": 0.8656074122184743,
      "learning_rate": 1.5749491828486484e-05,
      "loss": 0.6354,
      "step": 3620
    },
    {
      "epoch": 0.3260032861419343,
      "grad_norm": 0.7568109656019252,
      "learning_rate": 1.5747105603422013e-05,
      "loss": 0.5936,
      "step": 3621
    },
    {
      "epoch": 0.3260933174278062,
      "grad_norm": 0.7387189002542661,
      "learning_rate": 1.574471888962011e-05,
      "loss": 0.5938,
      "step": 3622
    },
    {
      "epoch": 0.326183348713678,
      "grad_norm": 0.7039490752915276,
      "learning_rate": 1.5742331687283748e-05,
      "loss": 0.5965,
      "step": 3623
    },
    {
      "epoch": 0.32627337999954986,
      "grad_norm": 0.7351813217486803,
      "learning_rate": 1.5739943996615936e-05,
      "loss": 0.5527,
      "step": 3624
    },
    {
      "epoch": 0.3263634112854217,
      "grad_norm": 0.7510070909289848,
      "learning_rate": 1.573755581781972e-05,
      "loss": 0.5321,
      "step": 3625
    },
    {
      "epoch": 0.32645344257129355,
      "grad_norm": 0.7714200747246756,
      "learning_rate": 1.573516715109819e-05,
      "loss": 0.5543,
      "step": 3626
    },
    {
      "epoch": 0.32654347385716537,
      "grad_norm": 0.6325535190706498,
      "learning_rate": 1.573277799665449e-05,
      "loss": 0.4959,
      "step": 3627
    },
    {
      "epoch": 0.3266335051430372,
      "grad_norm": 0.7530541932993718,
      "learning_rate": 1.573038835469179e-05,
      "loss": 0.5224,
      "step": 3628
    },
    {
      "epoch": 0.32672353642890906,
      "grad_norm": 0.8757348986629995,
      "learning_rate": 1.5727998225413303e-05,
      "loss": 0.5137,
      "step": 3629
    },
    {
      "epoch": 0.3268135677147809,
      "grad_norm": 0.6468846647462657,
      "learning_rate": 1.572560760902229e-05,
      "loss": 0.5917,
      "step": 3630
    },
    {
      "epoch": 0.32690359900065274,
      "grad_norm": 0.7821544672574362,
      "learning_rate": 1.572321650572205e-05,
      "loss": 0.5953,
      "step": 3631
    },
    {
      "epoch": 0.32699363028652456,
      "grad_norm": 0.9787683848885022,
      "learning_rate": 1.5720824915715928e-05,
      "loss": 0.6338,
      "step": 3632
    },
    {
      "epoch": 0.32708366157239643,
      "grad_norm": 0.6216836311144448,
      "learning_rate": 1.57184328392073e-05,
      "loss": 0.5362,
      "step": 3633
    },
    {
      "epoch": 0.32717369285826825,
      "grad_norm": 0.94031023776864,
      "learning_rate": 1.5716040276399593e-05,
      "loss": 0.5177,
      "step": 3634
    },
    {
      "epoch": 0.32726372414414007,
      "grad_norm": 0.8337025970408554,
      "learning_rate": 1.571364722749627e-05,
      "loss": 0.5249,
      "step": 3635
    },
    {
      "epoch": 0.32735375543001194,
      "grad_norm": 0.8526742777070583,
      "learning_rate": 1.5711253692700842e-05,
      "loss": 0.6869,
      "step": 3636
    },
    {
      "epoch": 0.32744378671588376,
      "grad_norm": 0.8858020182826565,
      "learning_rate": 1.5708859672216852e-05,
      "loss": 0.5463,
      "step": 3637
    },
    {
      "epoch": 0.32753381800175563,
      "grad_norm": 1.087752883449301,
      "learning_rate": 1.5706465166247892e-05,
      "loss": 0.6192,
      "step": 3638
    },
    {
      "epoch": 0.32762384928762744,
      "grad_norm": 0.6439733573598119,
      "learning_rate": 1.570407017499759e-05,
      "loss": 0.5623,
      "step": 3639
    },
    {
      "epoch": 0.3277138805734993,
      "grad_norm": 0.861540801398508,
      "learning_rate": 1.570167469866962e-05,
      "loss": 0.5902,
      "step": 3640
    },
    {
      "epoch": 0.32780391185937113,
      "grad_norm": 0.7933676351292029,
      "learning_rate": 1.569927873746769e-05,
      "loss": 0.5413,
      "step": 3641
    },
    {
      "epoch": 0.32789394314524295,
      "grad_norm": 0.6628696423836705,
      "learning_rate": 1.5696882291595557e-05,
      "loss": 0.529,
      "step": 3642
    },
    {
      "epoch": 0.3279839744311148,
      "grad_norm": 0.7398405990412641,
      "learning_rate": 1.569448536125702e-05,
      "loss": 0.5164,
      "step": 3643
    },
    {
      "epoch": 0.32807400571698664,
      "grad_norm": 0.7278695950699618,
      "learning_rate": 1.569208794665591e-05,
      "loss": 0.6201,
      "step": 3644
    },
    {
      "epoch": 0.3281640370028585,
      "grad_norm": 0.6932088852001921,
      "learning_rate": 1.5689690047996108e-05,
      "loss": 0.6165,
      "step": 3645
    },
    {
      "epoch": 0.3282540682887303,
      "grad_norm": 0.9054172462743768,
      "learning_rate": 1.568729166548153e-05,
      "loss": 0.5889,
      "step": 3646
    },
    {
      "epoch": 0.3283440995746022,
      "grad_norm": 0.6787467199801172,
      "learning_rate": 1.5684892799316137e-05,
      "loss": 0.4943,
      "step": 3647
    },
    {
      "epoch": 0.328434130860474,
      "grad_norm": 1.2112568968103608,
      "learning_rate": 1.568249344970393e-05,
      "loss": 0.6267,
      "step": 3648
    },
    {
      "epoch": 0.32852416214634583,
      "grad_norm": 0.8994803113508657,
      "learning_rate": 1.5680093616848955e-05,
      "loss": 0.6358,
      "step": 3649
    },
    {
      "epoch": 0.3286141934322177,
      "grad_norm": 0.7835262813140851,
      "learning_rate": 1.567769330095529e-05,
      "loss": 0.4968,
      "step": 3650
    },
    {
      "epoch": 0.3287042247180895,
      "grad_norm": 0.7838786294453689,
      "learning_rate": 1.5675292502227058e-05,
      "loss": 0.5867,
      "step": 3651
    },
    {
      "epoch": 0.3287942560039614,
      "grad_norm": 0.6752073981887556,
      "learning_rate": 1.567289122086843e-05,
      "loss": 0.5751,
      "step": 3652
    },
    {
      "epoch": 0.3288842872898332,
      "grad_norm": 0.7203960649712071,
      "learning_rate": 1.567048945708361e-05,
      "loss": 0.5427,
      "step": 3653
    },
    {
      "epoch": 0.3289743185757051,
      "grad_norm": 0.9075549809810056,
      "learning_rate": 1.5668087211076844e-05,
      "loss": 0.579,
      "step": 3654
    },
    {
      "epoch": 0.3290643498615769,
      "grad_norm": 0.6793164101772042,
      "learning_rate": 1.5665684483052425e-05,
      "loss": 0.6007,
      "step": 3655
    },
    {
      "epoch": 0.3291543811474487,
      "grad_norm": 1.3595284852259535,
      "learning_rate": 1.566328127321468e-05,
      "loss": 0.622,
      "step": 3656
    },
    {
      "epoch": 0.3292444124333206,
      "grad_norm": 0.7096409111637776,
      "learning_rate": 1.5660877581767976e-05,
      "loss": 0.6109,
      "step": 3657
    },
    {
      "epoch": 0.3293344437191924,
      "grad_norm": 0.7007461420558205,
      "learning_rate": 1.5658473408916728e-05,
      "loss": 0.5313,
      "step": 3658
    },
    {
      "epoch": 0.3294244750050643,
      "grad_norm": 0.9067794138370807,
      "learning_rate": 1.5656068754865388e-05,
      "loss": 0.5411,
      "step": 3659
    },
    {
      "epoch": 0.3295145062909361,
      "grad_norm": 0.7589199467130765,
      "learning_rate": 1.5653663619818447e-05,
      "loss": 0.5679,
      "step": 3660
    },
    {
      "epoch": 0.32960453757680797,
      "grad_norm": 0.7150659319591001,
      "learning_rate": 1.5651258003980444e-05,
      "loss": 0.5334,
      "step": 3661
    },
    {
      "epoch": 0.3296945688626798,
      "grad_norm": 0.61710808000621,
      "learning_rate": 1.5648851907555953e-05,
      "loss": 0.474,
      "step": 3662
    },
    {
      "epoch": 0.3297846001485516,
      "grad_norm": 0.6457833663692567,
      "learning_rate": 1.5646445330749584e-05,
      "loss": 0.5322,
      "step": 3663
    },
    {
      "epoch": 0.32987463143442347,
      "grad_norm": 0.8837843799993912,
      "learning_rate": 1.5644038273766004e-05,
      "loss": 0.5438,
      "step": 3664
    },
    {
      "epoch": 0.3299646627202953,
      "grad_norm": 0.8350633124403847,
      "learning_rate": 1.5641630736809896e-05,
      "loss": 0.5431,
      "step": 3665
    },
    {
      "epoch": 0.33005469400616716,
      "grad_norm": 0.8984756292377879,
      "learning_rate": 1.5639222720086013e-05,
      "loss": 0.5179,
      "step": 3666
    },
    {
      "epoch": 0.330144725292039,
      "grad_norm": 0.5695187231182933,
      "learning_rate": 1.5636814223799123e-05,
      "loss": 0.4292,
      "step": 3667
    },
    {
      "epoch": 0.33023475657791085,
      "grad_norm": 0.8764706781702517,
      "learning_rate": 1.5634405248154058e-05,
      "loss": 0.587,
      "step": 3668
    },
    {
      "epoch": 0.33032478786378267,
      "grad_norm": 0.7469907480709251,
      "learning_rate": 1.5631995793355668e-05,
      "loss": 0.4976,
      "step": 3669
    },
    {
      "epoch": 0.3304148191496545,
      "grad_norm": 0.8396337620898388,
      "learning_rate": 1.5629585859608858e-05,
      "loss": 0.6032,
      "step": 3670
    },
    {
      "epoch": 0.33050485043552635,
      "grad_norm": 0.7919658827102775,
      "learning_rate": 1.5627175447118576e-05,
      "loss": 0.5226,
      "step": 3671
    },
    {
      "epoch": 0.33059488172139817,
      "grad_norm": 0.8023930213443342,
      "learning_rate": 1.5624764556089793e-05,
      "loss": 0.6468,
      "step": 3672
    },
    {
      "epoch": 0.33068491300727004,
      "grad_norm": 0.7706565428065029,
      "learning_rate": 1.5622353186727542e-05,
      "loss": 0.5486,
      "step": 3673
    },
    {
      "epoch": 0.33077494429314186,
      "grad_norm": 0.7243432035997681,
      "learning_rate": 1.5619941339236887e-05,
      "loss": 0.5455,
      "step": 3674
    },
    {
      "epoch": 0.33086497557901373,
      "grad_norm": 0.6861325978492272,
      "learning_rate": 1.561752901382293e-05,
      "loss": 0.4817,
      "step": 3675
    },
    {
      "epoch": 0.33095500686488555,
      "grad_norm": 0.9046493212857594,
      "learning_rate": 1.5615116210690815e-05,
      "loss": 0.5417,
      "step": 3676
    },
    {
      "epoch": 0.33104503815075736,
      "grad_norm": 0.629925544762277,
      "learning_rate": 1.5612702930045733e-05,
      "loss": 0.564,
      "step": 3677
    },
    {
      "epoch": 0.33113506943662924,
      "grad_norm": 0.688933731688225,
      "learning_rate": 1.561028917209291e-05,
      "loss": 0.5787,
      "step": 3678
    },
    {
      "epoch": 0.33122510072250105,
      "grad_norm": 0.8189037046097983,
      "learning_rate": 1.560787493703761e-05,
      "loss": 0.5366,
      "step": 3679
    },
    {
      "epoch": 0.3313151320083729,
      "grad_norm": 0.9488319765983485,
      "learning_rate": 1.5605460225085144e-05,
      "loss": 0.6138,
      "step": 3680
    },
    {
      "epoch": 0.33140516329424474,
      "grad_norm": 0.919014812886082,
      "learning_rate": 1.5603045036440858e-05,
      "loss": 0.587,
      "step": 3681
    },
    {
      "epoch": 0.3314951945801166,
      "grad_norm": 0.7619945876241375,
      "learning_rate": 1.5600629371310144e-05,
      "loss": 0.6513,
      "step": 3682
    },
    {
      "epoch": 0.33158522586598843,
      "grad_norm": 1.0712390258843723,
      "learning_rate": 1.559821322989843e-05,
      "loss": 0.6426,
      "step": 3683
    },
    {
      "epoch": 0.33167525715186025,
      "grad_norm": 0.9765331425202886,
      "learning_rate": 1.559579661241119e-05,
      "loss": 0.5498,
      "step": 3684
    },
    {
      "epoch": 0.3317652884377321,
      "grad_norm": 0.8053230109884537,
      "learning_rate": 1.5593379519053928e-05,
      "loss": 0.5435,
      "step": 3685
    },
    {
      "epoch": 0.33185531972360394,
      "grad_norm": 0.8680700291611406,
      "learning_rate": 1.5590961950032198e-05,
      "loss": 0.5892,
      "step": 3686
    },
    {
      "epoch": 0.3319453510094758,
      "grad_norm": 1.1949499984031489,
      "learning_rate": 1.5588543905551597e-05,
      "loss": 0.5803,
      "step": 3687
    },
    {
      "epoch": 0.3320353822953476,
      "grad_norm": 1.0315654544471708,
      "learning_rate": 1.558612538581775e-05,
      "loss": 0.6104,
      "step": 3688
    },
    {
      "epoch": 0.3321254135812195,
      "grad_norm": 0.8059739376683651,
      "learning_rate": 1.558370639103633e-05,
      "loss": 0.5291,
      "step": 3689
    },
    {
      "epoch": 0.3322154448670913,
      "grad_norm": 0.7034977084291621,
      "learning_rate": 1.558128692141305e-05,
      "loss": 0.626,
      "step": 3690
    },
    {
      "epoch": 0.33230547615296313,
      "grad_norm": 0.7455026760647295,
      "learning_rate": 1.5578866977153665e-05,
      "loss": 0.6639,
      "step": 3691
    },
    {
      "epoch": 0.332395507438835,
      "grad_norm": 0.6612494100722632,
      "learning_rate": 1.5576446558463972e-05,
      "loss": 0.5342,
      "step": 3692
    },
    {
      "epoch": 0.3324855387247068,
      "grad_norm": 0.6772127929561289,
      "learning_rate": 1.5574025665549798e-05,
      "loss": 0.5715,
      "step": 3693
    },
    {
      "epoch": 0.3325755700105787,
      "grad_norm": 0.7079760971241725,
      "learning_rate": 1.557160429861702e-05,
      "loss": 0.6165,
      "step": 3694
    },
    {
      "epoch": 0.3326656012964505,
      "grad_norm": 0.8103997702782083,
      "learning_rate": 1.556918245787155e-05,
      "loss": 0.5784,
      "step": 3695
    },
    {
      "epoch": 0.3327556325823224,
      "grad_norm": 0.8191875978110005,
      "learning_rate": 1.5566760143519354e-05,
      "loss": 0.4908,
      "step": 3696
    },
    {
      "epoch": 0.3328456638681942,
      "grad_norm": 0.6874609647973705,
      "learning_rate": 1.5564337355766412e-05,
      "loss": 0.585,
      "step": 3697
    },
    {
      "epoch": 0.332935695154066,
      "grad_norm": 0.6325503309099753,
      "learning_rate": 1.5561914094818768e-05,
      "loss": 0.5186,
      "step": 3698
    },
    {
      "epoch": 0.3330257264399379,
      "grad_norm": 0.8936305776338032,
      "learning_rate": 1.5559490360882495e-05,
      "loss": 0.5783,
      "step": 3699
    },
    {
      "epoch": 0.3331157577258097,
      "grad_norm": 0.7509921704999964,
      "learning_rate": 1.555706615416371e-05,
      "loss": 0.576,
      "step": 3700
    },
    {
      "epoch": 0.3332057890116816,
      "grad_norm": 0.7088955442238056,
      "learning_rate": 1.5554641474868572e-05,
      "loss": 0.6207,
      "step": 3701
    },
    {
      "epoch": 0.3332958202975534,
      "grad_norm": 0.7158222736669889,
      "learning_rate": 1.555221632320327e-05,
      "loss": 0.5538,
      "step": 3702
    },
    {
      "epoch": 0.33338585158342526,
      "grad_norm": 0.9096502838911873,
      "learning_rate": 1.5549790699374045e-05,
      "loss": 0.5219,
      "step": 3703
    },
    {
      "epoch": 0.3334758828692971,
      "grad_norm": 0.7504552014846099,
      "learning_rate": 1.5547364603587172e-05,
      "loss": 0.5977,
      "step": 3704
    },
    {
      "epoch": 0.3335659141551689,
      "grad_norm": 0.8961099014776903,
      "learning_rate": 1.5544938036048968e-05,
      "loss": 0.614,
      "step": 3705
    },
    {
      "epoch": 0.33365594544104077,
      "grad_norm": 0.7469267294519182,
      "learning_rate": 1.5542510996965795e-05,
      "loss": 0.607,
      "step": 3706
    },
    {
      "epoch": 0.3337459767269126,
      "grad_norm": 0.7318398677655585,
      "learning_rate": 1.554008348654404e-05,
      "loss": 0.5305,
      "step": 3707
    },
    {
      "epoch": 0.33383600801278446,
      "grad_norm": 0.7272950993233481,
      "learning_rate": 1.553765550499015e-05,
      "loss": 0.4999,
      "step": 3708
    },
    {
      "epoch": 0.3339260392986563,
      "grad_norm": 0.7371561355446776,
      "learning_rate": 1.553522705251059e-05,
      "loss": 0.5212,
      "step": 3709
    },
    {
      "epoch": 0.33401607058452815,
      "grad_norm": 0.5597671091461485,
      "learning_rate": 1.553279812931189e-05,
      "loss": 0.4903,
      "step": 3710
    },
    {
      "epoch": 0.33410610187039996,
      "grad_norm": 0.7785163791537206,
      "learning_rate": 1.55303687356006e-05,
      "loss": 0.5198,
      "step": 3711
    },
    {
      "epoch": 0.3341961331562718,
      "grad_norm": 0.7122112318751571,
      "learning_rate": 1.552793887158332e-05,
      "loss": 0.5286,
      "step": 3712
    },
    {
      "epoch": 0.33428616444214365,
      "grad_norm": 0.7454569737573495,
      "learning_rate": 1.5525508537466685e-05,
      "loss": 0.5157,
      "step": 3713
    },
    {
      "epoch": 0.33437619572801547,
      "grad_norm": 0.8193288140657206,
      "learning_rate": 1.5523077733457373e-05,
      "loss": 0.5352,
      "step": 3714
    },
    {
      "epoch": 0.33446622701388734,
      "grad_norm": 2.1283786366023945,
      "learning_rate": 1.5520646459762102e-05,
      "loss": 0.5593,
      "step": 3715
    },
    {
      "epoch": 0.33455625829975916,
      "grad_norm": 0.7905846931420859,
      "learning_rate": 1.551821471658763e-05,
      "loss": 0.4615,
      "step": 3716
    },
    {
      "epoch": 0.33464628958563103,
      "grad_norm": 0.6545909457498541,
      "learning_rate": 1.5515782504140744e-05,
      "loss": 0.5767,
      "step": 3717
    },
    {
      "epoch": 0.33473632087150285,
      "grad_norm": 0.6728865591729094,
      "learning_rate": 1.5513349822628295e-05,
      "loss": 0.5645,
      "step": 3718
    },
    {
      "epoch": 0.33482635215737466,
      "grad_norm": 0.9198530800147666,
      "learning_rate": 1.5510916672257153e-05,
      "loss": 0.6269,
      "step": 3719
    },
    {
      "epoch": 0.33491638344324653,
      "grad_norm": 0.744516019374396,
      "learning_rate": 1.5508483053234234e-05,
      "loss": 0.5223,
      "step": 3720
    },
    {
      "epoch": 0.33500641472911835,
      "grad_norm": 0.7896986578328228,
      "learning_rate": 1.5506048965766497e-05,
      "loss": 0.5364,
      "step": 3721
    },
    {
      "epoch": 0.3350964460149902,
      "grad_norm": 0.7028802946523364,
      "learning_rate": 1.5503614410060935e-05,
      "loss": 0.5054,
      "step": 3722
    },
    {
      "epoch": 0.33518647730086204,
      "grad_norm": 0.701656989497229,
      "learning_rate": 1.550117938632459e-05,
      "loss": 0.5277,
      "step": 3723
    },
    {
      "epoch": 0.3352765085867339,
      "grad_norm": 0.7609216690962068,
      "learning_rate": 1.5498743894764533e-05,
      "loss": 0.5683,
      "step": 3724
    },
    {
      "epoch": 0.33536653987260573,
      "grad_norm": 0.7891541543197316,
      "learning_rate": 1.549630793558788e-05,
      "loss": 0.6175,
      "step": 3725
    },
    {
      "epoch": 0.33545657115847755,
      "grad_norm": 0.7387760029151303,
      "learning_rate": 1.5493871509001785e-05,
      "loss": 0.5528,
      "step": 3726
    },
    {
      "epoch": 0.3355466024443494,
      "grad_norm": 0.6505713042269357,
      "learning_rate": 1.549143461521345e-05,
      "loss": 0.4731,
      "step": 3727
    },
    {
      "epoch": 0.33563663373022123,
      "grad_norm": 0.6603959535896569,
      "learning_rate": 1.54889972544301e-05,
      "loss": 0.5464,
      "step": 3728
    },
    {
      "epoch": 0.3357266650160931,
      "grad_norm": 0.9541721352973066,
      "learning_rate": 1.5486559426859018e-05,
      "loss": 0.5351,
      "step": 3729
    },
    {
      "epoch": 0.3358166963019649,
      "grad_norm": 0.7427761267574151,
      "learning_rate": 1.5484121132707515e-05,
      "loss": 0.5706,
      "step": 3730
    },
    {
      "epoch": 0.3359067275878368,
      "grad_norm": 0.844324853794475,
      "learning_rate": 1.5481682372182945e-05,
      "loss": 0.6259,
      "step": 3731
    },
    {
      "epoch": 0.3359967588737086,
      "grad_norm": 0.8034240251567945,
      "learning_rate": 1.5479243145492705e-05,
      "loss": 0.6005,
      "step": 3732
    },
    {
      "epoch": 0.33608679015958043,
      "grad_norm": 0.9601168209888862,
      "learning_rate": 1.5476803452844223e-05,
      "loss": 0.5919,
      "step": 3733
    },
    {
      "epoch": 0.3361768214454523,
      "grad_norm": 0.9392920780646867,
      "learning_rate": 1.5474363294444974e-05,
      "loss": 0.5833,
      "step": 3734
    },
    {
      "epoch": 0.3362668527313241,
      "grad_norm": 0.6752734352323893,
      "learning_rate": 1.5471922670502472e-05,
      "loss": 0.5424,
      "step": 3735
    },
    {
      "epoch": 0.336356884017196,
      "grad_norm": 0.6797213360926753,
      "learning_rate": 1.5469481581224274e-05,
      "loss": 0.5321,
      "step": 3736
    },
    {
      "epoch": 0.3364469153030678,
      "grad_norm": 0.6593090054590939,
      "learning_rate": 1.546704002681796e-05,
      "loss": 0.4844,
      "step": 3737
    },
    {
      "epoch": 0.3365369465889397,
      "grad_norm": 0.8835487711507152,
      "learning_rate": 1.5464598007491165e-05,
      "loss": 0.5347,
      "step": 3738
    },
    {
      "epoch": 0.3366269778748115,
      "grad_norm": 0.6924353697328409,
      "learning_rate": 1.5462155523451565e-05,
      "loss": 0.4719,
      "step": 3739
    },
    {
      "epoch": 0.3367170091606833,
      "grad_norm": 0.5972307876875568,
      "learning_rate": 1.545971257490687e-05,
      "loss": 0.6067,
      "step": 3740
    },
    {
      "epoch": 0.3368070404465552,
      "grad_norm": 0.5891221624262487,
      "learning_rate": 1.5457269162064827e-05,
      "loss": 0.4331,
      "step": 3741
    },
    {
      "epoch": 0.336897071732427,
      "grad_norm": 0.8398206670736919,
      "learning_rate": 1.5454825285133223e-05,
      "loss": 0.5765,
      "step": 3742
    },
    {
      "epoch": 0.3369871030182989,
      "grad_norm": 0.9008956715156452,
      "learning_rate": 1.5452380944319892e-05,
      "loss": 0.5315,
      "step": 3743
    },
    {
      "epoch": 0.3370771343041707,
      "grad_norm": 0.7751789211072075,
      "learning_rate": 1.54499361398327e-05,
      "loss": 0.528,
      "step": 3744
    },
    {
      "epoch": 0.33716716559004256,
      "grad_norm": 0.6417511071367399,
      "learning_rate": 1.544749087187955e-05,
      "loss": 0.5342,
      "step": 3745
    },
    {
      "epoch": 0.3372571968759144,
      "grad_norm": 0.8625701952566501,
      "learning_rate": 1.54450451406684e-05,
      "loss": 0.5522,
      "step": 3746
    },
    {
      "epoch": 0.3373472281617862,
      "grad_norm": 0.6526603447339616,
      "learning_rate": 1.5442598946407227e-05,
      "loss": 0.5032,
      "step": 3747
    },
    {
      "epoch": 0.33743725944765807,
      "grad_norm": 0.8420957371235955,
      "learning_rate": 1.5440152289304063e-05,
      "loss": 0.5736,
      "step": 3748
    },
    {
      "epoch": 0.3375272907335299,
      "grad_norm": 0.7350314328783261,
      "learning_rate": 1.5437705169566967e-05,
      "loss": 0.5546,
      "step": 3749
    },
    {
      "epoch": 0.33761732201940176,
      "grad_norm": 0.8523326820834928,
      "learning_rate": 1.5435257587404047e-05,
      "loss": 0.5593,
      "step": 3750
    },
    {
      "epoch": 0.33770735330527357,
      "grad_norm": 0.5658405682142744,
      "learning_rate": 1.5432809543023444e-05,
      "loss": 0.5409,
      "step": 3751
    },
    {
      "epoch": 0.33779738459114544,
      "grad_norm": 0.6628655690642511,
      "learning_rate": 1.5430361036633352e-05,
      "loss": 0.568,
      "step": 3752
    },
    {
      "epoch": 0.33788741587701726,
      "grad_norm": 0.7489371810571305,
      "learning_rate": 1.5427912068441978e-05,
      "loss": 0.5763,
      "step": 3753
    },
    {
      "epoch": 0.3379774471628891,
      "grad_norm": 0.8972044859580034,
      "learning_rate": 1.5425462638657597e-05,
      "loss": 0.5468,
      "step": 3754
    },
    {
      "epoch": 0.33806747844876095,
      "grad_norm": 0.6749086753062723,
      "learning_rate": 1.54230127474885e-05,
      "loss": 0.582,
      "step": 3755
    },
    {
      "epoch": 0.33815750973463277,
      "grad_norm": 0.640977891959064,
      "learning_rate": 1.5420562395143034e-05,
      "loss": 0.5552,
      "step": 3756
    },
    {
      "epoch": 0.33824754102050464,
      "grad_norm": 0.739158866203238,
      "learning_rate": 1.5418111581829575e-05,
      "loss": 0.6488,
      "step": 3757
    },
    {
      "epoch": 0.33833757230637646,
      "grad_norm": 0.7498332218438829,
      "learning_rate": 1.5415660307756542e-05,
      "loss": 0.4975,
      "step": 3758
    },
    {
      "epoch": 0.3384276035922483,
      "grad_norm": 0.6433593269993952,
      "learning_rate": 1.5413208573132394e-05,
      "loss": 0.579,
      "step": 3759
    },
    {
      "epoch": 0.33851763487812014,
      "grad_norm": 0.6546259414041241,
      "learning_rate": 1.541075637816563e-05,
      "loss": 0.5407,
      "step": 3760
    },
    {
      "epoch": 0.33860766616399196,
      "grad_norm": 0.7986238012166377,
      "learning_rate": 1.5408303723064784e-05,
      "loss": 0.5011,
      "step": 3761
    },
    {
      "epoch": 0.33869769744986383,
      "grad_norm": 0.8746163635521228,
      "learning_rate": 1.5405850608038428e-05,
      "loss": 0.509,
      "step": 3762
    },
    {
      "epoch": 0.33878772873573565,
      "grad_norm": 0.6339246980555198,
      "learning_rate": 1.540339703329518e-05,
      "loss": 0.479,
      "step": 3763
    },
    {
      "epoch": 0.3388777600216075,
      "grad_norm": 0.8393590781129463,
      "learning_rate": 1.5400942999043694e-05,
      "loss": 0.5796,
      "step": 3764
    },
    {
      "epoch": 0.33896779130747934,
      "grad_norm": 0.7209141328356711,
      "learning_rate": 1.539848850549266e-05,
      "loss": 0.5279,
      "step": 3765
    },
    {
      "epoch": 0.3390578225933512,
      "grad_norm": 0.8328431928932443,
      "learning_rate": 1.5396033552850815e-05,
      "loss": 0.585,
      "step": 3766
    },
    {
      "epoch": 0.339147853879223,
      "grad_norm": 1.0299807669410255,
      "learning_rate": 1.5393578141326923e-05,
      "loss": 0.5344,
      "step": 3767
    },
    {
      "epoch": 0.33923788516509484,
      "grad_norm": 0.8935235112678125,
      "learning_rate": 1.5391122271129798e-05,
      "loss": 0.646,
      "step": 3768
    },
    {
      "epoch": 0.3393279164509667,
      "grad_norm": 0.6930820746613116,
      "learning_rate": 1.538866594246829e-05,
      "loss": 0.5049,
      "step": 3769
    },
    {
      "epoch": 0.33941794773683853,
      "grad_norm": 0.7342086267849954,
      "learning_rate": 1.538620915555128e-05,
      "loss": 0.5826,
      "step": 3770
    },
    {
      "epoch": 0.3395079790227104,
      "grad_norm": 0.7207224639289033,
      "learning_rate": 1.5383751910587702e-05,
      "loss": 0.5409,
      "step": 3771
    },
    {
      "epoch": 0.3395980103085822,
      "grad_norm": 0.6625402750762746,
      "learning_rate": 1.5381294207786514e-05,
      "loss": 0.4935,
      "step": 3772
    },
    {
      "epoch": 0.3396880415944541,
      "grad_norm": 0.7742222367356819,
      "learning_rate": 1.5378836047356725e-05,
      "loss": 0.5828,
      "step": 3773
    },
    {
      "epoch": 0.3397780728803259,
      "grad_norm": 0.6870396989071406,
      "learning_rate": 1.5376377429507386e-05,
      "loss": 0.567,
      "step": 3774
    },
    {
      "epoch": 0.3398681041661977,
      "grad_norm": 1.004183054554942,
      "learning_rate": 1.5373918354447565e-05,
      "loss": 0.5605,
      "step": 3775
    },
    {
      "epoch": 0.3399581354520696,
      "grad_norm": 0.7412526743154565,
      "learning_rate": 1.5371458822386394e-05,
      "loss": 0.5051,
      "step": 3776
    },
    {
      "epoch": 0.3400481667379414,
      "grad_norm": 0.8579305010467163,
      "learning_rate": 1.536899883353303e-05,
      "loss": 0.7188,
      "step": 3777
    },
    {
      "epoch": 0.3401381980238133,
      "grad_norm": 0.8205832284678423,
      "learning_rate": 1.536653838809667e-05,
      "loss": 0.5783,
      "step": 3778
    },
    {
      "epoch": 0.3402282293096851,
      "grad_norm": 0.7581392505321625,
      "learning_rate": 1.5364077486286553e-05,
      "loss": 0.6545,
      "step": 3779
    },
    {
      "epoch": 0.340318260595557,
      "grad_norm": 0.8383772456757963,
      "learning_rate": 1.5361616128311955e-05,
      "loss": 0.5238,
      "step": 3780
    },
    {
      "epoch": 0.3404082918814288,
      "grad_norm": 0.6020675948667454,
      "learning_rate": 1.5359154314382192e-05,
      "loss": 0.4479,
      "step": 3781
    },
    {
      "epoch": 0.3404983231673006,
      "grad_norm": 0.6456155759445958,
      "learning_rate": 1.535669204470662e-05,
      "loss": 0.5341,
      "step": 3782
    },
    {
      "epoch": 0.3405883544531725,
      "grad_norm": 0.7834108594081151,
      "learning_rate": 1.5354229319494626e-05,
      "loss": 0.5192,
      "step": 3783
    },
    {
      "epoch": 0.3406783857390443,
      "grad_norm": 0.6567964310886658,
      "learning_rate": 1.535176613895565e-05,
      "loss": 0.589,
      "step": 3784
    },
    {
      "epoch": 0.34076841702491617,
      "grad_norm": 0.7146561048409767,
      "learning_rate": 1.534930250329916e-05,
      "loss": 0.602,
      "step": 3785
    },
    {
      "epoch": 0.340858448310788,
      "grad_norm": 0.6657541257603481,
      "learning_rate": 1.534683841273466e-05,
      "loss": 0.5543,
      "step": 3786
    },
    {
      "epoch": 0.34094847959665986,
      "grad_norm": 0.7906207819463507,
      "learning_rate": 1.53443738674717e-05,
      "loss": 0.534,
      "step": 3787
    },
    {
      "epoch": 0.3410385108825317,
      "grad_norm": 0.7761083434285763,
      "learning_rate": 1.5341908867719875e-05,
      "loss": 0.6422,
      "step": 3788
    },
    {
      "epoch": 0.34112854216840355,
      "grad_norm": 0.6638324878820231,
      "learning_rate": 1.5339443413688796e-05,
      "loss": 0.6244,
      "step": 3789
    },
    {
      "epoch": 0.34121857345427536,
      "grad_norm": 0.665182056639804,
      "learning_rate": 1.5336977505588135e-05,
      "loss": 0.4962,
      "step": 3790
    },
    {
      "epoch": 0.3413086047401472,
      "grad_norm": 0.7044303221806095,
      "learning_rate": 1.5334511143627596e-05,
      "loss": 0.4954,
      "step": 3791
    },
    {
      "epoch": 0.34139863602601905,
      "grad_norm": 0.8048571139955236,
      "learning_rate": 1.5332044328016916e-05,
      "loss": 0.6272,
      "step": 3792
    },
    {
      "epoch": 0.34148866731189087,
      "grad_norm": 0.8449534558171625,
      "learning_rate": 1.5329577058965874e-05,
      "loss": 0.6054,
      "step": 3793
    },
    {
      "epoch": 0.34157869859776274,
      "grad_norm": 0.6257257272294396,
      "learning_rate": 1.5327109336684286e-05,
      "loss": 0.5583,
      "step": 3794
    },
    {
      "epoch": 0.34166872988363456,
      "grad_norm": 0.809877619352253,
      "learning_rate": 1.5324641161382017e-05,
      "loss": 0.5254,
      "step": 3795
    },
    {
      "epoch": 0.34175876116950643,
      "grad_norm": 0.6012802641089606,
      "learning_rate": 1.532217253326896e-05,
      "loss": 0.6022,
      "step": 3796
    },
    {
      "epoch": 0.34184879245537825,
      "grad_norm": 0.7392747371416007,
      "learning_rate": 1.531970345255504e-05,
      "loss": 0.5148,
      "step": 3797
    },
    {
      "epoch": 0.34193882374125006,
      "grad_norm": 0.7677566821645709,
      "learning_rate": 1.531723391945024e-05,
      "loss": 0.5384,
      "step": 3798
    },
    {
      "epoch": 0.34202885502712194,
      "grad_norm": 0.6381934950786664,
      "learning_rate": 1.531476393416456e-05,
      "loss": 0.5308,
      "step": 3799
    },
    {
      "epoch": 0.34211888631299375,
      "grad_norm": 0.6280114943026037,
      "learning_rate": 1.531229349690806e-05,
      "loss": 0.5791,
      "step": 3800
    },
    {
      "epoch": 0.3422089175988656,
      "grad_norm": 0.6910377273544489,
      "learning_rate": 1.530982260789082e-05,
      "loss": 0.5438,
      "step": 3801
    },
    {
      "epoch": 0.34229894888473744,
      "grad_norm": 0.792892896800648,
      "learning_rate": 1.5307351267322966e-05,
      "loss": 0.5391,
      "step": 3802
    },
    {
      "epoch": 0.3423889801706093,
      "grad_norm": 0.7615505313922064,
      "learning_rate": 1.5304879475414673e-05,
      "loss": 0.4866,
      "step": 3803
    },
    {
      "epoch": 0.34247901145648113,
      "grad_norm": 0.9180347742587159,
      "learning_rate": 1.5302407232376127e-05,
      "loss": 0.5456,
      "step": 3804
    },
    {
      "epoch": 0.34256904274235295,
      "grad_norm": 0.7116340425322799,
      "learning_rate": 1.5299934538417583e-05,
      "loss": 0.46,
      "step": 3805
    },
    {
      "epoch": 0.3426590740282248,
      "grad_norm": 0.8623575425180212,
      "learning_rate": 1.529746139374931e-05,
      "loss": 0.546,
      "step": 3806
    },
    {
      "epoch": 0.34274910531409664,
      "grad_norm": 0.8986135045652405,
      "learning_rate": 1.5294987798581632e-05,
      "loss": 0.4903,
      "step": 3807
    },
    {
      "epoch": 0.3428391365999685,
      "grad_norm": 0.7205292627259953,
      "learning_rate": 1.5292513753124903e-05,
      "loss": 0.5294,
      "step": 3808
    },
    {
      "epoch": 0.3429291678858403,
      "grad_norm": 0.6525588854415544,
      "learning_rate": 1.529003925758952e-05,
      "loss": 0.5348,
      "step": 3809
    },
    {
      "epoch": 0.3430191991717122,
      "grad_norm": 0.7371920265580929,
      "learning_rate": 1.5287564312185916e-05,
      "loss": 0.6021,
      "step": 3810
    },
    {
      "epoch": 0.343109230457584,
      "grad_norm": 0.8828216343151055,
      "learning_rate": 1.5285088917124555e-05,
      "loss": 0.4535,
      "step": 3811
    },
    {
      "epoch": 0.34319926174345583,
      "grad_norm": 0.6092326129724203,
      "learning_rate": 1.5282613072615956e-05,
      "loss": 0.4841,
      "step": 3812
    },
    {
      "epoch": 0.3432892930293277,
      "grad_norm": 0.8558080061764001,
      "learning_rate": 1.528013677887066e-05,
      "loss": 0.6275,
      "step": 3813
    },
    {
      "epoch": 0.3433793243151995,
      "grad_norm": 0.7840959458048741,
      "learning_rate": 1.5277660036099248e-05,
      "loss": 0.5597,
      "step": 3814
    },
    {
      "epoch": 0.3434693556010714,
      "grad_norm": 0.8210317350611656,
      "learning_rate": 1.5275182844512353e-05,
      "loss": 0.5711,
      "step": 3815
    },
    {
      "epoch": 0.3435593868869432,
      "grad_norm": 0.6591129563085304,
      "learning_rate": 1.5272705204320634e-05,
      "loss": 0.4526,
      "step": 3816
    },
    {
      "epoch": 0.3436494181728151,
      "grad_norm": 0.7494058573467898,
      "learning_rate": 1.527022711573479e-05,
      "loss": 0.4771,
      "step": 3817
    },
    {
      "epoch": 0.3437394494586869,
      "grad_norm": 0.8678581062869801,
      "learning_rate": 1.5267748578965557e-05,
      "loss": 0.5076,
      "step": 3818
    },
    {
      "epoch": 0.3438294807445587,
      "grad_norm": 0.7852501656237776,
      "learning_rate": 1.526526959422372e-05,
      "loss": 0.559,
      "step": 3819
    },
    {
      "epoch": 0.3439195120304306,
      "grad_norm": 0.782450755204641,
      "learning_rate": 1.5262790161720082e-05,
      "loss": 0.5791,
      "step": 3820
    },
    {
      "epoch": 0.3440095433163024,
      "grad_norm": 0.7482048100137553,
      "learning_rate": 1.5260310281665503e-05,
      "loss": 0.6951,
      "step": 3821
    },
    {
      "epoch": 0.3440995746021743,
      "grad_norm": 0.6811033836324691,
      "learning_rate": 1.5257829954270868e-05,
      "loss": 0.5764,
      "step": 3822
    },
    {
      "epoch": 0.3441896058880461,
      "grad_norm": 0.6680755367602073,
      "learning_rate": 1.5255349179747114e-05,
      "loss": 0.5294,
      "step": 3823
    },
    {
      "epoch": 0.34427963717391796,
      "grad_norm": 0.6283610594316641,
      "learning_rate": 1.52528679583052e-05,
      "loss": 0.5385,
      "step": 3824
    },
    {
      "epoch": 0.3443696684597898,
      "grad_norm": 0.9292776589650622,
      "learning_rate": 1.5250386290156132e-05,
      "loss": 0.5555,
      "step": 3825
    },
    {
      "epoch": 0.3444596997456616,
      "grad_norm": 0.8819654554824786,
      "learning_rate": 1.5247904175510955e-05,
      "loss": 0.5419,
      "step": 3826
    },
    {
      "epoch": 0.34454973103153347,
      "grad_norm": 0.8023092300159719,
      "learning_rate": 1.5245421614580749e-05,
      "loss": 0.5338,
      "step": 3827
    },
    {
      "epoch": 0.3446397623174053,
      "grad_norm": 1.1041313440383251,
      "learning_rate": 1.524293860757663e-05,
      "loss": 0.5398,
      "step": 3828
    },
    {
      "epoch": 0.34472979360327716,
      "grad_norm": 0.647322298519605,
      "learning_rate": 1.5240455154709757e-05,
      "loss": 0.5413,
      "step": 3829
    },
    {
      "epoch": 0.344819824889149,
      "grad_norm": 0.7057370513410121,
      "learning_rate": 1.5237971256191325e-05,
      "loss": 0.5217,
      "step": 3830
    },
    {
      "epoch": 0.34490985617502085,
      "grad_norm": 0.7742200856127812,
      "learning_rate": 1.5235486912232561e-05,
      "loss": 0.5599,
      "step": 3831
    },
    {
      "epoch": 0.34499988746089266,
      "grad_norm": 0.7961745648697885,
      "learning_rate": 1.5233002123044743e-05,
      "loss": 0.6294,
      "step": 3832
    },
    {
      "epoch": 0.3450899187467645,
      "grad_norm": 0.7860604522500758,
      "learning_rate": 1.5230516888839173e-05,
      "loss": 0.6169,
      "step": 3833
    },
    {
      "epoch": 0.34517995003263635,
      "grad_norm": 0.8341154383863054,
      "learning_rate": 1.52280312098272e-05,
      "loss": 0.5938,
      "step": 3834
    },
    {
      "epoch": 0.34526998131850817,
      "grad_norm": 0.8064298139340863,
      "learning_rate": 1.5225545086220205e-05,
      "loss": 0.5109,
      "step": 3835
    },
    {
      "epoch": 0.34536001260438004,
      "grad_norm": 0.7853605102142536,
      "learning_rate": 1.5223058518229614e-05,
      "loss": 0.5485,
      "step": 3836
    },
    {
      "epoch": 0.34545004389025186,
      "grad_norm": 1.0283568626299544,
      "learning_rate": 1.5220571506066884e-05,
      "loss": 0.471,
      "step": 3837
    },
    {
      "epoch": 0.34554007517612373,
      "grad_norm": 0.7656029088333662,
      "learning_rate": 1.5218084049943508e-05,
      "loss": 0.5534,
      "step": 3838
    },
    {
      "epoch": 0.34563010646199555,
      "grad_norm": 0.688018161368778,
      "learning_rate": 1.5215596150071024e-05,
      "loss": 0.5851,
      "step": 3839
    },
    {
      "epoch": 0.34572013774786736,
      "grad_norm": 0.7658268798893819,
      "learning_rate": 1.5213107806661007e-05,
      "loss": 0.5043,
      "step": 3840
    },
    {
      "epoch": 0.34581016903373923,
      "grad_norm": 0.7492722781180907,
      "learning_rate": 1.5210619019925066e-05,
      "loss": 0.6168,
      "step": 3841
    },
    {
      "epoch": 0.34590020031961105,
      "grad_norm": 0.664998525275791,
      "learning_rate": 1.5208129790074844e-05,
      "loss": 0.5634,
      "step": 3842
    },
    {
      "epoch": 0.3459902316054829,
      "grad_norm": 0.6142529308578262,
      "learning_rate": 1.5205640117322032e-05,
      "loss": 0.5665,
      "step": 3843
    },
    {
      "epoch": 0.34608026289135474,
      "grad_norm": 0.7761638455476264,
      "learning_rate": 1.5203150001878355e-05,
      "loss": 0.6211,
      "step": 3844
    },
    {
      "epoch": 0.3461702941772266,
      "grad_norm": 0.9058396182451242,
      "learning_rate": 1.5200659443955568e-05,
      "loss": 0.5323,
      "step": 3845
    },
    {
      "epoch": 0.34626032546309843,
      "grad_norm": 0.594945666562359,
      "learning_rate": 1.5198168443765476e-05,
      "loss": 0.5403,
      "step": 3846
    },
    {
      "epoch": 0.34635035674897025,
      "grad_norm": 0.7126227390836927,
      "learning_rate": 1.5195677001519909e-05,
      "loss": 0.5286,
      "step": 3847
    },
    {
      "epoch": 0.3464403880348421,
      "grad_norm": 0.8522444453282372,
      "learning_rate": 1.5193185117430745e-05,
      "loss": 0.4909,
      "step": 3848
    },
    {
      "epoch": 0.34653041932071393,
      "grad_norm": 0.8453037580123021,
      "learning_rate": 1.5190692791709891e-05,
      "loss": 0.5698,
      "step": 3849
    },
    {
      "epoch": 0.3466204506065858,
      "grad_norm": 0.6797881825365951,
      "learning_rate": 1.5188200024569302e-05,
      "loss": 0.6344,
      "step": 3850
    },
    {
      "epoch": 0.3467104818924576,
      "grad_norm": 0.7180301213782094,
      "learning_rate": 1.5185706816220959e-05,
      "loss": 0.587,
      "step": 3851
    },
    {
      "epoch": 0.3468005131783295,
      "grad_norm": 0.7101692681233999,
      "learning_rate": 1.518321316687689e-05,
      "loss": 0.5708,
      "step": 3852
    },
    {
      "epoch": 0.3468905444642013,
      "grad_norm": 1.0347769650241665,
      "learning_rate": 1.5180719076749153e-05,
      "loss": 0.6381,
      "step": 3853
    },
    {
      "epoch": 0.34698057575007313,
      "grad_norm": 0.778366532300099,
      "learning_rate": 1.5178224546049851e-05,
      "loss": 0.6424,
      "step": 3854
    },
    {
      "epoch": 0.347070607035945,
      "grad_norm": 0.671129642063251,
      "learning_rate": 1.5175729574991118e-05,
      "loss": 0.6088,
      "step": 3855
    },
    {
      "epoch": 0.3471606383218168,
      "grad_norm": 0.8408714859781452,
      "learning_rate": 1.5173234163785127e-05,
      "loss": 0.5726,
      "step": 3856
    },
    {
      "epoch": 0.3472506696076887,
      "grad_norm": 0.8139839984354702,
      "learning_rate": 1.5170738312644092e-05,
      "loss": 0.6088,
      "step": 3857
    },
    {
      "epoch": 0.3473407008935605,
      "grad_norm": 0.6211063664050605,
      "learning_rate": 1.516824202178026e-05,
      "loss": 0.5794,
      "step": 3858
    },
    {
      "epoch": 0.3474307321794324,
      "grad_norm": 0.7174147568299863,
      "learning_rate": 1.5165745291405913e-05,
      "loss": 0.5187,
      "step": 3859
    },
    {
      "epoch": 0.3475207634653042,
      "grad_norm": 0.9282874989787343,
      "learning_rate": 1.5163248121733385e-05,
      "loss": 0.5112,
      "step": 3860
    },
    {
      "epoch": 0.347610794751176,
      "grad_norm": 0.7989444436758021,
      "learning_rate": 1.5160750512975027e-05,
      "loss": 0.5834,
      "step": 3861
    },
    {
      "epoch": 0.3477008260370479,
      "grad_norm": 0.6701222328026785,
      "learning_rate": 1.5158252465343242e-05,
      "loss": 0.4994,
      "step": 3862
    },
    {
      "epoch": 0.3477908573229197,
      "grad_norm": 0.7534460749930006,
      "learning_rate": 1.5155753979050465e-05,
      "loss": 0.6208,
      "step": 3863
    },
    {
      "epoch": 0.34788088860879157,
      "grad_norm": 0.8667369304515027,
      "learning_rate": 1.5153255054309165e-05,
      "loss": 0.6142,
      "step": 3864
    },
    {
      "epoch": 0.3479709198946634,
      "grad_norm": 0.8851793719094004,
      "learning_rate": 1.515075569133186e-05,
      "loss": 0.4536,
      "step": 3865
    },
    {
      "epoch": 0.34806095118053526,
      "grad_norm": 0.6226592779611763,
      "learning_rate": 1.5148255890331087e-05,
      "loss": 0.5479,
      "step": 3866
    },
    {
      "epoch": 0.3481509824664071,
      "grad_norm": 0.8437713163505997,
      "learning_rate": 1.514575565151944e-05,
      "loss": 0.6208,
      "step": 3867
    },
    {
      "epoch": 0.3482410137522789,
      "grad_norm": 0.7378023447258965,
      "learning_rate": 1.5143254975109538e-05,
      "loss": 0.4474,
      "step": 3868
    },
    {
      "epoch": 0.34833104503815077,
      "grad_norm": 0.6703965910257099,
      "learning_rate": 1.5140753861314038e-05,
      "loss": 0.5457,
      "step": 3869
    },
    {
      "epoch": 0.3484210763240226,
      "grad_norm": 0.6652349232951313,
      "learning_rate": 1.5138252310345637e-05,
      "loss": 0.5325,
      "step": 3870
    },
    {
      "epoch": 0.34851110760989445,
      "grad_norm": 0.7880036069243845,
      "learning_rate": 1.5135750322417069e-05,
      "loss": 0.5421,
      "step": 3871
    },
    {
      "epoch": 0.34860113889576627,
      "grad_norm": 0.7110379507396365,
      "learning_rate": 1.5133247897741106e-05,
      "loss": 0.6157,
      "step": 3872
    },
    {
      "epoch": 0.34869117018163814,
      "grad_norm": 0.70654885247011,
      "learning_rate": 1.5130745036530554e-05,
      "loss": 0.5469,
      "step": 3873
    },
    {
      "epoch": 0.34878120146750996,
      "grad_norm": 0.6788930599281322,
      "learning_rate": 1.5128241738998254e-05,
      "loss": 0.522,
      "step": 3874
    },
    {
      "epoch": 0.3488712327533818,
      "grad_norm": 0.821018740600321,
      "learning_rate": 1.51257380053571e-05,
      "loss": 0.5488,
      "step": 3875
    },
    {
      "epoch": 0.34896126403925365,
      "grad_norm": 0.6634349715657558,
      "learning_rate": 1.512323383582e-05,
      "loss": 0.4842,
      "step": 3876
    },
    {
      "epoch": 0.34905129532512547,
      "grad_norm": 0.740478311189687,
      "learning_rate": 1.512072923059991e-05,
      "loss": 0.5603,
      "step": 3877
    },
    {
      "epoch": 0.34914132661099734,
      "grad_norm": 0.6905493809191036,
      "learning_rate": 1.511822418990983e-05,
      "loss": 0.5977,
      "step": 3878
    },
    {
      "epoch": 0.34923135789686915,
      "grad_norm": 0.7175920793166988,
      "learning_rate": 1.5115718713962786e-05,
      "loss": 0.5656,
      "step": 3879
    },
    {
      "epoch": 0.349321389182741,
      "grad_norm": 0.5452921601339253,
      "learning_rate": 1.5113212802971844e-05,
      "loss": 0.5048,
      "step": 3880
    },
    {
      "epoch": 0.34941142046861284,
      "grad_norm": 0.7425115472931031,
      "learning_rate": 1.5110706457150111e-05,
      "loss": 0.5985,
      "step": 3881
    },
    {
      "epoch": 0.34950145175448466,
      "grad_norm": 0.7080743868127539,
      "learning_rate": 1.5108199676710732e-05,
      "loss": 0.5325,
      "step": 3882
    },
    {
      "epoch": 0.34959148304035653,
      "grad_norm": 0.8205880090224681,
      "learning_rate": 1.5105692461866874e-05,
      "loss": 0.4505,
      "step": 3883
    },
    {
      "epoch": 0.34968151432622835,
      "grad_norm": 0.7033632957715243,
      "learning_rate": 1.5103184812831764e-05,
      "loss": 0.4988,
      "step": 3884
    },
    {
      "epoch": 0.3497715456121002,
      "grad_norm": 0.9012392520164713,
      "learning_rate": 1.5100676729818645e-05,
      "loss": 0.5782,
      "step": 3885
    },
    {
      "epoch": 0.34986157689797204,
      "grad_norm": 0.7007768470277717,
      "learning_rate": 1.5098168213040812e-05,
      "loss": 0.5188,
      "step": 3886
    },
    {
      "epoch": 0.3499516081838439,
      "grad_norm": 0.7152259540206177,
      "learning_rate": 1.5095659262711588e-05,
      "loss": 0.5531,
      "step": 3887
    },
    {
      "epoch": 0.3500416394697157,
      "grad_norm": 0.6314055973443176,
      "learning_rate": 1.5093149879044333e-05,
      "loss": 0.6412,
      "step": 3888
    },
    {
      "epoch": 0.35013167075558754,
      "grad_norm": 0.6428424668395417,
      "learning_rate": 1.5090640062252455e-05,
      "loss": 0.6119,
      "step": 3889
    },
    {
      "epoch": 0.3502217020414594,
      "grad_norm": 0.6802184159013717,
      "learning_rate": 1.5088129812549382e-05,
      "loss": 0.5863,
      "step": 3890
    },
    {
      "epoch": 0.35031173332733123,
      "grad_norm": 0.8350449017572815,
      "learning_rate": 1.508561913014859e-05,
      "loss": 0.6132,
      "step": 3891
    },
    {
      "epoch": 0.3504017646132031,
      "grad_norm": 0.6684666837312927,
      "learning_rate": 1.508310801526359e-05,
      "loss": 0.5414,
      "step": 3892
    },
    {
      "epoch": 0.3504917958990749,
      "grad_norm": 0.7373193348801741,
      "learning_rate": 1.5080596468107928e-05,
      "loss": 0.555,
      "step": 3893
    },
    {
      "epoch": 0.3505818271849468,
      "grad_norm": 0.650942722810677,
      "learning_rate": 1.5078084488895188e-05,
      "loss": 0.5437,
      "step": 3894
    },
    {
      "epoch": 0.3506718584708186,
      "grad_norm": 0.6483708230765797,
      "learning_rate": 1.5075572077838989e-05,
      "loss": 0.5347,
      "step": 3895
    },
    {
      "epoch": 0.3507618897566904,
      "grad_norm": 0.9276401870843968,
      "learning_rate": 1.5073059235152991e-05,
      "loss": 0.5415,
      "step": 3896
    },
    {
      "epoch": 0.3508519210425623,
      "grad_norm": 0.6508110781858626,
      "learning_rate": 1.5070545961050886e-05,
      "loss": 0.5581,
      "step": 3897
    },
    {
      "epoch": 0.3509419523284341,
      "grad_norm": 0.6573811304760233,
      "learning_rate": 1.50680322557464e-05,
      "loss": 0.4908,
      "step": 3898
    },
    {
      "epoch": 0.351031983614306,
      "grad_norm": 0.6689427289817165,
      "learning_rate": 1.5065518119453307e-05,
      "loss": 0.4698,
      "step": 3899
    },
    {
      "epoch": 0.3511220149001778,
      "grad_norm": 0.5843212484286661,
      "learning_rate": 1.5063003552385409e-05,
      "loss": 0.4816,
      "step": 3900
    },
    {
      "epoch": 0.3512120461860497,
      "grad_norm": 0.7334147554888151,
      "learning_rate": 1.5060488554756541e-05,
      "loss": 0.6295,
      "step": 3901
    },
    {
      "epoch": 0.3513020774719215,
      "grad_norm": 0.6489999994878809,
      "learning_rate": 1.5057973126780585e-05,
      "loss": 0.6003,
      "step": 3902
    },
    {
      "epoch": 0.3513921087577933,
      "grad_norm": 0.7761256580659098,
      "learning_rate": 1.5055457268671455e-05,
      "loss": 0.5416,
      "step": 3903
    },
    {
      "epoch": 0.3514821400436652,
      "grad_norm": 0.6157674809213259,
      "learning_rate": 1.50529409806431e-05,
      "loss": 0.5223,
      "step": 3904
    },
    {
      "epoch": 0.351572171329537,
      "grad_norm": 0.6568493352944815,
      "learning_rate": 1.5050424262909506e-05,
      "loss": 0.4886,
      "step": 3905
    },
    {
      "epoch": 0.35166220261540887,
      "grad_norm": 0.7788046793405823,
      "learning_rate": 1.5047907115684695e-05,
      "loss": 0.5986,
      "step": 3906
    },
    {
      "epoch": 0.3517522339012807,
      "grad_norm": 0.6991095549111539,
      "learning_rate": 1.504538953918273e-05,
      "loss": 0.5484,
      "step": 3907
    },
    {
      "epoch": 0.35184226518715256,
      "grad_norm": 0.8074880329529194,
      "learning_rate": 1.5042871533617706e-05,
      "loss": 0.5677,
      "step": 3908
    },
    {
      "epoch": 0.3519322964730244,
      "grad_norm": 0.6500633403714531,
      "learning_rate": 1.5040353099203756e-05,
      "loss": 0.6057,
      "step": 3909
    },
    {
      "epoch": 0.3520223277588962,
      "grad_norm": 1.1402313118101484,
      "learning_rate": 1.5037834236155049e-05,
      "loss": 0.6611,
      "step": 3910
    },
    {
      "epoch": 0.35211235904476806,
      "grad_norm": 0.838902886505234,
      "learning_rate": 1.5035314944685788e-05,
      "loss": 0.4628,
      "step": 3911
    },
    {
      "epoch": 0.3522023903306399,
      "grad_norm": 0.7687008455869215,
      "learning_rate": 1.5032795225010217e-05,
      "loss": 0.5461,
      "step": 3912
    },
    {
      "epoch": 0.35229242161651175,
      "grad_norm": 0.7086146893958233,
      "learning_rate": 1.5030275077342618e-05,
      "loss": 0.5347,
      "step": 3913
    },
    {
      "epoch": 0.35238245290238357,
      "grad_norm": 0.7961217556892914,
      "learning_rate": 1.50277545018973e-05,
      "loss": 0.5638,
      "step": 3914
    },
    {
      "epoch": 0.35247248418825544,
      "grad_norm": 0.8007333698474414,
      "learning_rate": 1.5025233498888618e-05,
      "loss": 0.6512,
      "step": 3915
    },
    {
      "epoch": 0.35256251547412726,
      "grad_norm": 0.7644207417309681,
      "learning_rate": 1.502271206853096e-05,
      "loss": 0.5918,
      "step": 3916
    },
    {
      "epoch": 0.3526525467599991,
      "grad_norm": 0.6272580110110885,
      "learning_rate": 1.502019021103875e-05,
      "loss": 0.4772,
      "step": 3917
    },
    {
      "epoch": 0.35274257804587095,
      "grad_norm": 0.5260285349935441,
      "learning_rate": 1.5017667926626445e-05,
      "loss": 0.4464,
      "step": 3918
    },
    {
      "epoch": 0.35283260933174276,
      "grad_norm": 0.7425142643677283,
      "learning_rate": 1.5015145215508544e-05,
      "loss": 0.5332,
      "step": 3919
    },
    {
      "epoch": 0.35292264061761464,
      "grad_norm": 0.7232850046572957,
      "learning_rate": 1.5012622077899578e-05,
      "loss": 0.5348,
      "step": 3920
    },
    {
      "epoch": 0.35301267190348645,
      "grad_norm": 0.7127743809896421,
      "learning_rate": 1.5010098514014123e-05,
      "loss": 0.5668,
      "step": 3921
    },
    {
      "epoch": 0.3531027031893583,
      "grad_norm": 0.6613915141715057,
      "learning_rate": 1.5007574524066776e-05,
      "loss": 0.5521,
      "step": 3922
    },
    {
      "epoch": 0.35319273447523014,
      "grad_norm": 0.8156133659992091,
      "learning_rate": 1.5005050108272181e-05,
      "loss": 0.6028,
      "step": 3923
    },
    {
      "epoch": 0.35328276576110196,
      "grad_norm": 0.6151841491655448,
      "learning_rate": 1.500252526684502e-05,
      "loss": 0.4953,
      "step": 3924
    },
    {
      "epoch": 0.35337279704697383,
      "grad_norm": 0.6346436996291495,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.6833,
      "step": 3925
    },
    {
      "epoch": 0.35346282833284565,
      "grad_norm": 0.7862722871576374,
      "learning_rate": 1.499747430795188e-05,
      "loss": 0.5307,
      "step": 3926
    },
    {
      "epoch": 0.3535528596187175,
      "grad_norm": 0.6892490739729878,
      "learning_rate": 1.499494819091544e-05,
      "loss": 0.5983,
      "step": 3927
    },
    {
      "epoch": 0.35364289090458934,
      "grad_norm": 0.8171176282883758,
      "learning_rate": 1.4992421649105505e-05,
      "loss": 0.6116,
      "step": 3928
    },
    {
      "epoch": 0.3537329221904612,
      "grad_norm": 0.7248658827254338,
      "learning_rate": 1.498989468273693e-05,
      "loss": 0.5348,
      "step": 3929
    },
    {
      "epoch": 0.353822953476333,
      "grad_norm": 0.8394790664203822,
      "learning_rate": 1.4987367292024613e-05,
      "loss": 0.5578,
      "step": 3930
    },
    {
      "epoch": 0.35391298476220484,
      "grad_norm": 0.7213536558332518,
      "learning_rate": 1.4984839477183488e-05,
      "loss": 0.608,
      "step": 3931
    },
    {
      "epoch": 0.3540030160480767,
      "grad_norm": 0.7104128053598228,
      "learning_rate": 1.4982311238428518e-05,
      "loss": 0.5297,
      "step": 3932
    },
    {
      "epoch": 0.35409304733394853,
      "grad_norm": 0.7666772620712893,
      "learning_rate": 1.4979782575974704e-05,
      "loss": 0.5502,
      "step": 3933
    },
    {
      "epoch": 0.3541830786198204,
      "grad_norm": 0.7796088994009162,
      "learning_rate": 1.497725349003709e-05,
      "loss": 0.6229,
      "step": 3934
    },
    {
      "epoch": 0.3542731099056922,
      "grad_norm": 0.9149563879657022,
      "learning_rate": 1.4974723980830748e-05,
      "loss": 0.5586,
      "step": 3935
    },
    {
      "epoch": 0.3543631411915641,
      "grad_norm": 0.8254268962559551,
      "learning_rate": 1.4972194048570788e-05,
      "loss": 0.6015,
      "step": 3936
    },
    {
      "epoch": 0.3544531724774359,
      "grad_norm": 0.7560713011302055,
      "learning_rate": 1.4969663693472358e-05,
      "loss": 0.6492,
      "step": 3937
    },
    {
      "epoch": 0.3545432037633077,
      "grad_norm": 0.7659889493297154,
      "learning_rate": 1.4967132915750643e-05,
      "loss": 0.5575,
      "step": 3938
    },
    {
      "epoch": 0.3546332350491796,
      "grad_norm": 0.6709862109634025,
      "learning_rate": 1.4964601715620863e-05,
      "loss": 0.558,
      "step": 3939
    },
    {
      "epoch": 0.3547232663350514,
      "grad_norm": 0.749915111483068,
      "learning_rate": 1.4962070093298267e-05,
      "loss": 0.586,
      "step": 3940
    },
    {
      "epoch": 0.3548132976209233,
      "grad_norm": 0.7199237571813615,
      "learning_rate": 1.4959538048998153e-05,
      "loss": 0.5226,
      "step": 3941
    },
    {
      "epoch": 0.3549033289067951,
      "grad_norm": 0.6953475243600464,
      "learning_rate": 1.495700558293584e-05,
      "loss": 0.5508,
      "step": 3942
    },
    {
      "epoch": 0.354993360192667,
      "grad_norm": 0.5866840462260806,
      "learning_rate": 1.4954472695326694e-05,
      "loss": 0.5761,
      "step": 3943
    },
    {
      "epoch": 0.3550833914785388,
      "grad_norm": 0.557294319113468,
      "learning_rate": 1.4951939386386113e-05,
      "loss": 0.4091,
      "step": 3944
    },
    {
      "epoch": 0.3551734227644106,
      "grad_norm": 0.7111057474118729,
      "learning_rate": 1.4949405656329537e-05,
      "loss": 0.5673,
      "step": 3945
    },
    {
      "epoch": 0.3552634540502825,
      "grad_norm": 0.8324189441094995,
      "learning_rate": 1.4946871505372426e-05,
      "loss": 0.573,
      "step": 3946
    },
    {
      "epoch": 0.3553534853361543,
      "grad_norm": 0.735739099787992,
      "learning_rate": 1.4944336933730292e-05,
      "loss": 0.5846,
      "step": 3947
    },
    {
      "epoch": 0.35544351662202617,
      "grad_norm": 0.7330262875336208,
      "learning_rate": 1.4941801941618676e-05,
      "loss": 0.5125,
      "step": 3948
    },
    {
      "epoch": 0.355533547907898,
      "grad_norm": 0.7313675678713764,
      "learning_rate": 1.4939266529253153e-05,
      "loss": 0.5272,
      "step": 3949
    },
    {
      "epoch": 0.35562357919376986,
      "grad_norm": 0.759367197044653,
      "learning_rate": 1.4936730696849338e-05,
      "loss": 0.6214,
      "step": 3950
    },
    {
      "epoch": 0.3557136104796417,
      "grad_norm": 0.627249262037424,
      "learning_rate": 1.4934194444622877e-05,
      "loss": 0.5731,
      "step": 3951
    },
    {
      "epoch": 0.3558036417655135,
      "grad_norm": 0.6826432821961115,
      "learning_rate": 1.4931657772789458e-05,
      "loss": 0.4957,
      "step": 3952
    },
    {
      "epoch": 0.35589367305138536,
      "grad_norm": 0.6636328639053466,
      "learning_rate": 1.4929120681564802e-05,
      "loss": 0.5711,
      "step": 3953
    },
    {
      "epoch": 0.3559837043372572,
      "grad_norm": 0.6621819954914026,
      "learning_rate": 1.4926583171164658e-05,
      "loss": 0.6132,
      "step": 3954
    },
    {
      "epoch": 0.35607373562312905,
      "grad_norm": 0.616779340292264,
      "learning_rate": 1.4924045241804826e-05,
      "loss": 0.5618,
      "step": 3955
    },
    {
      "epoch": 0.35616376690900087,
      "grad_norm": 0.8908589568298293,
      "learning_rate": 1.4921506893701128e-05,
      "loss": 0.5123,
      "step": 3956
    },
    {
      "epoch": 0.35625379819487274,
      "grad_norm": 0.748810441655897,
      "learning_rate": 1.4918968127069426e-05,
      "loss": 0.554,
      "step": 3957
    },
    {
      "epoch": 0.35634382948074456,
      "grad_norm": 0.6933873905531543,
      "learning_rate": 1.4916428942125621e-05,
      "loss": 0.6404,
      "step": 3958
    },
    {
      "epoch": 0.3564338607666164,
      "grad_norm": 0.9170824007146573,
      "learning_rate": 1.4913889339085647e-05,
      "loss": 0.6332,
      "step": 3959
    },
    {
      "epoch": 0.35652389205248824,
      "grad_norm": 0.659555339335281,
      "learning_rate": 1.4911349318165472e-05,
      "loss": 0.5432,
      "step": 3960
    },
    {
      "epoch": 0.35661392333836006,
      "grad_norm": 0.7300442199016262,
      "learning_rate": 1.49088088795811e-05,
      "loss": 0.5303,
      "step": 3961
    },
    {
      "epoch": 0.35670395462423193,
      "grad_norm": 0.7637005885690729,
      "learning_rate": 1.4906268023548575e-05,
      "loss": 0.5197,
      "step": 3962
    },
    {
      "epoch": 0.35679398591010375,
      "grad_norm": 0.7120502413057106,
      "learning_rate": 1.4903726750283969e-05,
      "loss": 0.5467,
      "step": 3963
    },
    {
      "epoch": 0.3568840171959756,
      "grad_norm": 1.0769693997390883,
      "learning_rate": 1.4901185060003395e-05,
      "loss": 0.5801,
      "step": 3964
    },
    {
      "epoch": 0.35697404848184744,
      "grad_norm": 0.6603801947090745,
      "learning_rate": 1.4898642952923e-05,
      "loss": 0.5773,
      "step": 3965
    },
    {
      "epoch": 0.35706407976771926,
      "grad_norm": 0.8324215000465627,
      "learning_rate": 1.489610042925897e-05,
      "loss": 0.6135,
      "step": 3966
    },
    {
      "epoch": 0.35715411105359113,
      "grad_norm": 0.674080288479018,
      "learning_rate": 1.4893557489227518e-05,
      "loss": 0.5436,
      "step": 3967
    },
    {
      "epoch": 0.35724414233946294,
      "grad_norm": 0.7057260735491769,
      "learning_rate": 1.4891014133044895e-05,
      "loss": 0.5482,
      "step": 3968
    },
    {
      "epoch": 0.3573341736253348,
      "grad_norm": 0.8004277604670068,
      "learning_rate": 1.4888470360927396e-05,
      "loss": 0.6277,
      "step": 3969
    },
    {
      "epoch": 0.35742420491120663,
      "grad_norm": 0.8066171161453483,
      "learning_rate": 1.488592617309134e-05,
      "loss": 0.5407,
      "step": 3970
    },
    {
      "epoch": 0.3575142361970785,
      "grad_norm": 0.8047994886656747,
      "learning_rate": 1.488338156975309e-05,
      "loss": 0.518,
      "step": 3971
    },
    {
      "epoch": 0.3576042674829503,
      "grad_norm": 0.6213000030625947,
      "learning_rate": 1.4880836551129038e-05,
      "loss": 0.6049,
      "step": 3972
    },
    {
      "epoch": 0.35769429876882214,
      "grad_norm": 0.7532486566289577,
      "learning_rate": 1.4878291117435617e-05,
      "loss": 0.5601,
      "step": 3973
    },
    {
      "epoch": 0.357784330054694,
      "grad_norm": 0.7108430366348436,
      "learning_rate": 1.487574526888929e-05,
      "loss": 0.5791,
      "step": 3974
    },
    {
      "epoch": 0.3578743613405658,
      "grad_norm": 0.6863030587426623,
      "learning_rate": 1.4873199005706555e-05,
      "loss": 0.5595,
      "step": 3975
    },
    {
      "epoch": 0.3579643926264377,
      "grad_norm": 0.7018851283399977,
      "learning_rate": 1.4870652328103953e-05,
      "loss": 0.5586,
      "step": 3976
    },
    {
      "epoch": 0.3580544239123095,
      "grad_norm": 0.6448905279481241,
      "learning_rate": 1.4868105236298053e-05,
      "loss": 0.5203,
      "step": 3977
    },
    {
      "epoch": 0.3581444551981814,
      "grad_norm": 0.9243606084565785,
      "learning_rate": 1.486555773050546e-05,
      "loss": 0.5638,
      "step": 3978
    },
    {
      "epoch": 0.3582344864840532,
      "grad_norm": 0.7042708763137857,
      "learning_rate": 1.4863009810942814e-05,
      "loss": 0.6686,
      "step": 3979
    },
    {
      "epoch": 0.358324517769925,
      "grad_norm": 0.9262550257318443,
      "learning_rate": 1.48604614778268e-05,
      "loss": 0.5559,
      "step": 3980
    },
    {
      "epoch": 0.3584145490557969,
      "grad_norm": 1.0388800203360589,
      "learning_rate": 1.4857912731374119e-05,
      "loss": 0.5897,
      "step": 3981
    },
    {
      "epoch": 0.3585045803416687,
      "grad_norm": 0.6587299114703528,
      "learning_rate": 1.4855363571801523e-05,
      "loss": 0.5642,
      "step": 3982
    },
    {
      "epoch": 0.3585946116275406,
      "grad_norm": 1.135988312530086,
      "learning_rate": 1.4852813999325797e-05,
      "loss": 0.6875,
      "step": 3983
    },
    {
      "epoch": 0.3586846429134124,
      "grad_norm": 0.8167728214337237,
      "learning_rate": 1.4850264014163752e-05,
      "loss": 0.5727,
      "step": 3984
    },
    {
      "epoch": 0.35877467419928427,
      "grad_norm": 0.7507955566856325,
      "learning_rate": 1.4847713616532244e-05,
      "loss": 0.6549,
      "step": 3985
    },
    {
      "epoch": 0.3588647054851561,
      "grad_norm": 0.7902777425233666,
      "learning_rate": 1.4845162806648159e-05,
      "loss": 0.5784,
      "step": 3986
    },
    {
      "epoch": 0.3589547367710279,
      "grad_norm": 0.7334118115075394,
      "learning_rate": 1.4842611584728423e-05,
      "loss": 0.5824,
      "step": 3987
    },
    {
      "epoch": 0.3590447680568998,
      "grad_norm": 0.714061955707572,
      "learning_rate": 1.4840059950989992e-05,
      "loss": 0.5848,
      "step": 3988
    },
    {
      "epoch": 0.3591347993427716,
      "grad_norm": 1.0421045939256692,
      "learning_rate": 1.4837507905649852e-05,
      "loss": 0.5558,
      "step": 3989
    },
    {
      "epoch": 0.35922483062864347,
      "grad_norm": 0.7219396037217872,
      "learning_rate": 1.4834955448925041e-05,
      "loss": 0.6406,
      "step": 3990
    },
    {
      "epoch": 0.3593148619145153,
      "grad_norm": 0.6655694054252177,
      "learning_rate": 1.4832402581032614e-05,
      "loss": 0.632,
      "step": 3991
    },
    {
      "epoch": 0.35940489320038715,
      "grad_norm": 0.6491381289002547,
      "learning_rate": 1.4829849302189669e-05,
      "loss": 0.5527,
      "step": 3992
    },
    {
      "epoch": 0.35949492448625897,
      "grad_norm": 0.770117578664263,
      "learning_rate": 1.4827295612613344e-05,
      "loss": 0.535,
      "step": 3993
    },
    {
      "epoch": 0.35958495577213084,
      "grad_norm": 0.8559991766890479,
      "learning_rate": 1.4824741512520801e-05,
      "loss": 0.5488,
      "step": 3994
    },
    {
      "epoch": 0.35967498705800266,
      "grad_norm": 0.718898726297813,
      "learning_rate": 1.4822187002129244e-05,
      "loss": 0.6367,
      "step": 3995
    },
    {
      "epoch": 0.3597650183438745,
      "grad_norm": 0.5643080464839195,
      "learning_rate": 1.481963208165591e-05,
      "loss": 0.5746,
      "step": 3996
    },
    {
      "epoch": 0.35985504962974635,
      "grad_norm": 0.7783119171835077,
      "learning_rate": 1.4817076751318074e-05,
      "loss": 0.6324,
      "step": 3997
    },
    {
      "epoch": 0.35994508091561817,
      "grad_norm": 0.6501228307658364,
      "learning_rate": 1.4814521011333036e-05,
      "loss": 0.501,
      "step": 3998
    },
    {
      "epoch": 0.36003511220149004,
      "grad_norm": 0.6585703857959727,
      "learning_rate": 1.4811964861918142e-05,
      "loss": 0.6246,
      "step": 3999
    },
    {
      "epoch": 0.36012514348736185,
      "grad_norm": 0.8484260694490955,
      "learning_rate": 1.4809408303290768e-05,
      "loss": 0.6517,
      "step": 4000
    },
    {
      "epoch": 0.3602151747732337,
      "grad_norm": 0.8333761741347951,
      "learning_rate": 1.4806851335668328e-05,
      "loss": 0.5693,
      "step": 4001
    },
    {
      "epoch": 0.36030520605910554,
      "grad_norm": 0.8275875342082081,
      "learning_rate": 1.4804293959268263e-05,
      "loss": 0.5981,
      "step": 4002
    },
    {
      "epoch": 0.36039523734497736,
      "grad_norm": 0.9679569390300515,
      "learning_rate": 1.4801736174308058e-05,
      "loss": 0.5743,
      "step": 4003
    },
    {
      "epoch": 0.36048526863084923,
      "grad_norm": 0.6993784529069415,
      "learning_rate": 1.4799177981005229e-05,
      "loss": 0.5612,
      "step": 4004
    },
    {
      "epoch": 0.36057529991672105,
      "grad_norm": 0.938400305345404,
      "learning_rate": 1.4796619379577322e-05,
      "loss": 0.6136,
      "step": 4005
    },
    {
      "epoch": 0.3606653312025929,
      "grad_norm": 0.6493991269840196,
      "learning_rate": 1.4794060370241923e-05,
      "loss": 0.535,
      "step": 4006
    },
    {
      "epoch": 0.36075536248846474,
      "grad_norm": 0.5707756659502404,
      "learning_rate": 1.4791500953216656e-05,
      "loss": 0.471,
      "step": 4007
    },
    {
      "epoch": 0.3608453937743366,
      "grad_norm": 0.8217136726235819,
      "learning_rate": 1.4788941128719173e-05,
      "loss": 0.5597,
      "step": 4008
    },
    {
      "epoch": 0.3609354250602084,
      "grad_norm": 0.6444379778653584,
      "learning_rate": 1.478638089696716e-05,
      "loss": 0.4772,
      "step": 4009
    },
    {
      "epoch": 0.36102545634608024,
      "grad_norm": 0.6211297661986764,
      "learning_rate": 1.4783820258178345e-05,
      "loss": 0.5484,
      "step": 4010
    },
    {
      "epoch": 0.3611154876319521,
      "grad_norm": 0.655648347384168,
      "learning_rate": 1.4781259212570485e-05,
      "loss": 0.4958,
      "step": 4011
    },
    {
      "epoch": 0.36120551891782393,
      "grad_norm": 0.6845518427610827,
      "learning_rate": 1.4778697760361373e-05,
      "loss": 0.572,
      "step": 4012
    },
    {
      "epoch": 0.3612955502036958,
      "grad_norm": 0.6471669741699164,
      "learning_rate": 1.4776135901768833e-05,
      "loss": 0.5057,
      "step": 4013
    },
    {
      "epoch": 0.3613855814895676,
      "grad_norm": 0.7257571434963592,
      "learning_rate": 1.4773573637010735e-05,
      "loss": 0.634,
      "step": 4014
    },
    {
      "epoch": 0.3614756127754395,
      "grad_norm": 0.7168639172845946,
      "learning_rate": 1.4771010966304968e-05,
      "loss": 0.67,
      "step": 4015
    },
    {
      "epoch": 0.3615656440613113,
      "grad_norm": 0.7443714334283794,
      "learning_rate": 1.4768447889869468e-05,
      "loss": 0.5281,
      "step": 4016
    },
    {
      "epoch": 0.3616556753471831,
      "grad_norm": 0.7080427450470144,
      "learning_rate": 1.4765884407922196e-05,
      "loss": 0.5389,
      "step": 4017
    },
    {
      "epoch": 0.361745706633055,
      "grad_norm": 0.9111794438118954,
      "learning_rate": 1.4763320520681157e-05,
      "loss": 0.5706,
      "step": 4018
    },
    {
      "epoch": 0.3618357379189268,
      "grad_norm": 0.7215316136506604,
      "learning_rate": 1.4760756228364381e-05,
      "loss": 0.5086,
      "step": 4019
    },
    {
      "epoch": 0.3619257692047987,
      "grad_norm": 0.6338227200920513,
      "learning_rate": 1.475819153118994e-05,
      "loss": 0.601,
      "step": 4020
    },
    {
      "epoch": 0.3620158004906705,
      "grad_norm": 0.5573586003416169,
      "learning_rate": 1.4755626429375935e-05,
      "loss": 0.6112,
      "step": 4021
    },
    {
      "epoch": 0.3621058317765424,
      "grad_norm": 0.6518652478343575,
      "learning_rate": 1.475306092314051e-05,
      "loss": 0.4241,
      "step": 4022
    },
    {
      "epoch": 0.3621958630624142,
      "grad_norm": 0.8068161024179173,
      "learning_rate": 1.4750495012701828e-05,
      "loss": 0.5373,
      "step": 4023
    },
    {
      "epoch": 0.362285894348286,
      "grad_norm": 0.8066425946839549,
      "learning_rate": 1.4747928698278102e-05,
      "loss": 0.5874,
      "step": 4024
    },
    {
      "epoch": 0.3623759256341579,
      "grad_norm": 0.8740614768892964,
      "learning_rate": 1.4745361980087574e-05,
      "loss": 0.5314,
      "step": 4025
    },
    {
      "epoch": 0.3624659569200297,
      "grad_norm": 0.841605850708518,
      "learning_rate": 1.4742794858348512e-05,
      "loss": 0.5801,
      "step": 4026
    },
    {
      "epoch": 0.36255598820590157,
      "grad_norm": 0.7148962732895462,
      "learning_rate": 1.4740227333279232e-05,
      "loss": 0.4997,
      "step": 4027
    },
    {
      "epoch": 0.3626460194917734,
      "grad_norm": 0.5648986927749655,
      "learning_rate": 1.4737659405098077e-05,
      "loss": 0.4207,
      "step": 4028
    },
    {
      "epoch": 0.36273605077764526,
      "grad_norm": 0.7274427971001314,
      "learning_rate": 1.4735091074023425e-05,
      "loss": 0.5185,
      "step": 4029
    },
    {
      "epoch": 0.3628260820635171,
      "grad_norm": 0.8209291099908929,
      "learning_rate": 1.4732522340273686e-05,
      "loss": 0.5788,
      "step": 4030
    },
    {
      "epoch": 0.3629161133493889,
      "grad_norm": 0.684312798970206,
      "learning_rate": 1.4729953204067309e-05,
      "loss": 0.5427,
      "step": 4031
    },
    {
      "epoch": 0.36300614463526076,
      "grad_norm": 0.8084928091544669,
      "learning_rate": 1.4727383665622777e-05,
      "loss": 0.5976,
      "step": 4032
    },
    {
      "epoch": 0.3630961759211326,
      "grad_norm": 0.7245635107515733,
      "learning_rate": 1.4724813725158598e-05,
      "loss": 0.6315,
      "step": 4033
    },
    {
      "epoch": 0.36318620720700445,
      "grad_norm": 0.692413592991066,
      "learning_rate": 1.4722243382893328e-05,
      "loss": 0.5534,
      "step": 4034
    },
    {
      "epoch": 0.36327623849287627,
      "grad_norm": 0.9457741975740953,
      "learning_rate": 1.471967263904555e-05,
      "loss": 0.5791,
      "step": 4035
    },
    {
      "epoch": 0.36336626977874814,
      "grad_norm": 0.8248207624803411,
      "learning_rate": 1.4717101493833879e-05,
      "loss": 0.5719,
      "step": 4036
    },
    {
      "epoch": 0.36345630106461996,
      "grad_norm": 0.8951342245692975,
      "learning_rate": 1.4714529947476967e-05,
      "loss": 0.6465,
      "step": 4037
    },
    {
      "epoch": 0.3635463323504918,
      "grad_norm": 0.762717611838056,
      "learning_rate": 1.4711958000193502e-05,
      "loss": 0.6025,
      "step": 4038
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.8274649275169889,
      "learning_rate": 1.4709385652202204e-05,
      "loss": 0.5397,
      "step": 4039
    },
    {
      "epoch": 0.36372639492223546,
      "grad_norm": 0.713246809994466,
      "learning_rate": 1.4706812903721824e-05,
      "loss": 0.5174,
      "step": 4040
    },
    {
      "epoch": 0.36381642620810734,
      "grad_norm": 0.8499056052862073,
      "learning_rate": 1.4704239754971153e-05,
      "loss": 0.5422,
      "step": 4041
    },
    {
      "epoch": 0.36390645749397915,
      "grad_norm": 0.7189365807871169,
      "learning_rate": 1.4701666206169009e-05,
      "loss": 0.5062,
      "step": 4042
    },
    {
      "epoch": 0.363996488779851,
      "grad_norm": 0.6077423785356466,
      "learning_rate": 1.4699092257534258e-05,
      "loss": 0.5297,
      "step": 4043
    },
    {
      "epoch": 0.36408652006572284,
      "grad_norm": 0.8243611349368277,
      "learning_rate": 1.4696517909285777e-05,
      "loss": 0.6232,
      "step": 4044
    },
    {
      "epoch": 0.36417655135159466,
      "grad_norm": 0.6947490854624976,
      "learning_rate": 1.4693943161642498e-05,
      "loss": 0.5441,
      "step": 4045
    },
    {
      "epoch": 0.36426658263746653,
      "grad_norm": 0.6477846236315031,
      "learning_rate": 1.4691368014823382e-05,
      "loss": 0.5194,
      "step": 4046
    },
    {
      "epoch": 0.36435661392333835,
      "grad_norm": 0.7396608257093594,
      "learning_rate": 1.4688792469047416e-05,
      "loss": 0.5974,
      "step": 4047
    },
    {
      "epoch": 0.3644466452092102,
      "grad_norm": 0.6050626568839896,
      "learning_rate": 1.4686216524533623e-05,
      "loss": 0.5658,
      "step": 4048
    },
    {
      "epoch": 0.36453667649508203,
      "grad_norm": 0.6648392649603584,
      "learning_rate": 1.4683640181501067e-05,
      "loss": 0.53,
      "step": 4049
    },
    {
      "epoch": 0.3646267077809539,
      "grad_norm": 0.6380633069788518,
      "learning_rate": 1.4681063440168846e-05,
      "loss": 0.4833,
      "step": 4050
    },
    {
      "epoch": 0.3647167390668257,
      "grad_norm": 0.6526726930364368,
      "learning_rate": 1.467848630075608e-05,
      "loss": 0.5478,
      "step": 4051
    },
    {
      "epoch": 0.36480677035269754,
      "grad_norm": 0.6957779191782819,
      "learning_rate": 1.4675908763481934e-05,
      "loss": 0.5833,
      "step": 4052
    },
    {
      "epoch": 0.3648968016385694,
      "grad_norm": 0.954460669103796,
      "learning_rate": 1.4673330828565606e-05,
      "loss": 0.6557,
      "step": 4053
    },
    {
      "epoch": 0.36498683292444123,
      "grad_norm": 0.700178007122905,
      "learning_rate": 1.4670752496226321e-05,
      "loss": 0.5566,
      "step": 4054
    },
    {
      "epoch": 0.3650768642103131,
      "grad_norm": 0.8653344206409197,
      "learning_rate": 1.4668173766683342e-05,
      "loss": 0.5937,
      "step": 4055
    },
    {
      "epoch": 0.3651668954961849,
      "grad_norm": 0.7983977284976967,
      "learning_rate": 1.4665594640155969e-05,
      "loss": 0.5569,
      "step": 4056
    },
    {
      "epoch": 0.3652569267820568,
      "grad_norm": 0.6838688559519848,
      "learning_rate": 1.4663015116863531e-05,
      "loss": 0.5308,
      "step": 4057
    },
    {
      "epoch": 0.3653469580679286,
      "grad_norm": 0.8884101676363858,
      "learning_rate": 1.4660435197025391e-05,
      "loss": 0.5561,
      "step": 4058
    },
    {
      "epoch": 0.3654369893538004,
      "grad_norm": 0.6629791306517594,
      "learning_rate": 1.4657854880860946e-05,
      "loss": 0.4956,
      "step": 4059
    },
    {
      "epoch": 0.3655270206396723,
      "grad_norm": 0.9267818792586477,
      "learning_rate": 1.4655274168589635e-05,
      "loss": 0.6258,
      "step": 4060
    },
    {
      "epoch": 0.3656170519255441,
      "grad_norm": 0.819763275377176,
      "learning_rate": 1.4652693060430914e-05,
      "loss": 0.5,
      "step": 4061
    },
    {
      "epoch": 0.365707083211416,
      "grad_norm": 0.6360686626595905,
      "learning_rate": 1.4650111556604287e-05,
      "loss": 0.4943,
      "step": 4062
    },
    {
      "epoch": 0.3657971144972878,
      "grad_norm": 0.768647653204168,
      "learning_rate": 1.4647529657329284e-05,
      "loss": 0.5428,
      "step": 4063
    },
    {
      "epoch": 0.3658871457831597,
      "grad_norm": 0.9617261839269631,
      "learning_rate": 1.4644947362825477e-05,
      "loss": 0.691,
      "step": 4064
    },
    {
      "epoch": 0.3659771770690315,
      "grad_norm": 0.7089183520695017,
      "learning_rate": 1.4642364673312458e-05,
      "loss": 0.4329,
      "step": 4065
    },
    {
      "epoch": 0.3660672083549033,
      "grad_norm": 0.733395913752853,
      "learning_rate": 1.4639781589009867e-05,
      "loss": 0.5479,
      "step": 4066
    },
    {
      "epoch": 0.3661572396407752,
      "grad_norm": 0.7918954068589477,
      "learning_rate": 1.4637198110137369e-05,
      "loss": 0.6321,
      "step": 4067
    },
    {
      "epoch": 0.366247270926647,
      "grad_norm": 0.6850865936704583,
      "learning_rate": 1.4634614236914662e-05,
      "loss": 0.5686,
      "step": 4068
    },
    {
      "epoch": 0.36633730221251887,
      "grad_norm": 0.839631590197893,
      "learning_rate": 1.4632029969561483e-05,
      "loss": 0.5197,
      "step": 4069
    },
    {
      "epoch": 0.3664273334983907,
      "grad_norm": 0.7161607838464503,
      "learning_rate": 1.4629445308297601e-05,
      "loss": 0.4482,
      "step": 4070
    },
    {
      "epoch": 0.36651736478426256,
      "grad_norm": 0.6828807538910596,
      "learning_rate": 1.4626860253342816e-05,
      "loss": 0.5658,
      "step": 4071
    },
    {
      "epoch": 0.3666073960701344,
      "grad_norm": 0.6266761332278741,
      "learning_rate": 1.4624274804916958e-05,
      "loss": 0.6066,
      "step": 4072
    },
    {
      "epoch": 0.3666974273560062,
      "grad_norm": 0.788703358855722,
      "learning_rate": 1.4621688963239901e-05,
      "loss": 0.535,
      "step": 4073
    },
    {
      "epoch": 0.36678745864187806,
      "grad_norm": 0.9559015947347039,
      "learning_rate": 1.461910272853155e-05,
      "loss": 0.5674,
      "step": 4074
    },
    {
      "epoch": 0.3668774899277499,
      "grad_norm": 0.7474729162539819,
      "learning_rate": 1.4616516101011828e-05,
      "loss": 0.5762,
      "step": 4075
    },
    {
      "epoch": 0.36696752121362175,
      "grad_norm": 0.5933815622252369,
      "learning_rate": 1.461392908090071e-05,
      "loss": 0.5167,
      "step": 4076
    },
    {
      "epoch": 0.36705755249949357,
      "grad_norm": 0.9902057884657285,
      "learning_rate": 1.4611341668418199e-05,
      "loss": 0.5529,
      "step": 4077
    },
    {
      "epoch": 0.36714758378536544,
      "grad_norm": 0.8860509701329766,
      "learning_rate": 1.4608753863784333e-05,
      "loss": 0.6342,
      "step": 4078
    },
    {
      "epoch": 0.36723761507123726,
      "grad_norm": 0.656548226952207,
      "learning_rate": 1.4606165667219174e-05,
      "loss": 0.6209,
      "step": 4079
    },
    {
      "epoch": 0.36732764635710907,
      "grad_norm": 0.7794872889785831,
      "learning_rate": 1.4603577078942826e-05,
      "loss": 0.5901,
      "step": 4080
    },
    {
      "epoch": 0.36741767764298094,
      "grad_norm": 0.6782824483052711,
      "learning_rate": 1.4600988099175425e-05,
      "loss": 0.5876,
      "step": 4081
    },
    {
      "epoch": 0.36750770892885276,
      "grad_norm": 0.866244225747887,
      "learning_rate": 1.459839872813714e-05,
      "loss": 0.587,
      "step": 4082
    },
    {
      "epoch": 0.36759774021472463,
      "grad_norm": 0.5775915627696292,
      "learning_rate": 1.459580896604817e-05,
      "loss": 0.6094,
      "step": 4083
    },
    {
      "epoch": 0.36768777150059645,
      "grad_norm": 0.7260043273568417,
      "learning_rate": 1.4593218813128753e-05,
      "loss": 0.6105,
      "step": 4084
    },
    {
      "epoch": 0.3677778027864683,
      "grad_norm": 0.7570775106327932,
      "learning_rate": 1.4590628269599156e-05,
      "loss": 0.6189,
      "step": 4085
    },
    {
      "epoch": 0.36786783407234014,
      "grad_norm": 0.6618379467302563,
      "learning_rate": 1.4588037335679682e-05,
      "loss": 0.5538,
      "step": 4086
    },
    {
      "epoch": 0.36795786535821196,
      "grad_norm": 0.5655416092684064,
      "learning_rate": 1.458544601159066e-05,
      "loss": 0.5483,
      "step": 4087
    },
    {
      "epoch": 0.3680478966440838,
      "grad_norm": 0.6612570980285615,
      "learning_rate": 1.4582854297552467e-05,
      "loss": 0.5004,
      "step": 4088
    },
    {
      "epoch": 0.36813792792995564,
      "grad_norm": 0.9010089909249162,
      "learning_rate": 1.4580262193785495e-05,
      "loss": 0.5719,
      "step": 4089
    },
    {
      "epoch": 0.3682279592158275,
      "grad_norm": 0.6717145173274511,
      "learning_rate": 1.4577669700510182e-05,
      "loss": 0.5469,
      "step": 4090
    },
    {
      "epoch": 0.36831799050169933,
      "grad_norm": 0.7860940811326101,
      "learning_rate": 1.4575076817946996e-05,
      "loss": 0.538,
      "step": 4091
    },
    {
      "epoch": 0.3684080217875712,
      "grad_norm": 0.7847985863164534,
      "learning_rate": 1.4572483546316439e-05,
      "loss": 0.4873,
      "step": 4092
    },
    {
      "epoch": 0.368498053073443,
      "grad_norm": 0.7439093185605788,
      "learning_rate": 1.456988988583904e-05,
      "loss": 0.5752,
      "step": 4093
    },
    {
      "epoch": 0.36858808435931484,
      "grad_norm": 0.734717458008535,
      "learning_rate": 1.4567295836735366e-05,
      "loss": 0.549,
      "step": 4094
    },
    {
      "epoch": 0.3686781156451867,
      "grad_norm": 0.652976070652647,
      "learning_rate": 1.456470139922602e-05,
      "loss": 0.5396,
      "step": 4095
    },
    {
      "epoch": 0.3687681469310585,
      "grad_norm": 0.6769500361228226,
      "learning_rate": 1.4562106573531632e-05,
      "loss": 0.6312,
      "step": 4096
    },
    {
      "epoch": 0.3688581782169304,
      "grad_norm": 0.8933061780100113,
      "learning_rate": 1.4559511359872866e-05,
      "loss": 0.5948,
      "step": 4097
    },
    {
      "epoch": 0.3689482095028022,
      "grad_norm": 1.1628753003158547,
      "learning_rate": 1.4556915758470425e-05,
      "loss": 0.5957,
      "step": 4098
    },
    {
      "epoch": 0.3690382407886741,
      "grad_norm": 0.8368058442444644,
      "learning_rate": 1.455431976954504e-05,
      "loss": 0.5933,
      "step": 4099
    },
    {
      "epoch": 0.3691282720745459,
      "grad_norm": 0.6813900281854148,
      "learning_rate": 1.4551723393317472e-05,
      "loss": 0.5425,
      "step": 4100
    },
    {
      "epoch": 0.3692183033604177,
      "grad_norm": 0.7609571984505794,
      "learning_rate": 1.4549126630008519e-05,
      "loss": 0.5992,
      "step": 4101
    },
    {
      "epoch": 0.3693083346462896,
      "grad_norm": 0.6546397225448407,
      "learning_rate": 1.4546529479839016e-05,
      "loss": 0.5663,
      "step": 4102
    },
    {
      "epoch": 0.3693983659321614,
      "grad_norm": 0.7505708847570948,
      "learning_rate": 1.4543931943029822e-05,
      "loss": 0.5805,
      "step": 4103
    },
    {
      "epoch": 0.3694883972180333,
      "grad_norm": 0.6632765014221539,
      "learning_rate": 1.4541334019801832e-05,
      "loss": 0.5564,
      "step": 4104
    },
    {
      "epoch": 0.3695784285039051,
      "grad_norm": 0.6395631723606349,
      "learning_rate": 1.4538735710375975e-05,
      "loss": 0.5895,
      "step": 4105
    },
    {
      "epoch": 0.36966845978977697,
      "grad_norm": 0.9276444081160724,
      "learning_rate": 1.4536137014973219e-05,
      "loss": 0.5928,
      "step": 4106
    },
    {
      "epoch": 0.3697584910756488,
      "grad_norm": 0.6910528095318231,
      "learning_rate": 1.4533537933814554e-05,
      "loss": 0.5983,
      "step": 4107
    },
    {
      "epoch": 0.3698485223615206,
      "grad_norm": 0.7419829543224128,
      "learning_rate": 1.4530938467121007e-05,
      "loss": 0.5581,
      "step": 4108
    },
    {
      "epoch": 0.3699385536473925,
      "grad_norm": 0.6203608447372414,
      "learning_rate": 1.4528338615113642e-05,
      "loss": 0.5708,
      "step": 4109
    },
    {
      "epoch": 0.3700285849332643,
      "grad_norm": 0.5931555356011402,
      "learning_rate": 1.4525738378013545e-05,
      "loss": 0.5637,
      "step": 4110
    },
    {
      "epoch": 0.37011861621913617,
      "grad_norm": 0.6658328840324851,
      "learning_rate": 1.4523137756041847e-05,
      "loss": 0.5358,
      "step": 4111
    },
    {
      "epoch": 0.370208647505008,
      "grad_norm": 0.7051714179959399,
      "learning_rate": 1.4520536749419708e-05,
      "loss": 0.5464,
      "step": 4112
    },
    {
      "epoch": 0.37029867879087985,
      "grad_norm": 0.7658876346855498,
      "learning_rate": 1.4517935358368318e-05,
      "loss": 0.6832,
      "step": 4113
    },
    {
      "epoch": 0.37038871007675167,
      "grad_norm": 0.8211340237598391,
      "learning_rate": 1.4515333583108896e-05,
      "loss": 0.5154,
      "step": 4114
    },
    {
      "epoch": 0.3704787413626235,
      "grad_norm": 0.7859017271650343,
      "learning_rate": 1.4512731423862702e-05,
      "loss": 0.5907,
      "step": 4115
    },
    {
      "epoch": 0.37056877264849536,
      "grad_norm": 0.762813462435444,
      "learning_rate": 1.451012888085103e-05,
      "loss": 0.5944,
      "step": 4116
    },
    {
      "epoch": 0.3706588039343672,
      "grad_norm": 0.5473672519896271,
      "learning_rate": 1.4507525954295194e-05,
      "loss": 0.4946,
      "step": 4117
    },
    {
      "epoch": 0.37074883522023905,
      "grad_norm": 0.7807696227276125,
      "learning_rate": 1.4504922644416553e-05,
      "loss": 0.5689,
      "step": 4118
    },
    {
      "epoch": 0.37083886650611086,
      "grad_norm": 0.7060584868839833,
      "learning_rate": 1.4502318951436491e-05,
      "loss": 0.556,
      "step": 4119
    },
    {
      "epoch": 0.37092889779198274,
      "grad_norm": 0.7038873223968355,
      "learning_rate": 1.4499714875576434e-05,
      "loss": 0.4575,
      "step": 4120
    },
    {
      "epoch": 0.37101892907785455,
      "grad_norm": 0.9218927351985599,
      "learning_rate": 1.4497110417057827e-05,
      "loss": 0.6514,
      "step": 4121
    },
    {
      "epoch": 0.37110896036372637,
      "grad_norm": 0.8804413304352194,
      "learning_rate": 1.4494505576102156e-05,
      "loss": 0.6904,
      "step": 4122
    },
    {
      "epoch": 0.37119899164959824,
      "grad_norm": 0.7640508491289928,
      "learning_rate": 1.4491900352930944e-05,
      "loss": 0.5972,
      "step": 4123
    },
    {
      "epoch": 0.37128902293547006,
      "grad_norm": 0.7290691212142688,
      "learning_rate": 1.4489294747765735e-05,
      "loss": 0.5085,
      "step": 4124
    },
    {
      "epoch": 0.37137905422134193,
      "grad_norm": 0.8437904971332568,
      "learning_rate": 1.448668876082811e-05,
      "loss": 0.6018,
      "step": 4125
    },
    {
      "epoch": 0.37146908550721375,
      "grad_norm": 0.6389760175630195,
      "learning_rate": 1.448408239233969e-05,
      "loss": 0.4969,
      "step": 4126
    },
    {
      "epoch": 0.3715591167930856,
      "grad_norm": 0.6814041653277172,
      "learning_rate": 1.4481475642522121e-05,
      "loss": 0.5645,
      "step": 4127
    },
    {
      "epoch": 0.37164914807895744,
      "grad_norm": 0.8661534175363983,
      "learning_rate": 1.4478868511597077e-05,
      "loss": 0.5788,
      "step": 4128
    },
    {
      "epoch": 0.37173917936482925,
      "grad_norm": 0.6670486357606504,
      "learning_rate": 1.4476260999786274e-05,
      "loss": 0.6156,
      "step": 4129
    },
    {
      "epoch": 0.3718292106507011,
      "grad_norm": 0.8111188732108024,
      "learning_rate": 1.4473653107311458e-05,
      "loss": 0.7007,
      "step": 4130
    },
    {
      "epoch": 0.37191924193657294,
      "grad_norm": 0.5349579524754478,
      "learning_rate": 1.4471044834394404e-05,
      "loss": 0.4835,
      "step": 4131
    },
    {
      "epoch": 0.3720092732224448,
      "grad_norm": 0.8490028250826497,
      "learning_rate": 1.446843618125692e-05,
      "loss": 0.5775,
      "step": 4132
    },
    {
      "epoch": 0.37209930450831663,
      "grad_norm": 0.6403860300172928,
      "learning_rate": 1.4465827148120846e-05,
      "loss": 0.4927,
      "step": 4133
    },
    {
      "epoch": 0.3721893357941885,
      "grad_norm": 0.6827066708888462,
      "learning_rate": 1.4463217735208061e-05,
      "loss": 0.5618,
      "step": 4134
    },
    {
      "epoch": 0.3722793670800603,
      "grad_norm": 0.8821523193369347,
      "learning_rate": 1.4460607942740468e-05,
      "loss": 0.5523,
      "step": 4135
    },
    {
      "epoch": 0.37236939836593214,
      "grad_norm": 0.7122700807047239,
      "learning_rate": 1.4457997770940006e-05,
      "loss": 0.5177,
      "step": 4136
    },
    {
      "epoch": 0.372459429651804,
      "grad_norm": 0.7103900824285537,
      "learning_rate": 1.4455387220028648e-05,
      "loss": 0.5091,
      "step": 4137
    },
    {
      "epoch": 0.3725494609376758,
      "grad_norm": 0.5925393189263004,
      "learning_rate": 1.445277629022839e-05,
      "loss": 0.6107,
      "step": 4138
    },
    {
      "epoch": 0.3726394922235477,
      "grad_norm": 0.9836499304465874,
      "learning_rate": 1.4450164981761272e-05,
      "loss": 0.5917,
      "step": 4139
    },
    {
      "epoch": 0.3727295235094195,
      "grad_norm": 0.6205032902395354,
      "learning_rate": 1.4447553294849362e-05,
      "loss": 0.5199,
      "step": 4140
    },
    {
      "epoch": 0.3728195547952914,
      "grad_norm": 0.8316745233120959,
      "learning_rate": 1.444494122971476e-05,
      "loss": 0.6243,
      "step": 4141
    },
    {
      "epoch": 0.3729095860811632,
      "grad_norm": 0.8142858450919872,
      "learning_rate": 1.4442328786579591e-05,
      "loss": 0.6404,
      "step": 4142
    },
    {
      "epoch": 0.372999617367035,
      "grad_norm": 0.8722964754165169,
      "learning_rate": 1.4439715965666026e-05,
      "loss": 0.5804,
      "step": 4143
    },
    {
      "epoch": 0.3730896486529069,
      "grad_norm": 0.822832055140612,
      "learning_rate": 1.443710276719626e-05,
      "loss": 0.5451,
      "step": 4144
    },
    {
      "epoch": 0.3731796799387787,
      "grad_norm": 0.6365829795182005,
      "learning_rate": 1.4434489191392518e-05,
      "loss": 0.5634,
      "step": 4145
    },
    {
      "epoch": 0.3732697112246506,
      "grad_norm": 0.697509646718209,
      "learning_rate": 1.4431875238477059e-05,
      "loss": 0.5131,
      "step": 4146
    },
    {
      "epoch": 0.3733597425105224,
      "grad_norm": 0.8268258548409263,
      "learning_rate": 1.4429260908672182e-05,
      "loss": 0.6458,
      "step": 4147
    },
    {
      "epoch": 0.37344977379639427,
      "grad_norm": 0.8115864215917655,
      "learning_rate": 1.4426646202200204e-05,
      "loss": 0.6037,
      "step": 4148
    },
    {
      "epoch": 0.3735398050822661,
      "grad_norm": 0.8405745868192881,
      "learning_rate": 1.4424031119283485e-05,
      "loss": 0.5953,
      "step": 4149
    },
    {
      "epoch": 0.3736298363681379,
      "grad_norm": 0.7831687372701621,
      "learning_rate": 1.4421415660144416e-05,
      "loss": 0.6391,
      "step": 4150
    },
    {
      "epoch": 0.3737198676540098,
      "grad_norm": 0.6893353048328329,
      "learning_rate": 1.4418799825005412e-05,
      "loss": 0.4844,
      "step": 4151
    },
    {
      "epoch": 0.3738098989398816,
      "grad_norm": 0.6297299530420626,
      "learning_rate": 1.4416183614088925e-05,
      "loss": 0.5557,
      "step": 4152
    },
    {
      "epoch": 0.37389993022575346,
      "grad_norm": 0.7062711771991862,
      "learning_rate": 1.4413567027617442e-05,
      "loss": 0.6065,
      "step": 4153
    },
    {
      "epoch": 0.3739899615116253,
      "grad_norm": 0.8231164678512228,
      "learning_rate": 1.4410950065813479e-05,
      "loss": 0.5692,
      "step": 4154
    },
    {
      "epoch": 0.37407999279749715,
      "grad_norm": 0.7136682005682965,
      "learning_rate": 1.4408332728899583e-05,
      "loss": 0.526,
      "step": 4155
    },
    {
      "epoch": 0.37417002408336897,
      "grad_norm": 0.7317778115714064,
      "learning_rate": 1.4405715017098333e-05,
      "loss": 0.542,
      "step": 4156
    },
    {
      "epoch": 0.3742600553692408,
      "grad_norm": 0.6382261798991046,
      "learning_rate": 1.4403096930632345e-05,
      "loss": 0.5504,
      "step": 4157
    },
    {
      "epoch": 0.37435008665511266,
      "grad_norm": 0.664570283167861,
      "learning_rate": 1.4400478469724259e-05,
      "loss": 0.5524,
      "step": 4158
    },
    {
      "epoch": 0.3744401179409845,
      "grad_norm": 0.8148013266348381,
      "learning_rate": 1.4397859634596751e-05,
      "loss": 0.6205,
      "step": 4159
    },
    {
      "epoch": 0.37453014922685635,
      "grad_norm": 0.8935865248759642,
      "learning_rate": 1.439524042547253e-05,
      "loss": 0.5942,
      "step": 4160
    },
    {
      "epoch": 0.37462018051272816,
      "grad_norm": 0.8397493924960081,
      "learning_rate": 1.4392620842574332e-05,
      "loss": 0.5732,
      "step": 4161
    },
    {
      "epoch": 0.37471021179860003,
      "grad_norm": 0.8152237693451216,
      "learning_rate": 1.4390000886124933e-05,
      "loss": 0.5422,
      "step": 4162
    },
    {
      "epoch": 0.37480024308447185,
      "grad_norm": 0.6904795072818655,
      "learning_rate": 1.4387380556347131e-05,
      "loss": 0.5713,
      "step": 4163
    },
    {
      "epoch": 0.37489027437034367,
      "grad_norm": 0.6512739054996671,
      "learning_rate": 1.4384759853463761e-05,
      "loss": 0.4639,
      "step": 4164
    },
    {
      "epoch": 0.37498030565621554,
      "grad_norm": 0.7540757822859169,
      "learning_rate": 1.4382138777697691e-05,
      "loss": 0.5426,
      "step": 4165
    },
    {
      "epoch": 0.37507033694208736,
      "grad_norm": 0.6859414827569694,
      "learning_rate": 1.4379517329271822e-05,
      "loss": 0.528,
      "step": 4166
    },
    {
      "epoch": 0.37516036822795923,
      "grad_norm": 0.859639657979861,
      "learning_rate": 1.4376895508409074e-05,
      "loss": 0.5767,
      "step": 4167
    },
    {
      "epoch": 0.37525039951383105,
      "grad_norm": 0.6057827904074248,
      "learning_rate": 1.4374273315332416e-05,
      "loss": 0.486,
      "step": 4168
    },
    {
      "epoch": 0.3753404307997029,
      "grad_norm": 0.7478691162623249,
      "learning_rate": 1.4371650750264842e-05,
      "loss": 0.6006,
      "step": 4169
    },
    {
      "epoch": 0.37543046208557473,
      "grad_norm": 0.7449871727777123,
      "learning_rate": 1.4369027813429371e-05,
      "loss": 0.5208,
      "step": 4170
    },
    {
      "epoch": 0.37552049337144655,
      "grad_norm": 0.8534197956061942,
      "learning_rate": 1.4366404505049061e-05,
      "loss": 0.601,
      "step": 4171
    },
    {
      "epoch": 0.3756105246573184,
      "grad_norm": 1.0548440082945687,
      "learning_rate": 1.4363780825347005e-05,
      "loss": 0.5642,
      "step": 4172
    },
    {
      "epoch": 0.37570055594319024,
      "grad_norm": 0.6128621427800802,
      "learning_rate": 1.4361156774546314e-05,
      "loss": 0.5309,
      "step": 4173
    },
    {
      "epoch": 0.3757905872290621,
      "grad_norm": 0.8185082193076891,
      "learning_rate": 1.4358532352870142e-05,
      "loss": 0.5208,
      "step": 4174
    },
    {
      "epoch": 0.37588061851493393,
      "grad_norm": 0.7087583115885131,
      "learning_rate": 1.4355907560541675e-05,
      "loss": 0.5191,
      "step": 4175
    },
    {
      "epoch": 0.3759706498008058,
      "grad_norm": 0.8029327940903233,
      "learning_rate": 1.4353282397784125e-05,
      "loss": 0.5681,
      "step": 4176
    },
    {
      "epoch": 0.3760606810866776,
      "grad_norm": 0.8937753101444313,
      "learning_rate": 1.4350656864820733e-05,
      "loss": 0.4636,
      "step": 4177
    },
    {
      "epoch": 0.37615071237254943,
      "grad_norm": 0.6600287842104511,
      "learning_rate": 1.4348030961874781e-05,
      "loss": 0.5198,
      "step": 4178
    },
    {
      "epoch": 0.3762407436584213,
      "grad_norm": 0.8860174328475858,
      "learning_rate": 1.434540468916958e-05,
      "loss": 0.6358,
      "step": 4179
    },
    {
      "epoch": 0.3763307749442931,
      "grad_norm": 0.9175489253820733,
      "learning_rate": 1.4342778046928462e-05,
      "loss": 0.5892,
      "step": 4180
    },
    {
      "epoch": 0.376420806230165,
      "grad_norm": 0.6555233207298813,
      "learning_rate": 1.4340151035374803e-05,
      "loss": 0.6198,
      "step": 4181
    },
    {
      "epoch": 0.3765108375160368,
      "grad_norm": 0.7298439944825467,
      "learning_rate": 1.4337523654732003e-05,
      "loss": 0.5314,
      "step": 4182
    },
    {
      "epoch": 0.3766008688019087,
      "grad_norm": 0.7553691365324375,
      "learning_rate": 1.4334895905223505e-05,
      "loss": 0.5194,
      "step": 4183
    },
    {
      "epoch": 0.3766909000877805,
      "grad_norm": 0.9959782257302113,
      "learning_rate": 1.433226778707276e-05,
      "loss": 0.6235,
      "step": 4184
    },
    {
      "epoch": 0.3767809313736523,
      "grad_norm": 0.7136965682820584,
      "learning_rate": 1.4329639300503275e-05,
      "loss": 0.5136,
      "step": 4185
    },
    {
      "epoch": 0.3768709626595242,
      "grad_norm": 0.7318179674517065,
      "learning_rate": 1.4327010445738579e-05,
      "loss": 0.4704,
      "step": 4186
    },
    {
      "epoch": 0.376960993945396,
      "grad_norm": 0.6813108745862541,
      "learning_rate": 1.4324381223002224e-05,
      "loss": 0.5678,
      "step": 4187
    },
    {
      "epoch": 0.3770510252312679,
      "grad_norm": 0.7840427177917546,
      "learning_rate": 1.4321751632517801e-05,
      "loss": 0.6106,
      "step": 4188
    },
    {
      "epoch": 0.3771410565171397,
      "grad_norm": 0.7914324548986387,
      "learning_rate": 1.4319121674508942e-05,
      "loss": 0.5056,
      "step": 4189
    },
    {
      "epoch": 0.37723108780301157,
      "grad_norm": 0.6876540364303639,
      "learning_rate": 1.4316491349199293e-05,
      "loss": 0.5296,
      "step": 4190
    },
    {
      "epoch": 0.3773211190888834,
      "grad_norm": 0.8210073349512967,
      "learning_rate": 1.4313860656812537e-05,
      "loss": 0.6781,
      "step": 4191
    },
    {
      "epoch": 0.3774111503747552,
      "grad_norm": 0.6341765880444726,
      "learning_rate": 1.4311229597572392e-05,
      "loss": 0.5042,
      "step": 4192
    },
    {
      "epoch": 0.37750118166062707,
      "grad_norm": 0.76539597845716,
      "learning_rate": 1.4308598171702608e-05,
      "loss": 0.5861,
      "step": 4193
    },
    {
      "epoch": 0.3775912129464989,
      "grad_norm": 0.79118606603338,
      "learning_rate": 1.4305966379426957e-05,
      "loss": 0.509,
      "step": 4194
    },
    {
      "epoch": 0.37768124423237076,
      "grad_norm": 0.8024998815472779,
      "learning_rate": 1.4303334220969251e-05,
      "loss": 0.5553,
      "step": 4195
    },
    {
      "epoch": 0.3777712755182426,
      "grad_norm": 0.7851602676319058,
      "learning_rate": 1.4300701696553332e-05,
      "loss": 0.5648,
      "step": 4196
    },
    {
      "epoch": 0.37786130680411445,
      "grad_norm": 0.6916928345097846,
      "learning_rate": 1.4298068806403071e-05,
      "loss": 0.5057,
      "step": 4197
    },
    {
      "epoch": 0.37795133808998627,
      "grad_norm": 0.8346236710246068,
      "learning_rate": 1.4295435550742372e-05,
      "loss": 0.5766,
      "step": 4198
    },
    {
      "epoch": 0.37804136937585814,
      "grad_norm": 0.6842365280497515,
      "learning_rate": 1.4292801929795162e-05,
      "loss": 0.5404,
      "step": 4199
    },
    {
      "epoch": 0.37813140066172996,
      "grad_norm": 0.7421276064098137,
      "learning_rate": 1.4290167943785413e-05,
      "loss": 0.5053,
      "step": 4200
    },
    {
      "epoch": 0.37822143194760177,
      "grad_norm": 0.8650014344576932,
      "learning_rate": 1.4287533592937117e-05,
      "loss": 0.6185,
      "step": 4201
    },
    {
      "epoch": 0.37831146323347364,
      "grad_norm": 0.7049942271829801,
      "learning_rate": 1.4284898877474302e-05,
      "loss": 0.5768,
      "step": 4202
    },
    {
      "epoch": 0.37840149451934546,
      "grad_norm": 0.9008725869724729,
      "learning_rate": 1.4282263797621024e-05,
      "loss": 0.4831,
      "step": 4203
    },
    {
      "epoch": 0.37849152580521733,
      "grad_norm": 0.6433375032799198,
      "learning_rate": 1.4279628353601378e-05,
      "loss": 0.5283,
      "step": 4204
    },
    {
      "epoch": 0.37858155709108915,
      "grad_norm": 0.7046843838191635,
      "learning_rate": 1.4276992545639477e-05,
      "loss": 0.4904,
      "step": 4205
    },
    {
      "epoch": 0.378671588376961,
      "grad_norm": 0.9507220332925311,
      "learning_rate": 1.4274356373959472e-05,
      "loss": 0.6426,
      "step": 4206
    },
    {
      "epoch": 0.37876161966283284,
      "grad_norm": 0.7034825729479056,
      "learning_rate": 1.4271719838785551e-05,
      "loss": 0.5459,
      "step": 4207
    },
    {
      "epoch": 0.37885165094870465,
      "grad_norm": 0.6770394833809629,
      "learning_rate": 1.426908294034192e-05,
      "loss": 0.5257,
      "step": 4208
    },
    {
      "epoch": 0.3789416822345765,
      "grad_norm": 0.8583715880101965,
      "learning_rate": 1.4266445678852825e-05,
      "loss": 0.5559,
      "step": 4209
    },
    {
      "epoch": 0.37903171352044834,
      "grad_norm": 0.7105778242552684,
      "learning_rate": 1.4263808054542541e-05,
      "loss": 0.519,
      "step": 4210
    },
    {
      "epoch": 0.3791217448063202,
      "grad_norm": 0.5784438316812862,
      "learning_rate": 1.4261170067635372e-05,
      "loss": 0.4021,
      "step": 4211
    },
    {
      "epoch": 0.37921177609219203,
      "grad_norm": 0.6911595325378141,
      "learning_rate": 1.4258531718355655e-05,
      "loss": 0.4425,
      "step": 4212
    },
    {
      "epoch": 0.3793018073780639,
      "grad_norm": 0.8535836589531077,
      "learning_rate": 1.4255893006927755e-05,
      "loss": 0.5868,
      "step": 4213
    },
    {
      "epoch": 0.3793918386639357,
      "grad_norm": 0.6635708314190408,
      "learning_rate": 1.4253253933576071e-05,
      "loss": 0.5137,
      "step": 4214
    },
    {
      "epoch": 0.37948186994980754,
      "grad_norm": 0.8465310358403395,
      "learning_rate": 1.4250614498525033e-05,
      "loss": 0.5418,
      "step": 4215
    },
    {
      "epoch": 0.3795719012356794,
      "grad_norm": 0.6762199916978489,
      "learning_rate": 1.4247974701999096e-05,
      "loss": 0.4961,
      "step": 4216
    },
    {
      "epoch": 0.3796619325215512,
      "grad_norm": 0.6844638473443448,
      "learning_rate": 1.4245334544222755e-05,
      "loss": 0.4954,
      "step": 4217
    },
    {
      "epoch": 0.3797519638074231,
      "grad_norm": 0.5972254897985446,
      "learning_rate": 1.424269402542053e-05,
      "loss": 0.4648,
      "step": 4218
    },
    {
      "epoch": 0.3798419950932949,
      "grad_norm": 0.7190649682502506,
      "learning_rate": 1.4240053145816968e-05,
      "loss": 0.5784,
      "step": 4219
    },
    {
      "epoch": 0.3799320263791668,
      "grad_norm": 0.8938204290691623,
      "learning_rate": 1.4237411905636653e-05,
      "loss": 0.5559,
      "step": 4220
    },
    {
      "epoch": 0.3800220576650386,
      "grad_norm": 0.7008790644184272,
      "learning_rate": 1.4234770305104203e-05,
      "loss": 0.5789,
      "step": 4221
    },
    {
      "epoch": 0.3801120889509104,
      "grad_norm": 0.6991029134448375,
      "learning_rate": 1.4232128344444251e-05,
      "loss": 0.5143,
      "step": 4222
    },
    {
      "epoch": 0.3802021202367823,
      "grad_norm": 0.7142425110035485,
      "learning_rate": 1.4229486023881477e-05,
      "loss": 0.6278,
      "step": 4223
    },
    {
      "epoch": 0.3802921515226541,
      "grad_norm": 0.7603090074935703,
      "learning_rate": 1.4226843343640589e-05,
      "loss": 0.5883,
      "step": 4224
    },
    {
      "epoch": 0.380382182808526,
      "grad_norm": 0.7298654550390519,
      "learning_rate": 1.4224200303946318e-05,
      "loss": 0.6006,
      "step": 4225
    },
    {
      "epoch": 0.3804722140943978,
      "grad_norm": 1.005681417186023,
      "learning_rate": 1.4221556905023427e-05,
      "loss": 0.6195,
      "step": 4226
    },
    {
      "epoch": 0.38056224538026967,
      "grad_norm": 0.7992046387893099,
      "learning_rate": 1.4218913147096718e-05,
      "loss": 0.5808,
      "step": 4227
    },
    {
      "epoch": 0.3806522766661415,
      "grad_norm": 0.9444075206124891,
      "learning_rate": 1.4216269030391017e-05,
      "loss": 0.5332,
      "step": 4228
    },
    {
      "epoch": 0.3807423079520133,
      "grad_norm": 0.8838928932408232,
      "learning_rate": 1.4213624555131173e-05,
      "loss": 0.6024,
      "step": 4229
    },
    {
      "epoch": 0.3808323392378852,
      "grad_norm": 0.6714128846195722,
      "learning_rate": 1.4210979721542086e-05,
      "loss": 0.5959,
      "step": 4230
    },
    {
      "epoch": 0.380922370523757,
      "grad_norm": 0.7569014222853915,
      "learning_rate": 1.4208334529848667e-05,
      "loss": 0.5814,
      "step": 4231
    },
    {
      "epoch": 0.38101240180962886,
      "grad_norm": 0.8053743470259406,
      "learning_rate": 1.4205688980275867e-05,
      "loss": 0.5595,
      "step": 4232
    },
    {
      "epoch": 0.3811024330955007,
      "grad_norm": 0.6979495234425059,
      "learning_rate": 1.4203043073048664e-05,
      "loss": 0.5737,
      "step": 4233
    },
    {
      "epoch": 0.38119246438137255,
      "grad_norm": 0.6779173620483114,
      "learning_rate": 1.420039680839207e-05,
      "loss": 0.5431,
      "step": 4234
    },
    {
      "epoch": 0.38128249566724437,
      "grad_norm": 0.9042108361240463,
      "learning_rate": 1.419775018653112e-05,
      "loss": 0.6314,
      "step": 4235
    },
    {
      "epoch": 0.3813725269531162,
      "grad_norm": 0.8044736965722338,
      "learning_rate": 1.419510320769089e-05,
      "loss": 0.5545,
      "step": 4236
    },
    {
      "epoch": 0.38146255823898806,
      "grad_norm": 0.7250042056482627,
      "learning_rate": 1.4192455872096478e-05,
      "loss": 0.6076,
      "step": 4237
    },
    {
      "epoch": 0.3815525895248599,
      "grad_norm": 0.7612162704872542,
      "learning_rate": 1.4189808179973015e-05,
      "loss": 0.5681,
      "step": 4238
    },
    {
      "epoch": 0.38164262081073175,
      "grad_norm": 0.9066065044653594,
      "learning_rate": 1.4187160131545663e-05,
      "loss": 0.6186,
      "step": 4239
    },
    {
      "epoch": 0.38173265209660356,
      "grad_norm": 0.6858198153469988,
      "learning_rate": 1.4184511727039612e-05,
      "loss": 0.5989,
      "step": 4240
    },
    {
      "epoch": 0.38182268338247544,
      "grad_norm": 0.8990934726903413,
      "learning_rate": 1.4181862966680086e-05,
      "loss": 0.5104,
      "step": 4241
    },
    {
      "epoch": 0.38191271466834725,
      "grad_norm": 0.6555985993723512,
      "learning_rate": 1.417921385069234e-05,
      "loss": 0.5601,
      "step": 4242
    },
    {
      "epoch": 0.38200274595421907,
      "grad_norm": 0.7661193973022546,
      "learning_rate": 1.417656437930165e-05,
      "loss": 0.4898,
      "step": 4243
    },
    {
      "epoch": 0.38209277724009094,
      "grad_norm": 0.7854149829089515,
      "learning_rate": 1.417391455273333e-05,
      "loss": 0.5013,
      "step": 4244
    },
    {
      "epoch": 0.38218280852596276,
      "grad_norm": 0.6436860376394363,
      "learning_rate": 1.4171264371212727e-05,
      "loss": 0.6249,
      "step": 4245
    },
    {
      "epoch": 0.38227283981183463,
      "grad_norm": 0.6682339792735731,
      "learning_rate": 1.4168613834965212e-05,
      "loss": 0.5975,
      "step": 4246
    },
    {
      "epoch": 0.38236287109770645,
      "grad_norm": 0.8861273930329168,
      "learning_rate": 1.4165962944216187e-05,
      "loss": 0.5912,
      "step": 4247
    },
    {
      "epoch": 0.3824529023835783,
      "grad_norm": 0.9466401589470259,
      "learning_rate": 1.4163311699191087e-05,
      "loss": 0.5234,
      "step": 4248
    },
    {
      "epoch": 0.38254293366945014,
      "grad_norm": 0.8032881839290491,
      "learning_rate": 1.4160660100115375e-05,
      "loss": 0.6222,
      "step": 4249
    },
    {
      "epoch": 0.38263296495532195,
      "grad_norm": 0.8743101965377632,
      "learning_rate": 1.4158008147214544e-05,
      "loss": 0.5666,
      "step": 4250
    },
    {
      "epoch": 0.3827229962411938,
      "grad_norm": 0.6875622386663632,
      "learning_rate": 1.4155355840714117e-05,
      "loss": 0.4428,
      "step": 4251
    },
    {
      "epoch": 0.38281302752706564,
      "grad_norm": 0.9625545428053254,
      "learning_rate": 1.4152703180839651e-05,
      "loss": 0.6588,
      "step": 4252
    },
    {
      "epoch": 0.3829030588129375,
      "grad_norm": 0.7556700933941338,
      "learning_rate": 1.4150050167816729e-05,
      "loss": 0.4939,
      "step": 4253
    },
    {
      "epoch": 0.38299309009880933,
      "grad_norm": 0.7641916592357274,
      "learning_rate": 1.4147396801870963e-05,
      "loss": 0.6187,
      "step": 4254
    },
    {
      "epoch": 0.3830831213846812,
      "grad_norm": 0.7697447858198689,
      "learning_rate": 1.4144743083227997e-05,
      "loss": 0.5356,
      "step": 4255
    },
    {
      "epoch": 0.383173152670553,
      "grad_norm": 0.8432321142478391,
      "learning_rate": 1.4142089012113508e-05,
      "loss": 0.6487,
      "step": 4256
    },
    {
      "epoch": 0.38326318395642484,
      "grad_norm": 0.6322073062495557,
      "learning_rate": 1.4139434588753193e-05,
      "loss": 0.5046,
      "step": 4257
    },
    {
      "epoch": 0.3833532152422967,
      "grad_norm": 0.6534492808473519,
      "learning_rate": 1.4136779813372792e-05,
      "loss": 0.5569,
      "step": 4258
    },
    {
      "epoch": 0.3834432465281685,
      "grad_norm": 0.6236910646678442,
      "learning_rate": 1.4134124686198067e-05,
      "loss": 0.4852,
      "step": 4259
    },
    {
      "epoch": 0.3835332778140404,
      "grad_norm": 0.6808270494180789,
      "learning_rate": 1.4131469207454813e-05,
      "loss": 0.4882,
      "step": 4260
    },
    {
      "epoch": 0.3836233090999122,
      "grad_norm": 0.6372655275778558,
      "learning_rate": 1.4128813377368851e-05,
      "loss": 0.5756,
      "step": 4261
    },
    {
      "epoch": 0.3837133403857841,
      "grad_norm": 0.745075658788727,
      "learning_rate": 1.4126157196166035e-05,
      "loss": 0.5569,
      "step": 4262
    },
    {
      "epoch": 0.3838033716716559,
      "grad_norm": 1.0920680933143023,
      "learning_rate": 1.4123500664072251e-05,
      "loss": 0.6252,
      "step": 4263
    },
    {
      "epoch": 0.3838934029575277,
      "grad_norm": 0.8380825891975295,
      "learning_rate": 1.4120843781313407e-05,
      "loss": 0.6051,
      "step": 4264
    },
    {
      "epoch": 0.3839834342433996,
      "grad_norm": 0.7208721212839747,
      "learning_rate": 1.4118186548115452e-05,
      "loss": 0.5078,
      "step": 4265
    },
    {
      "epoch": 0.3840734655292714,
      "grad_norm": 0.5941806459427574,
      "learning_rate": 1.4115528964704352e-05,
      "loss": 0.554,
      "step": 4266
    },
    {
      "epoch": 0.3841634968151433,
      "grad_norm": 0.7302651328826101,
      "learning_rate": 1.4112871031306118e-05,
      "loss": 0.5358,
      "step": 4267
    },
    {
      "epoch": 0.3842535281010151,
      "grad_norm": 0.6734287320836857,
      "learning_rate": 1.4110212748146774e-05,
      "loss": 0.5028,
      "step": 4268
    },
    {
      "epoch": 0.38434355938688697,
      "grad_norm": 0.5600812663951804,
      "learning_rate": 1.410755411545239e-05,
      "loss": 0.5099,
      "step": 4269
    },
    {
      "epoch": 0.3844335906727588,
      "grad_norm": 1.011333210238983,
      "learning_rate": 1.410489513344905e-05,
      "loss": 0.5163,
      "step": 4270
    },
    {
      "epoch": 0.3845236219586306,
      "grad_norm": 0.646108476118019,
      "learning_rate": 1.4102235802362879e-05,
      "loss": 0.5707,
      "step": 4271
    },
    {
      "epoch": 0.3846136532445025,
      "grad_norm": 0.8669887003120995,
      "learning_rate": 1.409957612242003e-05,
      "loss": 0.5555,
      "step": 4272
    },
    {
      "epoch": 0.3847036845303743,
      "grad_norm": 0.6939321473118115,
      "learning_rate": 1.4096916093846678e-05,
      "loss": 0.5356,
      "step": 4273
    },
    {
      "epoch": 0.38479371581624616,
      "grad_norm": 1.0192137465373092,
      "learning_rate": 1.4094255716869044e-05,
      "loss": 0.548,
      "step": 4274
    },
    {
      "epoch": 0.384883747102118,
      "grad_norm": 0.6685602936655384,
      "learning_rate": 1.4091594991713355e-05,
      "loss": 0.5321,
      "step": 4275
    },
    {
      "epoch": 0.38497377838798985,
      "grad_norm": 0.8286709377726105,
      "learning_rate": 1.408893391860589e-05,
      "loss": 0.5543,
      "step": 4276
    },
    {
      "epoch": 0.38506380967386167,
      "grad_norm": 0.6505269007483797,
      "learning_rate": 1.4086272497772946e-05,
      "loss": 0.5835,
      "step": 4277
    },
    {
      "epoch": 0.3851538409597335,
      "grad_norm": 0.7486069118748397,
      "learning_rate": 1.4083610729440851e-05,
      "loss": 0.5425,
      "step": 4278
    },
    {
      "epoch": 0.38524387224560536,
      "grad_norm": 0.7211809579280283,
      "learning_rate": 1.4080948613835963e-05,
      "loss": 0.6016,
      "step": 4279
    },
    {
      "epoch": 0.3853339035314772,
      "grad_norm": 0.9386015506448612,
      "learning_rate": 1.4078286151184672e-05,
      "loss": 0.5733,
      "step": 4280
    },
    {
      "epoch": 0.38542393481734905,
      "grad_norm": 0.6891177285736331,
      "learning_rate": 1.4075623341713395e-05,
      "loss": 0.5366,
      "step": 4281
    },
    {
      "epoch": 0.38551396610322086,
      "grad_norm": 0.6508456394014788,
      "learning_rate": 1.4072960185648576e-05,
      "loss": 0.4843,
      "step": 4282
    },
    {
      "epoch": 0.38560399738909273,
      "grad_norm": 0.6958780098342013,
      "learning_rate": 1.4070296683216696e-05,
      "loss": 0.4733,
      "step": 4283
    },
    {
      "epoch": 0.38569402867496455,
      "grad_norm": 0.676155588313943,
      "learning_rate": 1.4067632834644258e-05,
      "loss": 0.5074,
      "step": 4284
    },
    {
      "epoch": 0.38578405996083637,
      "grad_norm": 0.8283581171267069,
      "learning_rate": 1.4064968640157797e-05,
      "loss": 0.6336,
      "step": 4285
    },
    {
      "epoch": 0.38587409124670824,
      "grad_norm": 0.8120070912497528,
      "learning_rate": 1.406230409998388e-05,
      "loss": 0.498,
      "step": 4286
    },
    {
      "epoch": 0.38596412253258006,
      "grad_norm": 0.6195099050464619,
      "learning_rate": 1.4059639214349097e-05,
      "loss": 0.5213,
      "step": 4287
    },
    {
      "epoch": 0.38605415381845193,
      "grad_norm": 0.7236680798235351,
      "learning_rate": 1.405697398348008e-05,
      "loss": 0.5366,
      "step": 4288
    },
    {
      "epoch": 0.38614418510432374,
      "grad_norm": 0.7720705567221903,
      "learning_rate": 1.4054308407603471e-05,
      "loss": 0.6251,
      "step": 4289
    },
    {
      "epoch": 0.3862342163901956,
      "grad_norm": 0.9027774146985146,
      "learning_rate": 1.405164248694596e-05,
      "loss": 0.5316,
      "step": 4290
    },
    {
      "epoch": 0.38632424767606743,
      "grad_norm": 0.6415973114704042,
      "learning_rate": 1.4048976221734257e-05,
      "loss": 0.5459,
      "step": 4291
    },
    {
      "epoch": 0.38641427896193925,
      "grad_norm": 0.7518533016395917,
      "learning_rate": 1.40463096121951e-05,
      "loss": 0.6378,
      "step": 4292
    },
    {
      "epoch": 0.3865043102478111,
      "grad_norm": 0.6084274222708678,
      "learning_rate": 1.404364265855526e-05,
      "loss": 0.5202,
      "step": 4293
    },
    {
      "epoch": 0.38659434153368294,
      "grad_norm": 0.9156212921536853,
      "learning_rate": 1.4040975361041537e-05,
      "loss": 0.6192,
      "step": 4294
    },
    {
      "epoch": 0.3866843728195548,
      "grad_norm": 0.7725959463383955,
      "learning_rate": 1.4038307719880766e-05,
      "loss": 0.4383,
      "step": 4295
    },
    {
      "epoch": 0.38677440410542663,
      "grad_norm": 0.7667614162537315,
      "learning_rate": 1.4035639735299796e-05,
      "loss": 0.5325,
      "step": 4296
    },
    {
      "epoch": 0.3868644353912985,
      "grad_norm": 0.6377310212829874,
      "learning_rate": 1.4032971407525513e-05,
      "loss": 0.5434,
      "step": 4297
    },
    {
      "epoch": 0.3869544666771703,
      "grad_norm": 0.6684480577192053,
      "learning_rate": 1.4030302736784842e-05,
      "loss": 0.6006,
      "step": 4298
    },
    {
      "epoch": 0.38704449796304213,
      "grad_norm": 0.9038231699668563,
      "learning_rate": 1.4027633723304722e-05,
      "loss": 0.6235,
      "step": 4299
    },
    {
      "epoch": 0.387134529248914,
      "grad_norm": 0.6868028218624358,
      "learning_rate": 1.4024964367312128e-05,
      "loss": 0.4862,
      "step": 4300
    },
    {
      "epoch": 0.3872245605347858,
      "grad_norm": 0.866533502019823,
      "learning_rate": 1.4022294669034067e-05,
      "loss": 0.5354,
      "step": 4301
    },
    {
      "epoch": 0.3873145918206577,
      "grad_norm": 0.7520794312936431,
      "learning_rate": 1.401962462869757e-05,
      "loss": 0.5429,
      "step": 4302
    },
    {
      "epoch": 0.3874046231065295,
      "grad_norm": 0.9367992836555434,
      "learning_rate": 1.4016954246529697e-05,
      "loss": 0.5641,
      "step": 4303
    },
    {
      "epoch": 0.3874946543924014,
      "grad_norm": 0.7643666828106483,
      "learning_rate": 1.4014283522757541e-05,
      "loss": 0.5984,
      "step": 4304
    },
    {
      "epoch": 0.3875846856782732,
      "grad_norm": 0.6499990883167225,
      "learning_rate": 1.4011612457608221e-05,
      "loss": 0.518,
      "step": 4305
    },
    {
      "epoch": 0.387674716964145,
      "grad_norm": 1.4227264277639162,
      "learning_rate": 1.4008941051308888e-05,
      "loss": 0.54,
      "step": 4306
    },
    {
      "epoch": 0.3877647482500169,
      "grad_norm": 0.5784837077155399,
      "learning_rate": 1.4006269304086717e-05,
      "loss": 0.5337,
      "step": 4307
    },
    {
      "epoch": 0.3878547795358887,
      "grad_norm": 0.8131764743169478,
      "learning_rate": 1.4003597216168918e-05,
      "loss": 0.5257,
      "step": 4308
    },
    {
      "epoch": 0.3879448108217606,
      "grad_norm": 0.7100178266499891,
      "learning_rate": 1.4000924787782726e-05,
      "loss": 0.5458,
      "step": 4309
    },
    {
      "epoch": 0.3880348421076324,
      "grad_norm": 0.7902308163818205,
      "learning_rate": 1.3998252019155404e-05,
      "loss": 0.6296,
      "step": 4310
    },
    {
      "epoch": 0.38812487339350427,
      "grad_norm": 0.8107990800699999,
      "learning_rate": 1.399557891051425e-05,
      "loss": 0.5579,
      "step": 4311
    },
    {
      "epoch": 0.3882149046793761,
      "grad_norm": 0.8186392123759776,
      "learning_rate": 1.3992905462086586e-05,
      "loss": 0.5567,
      "step": 4312
    },
    {
      "epoch": 0.3883049359652479,
      "grad_norm": 0.8365432899812885,
      "learning_rate": 1.3990231674099757e-05,
      "loss": 0.5202,
      "step": 4313
    },
    {
      "epoch": 0.38839496725111977,
      "grad_norm": 0.9104247314682953,
      "learning_rate": 1.3987557546781152e-05,
      "loss": 0.5693,
      "step": 4314
    },
    {
      "epoch": 0.3884849985369916,
      "grad_norm": 0.7675019099761201,
      "learning_rate": 1.3984883080358174e-05,
      "loss": 0.5906,
      "step": 4315
    },
    {
      "epoch": 0.38857502982286346,
      "grad_norm": 0.7376081684852229,
      "learning_rate": 1.398220827505827e-05,
      "loss": 0.4874,
      "step": 4316
    },
    {
      "epoch": 0.3886650611087353,
      "grad_norm": 0.6903821186464386,
      "learning_rate": 1.3979533131108895e-05,
      "loss": 0.4975,
      "step": 4317
    },
    {
      "epoch": 0.38875509239460715,
      "grad_norm": 0.8192125363446394,
      "learning_rate": 1.3976857648737556e-05,
      "loss": 0.6559,
      "step": 4318
    },
    {
      "epoch": 0.38884512368047897,
      "grad_norm": 0.7997713079112733,
      "learning_rate": 1.3974181828171774e-05,
      "loss": 0.5052,
      "step": 4319
    },
    {
      "epoch": 0.3889351549663508,
      "grad_norm": 0.6560256167639026,
      "learning_rate": 1.3971505669639098e-05,
      "loss": 0.501,
      "step": 4320
    },
    {
      "epoch": 0.38902518625222265,
      "grad_norm": 0.799919769157837,
      "learning_rate": 1.3968829173367116e-05,
      "loss": 0.6325,
      "step": 4321
    },
    {
      "epoch": 0.38911521753809447,
      "grad_norm": 0.9982061017497431,
      "learning_rate": 1.3966152339583434e-05,
      "loss": 0.5605,
      "step": 4322
    },
    {
      "epoch": 0.38920524882396634,
      "grad_norm": 0.6575075150950087,
      "learning_rate": 1.3963475168515698e-05,
      "loss": 0.4835,
      "step": 4323
    },
    {
      "epoch": 0.38929528010983816,
      "grad_norm": 0.7232456888875501,
      "learning_rate": 1.396079766039157e-05,
      "loss": 0.5064,
      "step": 4324
    },
    {
      "epoch": 0.38938531139571003,
      "grad_norm": 0.7896065048345196,
      "learning_rate": 1.395811981543875e-05,
      "loss": 0.5232,
      "step": 4325
    },
    {
      "epoch": 0.38947534268158185,
      "grad_norm": 0.6914317750706348,
      "learning_rate": 1.3955441633884966e-05,
      "loss": 0.5161,
      "step": 4326
    },
    {
      "epoch": 0.38956537396745367,
      "grad_norm": 0.7089895043162111,
      "learning_rate": 1.3952763115957965e-05,
      "loss": 0.5548,
      "step": 4327
    },
    {
      "epoch": 0.38965540525332554,
      "grad_norm": 0.7009878342177578,
      "learning_rate": 1.3950084261885537e-05,
      "loss": 0.6059,
      "step": 4328
    },
    {
      "epoch": 0.38974543653919735,
      "grad_norm": 0.8214020707040421,
      "learning_rate": 1.3947405071895487e-05,
      "loss": 0.5685,
      "step": 4329
    },
    {
      "epoch": 0.3898354678250692,
      "grad_norm": 0.7019325898210971,
      "learning_rate": 1.3944725546215665e-05,
      "loss": 0.6025,
      "step": 4330
    },
    {
      "epoch": 0.38992549911094104,
      "grad_norm": 0.7421573453682156,
      "learning_rate": 1.3942045685073928e-05,
      "loss": 0.461,
      "step": 4331
    },
    {
      "epoch": 0.3900155303968129,
      "grad_norm": 0.7027900864984714,
      "learning_rate": 1.3939365488698178e-05,
      "loss": 0.538,
      "step": 4332
    },
    {
      "epoch": 0.39010556168268473,
      "grad_norm": 0.6493472128367186,
      "learning_rate": 1.3936684957316345e-05,
      "loss": 0.5732,
      "step": 4333
    },
    {
      "epoch": 0.39019559296855655,
      "grad_norm": 0.6713241913571053,
      "learning_rate": 1.3934004091156375e-05,
      "loss": 0.4658,
      "step": 4334
    },
    {
      "epoch": 0.3902856242544284,
      "grad_norm": 0.6827336722067384,
      "learning_rate": 1.3931322890446256e-05,
      "loss": 0.6969,
      "step": 4335
    },
    {
      "epoch": 0.39037565554030024,
      "grad_norm": 0.6520133465934629,
      "learning_rate": 1.3928641355413994e-05,
      "loss": 0.5516,
      "step": 4336
    },
    {
      "epoch": 0.3904656868261721,
      "grad_norm": 0.6377366890751522,
      "learning_rate": 1.3925959486287637e-05,
      "loss": 0.5533,
      "step": 4337
    },
    {
      "epoch": 0.3905557181120439,
      "grad_norm": 0.6950447930429114,
      "learning_rate": 1.3923277283295243e-05,
      "loss": 0.5724,
      "step": 4338
    },
    {
      "epoch": 0.3906457493979158,
      "grad_norm": 0.7574902344795984,
      "learning_rate": 1.3920594746664915e-05,
      "loss": 0.6649,
      "step": 4339
    },
    {
      "epoch": 0.3907357806837876,
      "grad_norm": 0.8753892494046401,
      "learning_rate": 1.3917911876624776e-05,
      "loss": 0.5905,
      "step": 4340
    },
    {
      "epoch": 0.39082581196965943,
      "grad_norm": 0.7241421694527648,
      "learning_rate": 1.3915228673402975e-05,
      "loss": 0.6131,
      "step": 4341
    },
    {
      "epoch": 0.3909158432555313,
      "grad_norm": 0.7819052515828426,
      "learning_rate": 1.3912545137227697e-05,
      "loss": 0.5951,
      "step": 4342
    },
    {
      "epoch": 0.3910058745414031,
      "grad_norm": 0.7064559627779087,
      "learning_rate": 1.3909861268327155e-05,
      "loss": 0.5952,
      "step": 4343
    },
    {
      "epoch": 0.391095905827275,
      "grad_norm": 0.7055218603231216,
      "learning_rate": 1.390717706692958e-05,
      "loss": 0.5793,
      "step": 4344
    },
    {
      "epoch": 0.3911859371131468,
      "grad_norm": 0.7882576401185191,
      "learning_rate": 1.3904492533263243e-05,
      "loss": 0.5057,
      "step": 4345
    },
    {
      "epoch": 0.3912759683990187,
      "grad_norm": 0.7697172979030611,
      "learning_rate": 1.3901807667556432e-05,
      "loss": 0.6915,
      "step": 4346
    },
    {
      "epoch": 0.3913659996848905,
      "grad_norm": 0.8678204959175939,
      "learning_rate": 1.389912247003748e-05,
      "loss": 0.5292,
      "step": 4347
    },
    {
      "epoch": 0.3914560309707623,
      "grad_norm": 0.917280543052425,
      "learning_rate": 1.389643694093473e-05,
      "loss": 0.5353,
      "step": 4348
    },
    {
      "epoch": 0.3915460622566342,
      "grad_norm": 0.9212203673693898,
      "learning_rate": 1.3893751080476561e-05,
      "loss": 0.5936,
      "step": 4349
    },
    {
      "epoch": 0.391636093542506,
      "grad_norm": 0.6115217363649382,
      "learning_rate": 1.389106488889138e-05,
      "loss": 0.5123,
      "step": 4350
    },
    {
      "epoch": 0.3917261248283779,
      "grad_norm": 0.8434139065817978,
      "learning_rate": 1.388837836640763e-05,
      "loss": 0.5351,
      "step": 4351
    },
    {
      "epoch": 0.3918161561142497,
      "grad_norm": 0.7566114381296078,
      "learning_rate": 1.388569151325377e-05,
      "loss": 0.5236,
      "step": 4352
    },
    {
      "epoch": 0.39190618740012156,
      "grad_norm": 0.6577528474386415,
      "learning_rate": 1.3883004329658288e-05,
      "loss": 0.5692,
      "step": 4353
    },
    {
      "epoch": 0.3919962186859934,
      "grad_norm": 0.703381606098781,
      "learning_rate": 1.3880316815849707e-05,
      "loss": 0.5892,
      "step": 4354
    },
    {
      "epoch": 0.3920862499718652,
      "grad_norm": 0.9448562751397224,
      "learning_rate": 1.3877628972056576e-05,
      "loss": 0.568,
      "step": 4355
    },
    {
      "epoch": 0.39217628125773707,
      "grad_norm": 0.7963561172342777,
      "learning_rate": 1.3874940798507467e-05,
      "loss": 0.5131,
      "step": 4356
    },
    {
      "epoch": 0.3922663125436089,
      "grad_norm": 0.8289184430660058,
      "learning_rate": 1.3872252295430989e-05,
      "loss": 0.5331,
      "step": 4357
    },
    {
      "epoch": 0.39235634382948076,
      "grad_norm": 0.8681743949941987,
      "learning_rate": 1.3869563463055769e-05,
      "loss": 0.5324,
      "step": 4358
    },
    {
      "epoch": 0.3924463751153526,
      "grad_norm": 0.7475752055570699,
      "learning_rate": 1.3866874301610472e-05,
      "loss": 0.5448,
      "step": 4359
    },
    {
      "epoch": 0.39253640640122445,
      "grad_norm": 0.9504238494256797,
      "learning_rate": 1.386418481132378e-05,
      "loss": 0.5627,
      "step": 4360
    },
    {
      "epoch": 0.39262643768709626,
      "grad_norm": 0.688131488285218,
      "learning_rate": 1.3861494992424414e-05,
      "loss": 0.504,
      "step": 4361
    },
    {
      "epoch": 0.3927164689729681,
      "grad_norm": 0.7974459097931759,
      "learning_rate": 1.3858804845141116e-05,
      "loss": 0.5909,
      "step": 4362
    },
    {
      "epoch": 0.39280650025883995,
      "grad_norm": 0.9729033071050641,
      "learning_rate": 1.3856114369702657e-05,
      "loss": 0.6759,
      "step": 4363
    },
    {
      "epoch": 0.39289653154471177,
      "grad_norm": 0.7617415041631265,
      "learning_rate": 1.3853423566337838e-05,
      "loss": 0.5483,
      "step": 4364
    },
    {
      "epoch": 0.39298656283058364,
      "grad_norm": 0.6918320590182246,
      "learning_rate": 1.3850732435275487e-05,
      "loss": 0.5483,
      "step": 4365
    },
    {
      "epoch": 0.39307659411645546,
      "grad_norm": 0.7346698348992958,
      "learning_rate": 1.3848040976744459e-05,
      "loss": 0.5988,
      "step": 4366
    },
    {
      "epoch": 0.39316662540232733,
      "grad_norm": 1.0112226548415646,
      "learning_rate": 1.3845349190973634e-05,
      "loss": 0.5402,
      "step": 4367
    },
    {
      "epoch": 0.39325665668819915,
      "grad_norm": 0.7056687221303062,
      "learning_rate": 1.384265707819193e-05,
      "loss": 0.4989,
      "step": 4368
    },
    {
      "epoch": 0.39334668797407096,
      "grad_norm": 0.6546506769341757,
      "learning_rate": 1.3839964638628281e-05,
      "loss": 0.4652,
      "step": 4369
    },
    {
      "epoch": 0.39343671925994284,
      "grad_norm": 0.8944796108372511,
      "learning_rate": 1.3837271872511652e-05,
      "loss": 0.6107,
      "step": 4370
    },
    {
      "epoch": 0.39352675054581465,
      "grad_norm": 0.7216609639531153,
      "learning_rate": 1.3834578780071042e-05,
      "loss": 0.519,
      "step": 4371
    },
    {
      "epoch": 0.3936167818316865,
      "grad_norm": 0.7260114335251088,
      "learning_rate": 1.383188536153547e-05,
      "loss": 0.5205,
      "step": 4372
    },
    {
      "epoch": 0.39370681311755834,
      "grad_norm": 0.7322470932077473,
      "learning_rate": 1.382919161713399e-05,
      "loss": 0.5615,
      "step": 4373
    },
    {
      "epoch": 0.3937968444034302,
      "grad_norm": 0.8285194546092476,
      "learning_rate": 1.3826497547095676e-05,
      "loss": 0.5276,
      "step": 4374
    },
    {
      "epoch": 0.39388687568930203,
      "grad_norm": 0.7817043113371644,
      "learning_rate": 1.3823803151649636e-05,
      "loss": 0.6195,
      "step": 4375
    },
    {
      "epoch": 0.39397690697517385,
      "grad_norm": 0.7845496101092758,
      "learning_rate": 1.3821108431025001e-05,
      "loss": 0.5303,
      "step": 4376
    },
    {
      "epoch": 0.3940669382610457,
      "grad_norm": 0.7392897012125061,
      "learning_rate": 1.381841338545093e-05,
      "loss": 0.5437,
      "step": 4377
    },
    {
      "epoch": 0.39415696954691753,
      "grad_norm": 0.6168787442851056,
      "learning_rate": 1.3815718015156616e-05,
      "loss": 0.603,
      "step": 4378
    },
    {
      "epoch": 0.3942470008327894,
      "grad_norm": 0.8615139420767665,
      "learning_rate": 1.3813022320371273e-05,
      "loss": 0.6172,
      "step": 4379
    },
    {
      "epoch": 0.3943370321186612,
      "grad_norm": 0.7055604689424171,
      "learning_rate": 1.3810326301324141e-05,
      "loss": 0.5229,
      "step": 4380
    },
    {
      "epoch": 0.3944270634045331,
      "grad_norm": 0.6893497214442172,
      "learning_rate": 1.3807629958244498e-05,
      "loss": 0.5891,
      "step": 4381
    },
    {
      "epoch": 0.3945170946904049,
      "grad_norm": 0.6397273909229735,
      "learning_rate": 1.3804933291361637e-05,
      "loss": 0.5427,
      "step": 4382
    },
    {
      "epoch": 0.39460712597627673,
      "grad_norm": 0.6553288865497511,
      "learning_rate": 1.3802236300904888e-05,
      "loss": 0.5688,
      "step": 4383
    },
    {
      "epoch": 0.3946971572621486,
      "grad_norm": 0.6913143228611037,
      "learning_rate": 1.37995389871036e-05,
      "loss": 0.5374,
      "step": 4384
    },
    {
      "epoch": 0.3947871885480204,
      "grad_norm": 0.6064993042141955,
      "learning_rate": 1.379684135018716e-05,
      "loss": 0.5346,
      "step": 4385
    },
    {
      "epoch": 0.3948772198338923,
      "grad_norm": 0.7284400556292598,
      "learning_rate": 1.3794143390384975e-05,
      "loss": 0.5922,
      "step": 4386
    },
    {
      "epoch": 0.3949672511197641,
      "grad_norm": 0.7641951037583541,
      "learning_rate": 1.3791445107926478e-05,
      "loss": 0.579,
      "step": 4387
    },
    {
      "epoch": 0.395057282405636,
      "grad_norm": 0.8554164015248596,
      "learning_rate": 1.3788746503041136e-05,
      "loss": 0.5571,
      "step": 4388
    },
    {
      "epoch": 0.3951473136915078,
      "grad_norm": 0.6745005730592225,
      "learning_rate": 1.3786047575958437e-05,
      "loss": 0.5355,
      "step": 4389
    },
    {
      "epoch": 0.3952373449773796,
      "grad_norm": 0.5860914943599237,
      "learning_rate": 1.3783348326907902e-05,
      "loss": 0.5858,
      "step": 4390
    },
    {
      "epoch": 0.3953273762632515,
      "grad_norm": 0.6262663372415034,
      "learning_rate": 1.3780648756119074e-05,
      "loss": 0.4561,
      "step": 4391
    },
    {
      "epoch": 0.3954174075491233,
      "grad_norm": 0.8246550601806322,
      "learning_rate": 1.377794886382153e-05,
      "loss": 0.6025,
      "step": 4392
    },
    {
      "epoch": 0.3955074388349952,
      "grad_norm": 0.7049417540928645,
      "learning_rate": 1.377524865024487e-05,
      "loss": 0.5467,
      "step": 4393
    },
    {
      "epoch": 0.395597470120867,
      "grad_norm": 0.8173093464381571,
      "learning_rate": 1.377254811561872e-05,
      "loss": 0.5439,
      "step": 4394
    },
    {
      "epoch": 0.39568750140673886,
      "grad_norm": 0.5883975911314597,
      "learning_rate": 1.3769847260172734e-05,
      "loss": 0.5334,
      "step": 4395
    },
    {
      "epoch": 0.3957775326926107,
      "grad_norm": 0.9338831840077463,
      "learning_rate": 1.37671460841366e-05,
      "loss": 0.5818,
      "step": 4396
    },
    {
      "epoch": 0.3958675639784825,
      "grad_norm": 0.7486643330147575,
      "learning_rate": 1.376444458774002e-05,
      "loss": 0.5431,
      "step": 4397
    },
    {
      "epoch": 0.39595759526435437,
      "grad_norm": 0.7288724498874353,
      "learning_rate": 1.3761742771212737e-05,
      "loss": 0.5621,
      "step": 4398
    },
    {
      "epoch": 0.3960476265502262,
      "grad_norm": 0.6370579947127074,
      "learning_rate": 1.3759040634784511e-05,
      "loss": 0.5036,
      "step": 4399
    },
    {
      "epoch": 0.39613765783609806,
      "grad_norm": 0.9820270885354794,
      "learning_rate": 1.3756338178685135e-05,
      "loss": 0.5153,
      "step": 4400
    },
    {
      "epoch": 0.3962276891219699,
      "grad_norm": 0.6162337767075848,
      "learning_rate": 1.3753635403144429e-05,
      "loss": 0.5082,
      "step": 4401
    },
    {
      "epoch": 0.39631772040784174,
      "grad_norm": 0.812286443381051,
      "learning_rate": 1.375093230839224e-05,
      "loss": 0.5247,
      "step": 4402
    },
    {
      "epoch": 0.39640775169371356,
      "grad_norm": 0.6550037127977604,
      "learning_rate": 1.3748228894658435e-05,
      "loss": 0.5708,
      "step": 4403
    },
    {
      "epoch": 0.39649778297958543,
      "grad_norm": 1.0819213018445193,
      "learning_rate": 1.3745525162172919e-05,
      "loss": 0.621,
      "step": 4404
    },
    {
      "epoch": 0.39658781426545725,
      "grad_norm": 0.6315546750331417,
      "learning_rate": 1.3742821111165616e-05,
      "loss": 0.5484,
      "step": 4405
    },
    {
      "epoch": 0.39667784555132907,
      "grad_norm": 0.7225734577786961,
      "learning_rate": 1.3740116741866483e-05,
      "loss": 0.517,
      "step": 4406
    },
    {
      "epoch": 0.39676787683720094,
      "grad_norm": 0.8578027108095085,
      "learning_rate": 1.3737412054505499e-05,
      "loss": 0.5438,
      "step": 4407
    },
    {
      "epoch": 0.39685790812307276,
      "grad_norm": 0.6235565769605681,
      "learning_rate": 1.3734707049312674e-05,
      "loss": 0.4894,
      "step": 4408
    },
    {
      "epoch": 0.39694793940894463,
      "grad_norm": 0.7078350291626528,
      "learning_rate": 1.373200172651804e-05,
      "loss": 0.6429,
      "step": 4409
    },
    {
      "epoch": 0.39703797069481644,
      "grad_norm": 0.5857718482837554,
      "learning_rate": 1.3729296086351665e-05,
      "loss": 0.5102,
      "step": 4410
    },
    {
      "epoch": 0.3971280019806883,
      "grad_norm": 0.5405224321932308,
      "learning_rate": 1.3726590129043632e-05,
      "loss": 0.4595,
      "step": 4411
    },
    {
      "epoch": 0.39721803326656013,
      "grad_norm": 0.7817621831382808,
      "learning_rate": 1.3723883854824066e-05,
      "loss": 0.5087,
      "step": 4412
    },
    {
      "epoch": 0.39730806455243195,
      "grad_norm": 0.6571151282861953,
      "learning_rate": 1.3721177263923098e-05,
      "loss": 0.4508,
      "step": 4413
    },
    {
      "epoch": 0.3973980958383038,
      "grad_norm": 0.7252392869358789,
      "learning_rate": 1.371847035657091e-05,
      "loss": 0.537,
      "step": 4414
    },
    {
      "epoch": 0.39748812712417564,
      "grad_norm": 0.791168303062044,
      "learning_rate": 1.371576313299769e-05,
      "loss": 0.5172,
      "step": 4415
    },
    {
      "epoch": 0.3975781584100475,
      "grad_norm": 0.9583139424789714,
      "learning_rate": 1.371305559343367e-05,
      "loss": 0.6622,
      "step": 4416
    },
    {
      "epoch": 0.3976681896959193,
      "grad_norm": 0.9051297529669445,
      "learning_rate": 1.3710347738109093e-05,
      "loss": 0.5964,
      "step": 4417
    },
    {
      "epoch": 0.3977582209817912,
      "grad_norm": 0.802438558512448,
      "learning_rate": 1.370763956725424e-05,
      "loss": 0.5918,
      "step": 4418
    },
    {
      "epoch": 0.397848252267663,
      "grad_norm": 0.6702990743669566,
      "learning_rate": 1.370493108109942e-05,
      "loss": 0.5348,
      "step": 4419
    },
    {
      "epoch": 0.39793828355353483,
      "grad_norm": 0.737515650966897,
      "learning_rate": 1.3702222279874957e-05,
      "loss": 0.5226,
      "step": 4420
    },
    {
      "epoch": 0.3980283148394067,
      "grad_norm": 0.685441540936663,
      "learning_rate": 1.3699513163811214e-05,
      "loss": 0.54,
      "step": 4421
    },
    {
      "epoch": 0.3981183461252785,
      "grad_norm": 0.6618267093328742,
      "learning_rate": 1.3696803733138575e-05,
      "loss": 0.4628,
      "step": 4422
    },
    {
      "epoch": 0.3982083774111504,
      "grad_norm": 0.6027578562842144,
      "learning_rate": 1.3694093988087449e-05,
      "loss": 0.552,
      "step": 4423
    },
    {
      "epoch": 0.3982984086970222,
      "grad_norm": 0.8146590310006496,
      "learning_rate": 1.3691383928888278e-05,
      "loss": 0.5678,
      "step": 4424
    },
    {
      "epoch": 0.3983884399828941,
      "grad_norm": 0.6801548081359614,
      "learning_rate": 1.3688673555771525e-05,
      "loss": 0.5403,
      "step": 4425
    },
    {
      "epoch": 0.3984784712687659,
      "grad_norm": 0.7447039364560855,
      "learning_rate": 1.3685962868967682e-05,
      "loss": 0.5651,
      "step": 4426
    },
    {
      "epoch": 0.3985685025546377,
      "grad_norm": 0.7222167264036162,
      "learning_rate": 1.3683251868707266e-05,
      "loss": 0.5396,
      "step": 4427
    },
    {
      "epoch": 0.3986585338405096,
      "grad_norm": 1.0512975457518872,
      "learning_rate": 1.3680540555220829e-05,
      "loss": 0.5654,
      "step": 4428
    },
    {
      "epoch": 0.3987485651263814,
      "grad_norm": 0.6878373678514428,
      "learning_rate": 1.3677828928738934e-05,
      "loss": 0.4842,
      "step": 4429
    },
    {
      "epoch": 0.3988385964122533,
      "grad_norm": 0.7006520578426928,
      "learning_rate": 1.3675116989492182e-05,
      "loss": 0.5621,
      "step": 4430
    },
    {
      "epoch": 0.3989286276981251,
      "grad_norm": 0.7585269086799308,
      "learning_rate": 1.3672404737711205e-05,
      "loss": 0.564,
      "step": 4431
    },
    {
      "epoch": 0.39901865898399697,
      "grad_norm": 0.93472686950393,
      "learning_rate": 1.3669692173626645e-05,
      "loss": 0.5844,
      "step": 4432
    },
    {
      "epoch": 0.3991086902698688,
      "grad_norm": 0.6560532262159336,
      "learning_rate": 1.3666979297469183e-05,
      "loss": 0.6173,
      "step": 4433
    },
    {
      "epoch": 0.3991987215557406,
      "grad_norm": 0.6890600457868408,
      "learning_rate": 1.3664266109469524e-05,
      "loss": 0.4684,
      "step": 4434
    },
    {
      "epoch": 0.39928875284161247,
      "grad_norm": 0.6617883419559603,
      "learning_rate": 1.3661552609858401e-05,
      "loss": 0.5366,
      "step": 4435
    },
    {
      "epoch": 0.3993787841274843,
      "grad_norm": 0.735614349360046,
      "learning_rate": 1.3658838798866572e-05,
      "loss": 0.5388,
      "step": 4436
    },
    {
      "epoch": 0.39946881541335616,
      "grad_norm": 0.7516250884981727,
      "learning_rate": 1.3656124676724819e-05,
      "loss": 0.5601,
      "step": 4437
    },
    {
      "epoch": 0.399558846699228,
      "grad_norm": 0.753739027136231,
      "learning_rate": 1.3653410243663953e-05,
      "loss": 0.5818,
      "step": 4438
    },
    {
      "epoch": 0.39964887798509985,
      "grad_norm": 0.678626770565688,
      "learning_rate": 1.365069549991481e-05,
      "loss": 0.6233,
      "step": 4439
    },
    {
      "epoch": 0.39973890927097167,
      "grad_norm": 0.9109708552086216,
      "learning_rate": 1.3647980445708255e-05,
      "loss": 0.5335,
      "step": 4440
    },
    {
      "epoch": 0.3998289405568435,
      "grad_norm": 0.6640787014514431,
      "learning_rate": 1.3645265081275178e-05,
      "loss": 0.5099,
      "step": 4441
    },
    {
      "epoch": 0.39991897184271535,
      "grad_norm": 0.6502209836687417,
      "learning_rate": 1.3642549406846497e-05,
      "loss": 0.5642,
      "step": 4442
    },
    {
      "epoch": 0.40000900312858717,
      "grad_norm": 0.667656185926329,
      "learning_rate": 1.363983342265315e-05,
      "loss": 0.5151,
      "step": 4443
    },
    {
      "epoch": 0.40009903441445904,
      "grad_norm": 0.733357816806706,
      "learning_rate": 1.3637117128926109e-05,
      "loss": 0.5515,
      "step": 4444
    },
    {
      "epoch": 0.40018906570033086,
      "grad_norm": 0.8352872430848685,
      "learning_rate": 1.3634400525896371e-05,
      "loss": 0.5449,
      "step": 4445
    },
    {
      "epoch": 0.40027909698620273,
      "grad_norm": 0.6130753304749809,
      "learning_rate": 1.3631683613794955e-05,
      "loss": 0.5174,
      "step": 4446
    },
    {
      "epoch": 0.40036912827207455,
      "grad_norm": 0.9465240612626603,
      "learning_rate": 1.3628966392852907e-05,
      "loss": 0.5439,
      "step": 4447
    },
    {
      "epoch": 0.40045915955794636,
      "grad_norm": 0.7055447134247915,
      "learning_rate": 1.3626248863301306e-05,
      "loss": 0.5902,
      "step": 4448
    },
    {
      "epoch": 0.40054919084381824,
      "grad_norm": 0.6572747734055733,
      "learning_rate": 1.362353102537125e-05,
      "loss": 0.6,
      "step": 4449
    },
    {
      "epoch": 0.40063922212969005,
      "grad_norm": 0.7549956388609187,
      "learning_rate": 1.3620812879293864e-05,
      "loss": 0.6106,
      "step": 4450
    },
    {
      "epoch": 0.4007292534155619,
      "grad_norm": 0.8026397885758775,
      "learning_rate": 1.36180944253003e-05,
      "loss": 0.491,
      "step": 4451
    },
    {
      "epoch": 0.40081928470143374,
      "grad_norm": 0.8866474537670251,
      "learning_rate": 1.3615375663621746e-05,
      "loss": 0.576,
      "step": 4452
    },
    {
      "epoch": 0.4009093159873056,
      "grad_norm": 0.7364019544575259,
      "learning_rate": 1.3612656594489397e-05,
      "loss": 0.5728,
      "step": 4453
    },
    {
      "epoch": 0.40099934727317743,
      "grad_norm": 0.7172937865063486,
      "learning_rate": 1.3609937218134489e-05,
      "loss": 0.502,
      "step": 4454
    },
    {
      "epoch": 0.40108937855904925,
      "grad_norm": 0.6155414142909043,
      "learning_rate": 1.3607217534788275e-05,
      "loss": 0.606,
      "step": 4455
    },
    {
      "epoch": 0.4011794098449211,
      "grad_norm": 0.6584665300527961,
      "learning_rate": 1.3604497544682047e-05,
      "loss": 0.6086,
      "step": 4456
    },
    {
      "epoch": 0.40126944113079294,
      "grad_norm": 0.6596968631011486,
      "learning_rate": 1.3601777248047105e-05,
      "loss": 0.5887,
      "step": 4457
    },
    {
      "epoch": 0.4013594724166648,
      "grad_norm": 0.674841298622158,
      "learning_rate": 1.3599056645114792e-05,
      "loss": 0.5308,
      "step": 4458
    },
    {
      "epoch": 0.4014495037025366,
      "grad_norm": 0.8666382176967238,
      "learning_rate": 1.3596335736116468e-05,
      "loss": 0.5118,
      "step": 4459
    },
    {
      "epoch": 0.4015395349884085,
      "grad_norm": 0.6940522747779081,
      "learning_rate": 1.3593614521283518e-05,
      "loss": 0.4569,
      "step": 4460
    },
    {
      "epoch": 0.4016295662742803,
      "grad_norm": 0.8035067715802392,
      "learning_rate": 1.3590893000847358e-05,
      "loss": 0.5686,
      "step": 4461
    },
    {
      "epoch": 0.40171959756015213,
      "grad_norm": 0.717802076676631,
      "learning_rate": 1.3588171175039427e-05,
      "loss": 0.4897,
      "step": 4462
    },
    {
      "epoch": 0.401809628846024,
      "grad_norm": 0.8423434309348926,
      "learning_rate": 1.3585449044091192e-05,
      "loss": 0.5635,
      "step": 4463
    },
    {
      "epoch": 0.4018996601318958,
      "grad_norm": 0.7284570319121202,
      "learning_rate": 1.3582726608234145e-05,
      "loss": 0.5661,
      "step": 4464
    },
    {
      "epoch": 0.4019896914177677,
      "grad_norm": 0.6438100705206738,
      "learning_rate": 1.3580003867699801e-05,
      "loss": 0.445,
      "step": 4465
    },
    {
      "epoch": 0.4020797227036395,
      "grad_norm": 0.7643332501432926,
      "learning_rate": 1.3577280822719705e-05,
      "loss": 0.6406,
      "step": 4466
    },
    {
      "epoch": 0.4021697539895114,
      "grad_norm": 0.6458477614305422,
      "learning_rate": 1.3574557473525427e-05,
      "loss": 0.5522,
      "step": 4467
    },
    {
      "epoch": 0.4022597852753832,
      "grad_norm": 0.776591277957957,
      "learning_rate": 1.3571833820348561e-05,
      "loss": 0.4987,
      "step": 4468
    },
    {
      "epoch": 0.402349816561255,
      "grad_norm": 0.9565995033019312,
      "learning_rate": 1.356910986342073e-05,
      "loss": 0.5569,
      "step": 4469
    },
    {
      "epoch": 0.4024398478471269,
      "grad_norm": 0.7699845859678778,
      "learning_rate": 1.3566385602973582e-05,
      "loss": 0.5692,
      "step": 4470
    },
    {
      "epoch": 0.4025298791329987,
      "grad_norm": 0.8813684565560997,
      "learning_rate": 1.3563661039238785e-05,
      "loss": 0.5344,
      "step": 4471
    },
    {
      "epoch": 0.4026199104188706,
      "grad_norm": 0.5349745264731847,
      "learning_rate": 1.356093617244804e-05,
      "loss": 0.567,
      "step": 4472
    },
    {
      "epoch": 0.4027099417047424,
      "grad_norm": 0.7270429727494155,
      "learning_rate": 1.3558211002833075e-05,
      "loss": 0.6013,
      "step": 4473
    },
    {
      "epoch": 0.40279997299061426,
      "grad_norm": 0.7500512080713255,
      "learning_rate": 1.3555485530625635e-05,
      "loss": 0.5846,
      "step": 4474
    },
    {
      "epoch": 0.4028900042764861,
      "grad_norm": 0.7970604386866216,
      "learning_rate": 1.3552759756057493e-05,
      "loss": 0.5687,
      "step": 4475
    },
    {
      "epoch": 0.4029800355623579,
      "grad_norm": 0.8198997363980525,
      "learning_rate": 1.3550033679360462e-05,
      "loss": 0.5414,
      "step": 4476
    },
    {
      "epoch": 0.40307006684822977,
      "grad_norm": 0.7137279037660862,
      "learning_rate": 1.3547307300766362e-05,
      "loss": 0.5399,
      "step": 4477
    },
    {
      "epoch": 0.4031600981341016,
      "grad_norm": 0.6963232378297868,
      "learning_rate": 1.3544580620507044e-05,
      "loss": 0.534,
      "step": 4478
    },
    {
      "epoch": 0.40325012941997346,
      "grad_norm": 1.8095785430849949,
      "learning_rate": 1.3541853638814388e-05,
      "loss": 0.4959,
      "step": 4479
    },
    {
      "epoch": 0.4033401607058453,
      "grad_norm": 0.6640678629829551,
      "learning_rate": 1.3539126355920305e-05,
      "loss": 0.5066,
      "step": 4480
    },
    {
      "epoch": 0.40343019199171715,
      "grad_norm": 0.9765305217037102,
      "learning_rate": 1.3536398772056714e-05,
      "loss": 0.5779,
      "step": 4481
    },
    {
      "epoch": 0.40352022327758896,
      "grad_norm": 0.6800412193219101,
      "learning_rate": 1.3533670887455574e-05,
      "loss": 0.5802,
      "step": 4482
    },
    {
      "epoch": 0.4036102545634608,
      "grad_norm": 0.8417921891508682,
      "learning_rate": 1.3530942702348872e-05,
      "loss": 0.6064,
      "step": 4483
    },
    {
      "epoch": 0.40370028584933265,
      "grad_norm": 3.0101471583071056,
      "learning_rate": 1.352821421696861e-05,
      "loss": 0.6681,
      "step": 4484
    },
    {
      "epoch": 0.40379031713520447,
      "grad_norm": 0.9387764228541104,
      "learning_rate": 1.3525485431546817e-05,
      "loss": 0.6214,
      "step": 4485
    },
    {
      "epoch": 0.40388034842107634,
      "grad_norm": 0.7421830889294697,
      "learning_rate": 1.3522756346315555e-05,
      "loss": 0.5614,
      "step": 4486
    },
    {
      "epoch": 0.40397037970694816,
      "grad_norm": 0.7249216600744611,
      "learning_rate": 1.3520026961506906e-05,
      "loss": 0.58,
      "step": 4487
    },
    {
      "epoch": 0.40406041099282003,
      "grad_norm": 1.1000814192151334,
      "learning_rate": 1.3517297277352978e-05,
      "loss": 0.5909,
      "step": 4488
    },
    {
      "epoch": 0.40415044227869185,
      "grad_norm": 0.7437714052299891,
      "learning_rate": 1.3514567294085908e-05,
      "loss": 0.5305,
      "step": 4489
    },
    {
      "epoch": 0.40424047356456366,
      "grad_norm": 0.6962805228701127,
      "learning_rate": 1.3511837011937851e-05,
      "loss": 0.5454,
      "step": 4490
    },
    {
      "epoch": 0.40433050485043553,
      "grad_norm": 0.9184456584485887,
      "learning_rate": 1.3509106431140992e-05,
      "loss": 0.5968,
      "step": 4491
    },
    {
      "epoch": 0.40442053613630735,
      "grad_norm": 0.7079906864602452,
      "learning_rate": 1.3506375551927546e-05,
      "loss": 0.5431,
      "step": 4492
    },
    {
      "epoch": 0.4045105674221792,
      "grad_norm": 0.705630511763147,
      "learning_rate": 1.3503644374529744e-05,
      "loss": 0.5191,
      "step": 4493
    },
    {
      "epoch": 0.40460059870805104,
      "grad_norm": 0.6326158557879411,
      "learning_rate": 1.3500912899179852e-05,
      "loss": 0.5662,
      "step": 4494
    },
    {
      "epoch": 0.4046906299939229,
      "grad_norm": 0.8588232904243214,
      "learning_rate": 1.349818112611015e-05,
      "loss": 0.6106,
      "step": 4495
    },
    {
      "epoch": 0.40478066127979473,
      "grad_norm": 0.8011235983080515,
      "learning_rate": 1.3495449055552951e-05,
      "loss": 0.554,
      "step": 4496
    },
    {
      "epoch": 0.40487069256566655,
      "grad_norm": 0.7866647493427995,
      "learning_rate": 1.3492716687740601e-05,
      "loss": 0.5377,
      "step": 4497
    },
    {
      "epoch": 0.4049607238515384,
      "grad_norm": 0.6537343893682651,
      "learning_rate": 1.3489984022905449e-05,
      "loss": 0.6278,
      "step": 4498
    },
    {
      "epoch": 0.40505075513741023,
      "grad_norm": 0.7549990220100681,
      "learning_rate": 1.348725106127989e-05,
      "loss": 0.5602,
      "step": 4499
    },
    {
      "epoch": 0.4051407864232821,
      "grad_norm": 0.7079848433234031,
      "learning_rate": 1.3484517803096334e-05,
      "loss": 0.5591,
      "step": 4500
    },
    {
      "epoch": 0.4052308177091539,
      "grad_norm": 0.7433055625763889,
      "learning_rate": 1.3481784248587224e-05,
      "loss": 0.5406,
      "step": 4501
    },
    {
      "epoch": 0.4053208489950258,
      "grad_norm": 0.6142395883938019,
      "learning_rate": 1.3479050397985018e-05,
      "loss": 0.5216,
      "step": 4502
    },
    {
      "epoch": 0.4054108802808976,
      "grad_norm": 0.6255592445921323,
      "learning_rate": 1.3476316251522206e-05,
      "loss": 0.5469,
      "step": 4503
    },
    {
      "epoch": 0.40550091156676943,
      "grad_norm": 0.7272862100407562,
      "learning_rate": 1.3473581809431302e-05,
      "loss": 0.6127,
      "step": 4504
    },
    {
      "epoch": 0.4055909428526413,
      "grad_norm": 0.7598929464731359,
      "learning_rate": 1.347084707194484e-05,
      "loss": 0.5801,
      "step": 4505
    },
    {
      "epoch": 0.4056809741385131,
      "grad_norm": 0.7381817183711324,
      "learning_rate": 1.3468112039295392e-05,
      "loss": 0.5753,
      "step": 4506
    },
    {
      "epoch": 0.405771005424385,
      "grad_norm": 0.8518861449580404,
      "learning_rate": 1.346537671171554e-05,
      "loss": 0.5409,
      "step": 4507
    },
    {
      "epoch": 0.4058610367102568,
      "grad_norm": 0.7569624663632905,
      "learning_rate": 1.3462641089437902e-05,
      "loss": 0.6278,
      "step": 4508
    },
    {
      "epoch": 0.4059510679961287,
      "grad_norm": 0.7434089025445602,
      "learning_rate": 1.3459905172695115e-05,
      "loss": 0.5849,
      "step": 4509
    },
    {
      "epoch": 0.4060410992820005,
      "grad_norm": 0.667718531084683,
      "learning_rate": 1.3457168961719844e-05,
      "loss": 0.4521,
      "step": 4510
    },
    {
      "epoch": 0.4061311305678723,
      "grad_norm": 0.8512296491105074,
      "learning_rate": 1.3454432456744782e-05,
      "loss": 0.5779,
      "step": 4511
    },
    {
      "epoch": 0.4062211618537442,
      "grad_norm": 0.8359886057128181,
      "learning_rate": 1.3451695658002632e-05,
      "loss": 0.689,
      "step": 4512
    },
    {
      "epoch": 0.406311193139616,
      "grad_norm": 0.654604416972999,
      "learning_rate": 1.3448958565726144e-05,
      "loss": 0.5611,
      "step": 4513
    },
    {
      "epoch": 0.4064012244254879,
      "grad_norm": 0.5834864667722766,
      "learning_rate": 1.3446221180148076e-05,
      "loss": 0.5318,
      "step": 4514
    },
    {
      "epoch": 0.4064912557113597,
      "grad_norm": 0.6757269246535161,
      "learning_rate": 1.344348350150122e-05,
      "loss": 0.6175,
      "step": 4515
    },
    {
      "epoch": 0.40658128699723156,
      "grad_norm": 0.8833396854058524,
      "learning_rate": 1.3440745530018388e-05,
      "loss": 0.6028,
      "step": 4516
    },
    {
      "epoch": 0.4066713182831034,
      "grad_norm": 0.8054886357277529,
      "learning_rate": 1.343800726593242e-05,
      "loss": 0.4773,
      "step": 4517
    },
    {
      "epoch": 0.4067613495689752,
      "grad_norm": 1.0112831627326049,
      "learning_rate": 1.3435268709476181e-05,
      "loss": 0.603,
      "step": 4518
    },
    {
      "epoch": 0.40685138085484707,
      "grad_norm": 0.9525108963089367,
      "learning_rate": 1.3432529860882558e-05,
      "loss": 0.5696,
      "step": 4519
    },
    {
      "epoch": 0.4069414121407189,
      "grad_norm": 0.6290941913663506,
      "learning_rate": 1.3429790720384461e-05,
      "loss": 0.5545,
      "step": 4520
    },
    {
      "epoch": 0.40703144342659076,
      "grad_norm": 0.8016548485204967,
      "learning_rate": 1.3427051288214832e-05,
      "loss": 0.4981,
      "step": 4521
    },
    {
      "epoch": 0.40712147471246257,
      "grad_norm": 0.6877627023548146,
      "learning_rate": 1.3424311564606636e-05,
      "loss": 0.5933,
      "step": 4522
    },
    {
      "epoch": 0.40721150599833444,
      "grad_norm": 1.0073375954978707,
      "learning_rate": 1.3421571549792858e-05,
      "loss": 0.6615,
      "step": 4523
    },
    {
      "epoch": 0.40730153728420626,
      "grad_norm": 0.6618177895044165,
      "learning_rate": 1.3418831244006506e-05,
      "loss": 0.4464,
      "step": 4524
    },
    {
      "epoch": 0.4073915685700781,
      "grad_norm": 0.7298312156911304,
      "learning_rate": 1.341609064748063e-05,
      "loss": 0.5691,
      "step": 4525
    },
    {
      "epoch": 0.40748159985594995,
      "grad_norm": 0.6327660917764054,
      "learning_rate": 1.341334976044828e-05,
      "loss": 0.4768,
      "step": 4526
    },
    {
      "epoch": 0.40757163114182177,
      "grad_norm": 0.7447953479238804,
      "learning_rate": 1.3410608583142547e-05,
      "loss": 0.5938,
      "step": 4527
    },
    {
      "epoch": 0.40766166242769364,
      "grad_norm": 0.6377866726251263,
      "learning_rate": 1.340786711579654e-05,
      "loss": 0.4204,
      "step": 4528
    },
    {
      "epoch": 0.40775169371356546,
      "grad_norm": 0.7645807595873034,
      "learning_rate": 1.34051253586434e-05,
      "loss": 0.6319,
      "step": 4529
    },
    {
      "epoch": 0.4078417249994373,
      "grad_norm": 0.7794138449959055,
      "learning_rate": 1.3402383311916282e-05,
      "loss": 0.5434,
      "step": 4530
    },
    {
      "epoch": 0.40793175628530914,
      "grad_norm": 0.7630106044207303,
      "learning_rate": 1.3399640975848379e-05,
      "loss": 0.4704,
      "step": 4531
    },
    {
      "epoch": 0.40802178757118096,
      "grad_norm": 0.9641684737316917,
      "learning_rate": 1.3396898350672895e-05,
      "loss": 0.5351,
      "step": 4532
    },
    {
      "epoch": 0.40811181885705283,
      "grad_norm": 0.7596608040739397,
      "learning_rate": 1.3394155436623067e-05,
      "loss": 0.6253,
      "step": 4533
    },
    {
      "epoch": 0.40820185014292465,
      "grad_norm": 0.6923478429526497,
      "learning_rate": 1.3391412233932148e-05,
      "loss": 0.5572,
      "step": 4534
    },
    {
      "epoch": 0.4082918814287965,
      "grad_norm": 0.5537660454277605,
      "learning_rate": 1.3388668742833431e-05,
      "loss": 0.5524,
      "step": 4535
    },
    {
      "epoch": 0.40838191271466834,
      "grad_norm": 0.7169485074041656,
      "learning_rate": 1.338592496356022e-05,
      "loss": 0.5886,
      "step": 4536
    },
    {
      "epoch": 0.4084719440005402,
      "grad_norm": 0.9296632098194824,
      "learning_rate": 1.3383180896345846e-05,
      "loss": 0.5431,
      "step": 4537
    },
    {
      "epoch": 0.408561975286412,
      "grad_norm": 0.931417590085901,
      "learning_rate": 1.338043654142367e-05,
      "loss": 0.6292,
      "step": 4538
    },
    {
      "epoch": 0.40865200657228384,
      "grad_norm": 0.7440914568818382,
      "learning_rate": 1.3377691899027071e-05,
      "loss": 0.6283,
      "step": 4539
    },
    {
      "epoch": 0.4087420378581557,
      "grad_norm": 0.71015748833275,
      "learning_rate": 1.3374946969389452e-05,
      "loss": 0.551,
      "step": 4540
    },
    {
      "epoch": 0.40883206914402753,
      "grad_norm": 0.733251544563366,
      "learning_rate": 1.3372201752744251e-05,
      "loss": 0.607,
      "step": 4541
    },
    {
      "epoch": 0.4089221004298994,
      "grad_norm": 0.6648470159515752,
      "learning_rate": 1.3369456249324916e-05,
      "loss": 0.5449,
      "step": 4542
    },
    {
      "epoch": 0.4090121317157712,
      "grad_norm": 0.7803422110956708,
      "learning_rate": 1.3366710459364933e-05,
      "loss": 0.5655,
      "step": 4543
    },
    {
      "epoch": 0.4091021630016431,
      "grad_norm": 0.6982941781750215,
      "learning_rate": 1.33639643830978e-05,
      "loss": 0.5197,
      "step": 4544
    },
    {
      "epoch": 0.4091921942875149,
      "grad_norm": 0.6123242295500931,
      "learning_rate": 1.3361218020757048e-05,
      "loss": 0.5107,
      "step": 4545
    },
    {
      "epoch": 0.4092822255733867,
      "grad_norm": 0.861885240348118,
      "learning_rate": 1.3358471372576229e-05,
      "loss": 0.6121,
      "step": 4546
    },
    {
      "epoch": 0.4093722568592586,
      "grad_norm": 0.7130383922271908,
      "learning_rate": 1.3355724438788915e-05,
      "loss": 0.5759,
      "step": 4547
    },
    {
      "epoch": 0.4094622881451304,
      "grad_norm": 0.7954746184212675,
      "learning_rate": 1.3352977219628716e-05,
      "loss": 0.5499,
      "step": 4548
    },
    {
      "epoch": 0.4095523194310023,
      "grad_norm": 0.7225218056713848,
      "learning_rate": 1.3350229715329248e-05,
      "loss": 0.5467,
      "step": 4549
    },
    {
      "epoch": 0.4096423507168741,
      "grad_norm": 0.6546887166222238,
      "learning_rate": 1.3347481926124168e-05,
      "loss": 0.4893,
      "step": 4550
    },
    {
      "epoch": 0.409732382002746,
      "grad_norm": 1.1496787947568317,
      "learning_rate": 1.3344733852247145e-05,
      "loss": 0.5334,
      "step": 4551
    },
    {
      "epoch": 0.4098224132886178,
      "grad_norm": 0.7274031493346476,
      "learning_rate": 1.3341985493931877e-05,
      "loss": 0.4973,
      "step": 4552
    },
    {
      "epoch": 0.4099124445744896,
      "grad_norm": 0.7308192864502567,
      "learning_rate": 1.333923685141209e-05,
      "loss": 0.5577,
      "step": 4553
    },
    {
      "epoch": 0.4100024758603615,
      "grad_norm": 0.7032978056326058,
      "learning_rate": 1.3336487924921526e-05,
      "loss": 0.5013,
      "step": 4554
    },
    {
      "epoch": 0.4100925071462333,
      "grad_norm": 0.6632668913456368,
      "learning_rate": 1.3333738714693958e-05,
      "loss": 0.5452,
      "step": 4555
    },
    {
      "epoch": 0.41018253843210517,
      "grad_norm": 0.6703434810856599,
      "learning_rate": 1.3330989220963177e-05,
      "loss": 0.5784,
      "step": 4556
    },
    {
      "epoch": 0.410272569717977,
      "grad_norm": 0.6690453205523542,
      "learning_rate": 1.332823944396301e-05,
      "loss": 0.5715,
      "step": 4557
    },
    {
      "epoch": 0.41036260100384886,
      "grad_norm": 0.8912505509193751,
      "learning_rate": 1.3325489383927288e-05,
      "loss": 0.5713,
      "step": 4558
    },
    {
      "epoch": 0.4104526322897207,
      "grad_norm": 0.7360938881785877,
      "learning_rate": 1.3322739041089885e-05,
      "loss": 0.527,
      "step": 4559
    },
    {
      "epoch": 0.4105426635755925,
      "grad_norm": 0.7649243510784057,
      "learning_rate": 1.3319988415684695e-05,
      "loss": 0.5468,
      "step": 4560
    },
    {
      "epoch": 0.41063269486146436,
      "grad_norm": 0.807221909748092,
      "learning_rate": 1.3317237507945624e-05,
      "loss": 0.6203,
      "step": 4561
    },
    {
      "epoch": 0.4107227261473362,
      "grad_norm": 0.7902530329639623,
      "learning_rate": 1.3314486318106616e-05,
      "loss": 0.5457,
      "step": 4562
    },
    {
      "epoch": 0.41081275743320805,
      "grad_norm": 0.7374139582378874,
      "learning_rate": 1.3311734846401636e-05,
      "loss": 0.6228,
      "step": 4563
    },
    {
      "epoch": 0.41090278871907987,
      "grad_norm": 0.7640825422335575,
      "learning_rate": 1.3308983093064666e-05,
      "loss": 0.517,
      "step": 4564
    },
    {
      "epoch": 0.41099282000495174,
      "grad_norm": 0.7700630980741939,
      "learning_rate": 1.3306231058329722e-05,
      "loss": 0.4681,
      "step": 4565
    },
    {
      "epoch": 0.41108285129082356,
      "grad_norm": 0.730018080235602,
      "learning_rate": 1.3303478742430831e-05,
      "loss": 0.5223,
      "step": 4566
    },
    {
      "epoch": 0.4111728825766954,
      "grad_norm": 0.7259751173609379,
      "learning_rate": 1.3300726145602062e-05,
      "loss": 0.4861,
      "step": 4567
    },
    {
      "epoch": 0.41126291386256725,
      "grad_norm": 0.7288103875393142,
      "learning_rate": 1.3297973268077488e-05,
      "loss": 0.5422,
      "step": 4568
    },
    {
      "epoch": 0.41135294514843906,
      "grad_norm": 0.761764231036222,
      "learning_rate": 1.3295220110091221e-05,
      "loss": 0.5042,
      "step": 4569
    },
    {
      "epoch": 0.41144297643431094,
      "grad_norm": 0.7385597934276296,
      "learning_rate": 1.3292466671877388e-05,
      "loss": 0.5381,
      "step": 4570
    },
    {
      "epoch": 0.41153300772018275,
      "grad_norm": 0.704098216235829,
      "learning_rate": 1.3289712953670149e-05,
      "loss": 0.5052,
      "step": 4571
    },
    {
      "epoch": 0.4116230390060546,
      "grad_norm": 0.6442725270488218,
      "learning_rate": 1.3286958955703675e-05,
      "loss": 0.5517,
      "step": 4572
    },
    {
      "epoch": 0.41171307029192644,
      "grad_norm": 0.7391434870839017,
      "learning_rate": 1.3284204678212169e-05,
      "loss": 0.4878,
      "step": 4573
    },
    {
      "epoch": 0.41180310157779826,
      "grad_norm": 0.7553156952214162,
      "learning_rate": 1.328145012142986e-05,
      "loss": 0.4786,
      "step": 4574
    },
    {
      "epoch": 0.41189313286367013,
      "grad_norm": 0.5881461466927266,
      "learning_rate": 1.3278695285590994e-05,
      "loss": 0.5453,
      "step": 4575
    },
    {
      "epoch": 0.41198316414954195,
      "grad_norm": 0.7236831789645122,
      "learning_rate": 1.3275940170929845e-05,
      "loss": 0.4046,
      "step": 4576
    },
    {
      "epoch": 0.4120731954354138,
      "grad_norm": 0.9932462447327425,
      "learning_rate": 1.327318477768071e-05,
      "loss": 0.5389,
      "step": 4577
    },
    {
      "epoch": 0.41216322672128564,
      "grad_norm": 0.827579666584656,
      "learning_rate": 1.3270429106077909e-05,
      "loss": 0.6455,
      "step": 4578
    },
    {
      "epoch": 0.4122532580071575,
      "grad_norm": 0.741336316417432,
      "learning_rate": 1.3267673156355785e-05,
      "loss": 0.5334,
      "step": 4579
    },
    {
      "epoch": 0.4123432892930293,
      "grad_norm": 0.7293082937869569,
      "learning_rate": 1.3264916928748707e-05,
      "loss": 0.6111,
      "step": 4580
    },
    {
      "epoch": 0.41243332057890114,
      "grad_norm": 0.6952206001399762,
      "learning_rate": 1.326216042349107e-05,
      "loss": 0.5415,
      "step": 4581
    },
    {
      "epoch": 0.412523351864773,
      "grad_norm": 0.6972163370516667,
      "learning_rate": 1.3259403640817281e-05,
      "loss": 0.5782,
      "step": 4582
    },
    {
      "epoch": 0.41261338315064483,
      "grad_norm": 0.6717674813762368,
      "learning_rate": 1.3256646580961782e-05,
      "loss": 0.5295,
      "step": 4583
    },
    {
      "epoch": 0.4127034144365167,
      "grad_norm": 0.7185859782745903,
      "learning_rate": 1.3253889244159038e-05,
      "loss": 0.5529,
      "step": 4584
    },
    {
      "epoch": 0.4127934457223885,
      "grad_norm": 0.6552427054939152,
      "learning_rate": 1.3251131630643531e-05,
      "loss": 0.6072,
      "step": 4585
    },
    {
      "epoch": 0.4128834770082604,
      "grad_norm": 0.5747964061876059,
      "learning_rate": 1.3248373740649771e-05,
      "loss": 0.5791,
      "step": 4586
    },
    {
      "epoch": 0.4129735082941322,
      "grad_norm": 1.2010496274880091,
      "learning_rate": 1.3245615574412291e-05,
      "loss": 0.6521,
      "step": 4587
    },
    {
      "epoch": 0.413063539580004,
      "grad_norm": 0.7219889994860837,
      "learning_rate": 1.324285713216565e-05,
      "loss": 0.5386,
      "step": 4588
    },
    {
      "epoch": 0.4131535708658759,
      "grad_norm": 0.5815820496481622,
      "learning_rate": 1.324009841414442e-05,
      "loss": 0.5738,
      "step": 4589
    },
    {
      "epoch": 0.4132436021517477,
      "grad_norm": 0.7988087931052945,
      "learning_rate": 1.3237339420583213e-05,
      "loss": 0.5972,
      "step": 4590
    },
    {
      "epoch": 0.4133336334376196,
      "grad_norm": 0.7609791097904443,
      "learning_rate": 1.3234580151716649e-05,
      "loss": 0.5232,
      "step": 4591
    },
    {
      "epoch": 0.4134236647234914,
      "grad_norm": 1.6130895246062311,
      "learning_rate": 1.3231820607779381e-05,
      "loss": 0.704,
      "step": 4592
    },
    {
      "epoch": 0.4135136960093633,
      "grad_norm": 0.7657535850068995,
      "learning_rate": 1.3229060789006083e-05,
      "loss": 0.5241,
      "step": 4593
    },
    {
      "epoch": 0.4136037272952351,
      "grad_norm": 0.6454585963490517,
      "learning_rate": 1.3226300695631448e-05,
      "loss": 0.4628,
      "step": 4594
    },
    {
      "epoch": 0.4136937585811069,
      "grad_norm": 0.9221505810656963,
      "learning_rate": 1.3223540327890204e-05,
      "loss": 0.6899,
      "step": 4595
    },
    {
      "epoch": 0.4137837898669788,
      "grad_norm": 0.8794319924512212,
      "learning_rate": 1.3220779686017085e-05,
      "loss": 0.5876,
      "step": 4596
    },
    {
      "epoch": 0.4138738211528506,
      "grad_norm": 0.7720755994071048,
      "learning_rate": 1.3218018770246858e-05,
      "loss": 0.5732,
      "step": 4597
    },
    {
      "epoch": 0.41396385243872247,
      "grad_norm": 0.8146446564417892,
      "learning_rate": 1.321525758081432e-05,
      "loss": 0.527,
      "step": 4598
    },
    {
      "epoch": 0.4140538837245943,
      "grad_norm": 0.7857111736122864,
      "learning_rate": 1.3212496117954283e-05,
      "loss": 0.5976,
      "step": 4599
    },
    {
      "epoch": 0.41414391501046616,
      "grad_norm": 0.8309770470374002,
      "learning_rate": 1.320973438190158e-05,
      "loss": 0.5204,
      "step": 4600
    },
    {
      "epoch": 0.414233946296338,
      "grad_norm": 0.6498839327407108,
      "learning_rate": 1.3206972372891069e-05,
      "loss": 0.6178,
      "step": 4601
    },
    {
      "epoch": 0.4143239775822098,
      "grad_norm": 0.8528414254731501,
      "learning_rate": 1.320421009115764e-05,
      "loss": 0.5279,
      "step": 4602
    },
    {
      "epoch": 0.41441400886808166,
      "grad_norm": 0.6649792994517238,
      "learning_rate": 1.3201447536936193e-05,
      "loss": 0.5274,
      "step": 4603
    },
    {
      "epoch": 0.4145040401539535,
      "grad_norm": 0.7440065236249364,
      "learning_rate": 1.319868471046166e-05,
      "loss": 0.5294,
      "step": 4604
    },
    {
      "epoch": 0.41459407143982535,
      "grad_norm": 0.5783865790545575,
      "learning_rate": 1.3195921611968993e-05,
      "loss": 0.5219,
      "step": 4605
    },
    {
      "epoch": 0.41468410272569717,
      "grad_norm": 0.6708459135466948,
      "learning_rate": 1.3193158241693168e-05,
      "loss": 0.4718,
      "step": 4606
    },
    {
      "epoch": 0.41477413401156904,
      "grad_norm": 0.7360140233851367,
      "learning_rate": 1.3190394599869186e-05,
      "loss": 0.5623,
      "step": 4607
    },
    {
      "epoch": 0.41486416529744086,
      "grad_norm": 0.7778420018869937,
      "learning_rate": 1.318763068673206e-05,
      "loss": 0.525,
      "step": 4608
    },
    {
      "epoch": 0.41495419658331273,
      "grad_norm": 0.8300266243932394,
      "learning_rate": 1.3184866502516846e-05,
      "loss": 0.4975,
      "step": 4609
    },
    {
      "epoch": 0.41504422786918455,
      "grad_norm": 0.6419535608478727,
      "learning_rate": 1.3182102047458606e-05,
      "loss": 0.5823,
      "step": 4610
    },
    {
      "epoch": 0.41513425915505636,
      "grad_norm": 0.7455646647410872,
      "learning_rate": 1.3179337321792432e-05,
      "loss": 0.5551,
      "step": 4611
    },
    {
      "epoch": 0.41522429044092823,
      "grad_norm": 0.7037104056873645,
      "learning_rate": 1.3176572325753435e-05,
      "loss": 0.5793,
      "step": 4612
    },
    {
      "epoch": 0.41531432172680005,
      "grad_norm": 0.6865963619552273,
      "learning_rate": 1.317380705957676e-05,
      "loss": 0.4019,
      "step": 4613
    },
    {
      "epoch": 0.4154043530126719,
      "grad_norm": 0.7596852554925149,
      "learning_rate": 1.3171041523497558e-05,
      "loss": 0.5845,
      "step": 4614
    },
    {
      "epoch": 0.41549438429854374,
      "grad_norm": 0.7950325279438821,
      "learning_rate": 1.3168275717751017e-05,
      "loss": 0.5058,
      "step": 4615
    },
    {
      "epoch": 0.4155844155844156,
      "grad_norm": 0.7406030809039537,
      "learning_rate": 1.3165509642572346e-05,
      "loss": 0.5938,
      "step": 4616
    },
    {
      "epoch": 0.41567444687028743,
      "grad_norm": 0.7817533934036882,
      "learning_rate": 1.3162743298196763e-05,
      "loss": 0.5886,
      "step": 4617
    },
    {
      "epoch": 0.41576447815615925,
      "grad_norm": 0.8263955668584706,
      "learning_rate": 1.3159976684859528e-05,
      "loss": 0.559,
      "step": 4618
    },
    {
      "epoch": 0.4158545094420311,
      "grad_norm": 0.6271858036793254,
      "learning_rate": 1.3157209802795911e-05,
      "loss": 0.5283,
      "step": 4619
    },
    {
      "epoch": 0.41594454072790293,
      "grad_norm": 0.6963032105176834,
      "learning_rate": 1.3154442652241218e-05,
      "loss": 0.5809,
      "step": 4620
    },
    {
      "epoch": 0.4160345720137748,
      "grad_norm": 0.6709343745594235,
      "learning_rate": 1.3151675233430759e-05,
      "loss": 0.5843,
      "step": 4621
    },
    {
      "epoch": 0.4161246032996466,
      "grad_norm": 0.7900522699573866,
      "learning_rate": 1.3148907546599881e-05,
      "loss": 0.5453,
      "step": 4622
    },
    {
      "epoch": 0.4162146345855185,
      "grad_norm": 0.7080633476742223,
      "learning_rate": 1.3146139591983954e-05,
      "loss": 0.5994,
      "step": 4623
    },
    {
      "epoch": 0.4163046658713903,
      "grad_norm": 0.7744122763974066,
      "learning_rate": 1.3143371369818358e-05,
      "loss": 0.6435,
      "step": 4624
    },
    {
      "epoch": 0.41639469715726213,
      "grad_norm": 0.8905297130873634,
      "learning_rate": 1.314060288033851e-05,
      "loss": 0.517,
      "step": 4625
    },
    {
      "epoch": 0.416484728443134,
      "grad_norm": 0.7370058777437453,
      "learning_rate": 1.3137834123779843e-05,
      "loss": 0.5462,
      "step": 4626
    },
    {
      "epoch": 0.4165747597290058,
      "grad_norm": 0.717076854026124,
      "learning_rate": 1.3135065100377816e-05,
      "loss": 0.5203,
      "step": 4627
    },
    {
      "epoch": 0.4166647910148777,
      "grad_norm": 0.7138404513603793,
      "learning_rate": 1.31322958103679e-05,
      "loss": 0.4888,
      "step": 4628
    },
    {
      "epoch": 0.4167548223007495,
      "grad_norm": 0.7549188584832592,
      "learning_rate": 1.3129526253985609e-05,
      "loss": 0.5811,
      "step": 4629
    },
    {
      "epoch": 0.4168448535866214,
      "grad_norm": 0.87350405347872,
      "learning_rate": 1.3126756431466463e-05,
      "loss": 0.5678,
      "step": 4630
    },
    {
      "epoch": 0.4169348848724932,
      "grad_norm": 0.6926863209773014,
      "learning_rate": 1.3123986343046004e-05,
      "loss": 0.5206,
      "step": 4631
    },
    {
      "epoch": 0.417024916158365,
      "grad_norm": 0.7023938619078091,
      "learning_rate": 1.3121215988959808e-05,
      "loss": 0.6171,
      "step": 4632
    },
    {
      "epoch": 0.4171149474442369,
      "grad_norm": 0.6202580712095106,
      "learning_rate": 1.3118445369443467e-05,
      "loss": 0.4705,
      "step": 4633
    },
    {
      "epoch": 0.4172049787301087,
      "grad_norm": 0.800597555909606,
      "learning_rate": 1.3115674484732596e-05,
      "loss": 0.5825,
      "step": 4634
    },
    {
      "epoch": 0.41729501001598057,
      "grad_norm": 0.9845093012167953,
      "learning_rate": 1.3112903335062831e-05,
      "loss": 0.5891,
      "step": 4635
    },
    {
      "epoch": 0.4173850413018524,
      "grad_norm": 0.603597525073869,
      "learning_rate": 1.3110131920669833e-05,
      "loss": 0.4849,
      "step": 4636
    },
    {
      "epoch": 0.41747507258772426,
      "grad_norm": 0.737710997346421,
      "learning_rate": 1.3107360241789288e-05,
      "loss": 0.4681,
      "step": 4637
    },
    {
      "epoch": 0.4175651038735961,
      "grad_norm": 0.7465304634208765,
      "learning_rate": 1.3104588298656897e-05,
      "loss": 0.5754,
      "step": 4638
    },
    {
      "epoch": 0.4176551351594679,
      "grad_norm": 0.7969073066604423,
      "learning_rate": 1.3101816091508389e-05,
      "loss": 0.5562,
      "step": 4639
    },
    {
      "epoch": 0.41774516644533977,
      "grad_norm": 0.7598595301261041,
      "learning_rate": 1.3099043620579513e-05,
      "loss": 0.5414,
      "step": 4640
    },
    {
      "epoch": 0.4178351977312116,
      "grad_norm": 0.7652582484275489,
      "learning_rate": 1.3096270886106047e-05,
      "loss": 0.5605,
      "step": 4641
    },
    {
      "epoch": 0.41792522901708345,
      "grad_norm": 0.6734180636412223,
      "learning_rate": 1.3093497888323778e-05,
      "loss": 0.5332,
      "step": 4642
    },
    {
      "epoch": 0.41801526030295527,
      "grad_norm": 0.9225889886035188,
      "learning_rate": 1.309072462746853e-05,
      "loss": 0.5279,
      "step": 4643
    },
    {
      "epoch": 0.41810529158882714,
      "grad_norm": 0.6352990898316468,
      "learning_rate": 1.3087951103776142e-05,
      "loss": 0.4869,
      "step": 4644
    },
    {
      "epoch": 0.41819532287469896,
      "grad_norm": 0.7743700267758128,
      "learning_rate": 1.3085177317482472e-05,
      "loss": 0.5154,
      "step": 4645
    },
    {
      "epoch": 0.4182853541605708,
      "grad_norm": 0.7881315700433165,
      "learning_rate": 1.3082403268823407e-05,
      "loss": 0.5713,
      "step": 4646
    },
    {
      "epoch": 0.41837538544644265,
      "grad_norm": 0.8628306833465663,
      "learning_rate": 1.3079628958034856e-05,
      "loss": 0.5758,
      "step": 4647
    },
    {
      "epoch": 0.41846541673231447,
      "grad_norm": 0.6514215942654714,
      "learning_rate": 1.3076854385352746e-05,
      "loss": 0.5665,
      "step": 4648
    },
    {
      "epoch": 0.41855544801818634,
      "grad_norm": 0.9210900559862014,
      "learning_rate": 1.3074079551013026e-05,
      "loss": 0.5029,
      "step": 4649
    },
    {
      "epoch": 0.41864547930405815,
      "grad_norm": 0.6803223602718048,
      "learning_rate": 1.3071304455251675e-05,
      "loss": 0.4777,
      "step": 4650
    },
    {
      "epoch": 0.41873551058993,
      "grad_norm": 0.7948296290801248,
      "learning_rate": 1.3068529098304685e-05,
      "loss": 0.5475,
      "step": 4651
    },
    {
      "epoch": 0.41882554187580184,
      "grad_norm": 0.8614378261661284,
      "learning_rate": 1.3065753480408074e-05,
      "loss": 0.5999,
      "step": 4652
    },
    {
      "epoch": 0.41891557316167366,
      "grad_norm": 0.6542786845938634,
      "learning_rate": 1.3062977601797884e-05,
      "loss": 0.4958,
      "step": 4653
    },
    {
      "epoch": 0.41900560444754553,
      "grad_norm": 0.7358985814370086,
      "learning_rate": 1.3060201462710177e-05,
      "loss": 0.6185,
      "step": 4654
    },
    {
      "epoch": 0.41909563573341735,
      "grad_norm": 0.8463595741824033,
      "learning_rate": 1.3057425063381039e-05,
      "loss": 0.5129,
      "step": 4655
    },
    {
      "epoch": 0.4191856670192892,
      "grad_norm": 0.6933647682287026,
      "learning_rate": 1.3054648404046573e-05,
      "loss": 0.5222,
      "step": 4656
    },
    {
      "epoch": 0.41927569830516104,
      "grad_norm": 0.663643191259069,
      "learning_rate": 1.305187148494291e-05,
      "loss": 0.5915,
      "step": 4657
    },
    {
      "epoch": 0.4193657295910329,
      "grad_norm": 0.6883138697258163,
      "learning_rate": 1.3049094306306205e-05,
      "loss": 0.5081,
      "step": 4658
    },
    {
      "epoch": 0.4194557608769047,
      "grad_norm": 0.6244345025203335,
      "learning_rate": 1.3046316868372622e-05,
      "loss": 0.5347,
      "step": 4659
    },
    {
      "epoch": 0.41954579216277654,
      "grad_norm": 0.8498756348245889,
      "learning_rate": 1.3043539171378362e-05,
      "loss": 0.5065,
      "step": 4660
    },
    {
      "epoch": 0.4196358234486484,
      "grad_norm": 0.7817722973525806,
      "learning_rate": 1.3040761215559642e-05,
      "loss": 0.6128,
      "step": 4661
    },
    {
      "epoch": 0.41972585473452023,
      "grad_norm": 0.7648309396635303,
      "learning_rate": 1.3037983001152701e-05,
      "loss": 0.6263,
      "step": 4662
    },
    {
      "epoch": 0.4198158860203921,
      "grad_norm": 0.5905291915546741,
      "learning_rate": 1.3035204528393797e-05,
      "loss": 0.5436,
      "step": 4663
    },
    {
      "epoch": 0.4199059173062639,
      "grad_norm": 0.6484763839416614,
      "learning_rate": 1.3032425797519215e-05,
      "loss": 0.4673,
      "step": 4664
    },
    {
      "epoch": 0.4199959485921358,
      "grad_norm": 0.6384185469480986,
      "learning_rate": 1.3029646808765261e-05,
      "loss": 0.6138,
      "step": 4665
    },
    {
      "epoch": 0.4200859798780076,
      "grad_norm": 1.028657648602886,
      "learning_rate": 1.3026867562368262e-05,
      "loss": 0.624,
      "step": 4666
    },
    {
      "epoch": 0.4201760111638794,
      "grad_norm": 0.6086496502359829,
      "learning_rate": 1.3024088058564563e-05,
      "loss": 0.5691,
      "step": 4667
    },
    {
      "epoch": 0.4202660424497513,
      "grad_norm": 0.7478862489832697,
      "learning_rate": 1.302130829759054e-05,
      "loss": 0.4524,
      "step": 4668
    },
    {
      "epoch": 0.4203560737356231,
      "grad_norm": 0.8843310478048618,
      "learning_rate": 1.3018528279682585e-05,
      "loss": 0.538,
      "step": 4669
    },
    {
      "epoch": 0.420446105021495,
      "grad_norm": 0.7620502693089759,
      "learning_rate": 1.301574800507711e-05,
      "loss": 0.5984,
      "step": 4670
    },
    {
      "epoch": 0.4205361363073668,
      "grad_norm": 0.761349372832721,
      "learning_rate": 1.3012967474010549e-05,
      "loss": 0.5158,
      "step": 4671
    },
    {
      "epoch": 0.4206261675932387,
      "grad_norm": 0.6227711753576674,
      "learning_rate": 1.3010186686719366e-05,
      "loss": 0.4816,
      "step": 4672
    },
    {
      "epoch": 0.4207161988791105,
      "grad_norm": 0.7104169976225359,
      "learning_rate": 1.3007405643440039e-05,
      "loss": 0.4949,
      "step": 4673
    },
    {
      "epoch": 0.4208062301649823,
      "grad_norm": 0.6735780667949043,
      "learning_rate": 1.3004624344409065e-05,
      "loss": 0.5036,
      "step": 4674
    },
    {
      "epoch": 0.4208962614508542,
      "grad_norm": 0.7931151592668324,
      "learning_rate": 1.3001842789862973e-05,
      "loss": 0.4734,
      "step": 4675
    },
    {
      "epoch": 0.420986292736726,
      "grad_norm": 0.8324669806945619,
      "learning_rate": 1.2999060980038307e-05,
      "loss": 0.6798,
      "step": 4676
    },
    {
      "epoch": 0.42107632402259787,
      "grad_norm": 0.6369177605539851,
      "learning_rate": 1.2996278915171631e-05,
      "loss": 0.5874,
      "step": 4677
    },
    {
      "epoch": 0.4211663553084697,
      "grad_norm": 0.8873081909423857,
      "learning_rate": 1.2993496595499537e-05,
      "loss": 0.6554,
      "step": 4678
    },
    {
      "epoch": 0.42125638659434156,
      "grad_norm": 0.6759250629126208,
      "learning_rate": 1.2990714021258635e-05,
      "loss": 0.5542,
      "step": 4679
    },
    {
      "epoch": 0.4213464178802134,
      "grad_norm": 0.6613973957830662,
      "learning_rate": 1.2987931192685551e-05,
      "loss": 0.5239,
      "step": 4680
    },
    {
      "epoch": 0.4214364491660852,
      "grad_norm": 0.608882032412367,
      "learning_rate": 1.2985148110016947e-05,
      "loss": 0.5125,
      "step": 4681
    },
    {
      "epoch": 0.42152648045195706,
      "grad_norm": 0.7589105569855854,
      "learning_rate": 1.2982364773489492e-05,
      "loss": 0.5453,
      "step": 4682
    },
    {
      "epoch": 0.4216165117378289,
      "grad_norm": 0.7219677578506306,
      "learning_rate": 1.2979581183339888e-05,
      "loss": 0.6326,
      "step": 4683
    },
    {
      "epoch": 0.42170654302370075,
      "grad_norm": 0.7911097644563047,
      "learning_rate": 1.2976797339804847e-05,
      "loss": 0.5358,
      "step": 4684
    },
    {
      "epoch": 0.42179657430957257,
      "grad_norm": 0.8457507639094697,
      "learning_rate": 1.2974013243121113e-05,
      "loss": 0.5752,
      "step": 4685
    },
    {
      "epoch": 0.42188660559544444,
      "grad_norm": 0.710663925890559,
      "learning_rate": 1.2971228893525448e-05,
      "loss": 0.5313,
      "step": 4686
    },
    {
      "epoch": 0.42197663688131626,
      "grad_norm": 0.811003839253761,
      "learning_rate": 1.296844429125463e-05,
      "loss": 0.5364,
      "step": 4687
    },
    {
      "epoch": 0.4220666681671881,
      "grad_norm": 0.6691656575752789,
      "learning_rate": 1.2965659436545466e-05,
      "loss": 0.5481,
      "step": 4688
    },
    {
      "epoch": 0.42215669945305995,
      "grad_norm": 0.6807276409789658,
      "learning_rate": 1.2962874329634782e-05,
      "loss": 0.5099,
      "step": 4689
    },
    {
      "epoch": 0.42224673073893176,
      "grad_norm": 0.6800341600441034,
      "learning_rate": 1.2960088970759428e-05,
      "loss": 0.5674,
      "step": 4690
    },
    {
      "epoch": 0.42233676202480364,
      "grad_norm": 0.6953578662825066,
      "learning_rate": 1.2957303360156268e-05,
      "loss": 0.5826,
      "step": 4691
    },
    {
      "epoch": 0.42242679331067545,
      "grad_norm": 0.6396167303304777,
      "learning_rate": 1.2954517498062194e-05,
      "loss": 0.521,
      "step": 4692
    },
    {
      "epoch": 0.4225168245965473,
      "grad_norm": 0.86310367480116,
      "learning_rate": 1.295173138471412e-05,
      "loss": 0.5469,
      "step": 4693
    },
    {
      "epoch": 0.42260685588241914,
      "grad_norm": 0.6645342819725824,
      "learning_rate": 1.2948945020348971e-05,
      "loss": 0.6071,
      "step": 4694
    },
    {
      "epoch": 0.42269688716829096,
      "grad_norm": 0.7042019841223571,
      "learning_rate": 1.2946158405203711e-05,
      "loss": 0.6181,
      "step": 4695
    },
    {
      "epoch": 0.42278691845416283,
      "grad_norm": 0.7630888195690677,
      "learning_rate": 1.2943371539515306e-05,
      "loss": 0.5749,
      "step": 4696
    },
    {
      "epoch": 0.42287694974003465,
      "grad_norm": 0.6299712906603144,
      "learning_rate": 1.2940584423520762e-05,
      "loss": 0.4876,
      "step": 4697
    },
    {
      "epoch": 0.4229669810259065,
      "grad_norm": 0.5162483191025673,
      "learning_rate": 1.2937797057457091e-05,
      "loss": 0.4655,
      "step": 4698
    },
    {
      "epoch": 0.42305701231177834,
      "grad_norm": 0.7251504781146081,
      "learning_rate": 1.2935009441561332e-05,
      "loss": 0.5581,
      "step": 4699
    },
    {
      "epoch": 0.4231470435976502,
      "grad_norm": 0.6446032290766326,
      "learning_rate": 1.2932221576070554e-05,
      "loss": 0.3964,
      "step": 4700
    },
    {
      "epoch": 0.423237074883522,
      "grad_norm": 0.6656631466757869,
      "learning_rate": 1.2929433461221827e-05,
      "loss": 0.512,
      "step": 4701
    },
    {
      "epoch": 0.42332710616939384,
      "grad_norm": 0.7197019692397799,
      "learning_rate": 1.292664509725226e-05,
      "loss": 0.5103,
      "step": 4702
    },
    {
      "epoch": 0.4234171374552657,
      "grad_norm": 0.8540857862502801,
      "learning_rate": 1.2923856484398977e-05,
      "loss": 0.5897,
      "step": 4703
    },
    {
      "epoch": 0.42350716874113753,
      "grad_norm": 0.6867700659893378,
      "learning_rate": 1.2921067622899124e-05,
      "loss": 0.5599,
      "step": 4704
    },
    {
      "epoch": 0.4235972000270094,
      "grad_norm": 0.715743433545286,
      "learning_rate": 1.2918278512989866e-05,
      "loss": 0.6186,
      "step": 4705
    },
    {
      "epoch": 0.4236872313128812,
      "grad_norm": 0.8211855380603172,
      "learning_rate": 1.2915489154908389e-05,
      "loss": 0.5559,
      "step": 4706
    },
    {
      "epoch": 0.4237772625987531,
      "grad_norm": 0.6530316141338536,
      "learning_rate": 1.291269954889191e-05,
      "loss": 0.5589,
      "step": 4707
    },
    {
      "epoch": 0.4238672938846249,
      "grad_norm": 0.7362018717756245,
      "learning_rate": 1.2909909695177647e-05,
      "loss": 0.6319,
      "step": 4708
    },
    {
      "epoch": 0.4239573251704967,
      "grad_norm": 0.6585916458930205,
      "learning_rate": 1.2907119594002856e-05,
      "loss": 0.5841,
      "step": 4709
    },
    {
      "epoch": 0.4240473564563686,
      "grad_norm": 0.8222768763173208,
      "learning_rate": 1.2904329245604812e-05,
      "loss": 0.5719,
      "step": 4710
    },
    {
      "epoch": 0.4241373877422404,
      "grad_norm": 0.7726429293983683,
      "learning_rate": 1.2901538650220809e-05,
      "loss": 0.5381,
      "step": 4711
    },
    {
      "epoch": 0.4242274190281123,
      "grad_norm": 0.700023216451102,
      "learning_rate": 1.2898747808088152e-05,
      "loss": 0.6296,
      "step": 4712
    },
    {
      "epoch": 0.4243174503139841,
      "grad_norm": 0.7697541275983495,
      "learning_rate": 1.2895956719444182e-05,
      "loss": 0.5701,
      "step": 4713
    },
    {
      "epoch": 0.424407481599856,
      "grad_norm": 0.6728055683047215,
      "learning_rate": 1.2893165384526258e-05,
      "loss": 0.4813,
      "step": 4714
    },
    {
      "epoch": 0.4244975128857278,
      "grad_norm": 1.1379062561209552,
      "learning_rate": 1.2890373803571752e-05,
      "loss": 0.6162,
      "step": 4715
    },
    {
      "epoch": 0.4245875441715996,
      "grad_norm": 0.7049707180516991,
      "learning_rate": 1.288758197681806e-05,
      "loss": 0.542,
      "step": 4716
    },
    {
      "epoch": 0.4246775754574715,
      "grad_norm": 0.8243570033134,
      "learning_rate": 1.2884789904502605e-05,
      "loss": 0.5215,
      "step": 4717
    },
    {
      "epoch": 0.4247676067433433,
      "grad_norm": 0.862861565395005,
      "learning_rate": 1.2881997586862828e-05,
      "loss": 0.6166,
      "step": 4718
    },
    {
      "epoch": 0.42485763802921517,
      "grad_norm": 1.2643098287653063,
      "learning_rate": 1.2879205024136186e-05,
      "loss": 0.6243,
      "step": 4719
    },
    {
      "epoch": 0.424947669315087,
      "grad_norm": 0.7087732315276742,
      "learning_rate": 1.2876412216560157e-05,
      "loss": 0.5545,
      "step": 4720
    },
    {
      "epoch": 0.42503770060095886,
      "grad_norm": 0.6820835777048784,
      "learning_rate": 1.2873619164372251e-05,
      "loss": 0.589,
      "step": 4721
    },
    {
      "epoch": 0.4251277318868307,
      "grad_norm": 0.8340026297259758,
      "learning_rate": 1.2870825867809983e-05,
      "loss": 0.5724,
      "step": 4722
    },
    {
      "epoch": 0.4252177631727025,
      "grad_norm": 0.823909142277615,
      "learning_rate": 1.2868032327110904e-05,
      "loss": 0.5896,
      "step": 4723
    },
    {
      "epoch": 0.42530779445857436,
      "grad_norm": 0.7173699719264846,
      "learning_rate": 1.2865238542512572e-05,
      "loss": 0.6263,
      "step": 4724
    },
    {
      "epoch": 0.4253978257444462,
      "grad_norm": 0.5531159982657646,
      "learning_rate": 1.286244451425258e-05,
      "loss": 0.5207,
      "step": 4725
    },
    {
      "epoch": 0.42548785703031805,
      "grad_norm": 0.8533475184877194,
      "learning_rate": 1.2859650242568526e-05,
      "loss": 0.5893,
      "step": 4726
    },
    {
      "epoch": 0.42557788831618987,
      "grad_norm": 0.7136346526593722,
      "learning_rate": 1.285685572769804e-05,
      "loss": 0.5417,
      "step": 4727
    },
    {
      "epoch": 0.42566791960206174,
      "grad_norm": 0.708688309281741,
      "learning_rate": 1.285406096987877e-05,
      "loss": 0.5704,
      "step": 4728
    },
    {
      "epoch": 0.42575795088793356,
      "grad_norm": 0.7827561316871519,
      "learning_rate": 1.285126596934838e-05,
      "loss": 0.5077,
      "step": 4729
    },
    {
      "epoch": 0.4258479821738054,
      "grad_norm": 0.9218032447856142,
      "learning_rate": 1.2848470726344564e-05,
      "loss": 0.5824,
      "step": 4730
    },
    {
      "epoch": 0.42593801345967724,
      "grad_norm": 0.607701027334219,
      "learning_rate": 1.2845675241105026e-05,
      "loss": 0.5059,
      "step": 4731
    },
    {
      "epoch": 0.42602804474554906,
      "grad_norm": 0.680171707000386,
      "learning_rate": 1.2842879513867504e-05,
      "loss": 0.5596,
      "step": 4732
    },
    {
      "epoch": 0.42611807603142093,
      "grad_norm": 0.7323725785712619,
      "learning_rate": 1.284008354486974e-05,
      "loss": 0.6273,
      "step": 4733
    },
    {
      "epoch": 0.42620810731729275,
      "grad_norm": 0.8232117730008928,
      "learning_rate": 1.2837287334349506e-05,
      "loss": 0.5005,
      "step": 4734
    },
    {
      "epoch": 0.4262981386031646,
      "grad_norm": 0.6508161243952624,
      "learning_rate": 1.2834490882544598e-05,
      "loss": 0.5955,
      "step": 4735
    },
    {
      "epoch": 0.42638816988903644,
      "grad_norm": 0.6762342540578719,
      "learning_rate": 1.2831694189692823e-05,
      "loss": 0.6092,
      "step": 4736
    },
    {
      "epoch": 0.42647820117490826,
      "grad_norm": 0.6919309574468497,
      "learning_rate": 1.2828897256032016e-05,
      "loss": 0.4702,
      "step": 4737
    },
    {
      "epoch": 0.42656823246078013,
      "grad_norm": 0.7246965798527759,
      "learning_rate": 1.282610008180003e-05,
      "loss": 0.5151,
      "step": 4738
    },
    {
      "epoch": 0.42665826374665194,
      "grad_norm": 0.7195870786491959,
      "learning_rate": 1.2823302667234741e-05,
      "loss": 0.5379,
      "step": 4739
    },
    {
      "epoch": 0.4267482950325238,
      "grad_norm": 0.773276304679366,
      "learning_rate": 1.2820505012574035e-05,
      "loss": 0.5936,
      "step": 4740
    },
    {
      "epoch": 0.42683832631839563,
      "grad_norm": 0.7100941788462111,
      "learning_rate": 1.2817707118055835e-05,
      "loss": 0.5835,
      "step": 4741
    },
    {
      "epoch": 0.4269283576042675,
      "grad_norm": 0.8380689702184609,
      "learning_rate": 1.2814908983918073e-05,
      "loss": 0.6885,
      "step": 4742
    },
    {
      "epoch": 0.4270183888901393,
      "grad_norm": 0.7286132240943282,
      "learning_rate": 1.28121106103987e-05,
      "loss": 0.5496,
      "step": 4743
    },
    {
      "epoch": 0.42710842017601114,
      "grad_norm": 0.6253225811541475,
      "learning_rate": 1.2809311997735697e-05,
      "loss": 0.5849,
      "step": 4744
    },
    {
      "epoch": 0.427198451461883,
      "grad_norm": 0.6938946844407377,
      "learning_rate": 1.2806513146167054e-05,
      "loss": 0.5756,
      "step": 4745
    },
    {
      "epoch": 0.4272884827477548,
      "grad_norm": 0.624217011821653,
      "learning_rate": 1.2803714055930794e-05,
      "loss": 0.5421,
      "step": 4746
    },
    {
      "epoch": 0.4273785140336267,
      "grad_norm": 0.7139176905290447,
      "learning_rate": 1.2800914727264948e-05,
      "loss": 0.5604,
      "step": 4747
    },
    {
      "epoch": 0.4274685453194985,
      "grad_norm": 1.010880889340162,
      "learning_rate": 1.2798115160407574e-05,
      "loss": 0.6635,
      "step": 4748
    },
    {
      "epoch": 0.4275585766053704,
      "grad_norm": 0.9449418146842198,
      "learning_rate": 1.279531535559675e-05,
      "loss": 0.6329,
      "step": 4749
    },
    {
      "epoch": 0.4276486078912422,
      "grad_norm": 0.677582421538952,
      "learning_rate": 1.2792515313070573e-05,
      "loss": 0.5445,
      "step": 4750
    },
    {
      "epoch": 0.427738639177114,
      "grad_norm": 0.8086472670001708,
      "learning_rate": 1.2789715033067158e-05,
      "loss": 0.641,
      "step": 4751
    },
    {
      "epoch": 0.4278286704629859,
      "grad_norm": 0.7735931033732902,
      "learning_rate": 1.2786914515824645e-05,
      "loss": 0.5559,
      "step": 4752
    },
    {
      "epoch": 0.4279187017488577,
      "grad_norm": 0.7719460880214708,
      "learning_rate": 1.2784113761581191e-05,
      "loss": 0.5402,
      "step": 4753
    },
    {
      "epoch": 0.4280087330347296,
      "grad_norm": 0.8480410623847461,
      "learning_rate": 1.2781312770574976e-05,
      "loss": 0.5009,
      "step": 4754
    },
    {
      "epoch": 0.4280987643206014,
      "grad_norm": 1.098045082320462,
      "learning_rate": 1.2778511543044195e-05,
      "loss": 0.5449,
      "step": 4755
    },
    {
      "epoch": 0.42818879560647327,
      "grad_norm": 1.0811345585021206,
      "learning_rate": 1.2775710079227067e-05,
      "loss": 0.5708,
      "step": 4756
    },
    {
      "epoch": 0.4282788268923451,
      "grad_norm": 0.6447295224858662,
      "learning_rate": 1.2772908379361828e-05,
      "loss": 0.5624,
      "step": 4757
    },
    {
      "epoch": 0.4283688581782169,
      "grad_norm": 0.7904304776214347,
      "learning_rate": 1.277010644368674e-05,
      "loss": 0.5141,
      "step": 4758
    },
    {
      "epoch": 0.4284588894640888,
      "grad_norm": 0.6797808039607344,
      "learning_rate": 1.276730427244008e-05,
      "loss": 0.5706,
      "step": 4759
    },
    {
      "epoch": 0.4285489207499606,
      "grad_norm": 0.7649268190284542,
      "learning_rate": 1.2764501865860149e-05,
      "loss": 0.5367,
      "step": 4760
    },
    {
      "epoch": 0.42863895203583247,
      "grad_norm": 0.7693222443426816,
      "learning_rate": 1.276169922418526e-05,
      "loss": 0.5112,
      "step": 4761
    },
    {
      "epoch": 0.4287289833217043,
      "grad_norm": 0.8356130307417825,
      "learning_rate": 1.2758896347653753e-05,
      "loss": 0.5429,
      "step": 4762
    },
    {
      "epoch": 0.42881901460757615,
      "grad_norm": 0.713323389497575,
      "learning_rate": 1.275609323650399e-05,
      "loss": 0.5106,
      "step": 4763
    },
    {
      "epoch": 0.42890904589344797,
      "grad_norm": 0.870645540501837,
      "learning_rate": 1.2753289890974345e-05,
      "loss": 0.4757,
      "step": 4764
    },
    {
      "epoch": 0.4289990771793198,
      "grad_norm": 1.2779271571322919,
      "learning_rate": 1.2750486311303218e-05,
      "loss": 0.5629,
      "step": 4765
    },
    {
      "epoch": 0.42908910846519166,
      "grad_norm": 0.6366514835650656,
      "learning_rate": 1.2747682497729029e-05,
      "loss": 0.5253,
      "step": 4766
    },
    {
      "epoch": 0.4291791397510635,
      "grad_norm": 0.7240310191289125,
      "learning_rate": 1.2744878450490215e-05,
      "loss": 0.5426,
      "step": 4767
    },
    {
      "epoch": 0.42926917103693535,
      "grad_norm": 0.7102983621602824,
      "learning_rate": 1.2742074169825229e-05,
      "loss": 0.5287,
      "step": 4768
    },
    {
      "epoch": 0.42935920232280717,
      "grad_norm": 0.6926239243431246,
      "learning_rate": 1.2739269655972555e-05,
      "loss": 0.5817,
      "step": 4769
    },
    {
      "epoch": 0.42944923360867904,
      "grad_norm": 0.9767874419195378,
      "learning_rate": 1.2736464909170692e-05,
      "loss": 0.5582,
      "step": 4770
    },
    {
      "epoch": 0.42953926489455085,
      "grad_norm": 0.7714945560568963,
      "learning_rate": 1.273365992965815e-05,
      "loss": 0.5454,
      "step": 4771
    },
    {
      "epoch": 0.42962929618042267,
      "grad_norm": 0.7066647915111568,
      "learning_rate": 1.273085471767347e-05,
      "loss": 0.5557,
      "step": 4772
    },
    {
      "epoch": 0.42971932746629454,
      "grad_norm": 1.2415784731355193,
      "learning_rate": 1.272804927345521e-05,
      "loss": 0.5285,
      "step": 4773
    },
    {
      "epoch": 0.42980935875216636,
      "grad_norm": 0.7435916046256903,
      "learning_rate": 1.2725243597241948e-05,
      "loss": 0.5101,
      "step": 4774
    },
    {
      "epoch": 0.42989939003803823,
      "grad_norm": 0.9403500249785219,
      "learning_rate": 1.2722437689272273e-05,
      "loss": 0.4975,
      "step": 4775
    },
    {
      "epoch": 0.42998942132391005,
      "grad_norm": 0.6434920128040527,
      "learning_rate": 1.271963154978481e-05,
      "loss": 0.5874,
      "step": 4776
    },
    {
      "epoch": 0.4300794526097819,
      "grad_norm": 0.646245555660773,
      "learning_rate": 1.2716825179018192e-05,
      "loss": 0.498,
      "step": 4777
    },
    {
      "epoch": 0.43016948389565374,
      "grad_norm": 1.7442696073686013,
      "learning_rate": 1.2714018577211071e-05,
      "loss": 0.6412,
      "step": 4778
    },
    {
      "epoch": 0.43025951518152555,
      "grad_norm": 0.7716038964907174,
      "learning_rate": 1.2711211744602128e-05,
      "loss": 0.5416,
      "step": 4779
    },
    {
      "epoch": 0.4303495464673974,
      "grad_norm": 0.9001255162893769,
      "learning_rate": 1.2708404681430054e-05,
      "loss": 0.5769,
      "step": 4780
    },
    {
      "epoch": 0.43043957775326924,
      "grad_norm": 0.6869665129433741,
      "learning_rate": 1.2705597387933564e-05,
      "loss": 0.5368,
      "step": 4781
    },
    {
      "epoch": 0.4305296090391411,
      "grad_norm": 0.7091068192189357,
      "learning_rate": 1.2702789864351391e-05,
      "loss": 0.6117,
      "step": 4782
    },
    {
      "epoch": 0.43061964032501293,
      "grad_norm": 0.6228225349463754,
      "learning_rate": 1.269998211092229e-05,
      "loss": 0.5008,
      "step": 4783
    },
    {
      "epoch": 0.4307096716108848,
      "grad_norm": 0.785840739400199,
      "learning_rate": 1.2697174127885034e-05,
      "loss": 0.6512,
      "step": 4784
    },
    {
      "epoch": 0.4307997028967566,
      "grad_norm": 0.7807915646555929,
      "learning_rate": 1.2694365915478414e-05,
      "loss": 0.5698,
      "step": 4785
    },
    {
      "epoch": 0.43088973418262844,
      "grad_norm": 0.8315403094615963,
      "learning_rate": 1.2691557473941246e-05,
      "loss": 0.5892,
      "step": 4786
    },
    {
      "epoch": 0.4309797654685003,
      "grad_norm": 0.74705542631566,
      "learning_rate": 1.2688748803512356e-05,
      "loss": 0.5142,
      "step": 4787
    },
    {
      "epoch": 0.4310697967543721,
      "grad_norm": 0.615565637925546,
      "learning_rate": 1.2685939904430602e-05,
      "loss": 0.5269,
      "step": 4788
    },
    {
      "epoch": 0.431159828040244,
      "grad_norm": 0.644695783007861,
      "learning_rate": 1.268313077693485e-05,
      "loss": 0.5046,
      "step": 4789
    },
    {
      "epoch": 0.4312498593261158,
      "grad_norm": 0.7672410681650867,
      "learning_rate": 1.2680321421263988e-05,
      "loss": 0.5481,
      "step": 4790
    },
    {
      "epoch": 0.4313398906119877,
      "grad_norm": 0.9449798010623597,
      "learning_rate": 1.2677511837656931e-05,
      "loss": 0.5644,
      "step": 4791
    },
    {
      "epoch": 0.4314299218978595,
      "grad_norm": 0.8531870127632842,
      "learning_rate": 1.2674702026352604e-05,
      "loss": 0.5602,
      "step": 4792
    },
    {
      "epoch": 0.4315199531837313,
      "grad_norm": 0.7451651480486408,
      "learning_rate": 1.2671891987589952e-05,
      "loss": 0.5513,
      "step": 4793
    },
    {
      "epoch": 0.4316099844696032,
      "grad_norm": 0.7127714710217583,
      "learning_rate": 1.2669081721607949e-05,
      "loss": 0.6373,
      "step": 4794
    },
    {
      "epoch": 0.431700015755475,
      "grad_norm": 0.6359293885137579,
      "learning_rate": 1.2666271228645584e-05,
      "loss": 0.5032,
      "step": 4795
    },
    {
      "epoch": 0.4317900470413469,
      "grad_norm": 0.6988625876410156,
      "learning_rate": 1.2663460508941851e-05,
      "loss": 0.4807,
      "step": 4796
    },
    {
      "epoch": 0.4318800783272187,
      "grad_norm": 0.7517027803847508,
      "learning_rate": 1.2660649562735786e-05,
      "loss": 0.6023,
      "step": 4797
    },
    {
      "epoch": 0.43197010961309057,
      "grad_norm": 0.8255420168533331,
      "learning_rate": 1.2657838390266427e-05,
      "loss": 0.5496,
      "step": 4798
    },
    {
      "epoch": 0.4320601408989624,
      "grad_norm": 0.6407009250320742,
      "learning_rate": 1.2655026991772846e-05,
      "loss": 0.5803,
      "step": 4799
    },
    {
      "epoch": 0.4321501721848342,
      "grad_norm": 0.85101641009568,
      "learning_rate": 1.2652215367494116e-05,
      "loss": 0.678,
      "step": 4800
    },
    {
      "epoch": 0.4322402034707061,
      "grad_norm": 0.9203367639666806,
      "learning_rate": 1.2649403517669345e-05,
      "loss": 0.6454,
      "step": 4801
    },
    {
      "epoch": 0.4323302347565779,
      "grad_norm": 0.698740464145224,
      "learning_rate": 1.264659144253766e-05,
      "loss": 0.4112,
      "step": 4802
    },
    {
      "epoch": 0.43242026604244976,
      "grad_norm": 0.6765062792453944,
      "learning_rate": 1.2643779142338186e-05,
      "loss": 0.5369,
      "step": 4803
    },
    {
      "epoch": 0.4325102973283216,
      "grad_norm": 0.6995518873053449,
      "learning_rate": 1.2640966617310097e-05,
      "loss": 0.5502,
      "step": 4804
    },
    {
      "epoch": 0.43260032861419345,
      "grad_norm": 0.7533426941012523,
      "learning_rate": 1.2638153867692569e-05,
      "loss": 0.6007,
      "step": 4805
    },
    {
      "epoch": 0.43269035990006527,
      "grad_norm": 0.7853040129686374,
      "learning_rate": 1.2635340893724795e-05,
      "loss": 0.5513,
      "step": 4806
    },
    {
      "epoch": 0.4327803911859371,
      "grad_norm": 0.7478607900324792,
      "learning_rate": 1.2632527695645993e-05,
      "loss": 0.4873,
      "step": 4807
    },
    {
      "epoch": 0.43287042247180896,
      "grad_norm": 0.563029650135375,
      "learning_rate": 1.2629714273695404e-05,
      "loss": 0.5034,
      "step": 4808
    },
    {
      "epoch": 0.4329604537576808,
      "grad_norm": 0.7057358565094441,
      "learning_rate": 1.2626900628112282e-05,
      "loss": 0.5484,
      "step": 4809
    },
    {
      "epoch": 0.43305048504355265,
      "grad_norm": 0.6851987718374899,
      "learning_rate": 1.2624086759135896e-05,
      "loss": 0.585,
      "step": 4810
    },
    {
      "epoch": 0.43314051632942446,
      "grad_norm": 0.722159213821423,
      "learning_rate": 1.2621272667005544e-05,
      "loss": 0.6098,
      "step": 4811
    },
    {
      "epoch": 0.43323054761529634,
      "grad_norm": 0.7789080689786623,
      "learning_rate": 1.261845835196054e-05,
      "loss": 0.5057,
      "step": 4812
    },
    {
      "epoch": 0.43332057890116815,
      "grad_norm": 0.7531545433797473,
      "learning_rate": 1.2615643814240207e-05,
      "loss": 0.5708,
      "step": 4813
    },
    {
      "epoch": 0.43341061018704,
      "grad_norm": 0.9006544168592189,
      "learning_rate": 1.2612829054083899e-05,
      "loss": 0.5565,
      "step": 4814
    },
    {
      "epoch": 0.43350064147291184,
      "grad_norm": 0.797512374316015,
      "learning_rate": 1.2610014071730986e-05,
      "loss": 0.6039,
      "step": 4815
    },
    {
      "epoch": 0.43359067275878366,
      "grad_norm": 0.8740366900594105,
      "learning_rate": 1.260719886742086e-05,
      "loss": 0.5399,
      "step": 4816
    },
    {
      "epoch": 0.43368070404465553,
      "grad_norm": 1.1780683885086602,
      "learning_rate": 1.2604383441392922e-05,
      "loss": 0.5284,
      "step": 4817
    },
    {
      "epoch": 0.43377073533052735,
      "grad_norm": 0.6778474638001435,
      "learning_rate": 1.2601567793886598e-05,
      "loss": 0.5684,
      "step": 4818
    },
    {
      "epoch": 0.4338607666163992,
      "grad_norm": 0.7877478175653662,
      "learning_rate": 1.2598751925141335e-05,
      "loss": 0.6291,
      "step": 4819
    },
    {
      "epoch": 0.43395079790227103,
      "grad_norm": 0.8229407087456757,
      "learning_rate": 1.2595935835396591e-05,
      "loss": 0.6279,
      "step": 4820
    },
    {
      "epoch": 0.4340408291881429,
      "grad_norm": 0.7853059188905782,
      "learning_rate": 1.2593119524891855e-05,
      "loss": 0.5426,
      "step": 4821
    },
    {
      "epoch": 0.4341308604740147,
      "grad_norm": 0.7377764190053773,
      "learning_rate": 1.2590302993866623e-05,
      "loss": 0.516,
      "step": 4822
    },
    {
      "epoch": 0.43422089175988654,
      "grad_norm": 0.8646018851070995,
      "learning_rate": 1.2587486242560417e-05,
      "loss": 0.5358,
      "step": 4823
    },
    {
      "epoch": 0.4343109230457584,
      "grad_norm": 0.6869100352750338,
      "learning_rate": 1.2584669271212772e-05,
      "loss": 0.5842,
      "step": 4824
    },
    {
      "epoch": 0.43440095433163023,
      "grad_norm": 0.8006762526875457,
      "learning_rate": 1.2581852080063248e-05,
      "loss": 0.5341,
      "step": 4825
    },
    {
      "epoch": 0.4344909856175021,
      "grad_norm": 0.7400699514213641,
      "learning_rate": 1.2579034669351424e-05,
      "loss": 0.5751,
      "step": 4826
    },
    {
      "epoch": 0.4345810169033739,
      "grad_norm": 0.9775424097178437,
      "learning_rate": 1.2576217039316884e-05,
      "loss": 0.5664,
      "step": 4827
    },
    {
      "epoch": 0.4346710481892458,
      "grad_norm": 0.6662118669735881,
      "learning_rate": 1.257339919019925e-05,
      "loss": 0.6035,
      "step": 4828
    },
    {
      "epoch": 0.4347610794751176,
      "grad_norm": 0.7691959537462701,
      "learning_rate": 1.257058112223815e-05,
      "loss": 0.5478,
      "step": 4829
    },
    {
      "epoch": 0.4348511107609894,
      "grad_norm": 0.7053693664113859,
      "learning_rate": 1.2567762835673239e-05,
      "loss": 0.5502,
      "step": 4830
    },
    {
      "epoch": 0.4349411420468613,
      "grad_norm": 0.6997395153341196,
      "learning_rate": 1.256494433074418e-05,
      "loss": 0.5899,
      "step": 4831
    },
    {
      "epoch": 0.4350311733327331,
      "grad_norm": 0.5423457135608251,
      "learning_rate": 1.256212560769066e-05,
      "loss": 0.5099,
      "step": 4832
    },
    {
      "epoch": 0.435121204618605,
      "grad_norm": 0.8113241678151684,
      "learning_rate": 1.2559306666752391e-05,
      "loss": 0.5304,
      "step": 4833
    },
    {
      "epoch": 0.4352112359044768,
      "grad_norm": 0.7086209292300353,
      "learning_rate": 1.255648750816909e-05,
      "loss": 0.5333,
      "step": 4834
    },
    {
      "epoch": 0.4353012671903487,
      "grad_norm": 1.4535379508599193,
      "learning_rate": 1.2553668132180508e-05,
      "loss": 0.5437,
      "step": 4835
    },
    {
      "epoch": 0.4353912984762205,
      "grad_norm": 0.726764434015259,
      "learning_rate": 1.2550848539026399e-05,
      "loss": 0.4847,
      "step": 4836
    },
    {
      "epoch": 0.4354813297620923,
      "grad_norm": 0.6691494608992207,
      "learning_rate": 1.2548028728946548e-05,
      "loss": 0.5608,
      "step": 4837
    },
    {
      "epoch": 0.4355713610479642,
      "grad_norm": 0.6750644293742601,
      "learning_rate": 1.2545208702180752e-05,
      "loss": 0.481,
      "step": 4838
    },
    {
      "epoch": 0.435661392333836,
      "grad_norm": 0.5995489719329868,
      "learning_rate": 1.2542388458968827e-05,
      "loss": 0.4781,
      "step": 4839
    },
    {
      "epoch": 0.43575142361970787,
      "grad_norm": 1.5886251774900098,
      "learning_rate": 1.253956799955061e-05,
      "loss": 0.5653,
      "step": 4840
    },
    {
      "epoch": 0.4358414549055797,
      "grad_norm": 0.7197934027097839,
      "learning_rate": 1.253674732416595e-05,
      "loss": 0.5714,
      "step": 4841
    },
    {
      "epoch": 0.43593148619145156,
      "grad_norm": 1.1919946730530915,
      "learning_rate": 1.2533926433054723e-05,
      "loss": 0.5835,
      "step": 4842
    },
    {
      "epoch": 0.4360215174773234,
      "grad_norm": 0.6521031886676109,
      "learning_rate": 1.2531105326456821e-05,
      "loss": 0.555,
      "step": 4843
    },
    {
      "epoch": 0.4361115487631952,
      "grad_norm": 0.7044655938210627,
      "learning_rate": 1.252828400461215e-05,
      "loss": 0.5301,
      "step": 4844
    },
    {
      "epoch": 0.43620158004906706,
      "grad_norm": 0.7380378128494642,
      "learning_rate": 1.2525462467760638e-05,
      "loss": 0.5384,
      "step": 4845
    },
    {
      "epoch": 0.4362916113349389,
      "grad_norm": 0.6848890096003492,
      "learning_rate": 1.2522640716142227e-05,
      "loss": 0.4778,
      "step": 4846
    },
    {
      "epoch": 0.43638164262081075,
      "grad_norm": 0.7126712820486516,
      "learning_rate": 1.2519818749996886e-05,
      "loss": 0.5397,
      "step": 4847
    },
    {
      "epoch": 0.43647167390668257,
      "grad_norm": 0.8398351778120351,
      "learning_rate": 1.2516996569564593e-05,
      "loss": 0.6078,
      "step": 4848
    },
    {
      "epoch": 0.43656170519255444,
      "grad_norm": 0.7403189194097237,
      "learning_rate": 1.2514174175085346e-05,
      "loss": 0.5127,
      "step": 4849
    },
    {
      "epoch": 0.43665173647842626,
      "grad_norm": 0.8102656976105613,
      "learning_rate": 1.2511351566799169e-05,
      "loss": 0.5548,
      "step": 4850
    },
    {
      "epoch": 0.43674176776429807,
      "grad_norm": 0.7151012747339234,
      "learning_rate": 1.2508528744946096e-05,
      "loss": 0.5974,
      "step": 4851
    },
    {
      "epoch": 0.43683179905016994,
      "grad_norm": 0.7082992209280313,
      "learning_rate": 1.2505705709766178e-05,
      "loss": 0.451,
      "step": 4852
    },
    {
      "epoch": 0.43692183033604176,
      "grad_norm": 0.7593435574834758,
      "learning_rate": 1.250288246149949e-05,
      "loss": 0.5206,
      "step": 4853
    },
    {
      "epoch": 0.43701186162191363,
      "grad_norm": 0.7499524194419823,
      "learning_rate": 1.2500059000386129e-05,
      "loss": 0.5558,
      "step": 4854
    },
    {
      "epoch": 0.43710189290778545,
      "grad_norm": 0.7997605952223913,
      "learning_rate": 1.2497235326666193e-05,
      "loss": 0.465,
      "step": 4855
    },
    {
      "epoch": 0.4371919241936573,
      "grad_norm": 0.8197498061893072,
      "learning_rate": 1.2494411440579814e-05,
      "loss": 0.5673,
      "step": 4856
    },
    {
      "epoch": 0.43728195547952914,
      "grad_norm": 0.7605868872500138,
      "learning_rate": 1.249158734236714e-05,
      "loss": 0.5443,
      "step": 4857
    },
    {
      "epoch": 0.43737198676540096,
      "grad_norm": 0.7413531241611602,
      "learning_rate": 1.2488763032268332e-05,
      "loss": 0.6005,
      "step": 4858
    },
    {
      "epoch": 0.4374620180512728,
      "grad_norm": 0.6523729453669249,
      "learning_rate": 1.2485938510523568e-05,
      "loss": 0.6281,
      "step": 4859
    },
    {
      "epoch": 0.43755204933714464,
      "grad_norm": 0.695982793427679,
      "learning_rate": 1.2483113777373049e-05,
      "loss": 0.5496,
      "step": 4860
    },
    {
      "epoch": 0.4376420806230165,
      "grad_norm": 0.6614883469759492,
      "learning_rate": 1.2480288833056995e-05,
      "loss": 0.541,
      "step": 4861
    },
    {
      "epoch": 0.43773211190888833,
      "grad_norm": 0.58575688313944,
      "learning_rate": 1.2477463677815635e-05,
      "loss": 0.5398,
      "step": 4862
    },
    {
      "epoch": 0.4378221431947602,
      "grad_norm": 0.642777585286188,
      "learning_rate": 1.2474638311889227e-05,
      "loss": 0.503,
      "step": 4863
    },
    {
      "epoch": 0.437912174480632,
      "grad_norm": 0.7163047139626499,
      "learning_rate": 1.247181273551804e-05,
      "loss": 0.544,
      "step": 4864
    },
    {
      "epoch": 0.43800220576650384,
      "grad_norm": 0.945770980989106,
      "learning_rate": 1.2468986948942365e-05,
      "loss": 0.5851,
      "step": 4865
    },
    {
      "epoch": 0.4380922370523757,
      "grad_norm": 0.6752650348789146,
      "learning_rate": 1.2466160952402506e-05,
      "loss": 0.4951,
      "step": 4866
    },
    {
      "epoch": 0.4381822683382475,
      "grad_norm": 0.59593008263526,
      "learning_rate": 1.2463334746138787e-05,
      "loss": 0.5603,
      "step": 4867
    },
    {
      "epoch": 0.4382722996241194,
      "grad_norm": 0.8385364033606385,
      "learning_rate": 1.2460508330391554e-05,
      "loss": 0.493,
      "step": 4868
    },
    {
      "epoch": 0.4383623309099912,
      "grad_norm": 0.848142693446707,
      "learning_rate": 1.2457681705401163e-05,
      "loss": 0.5519,
      "step": 4869
    },
    {
      "epoch": 0.4384523621958631,
      "grad_norm": 0.8284123305916445,
      "learning_rate": 1.2454854871407993e-05,
      "loss": 0.5792,
      "step": 4870
    },
    {
      "epoch": 0.4385423934817349,
      "grad_norm": 1.0300392446571038,
      "learning_rate": 1.2452027828652442e-05,
      "loss": 0.5152,
      "step": 4871
    },
    {
      "epoch": 0.4386324247676067,
      "grad_norm": 0.6244468301482754,
      "learning_rate": 1.2449200577374922e-05,
      "loss": 0.5765,
      "step": 4872
    },
    {
      "epoch": 0.4387224560534786,
      "grad_norm": 0.8688222857713492,
      "learning_rate": 1.2446373117815862e-05,
      "loss": 0.4689,
      "step": 4873
    },
    {
      "epoch": 0.4388124873393504,
      "grad_norm": 1.5921830089407003,
      "learning_rate": 1.2443545450215715e-05,
      "loss": 0.5301,
      "step": 4874
    },
    {
      "epoch": 0.4389025186252223,
      "grad_norm": 0.7649120046833878,
      "learning_rate": 1.2440717574814946e-05,
      "loss": 0.5934,
      "step": 4875
    },
    {
      "epoch": 0.4389925499110941,
      "grad_norm": 0.7657730870604322,
      "learning_rate": 1.2437889491854038e-05,
      "loss": 0.6058,
      "step": 4876
    },
    {
      "epoch": 0.43908258119696597,
      "grad_norm": 0.877574510988504,
      "learning_rate": 1.2435061201573493e-05,
      "loss": 0.6568,
      "step": 4877
    },
    {
      "epoch": 0.4391726124828378,
      "grad_norm": 0.8739230429857501,
      "learning_rate": 1.2432232704213831e-05,
      "loss": 0.6138,
      "step": 4878
    },
    {
      "epoch": 0.4392626437687096,
      "grad_norm": 0.6992068758537283,
      "learning_rate": 1.2429404000015593e-05,
      "loss": 0.5913,
      "step": 4879
    },
    {
      "epoch": 0.4393526750545815,
      "grad_norm": 0.7782442292014446,
      "learning_rate": 1.2426575089219326e-05,
      "loss": 0.5603,
      "step": 4880
    },
    {
      "epoch": 0.4394427063404533,
      "grad_norm": 0.7520475926505472,
      "learning_rate": 1.2423745972065609e-05,
      "loss": 0.5129,
      "step": 4881
    },
    {
      "epoch": 0.43953273762632517,
      "grad_norm": 0.7082534254979099,
      "learning_rate": 1.2420916648795029e-05,
      "loss": 0.5499,
      "step": 4882
    },
    {
      "epoch": 0.439622768912197,
      "grad_norm": 0.7496049380539181,
      "learning_rate": 1.2418087119648195e-05,
      "loss": 0.5086,
      "step": 4883
    },
    {
      "epoch": 0.43971280019806885,
      "grad_norm": 0.7985884917107088,
      "learning_rate": 1.2415257384865729e-05,
      "loss": 0.5844,
      "step": 4884
    },
    {
      "epoch": 0.43980283148394067,
      "grad_norm": 0.7159470155672558,
      "learning_rate": 1.2412427444688276e-05,
      "loss": 0.5473,
      "step": 4885
    },
    {
      "epoch": 0.4398928627698125,
      "grad_norm": 0.8032518460254906,
      "learning_rate": 1.2409597299356498e-05,
      "loss": 0.5716,
      "step": 4886
    },
    {
      "epoch": 0.43998289405568436,
      "grad_norm": 0.7666063612663206,
      "learning_rate": 1.2406766949111064e-05,
      "loss": 0.5133,
      "step": 4887
    },
    {
      "epoch": 0.4400729253415562,
      "grad_norm": 0.6662682810354447,
      "learning_rate": 1.2403936394192678e-05,
      "loss": 0.569,
      "step": 4888
    },
    {
      "epoch": 0.44016295662742805,
      "grad_norm": 0.5709566318301093,
      "learning_rate": 1.2401105634842047e-05,
      "loss": 0.4726,
      "step": 4889
    },
    {
      "epoch": 0.44025298791329986,
      "grad_norm": 0.5374869442995198,
      "learning_rate": 1.2398274671299904e-05,
      "loss": 0.4499,
      "step": 4890
    },
    {
      "epoch": 0.44034301919917174,
      "grad_norm": 0.6764208773083757,
      "learning_rate": 1.239544350380699e-05,
      "loss": 0.6237,
      "step": 4891
    },
    {
      "epoch": 0.44043305048504355,
      "grad_norm": 0.8171759816253695,
      "learning_rate": 1.2392612132604074e-05,
      "loss": 0.5904,
      "step": 4892
    },
    {
      "epoch": 0.44052308177091537,
      "grad_norm": 0.8699238857758911,
      "learning_rate": 1.2389780557931939e-05,
      "loss": 0.6332,
      "step": 4893
    },
    {
      "epoch": 0.44061311305678724,
      "grad_norm": 0.6775361400969933,
      "learning_rate": 1.238694878003138e-05,
      "loss": 0.5999,
      "step": 4894
    },
    {
      "epoch": 0.44070314434265906,
      "grad_norm": 0.7162636744671502,
      "learning_rate": 1.2384116799143213e-05,
      "loss": 0.5896,
      "step": 4895
    },
    {
      "epoch": 0.44079317562853093,
      "grad_norm": 0.7445021943542472,
      "learning_rate": 1.2381284615508276e-05,
      "loss": 0.6275,
      "step": 4896
    },
    {
      "epoch": 0.44088320691440275,
      "grad_norm": 0.7642447023611679,
      "learning_rate": 1.2378452229367415e-05,
      "loss": 0.5751,
      "step": 4897
    },
    {
      "epoch": 0.4409732382002746,
      "grad_norm": 0.8309920170165862,
      "learning_rate": 1.23756196409615e-05,
      "loss": 0.595,
      "step": 4898
    },
    {
      "epoch": 0.44106326948614644,
      "grad_norm": 0.8778438689520183,
      "learning_rate": 1.2372786850531413e-05,
      "loss": 0.6666,
      "step": 4899
    },
    {
      "epoch": 0.44115330077201825,
      "grad_norm": 0.8530026099374113,
      "learning_rate": 1.2369953858318064e-05,
      "loss": 0.5688,
      "step": 4900
    },
    {
      "epoch": 0.4412433320578901,
      "grad_norm": 0.6442013991851441,
      "learning_rate": 1.2367120664562364e-05,
      "loss": 0.5956,
      "step": 4901
    },
    {
      "epoch": 0.44133336334376194,
      "grad_norm": 0.6897889644399198,
      "learning_rate": 1.2364287269505252e-05,
      "loss": 0.5447,
      "step": 4902
    },
    {
      "epoch": 0.4414233946296338,
      "grad_norm": 1.0699341859802112,
      "learning_rate": 1.2361453673387685e-05,
      "loss": 0.5512,
      "step": 4903
    },
    {
      "epoch": 0.44151342591550563,
      "grad_norm": 0.7486851817366186,
      "learning_rate": 1.2358619876450631e-05,
      "loss": 0.5168,
      "step": 4904
    },
    {
      "epoch": 0.4416034572013775,
      "grad_norm": 0.7816687231462247,
      "learning_rate": 1.2355785878935076e-05,
      "loss": 0.6489,
      "step": 4905
    },
    {
      "epoch": 0.4416934884872493,
      "grad_norm": 0.8413815668730658,
      "learning_rate": 1.2352951681082028e-05,
      "loss": 0.524,
      "step": 4906
    },
    {
      "epoch": 0.44178351977312114,
      "grad_norm": 0.7532506971048922,
      "learning_rate": 1.235011728313251e-05,
      "loss": 0.5542,
      "step": 4907
    },
    {
      "epoch": 0.441873551058993,
      "grad_norm": 0.6709809163546565,
      "learning_rate": 1.2347282685327555e-05,
      "loss": 0.5843,
      "step": 4908
    },
    {
      "epoch": 0.4419635823448648,
      "grad_norm": 0.7526046821701851,
      "learning_rate": 1.2344447887908227e-05,
      "loss": 0.5815,
      "step": 4909
    },
    {
      "epoch": 0.4420536136307367,
      "grad_norm": 0.6857300001300594,
      "learning_rate": 1.2341612891115593e-05,
      "loss": 0.5355,
      "step": 4910
    },
    {
      "epoch": 0.4421436449166085,
      "grad_norm": 0.6757813608004605,
      "learning_rate": 1.2338777695190745e-05,
      "loss": 0.5249,
      "step": 4911
    },
    {
      "epoch": 0.4422336762024804,
      "grad_norm": 0.7764052419306778,
      "learning_rate": 1.2335942300374788e-05,
      "loss": 0.4712,
      "step": 4912
    },
    {
      "epoch": 0.4423237074883522,
      "grad_norm": 0.7586487355590489,
      "learning_rate": 1.2333106706908848e-05,
      "loss": 0.5017,
      "step": 4913
    },
    {
      "epoch": 0.442413738774224,
      "grad_norm": 0.7220141989838832,
      "learning_rate": 1.2330270915034066e-05,
      "loss": 0.5237,
      "step": 4914
    },
    {
      "epoch": 0.4425037700600959,
      "grad_norm": 0.6845103159678484,
      "learning_rate": 1.23274349249916e-05,
      "loss": 0.5705,
      "step": 4915
    },
    {
      "epoch": 0.4425938013459677,
      "grad_norm": 0.6508317817066042,
      "learning_rate": 1.2324598737022619e-05,
      "loss": 0.5887,
      "step": 4916
    },
    {
      "epoch": 0.4426838326318396,
      "grad_norm": 0.7515898844435023,
      "learning_rate": 1.232176235136832e-05,
      "loss": 0.4827,
      "step": 4917
    },
    {
      "epoch": 0.4427738639177114,
      "grad_norm": 0.6940081454263768,
      "learning_rate": 1.231892576826991e-05,
      "loss": 0.5809,
      "step": 4918
    },
    {
      "epoch": 0.44286389520358327,
      "grad_norm": 0.8273997247156669,
      "learning_rate": 1.231608898796861e-05,
      "loss": 0.571,
      "step": 4919
    },
    {
      "epoch": 0.4429539264894551,
      "grad_norm": 1.003690957521489,
      "learning_rate": 1.2313252010705665e-05,
      "loss": 0.5799,
      "step": 4920
    },
    {
      "epoch": 0.4430439577753269,
      "grad_norm": 0.6966172481769342,
      "learning_rate": 1.2310414836722335e-05,
      "loss": 0.5578,
      "step": 4921
    },
    {
      "epoch": 0.4431339890611988,
      "grad_norm": 0.8649487020373795,
      "learning_rate": 1.230757746625989e-05,
      "loss": 0.5829,
      "step": 4922
    },
    {
      "epoch": 0.4432240203470706,
      "grad_norm": 0.7100964516629774,
      "learning_rate": 1.2304739899559626e-05,
      "loss": 0.5214,
      "step": 4923
    },
    {
      "epoch": 0.44331405163294246,
      "grad_norm": 0.7022303520005555,
      "learning_rate": 1.2301902136862849e-05,
      "loss": 0.4603,
      "step": 4924
    },
    {
      "epoch": 0.4434040829188143,
      "grad_norm": 0.8716115541159617,
      "learning_rate": 1.2299064178410884e-05,
      "loss": 0.6353,
      "step": 4925
    },
    {
      "epoch": 0.44349411420468615,
      "grad_norm": 0.852428073312889,
      "learning_rate": 1.2296226024445075e-05,
      "loss": 0.618,
      "step": 4926
    },
    {
      "epoch": 0.44358414549055797,
      "grad_norm": 0.6781469632782471,
      "learning_rate": 1.2293387675206779e-05,
      "loss": 0.5788,
      "step": 4927
    },
    {
      "epoch": 0.4436741767764298,
      "grad_norm": 0.9365371665896727,
      "learning_rate": 1.2290549130937373e-05,
      "loss": 0.5434,
      "step": 4928
    },
    {
      "epoch": 0.44376420806230166,
      "grad_norm": 0.7135654114084746,
      "learning_rate": 1.2287710391878243e-05,
      "loss": 0.5055,
      "step": 4929
    },
    {
      "epoch": 0.4438542393481735,
      "grad_norm": 0.7912990438121703,
      "learning_rate": 1.2284871458270802e-05,
      "loss": 0.5991,
      "step": 4930
    },
    {
      "epoch": 0.44394427063404535,
      "grad_norm": 0.5722917758895278,
      "learning_rate": 1.2282032330356474e-05,
      "loss": 0.5683,
      "step": 4931
    },
    {
      "epoch": 0.44403430191991716,
      "grad_norm": 1.0745374553308604,
      "learning_rate": 1.22791930083767e-05,
      "loss": 0.6329,
      "step": 4932
    },
    {
      "epoch": 0.44412433320578903,
      "grad_norm": 0.7286971608264919,
      "learning_rate": 1.2276353492572937e-05,
      "loss": 0.5541,
      "step": 4933
    },
    {
      "epoch": 0.44421436449166085,
      "grad_norm": 0.735949417831264,
      "learning_rate": 1.2273513783186657e-05,
      "loss": 0.59,
      "step": 4934
    },
    {
      "epoch": 0.44430439577753267,
      "grad_norm": 0.6870106989728673,
      "learning_rate": 1.2270673880459356e-05,
      "loss": 0.5089,
      "step": 4935
    },
    {
      "epoch": 0.44439442706340454,
      "grad_norm": 0.6508266355615194,
      "learning_rate": 1.226783378463254e-05,
      "loss": 0.5044,
      "step": 4936
    },
    {
      "epoch": 0.44448445834927636,
      "grad_norm": 0.992071766373684,
      "learning_rate": 1.2264993495947726e-05,
      "loss": 0.7121,
      "step": 4937
    },
    {
      "epoch": 0.44457448963514823,
      "grad_norm": 0.6927410181986203,
      "learning_rate": 1.2262153014646462e-05,
      "loss": 0.518,
      "step": 4938
    },
    {
      "epoch": 0.44466452092102005,
      "grad_norm": 0.7388030522963525,
      "learning_rate": 1.22593123409703e-05,
      "loss": 0.5784,
      "step": 4939
    },
    {
      "epoch": 0.4447545522068919,
      "grad_norm": 0.7589954858679012,
      "learning_rate": 1.2256471475160812e-05,
      "loss": 0.5579,
      "step": 4940
    },
    {
      "epoch": 0.44484458349276373,
      "grad_norm": 0.748967206314077,
      "learning_rate": 1.225363041745959e-05,
      "loss": 0.5295,
      "step": 4941
    },
    {
      "epoch": 0.44493461477863555,
      "grad_norm": 0.7260384063780978,
      "learning_rate": 1.225078916810824e-05,
      "loss": 0.5389,
      "step": 4942
    },
    {
      "epoch": 0.4450246460645074,
      "grad_norm": 0.7431946439977586,
      "learning_rate": 1.224794772734838e-05,
      "loss": 0.5179,
      "step": 4943
    },
    {
      "epoch": 0.44511467735037924,
      "grad_norm": 0.699611515316819,
      "learning_rate": 1.2245106095421647e-05,
      "loss": 0.5743,
      "step": 4944
    },
    {
      "epoch": 0.4452047086362511,
      "grad_norm": 0.8429348506557339,
      "learning_rate": 1.2242264272569701e-05,
      "loss": 0.5163,
      "step": 4945
    },
    {
      "epoch": 0.44529473992212293,
      "grad_norm": 0.7379752406307337,
      "learning_rate": 1.2239422259034205e-05,
      "loss": 0.5569,
      "step": 4946
    },
    {
      "epoch": 0.4453847712079948,
      "grad_norm": 0.7605007226022059,
      "learning_rate": 1.2236580055056853e-05,
      "loss": 0.5761,
      "step": 4947
    },
    {
      "epoch": 0.4454748024938666,
      "grad_norm": 0.833802999997017,
      "learning_rate": 1.2233737660879343e-05,
      "loss": 0.5424,
      "step": 4948
    },
    {
      "epoch": 0.44556483377973843,
      "grad_norm": 0.8108293871723712,
      "learning_rate": 1.2230895076743399e-05,
      "loss": 0.5436,
      "step": 4949
    },
    {
      "epoch": 0.4456548650656103,
      "grad_norm": 1.8555074391676294,
      "learning_rate": 1.2228052302890746e-05,
      "loss": 0.557,
      "step": 4950
    },
    {
      "epoch": 0.4457448963514821,
      "grad_norm": 0.6193424544324057,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 0.585,
      "step": 4951
    },
    {
      "epoch": 0.445834927637354,
      "grad_norm": 0.8076532149023337,
      "learning_rate": 1.2222366187002361e-05,
      "loss": 0.5192,
      "step": 4952
    },
    {
      "epoch": 0.4459249589232258,
      "grad_norm": 0.86946697926494,
      "learning_rate": 1.2219522845450175e-05,
      "loss": 0.5159,
      "step": 4953
    },
    {
      "epoch": 0.4460149902090977,
      "grad_norm": 0.7535020583887336,
      "learning_rate": 1.2216679315148388e-05,
      "loss": 0.6126,
      "step": 4954
    },
    {
      "epoch": 0.4461050214949695,
      "grad_norm": 0.7268965669782965,
      "learning_rate": 1.2213835596338817e-05,
      "loss": 0.484,
      "step": 4955
    },
    {
      "epoch": 0.4461950527808413,
      "grad_norm": 0.6261876063472916,
      "learning_rate": 1.2210991689263292e-05,
      "loss": 0.4924,
      "step": 4956
    },
    {
      "epoch": 0.4462850840667132,
      "grad_norm": 0.7432058897635421,
      "learning_rate": 1.220814759416366e-05,
      "loss": 0.4577,
      "step": 4957
    },
    {
      "epoch": 0.446375115352585,
      "grad_norm": 0.7507595189271197,
      "learning_rate": 1.2205303311281784e-05,
      "loss": 0.5022,
      "step": 4958
    },
    {
      "epoch": 0.4464651466384569,
      "grad_norm": 0.6996193198261864,
      "learning_rate": 1.2202458840859548e-05,
      "loss": 0.4855,
      "step": 4959
    },
    {
      "epoch": 0.4465551779243287,
      "grad_norm": 0.842154865778401,
      "learning_rate": 1.2199614183138841e-05,
      "loss": 0.5401,
      "step": 4960
    },
    {
      "epoch": 0.44664520921020057,
      "grad_norm": 0.6461557693211628,
      "learning_rate": 1.219676933836158e-05,
      "loss": 0.4776,
      "step": 4961
    },
    {
      "epoch": 0.4467352404960724,
      "grad_norm": 0.763458411494469,
      "learning_rate": 1.2193924306769691e-05,
      "loss": 0.618,
      "step": 4962
    },
    {
      "epoch": 0.4468252717819442,
      "grad_norm": 0.7566899349816875,
      "learning_rate": 1.2191079088605118e-05,
      "loss": 0.5728,
      "step": 4963
    },
    {
      "epoch": 0.44691530306781607,
      "grad_norm": 0.838746477162962,
      "learning_rate": 1.2188233684109816e-05,
      "loss": 0.4801,
      "step": 4964
    },
    {
      "epoch": 0.4470053343536879,
      "grad_norm": 0.7006062107270883,
      "learning_rate": 1.2185388093525764e-05,
      "loss": 0.5892,
      "step": 4965
    },
    {
      "epoch": 0.44709536563955976,
      "grad_norm": 0.8221902993654929,
      "learning_rate": 1.2182542317094951e-05,
      "loss": 0.7605,
      "step": 4966
    },
    {
      "epoch": 0.4471853969254316,
      "grad_norm": 0.8120125427846919,
      "learning_rate": 1.2179696355059385e-05,
      "loss": 0.528,
      "step": 4967
    },
    {
      "epoch": 0.44727542821130345,
      "grad_norm": 1.414047784975303,
      "learning_rate": 1.2176850207661086e-05,
      "loss": 0.5006,
      "step": 4968
    },
    {
      "epoch": 0.44736545949717527,
      "grad_norm": 0.6931096830639955,
      "learning_rate": 1.2174003875142095e-05,
      "loss": 0.6183,
      "step": 4969
    },
    {
      "epoch": 0.4474554907830471,
      "grad_norm": 0.7198560605823037,
      "learning_rate": 1.2171157357744466e-05,
      "loss": 0.5565,
      "step": 4970
    },
    {
      "epoch": 0.44754552206891896,
      "grad_norm": 0.6275153223995622,
      "learning_rate": 1.2168310655710265e-05,
      "loss": 0.4957,
      "step": 4971
    },
    {
      "epoch": 0.44763555335479077,
      "grad_norm": 0.7158290912731771,
      "learning_rate": 1.2165463769281578e-05,
      "loss": 0.508,
      "step": 4972
    },
    {
      "epoch": 0.44772558464066264,
      "grad_norm": 0.600878515494105,
      "learning_rate": 1.2162616698700512e-05,
      "loss": 0.4983,
      "step": 4973
    },
    {
      "epoch": 0.44781561592653446,
      "grad_norm": 0.9503415216520518,
      "learning_rate": 1.2159769444209174e-05,
      "loss": 0.6395,
      "step": 4974
    },
    {
      "epoch": 0.44790564721240633,
      "grad_norm": 0.9688098765280805,
      "learning_rate": 1.2156922006049703e-05,
      "loss": 0.602,
      "step": 4975
    },
    {
      "epoch": 0.44799567849827815,
      "grad_norm": 0.7273316014763941,
      "learning_rate": 1.2154074384464245e-05,
      "loss": 0.4579,
      "step": 4976
    },
    {
      "epoch": 0.44808570978414997,
      "grad_norm": 0.7895967925156862,
      "learning_rate": 1.2151226579694966e-05,
      "loss": 0.5409,
      "step": 4977
    },
    {
      "epoch": 0.44817574107002184,
      "grad_norm": 0.872352767560518,
      "learning_rate": 1.2148378591984041e-05,
      "loss": 0.5349,
      "step": 4978
    },
    {
      "epoch": 0.44826577235589365,
      "grad_norm": 0.7852336062533878,
      "learning_rate": 1.2145530421573666e-05,
      "loss": 0.5753,
      "step": 4979
    },
    {
      "epoch": 0.4483558036417655,
      "grad_norm": 0.5655939993705557,
      "learning_rate": 1.2142682068706055e-05,
      "loss": 0.5263,
      "step": 4980
    },
    {
      "epoch": 0.44844583492763734,
      "grad_norm": 0.9387156118746599,
      "learning_rate": 1.2139833533623427e-05,
      "loss": 0.5548,
      "step": 4981
    },
    {
      "epoch": 0.4485358662135092,
      "grad_norm": 0.7944442870150755,
      "learning_rate": 1.2136984816568027e-05,
      "loss": 0.5852,
      "step": 4982
    },
    {
      "epoch": 0.44862589749938103,
      "grad_norm": 0.8477891920956685,
      "learning_rate": 1.213413591778211e-05,
      "loss": 0.6092,
      "step": 4983
    },
    {
      "epoch": 0.44871592878525285,
      "grad_norm": 0.6896731401281287,
      "learning_rate": 1.2131286837507954e-05,
      "loss": 0.4996,
      "step": 4984
    },
    {
      "epoch": 0.4488059600711247,
      "grad_norm": 0.775812679934321,
      "learning_rate": 1.2128437575987839e-05,
      "loss": 0.535,
      "step": 4985
    },
    {
      "epoch": 0.44889599135699654,
      "grad_norm": 0.6859842219144913,
      "learning_rate": 1.212558813346407e-05,
      "loss": 0.5907,
      "step": 4986
    },
    {
      "epoch": 0.4489860226428684,
      "grad_norm": 0.8520078990694533,
      "learning_rate": 1.2122738510178968e-05,
      "loss": 0.5881,
      "step": 4987
    },
    {
      "epoch": 0.4490760539287402,
      "grad_norm": 0.7216095483586836,
      "learning_rate": 1.2119888706374866e-05,
      "loss": 0.4912,
      "step": 4988
    },
    {
      "epoch": 0.4491660852146121,
      "grad_norm": 1.047950578204369,
      "learning_rate": 1.211703872229411e-05,
      "loss": 0.6397,
      "step": 4989
    },
    {
      "epoch": 0.4492561165004839,
      "grad_norm": 0.6935202672459079,
      "learning_rate": 1.2114188558179065e-05,
      "loss": 0.5468,
      "step": 4990
    },
    {
      "epoch": 0.44934614778635573,
      "grad_norm": 0.7747700198775819,
      "learning_rate": 1.2111338214272116e-05,
      "loss": 0.475,
      "step": 4991
    },
    {
      "epoch": 0.4494361790722276,
      "grad_norm": 0.6863838613249597,
      "learning_rate": 1.210848769081565e-05,
      "loss": 0.5262,
      "step": 4992
    },
    {
      "epoch": 0.4495262103580994,
      "grad_norm": 1.7340556216771903,
      "learning_rate": 1.2105636988052085e-05,
      "loss": 0.5347,
      "step": 4993
    },
    {
      "epoch": 0.4496162416439713,
      "grad_norm": 0.7224100662159365,
      "learning_rate": 1.2102786106223844e-05,
      "loss": 0.5915,
      "step": 4994
    },
    {
      "epoch": 0.4497062729298431,
      "grad_norm": 1.033391057581755,
      "learning_rate": 1.2099935045573362e-05,
      "loss": 0.5621,
      "step": 4995
    },
    {
      "epoch": 0.449796304215715,
      "grad_norm": 0.7467871503894108,
      "learning_rate": 1.2097083806343104e-05,
      "loss": 0.4897,
      "step": 4996
    },
    {
      "epoch": 0.4498863355015868,
      "grad_norm": 0.5844839752466816,
      "learning_rate": 1.2094232388775535e-05,
      "loss": 0.5068,
      "step": 4997
    },
    {
      "epoch": 0.4499763667874586,
      "grad_norm": 0.7844864858447896,
      "learning_rate": 1.2091380793113145e-05,
      "loss": 0.5952,
      "step": 4998
    },
    {
      "epoch": 0.4500663980733305,
      "grad_norm": 0.6641686236493017,
      "learning_rate": 1.2088529019598432e-05,
      "loss": 0.5556,
      "step": 4999
    },
    {
      "epoch": 0.4501564293592023,
      "grad_norm": 0.6434821809903848,
      "learning_rate": 1.2085677068473915e-05,
      "loss": 0.5816,
      "step": 5000
    },
    {
      "epoch": 0.4502464606450742,
      "grad_norm": 0.6093431820081578,
      "learning_rate": 1.2082824939982128e-05,
      "loss": 0.5246,
      "step": 5001
    },
    {
      "epoch": 0.450336491930946,
      "grad_norm": 0.6088579550534239,
      "learning_rate": 1.2079972634365608e-05,
      "loss": 0.5517,
      "step": 5002
    },
    {
      "epoch": 0.45042652321681786,
      "grad_norm": 0.7361013598728046,
      "learning_rate": 1.2077120151866927e-05,
      "loss": 0.5023,
      "step": 5003
    },
    {
      "epoch": 0.4505165545026897,
      "grad_norm": 0.6489982658120886,
      "learning_rate": 1.207426749272866e-05,
      "loss": 0.5769,
      "step": 5004
    },
    {
      "epoch": 0.4506065857885615,
      "grad_norm": 0.635632804791505,
      "learning_rate": 1.2071414657193397e-05,
      "loss": 0.5197,
      "step": 5005
    },
    {
      "epoch": 0.45069661707443337,
      "grad_norm": 0.8156037673491484,
      "learning_rate": 1.2068561645503745e-05,
      "loss": 0.5621,
      "step": 5006
    },
    {
      "epoch": 0.4507866483603052,
      "grad_norm": 0.9421434911816311,
      "learning_rate": 1.2065708457902326e-05,
      "loss": 0.5853,
      "step": 5007
    },
    {
      "epoch": 0.45087667964617706,
      "grad_norm": 0.6729782310794425,
      "learning_rate": 1.2062855094631777e-05,
      "loss": 0.5647,
      "step": 5008
    },
    {
      "epoch": 0.4509667109320489,
      "grad_norm": 0.7886750347985552,
      "learning_rate": 1.2060001555934748e-05,
      "loss": 0.4433,
      "step": 5009
    },
    {
      "epoch": 0.45105674221792075,
      "grad_norm": 0.6324203815875709,
      "learning_rate": 1.2057147842053908e-05,
      "loss": 0.5146,
      "step": 5010
    },
    {
      "epoch": 0.45114677350379256,
      "grad_norm": 0.6861259083044787,
      "learning_rate": 1.205429395323194e-05,
      "loss": 0.5087,
      "step": 5011
    },
    {
      "epoch": 0.4512368047896644,
      "grad_norm": 0.6718314310303273,
      "learning_rate": 1.205143988971154e-05,
      "loss": 0.5815,
      "step": 5012
    },
    {
      "epoch": 0.45132683607553625,
      "grad_norm": 0.6379745077281149,
      "learning_rate": 1.2048585651735416e-05,
      "loss": 0.5716,
      "step": 5013
    },
    {
      "epoch": 0.45141686736140807,
      "grad_norm": 0.7331392430642564,
      "learning_rate": 1.2045731239546295e-05,
      "loss": 0.6154,
      "step": 5014
    },
    {
      "epoch": 0.45150689864727994,
      "grad_norm": 0.7667914087153649,
      "learning_rate": 1.2042876653386922e-05,
      "loss": 0.5606,
      "step": 5015
    },
    {
      "epoch": 0.45159692993315176,
      "grad_norm": 0.7644654827917259,
      "learning_rate": 1.2040021893500047e-05,
      "loss": 0.4338,
      "step": 5016
    },
    {
      "epoch": 0.45168696121902363,
      "grad_norm": 0.6931388399842403,
      "learning_rate": 1.2037166960128443e-05,
      "loss": 0.5149,
      "step": 5017
    },
    {
      "epoch": 0.45177699250489545,
      "grad_norm": 0.7389032821095981,
      "learning_rate": 1.2034311853514897e-05,
      "loss": 0.6035,
      "step": 5018
    },
    {
      "epoch": 0.4518670237907673,
      "grad_norm": 0.692786044967474,
      "learning_rate": 1.203145657390221e-05,
      "loss": 0.6081,
      "step": 5019
    },
    {
      "epoch": 0.45195705507663914,
      "grad_norm": 0.7414874628566877,
      "learning_rate": 1.2028601121533192e-05,
      "loss": 0.5875,
      "step": 5020
    },
    {
      "epoch": 0.45204708636251095,
      "grad_norm": 0.7177174830198078,
      "learning_rate": 1.2025745496650676e-05,
      "loss": 0.5539,
      "step": 5021
    },
    {
      "epoch": 0.4521371176483828,
      "grad_norm": 0.6837590751342664,
      "learning_rate": 1.2022889699497507e-05,
      "loss": 0.4579,
      "step": 5022
    },
    {
      "epoch": 0.45222714893425464,
      "grad_norm": 0.7020337540099931,
      "learning_rate": 1.2020033730316536e-05,
      "loss": 0.4769,
      "step": 5023
    },
    {
      "epoch": 0.4523171802201265,
      "grad_norm": 0.6994902497245179,
      "learning_rate": 1.2017177589350645e-05,
      "loss": 0.5814,
      "step": 5024
    },
    {
      "epoch": 0.45240721150599833,
      "grad_norm": 0.6923879612944258,
      "learning_rate": 1.2014321276842722e-05,
      "loss": 0.4424,
      "step": 5025
    },
    {
      "epoch": 0.4524972427918702,
      "grad_norm": 0.7565146778919851,
      "learning_rate": 1.2011464793035666e-05,
      "loss": 0.6093,
      "step": 5026
    },
    {
      "epoch": 0.452587274077742,
      "grad_norm": 0.7476080417883527,
      "learning_rate": 1.2008608138172393e-05,
      "loss": 0.6037,
      "step": 5027
    },
    {
      "epoch": 0.45267730536361384,
      "grad_norm": 0.6816763748195791,
      "learning_rate": 1.2005751312495837e-05,
      "loss": 0.5151,
      "step": 5028
    },
    {
      "epoch": 0.4527673366494857,
      "grad_norm": 0.7658229615935285,
      "learning_rate": 1.2002894316248945e-05,
      "loss": 0.5739,
      "step": 5029
    },
    {
      "epoch": 0.4528573679353575,
      "grad_norm": 0.7787413408173735,
      "learning_rate": 1.2000037149674677e-05,
      "loss": 0.4861,
      "step": 5030
    },
    {
      "epoch": 0.4529473992212294,
      "grad_norm": 0.8456710845993712,
      "learning_rate": 1.1997179813016006e-05,
      "loss": 0.5882,
      "step": 5031
    },
    {
      "epoch": 0.4530374305071012,
      "grad_norm": 0.7544559180258346,
      "learning_rate": 1.1994322306515926e-05,
      "loss": 0.5851,
      "step": 5032
    },
    {
      "epoch": 0.4531274617929731,
      "grad_norm": 0.5989316173715326,
      "learning_rate": 1.1991464630417436e-05,
      "loss": 0.4559,
      "step": 5033
    },
    {
      "epoch": 0.4532174930788449,
      "grad_norm": 0.7143957638265969,
      "learning_rate": 1.198860678496356e-05,
      "loss": 0.476,
      "step": 5034
    },
    {
      "epoch": 0.4533075243647167,
      "grad_norm": 0.6887996892761746,
      "learning_rate": 1.1985748770397325e-05,
      "loss": 0.5176,
      "step": 5035
    },
    {
      "epoch": 0.4533975556505886,
      "grad_norm": 0.6553687162093537,
      "learning_rate": 1.1982890586961783e-05,
      "loss": 0.5211,
      "step": 5036
    },
    {
      "epoch": 0.4534875869364604,
      "grad_norm": 0.791440717233519,
      "learning_rate": 1.1980032234899995e-05,
      "loss": 0.611,
      "step": 5037
    },
    {
      "epoch": 0.4535776182223323,
      "grad_norm": 0.9132603666893354,
      "learning_rate": 1.1977173714455034e-05,
      "loss": 0.5089,
      "step": 5038
    },
    {
      "epoch": 0.4536676495082041,
      "grad_norm": 0.74793044168774,
      "learning_rate": 1.1974315025869998e-05,
      "loss": 0.6144,
      "step": 5039
    },
    {
      "epoch": 0.45375768079407597,
      "grad_norm": 1.22742378400387,
      "learning_rate": 1.197145616938798e-05,
      "loss": 0.5737,
      "step": 5040
    },
    {
      "epoch": 0.4538477120799478,
      "grad_norm": 0.7545734263232211,
      "learning_rate": 1.1968597145252108e-05,
      "loss": 0.5585,
      "step": 5041
    },
    {
      "epoch": 0.4539377433658196,
      "grad_norm": 0.8327494293535211,
      "learning_rate": 1.1965737953705511e-05,
      "loss": 0.5604,
      "step": 5042
    },
    {
      "epoch": 0.4540277746516915,
      "grad_norm": 0.5895290111717332,
      "learning_rate": 1.1962878594991341e-05,
      "loss": 0.5487,
      "step": 5043
    },
    {
      "epoch": 0.4541178059375633,
      "grad_norm": 0.8800285383700992,
      "learning_rate": 1.1960019069352755e-05,
      "loss": 0.4839,
      "step": 5044
    },
    {
      "epoch": 0.45420783722343516,
      "grad_norm": 0.6888651994599992,
      "learning_rate": 1.1957159377032928e-05,
      "loss": 0.5314,
      "step": 5045
    },
    {
      "epoch": 0.454297868509307,
      "grad_norm": 0.9927294732700853,
      "learning_rate": 1.1954299518275056e-05,
      "loss": 0.5245,
      "step": 5046
    },
    {
      "epoch": 0.45438789979517885,
      "grad_norm": 0.7731324118917332,
      "learning_rate": 1.1951439493322336e-05,
      "loss": 0.6409,
      "step": 5047
    },
    {
      "epoch": 0.45447793108105067,
      "grad_norm": 0.6473794350709137,
      "learning_rate": 1.1948579302417991e-05,
      "loss": 0.6287,
      "step": 5048
    },
    {
      "epoch": 0.4545679623669225,
      "grad_norm": 0.6990886755715394,
      "learning_rate": 1.1945718945805252e-05,
      "loss": 0.532,
      "step": 5049
    },
    {
      "epoch": 0.45465799365279436,
      "grad_norm": 0.9180990165049908,
      "learning_rate": 1.1942858423727368e-05,
      "loss": 0.5715,
      "step": 5050
    },
    {
      "epoch": 0.4547480249386662,
      "grad_norm": 0.7139773510408715,
      "learning_rate": 1.1939997736427596e-05,
      "loss": 0.5948,
      "step": 5051
    },
    {
      "epoch": 0.45483805622453805,
      "grad_norm": 0.7299818010720722,
      "learning_rate": 1.1937136884149209e-05,
      "loss": 0.4622,
      "step": 5052
    },
    {
      "epoch": 0.45492808751040986,
      "grad_norm": 0.8695512478449707,
      "learning_rate": 1.1934275867135502e-05,
      "loss": 0.6223,
      "step": 5053
    },
    {
      "epoch": 0.45501811879628173,
      "grad_norm": 0.6070551027981533,
      "learning_rate": 1.1931414685629775e-05,
      "loss": 0.6319,
      "step": 5054
    },
    {
      "epoch": 0.45510815008215355,
      "grad_norm": 0.6817085743560392,
      "learning_rate": 1.1928553339875339e-05,
      "loss": 0.4847,
      "step": 5055
    },
    {
      "epoch": 0.45519818136802537,
      "grad_norm": 0.6353940116414193,
      "learning_rate": 1.1925691830115531e-05,
      "loss": 0.4674,
      "step": 5056
    },
    {
      "epoch": 0.45528821265389724,
      "grad_norm": 0.8670669085074334,
      "learning_rate": 1.1922830156593697e-05,
      "loss": 0.5183,
      "step": 5057
    },
    {
      "epoch": 0.45537824393976906,
      "grad_norm": 0.8432637359610061,
      "learning_rate": 1.191996831955319e-05,
      "loss": 0.536,
      "step": 5058
    },
    {
      "epoch": 0.45546827522564093,
      "grad_norm": 0.7724275111268408,
      "learning_rate": 1.1917106319237386e-05,
      "loss": 0.5837,
      "step": 5059
    },
    {
      "epoch": 0.45555830651151275,
      "grad_norm": 0.7551412358420578,
      "learning_rate": 1.1914244155889672e-05,
      "loss": 0.5667,
      "step": 5060
    },
    {
      "epoch": 0.4556483377973846,
      "grad_norm": 0.6888577710988877,
      "learning_rate": 1.1911381829753443e-05,
      "loss": 0.5333,
      "step": 5061
    },
    {
      "epoch": 0.45573836908325643,
      "grad_norm": 0.6246132124186016,
      "learning_rate": 1.1908519341072117e-05,
      "loss": 0.5297,
      "step": 5062
    },
    {
      "epoch": 0.45582840036912825,
      "grad_norm": 0.8206323526232528,
      "learning_rate": 1.1905656690089121e-05,
      "loss": 0.5642,
      "step": 5063
    },
    {
      "epoch": 0.4559184316550001,
      "grad_norm": 0.7294981967524997,
      "learning_rate": 1.19027938770479e-05,
      "loss": 0.5444,
      "step": 5064
    },
    {
      "epoch": 0.45600846294087194,
      "grad_norm": 0.8095976693384555,
      "learning_rate": 1.1899930902191904e-05,
      "loss": 0.4922,
      "step": 5065
    },
    {
      "epoch": 0.4560984942267438,
      "grad_norm": 0.6744963495530589,
      "learning_rate": 1.1897067765764603e-05,
      "loss": 0.6509,
      "step": 5066
    },
    {
      "epoch": 0.45618852551261563,
      "grad_norm": 0.6744539586831888,
      "learning_rate": 1.1894204468009483e-05,
      "loss": 0.5268,
      "step": 5067
    },
    {
      "epoch": 0.4562785567984875,
      "grad_norm": 0.6446266162182785,
      "learning_rate": 1.1891341009170036e-05,
      "loss": 0.5382,
      "step": 5068
    },
    {
      "epoch": 0.4563685880843593,
      "grad_norm": 0.6932496011562531,
      "learning_rate": 1.1888477389489776e-05,
      "loss": 0.5238,
      "step": 5069
    },
    {
      "epoch": 0.45645861937023113,
      "grad_norm": 0.8152150143503881,
      "learning_rate": 1.1885613609212227e-05,
      "loss": 0.5494,
      "step": 5070
    },
    {
      "epoch": 0.456548650656103,
      "grad_norm": 0.8494037690858658,
      "learning_rate": 1.1882749668580928e-05,
      "loss": 0.5088,
      "step": 5071
    },
    {
      "epoch": 0.4566386819419748,
      "grad_norm": 0.6316238012239669,
      "learning_rate": 1.1879885567839426e-05,
      "loss": 0.499,
      "step": 5072
    },
    {
      "epoch": 0.4567287132278467,
      "grad_norm": 0.8073145665084862,
      "learning_rate": 1.1877021307231289e-05,
      "loss": 0.5449,
      "step": 5073
    },
    {
      "epoch": 0.4568187445137185,
      "grad_norm": 0.5934658105554365,
      "learning_rate": 1.1874156887000094e-05,
      "loss": 0.5047,
      "step": 5074
    },
    {
      "epoch": 0.4569087757995904,
      "grad_norm": 0.6649138350433249,
      "learning_rate": 1.1871292307389433e-05,
      "loss": 0.5667,
      "step": 5075
    },
    {
      "epoch": 0.4569988070854622,
      "grad_norm": 0.8096045724565941,
      "learning_rate": 1.1868427568642913e-05,
      "loss": 0.4571,
      "step": 5076
    },
    {
      "epoch": 0.457088838371334,
      "grad_norm": 0.6840627500988417,
      "learning_rate": 1.1865562671004152e-05,
      "loss": 0.511,
      "step": 5077
    },
    {
      "epoch": 0.4571788696572059,
      "grad_norm": 0.6524077629281385,
      "learning_rate": 1.1862697614716785e-05,
      "loss": 0.5777,
      "step": 5078
    },
    {
      "epoch": 0.4572689009430777,
      "grad_norm": 0.7096767527728858,
      "learning_rate": 1.1859832400024454e-05,
      "loss": 0.5444,
      "step": 5079
    },
    {
      "epoch": 0.4573589322289496,
      "grad_norm": 0.8232702473754602,
      "learning_rate": 1.1856967027170818e-05,
      "loss": 0.5525,
      "step": 5080
    },
    {
      "epoch": 0.4574489635148214,
      "grad_norm": 0.9425957969874929,
      "learning_rate": 1.1854101496399559e-05,
      "loss": 0.5467,
      "step": 5081
    },
    {
      "epoch": 0.45753899480069327,
      "grad_norm": 0.741373371405055,
      "learning_rate": 1.1851235807954354e-05,
      "loss": 0.5225,
      "step": 5082
    },
    {
      "epoch": 0.4576290260865651,
      "grad_norm": 0.8403396021746699,
      "learning_rate": 1.1848369962078907e-05,
      "loss": 0.5485,
      "step": 5083
    },
    {
      "epoch": 0.4577190573724369,
      "grad_norm": 0.7602201938504595,
      "learning_rate": 1.1845503959016929e-05,
      "loss": 0.5964,
      "step": 5084
    },
    {
      "epoch": 0.45780908865830877,
      "grad_norm": 0.6670104645001105,
      "learning_rate": 1.1842637799012151e-05,
      "loss": 0.5027,
      "step": 5085
    },
    {
      "epoch": 0.4578991199441806,
      "grad_norm": 0.7148549906697461,
      "learning_rate": 1.183977148230831e-05,
      "loss": 0.5853,
      "step": 5086
    },
    {
      "epoch": 0.45798915123005246,
      "grad_norm": 0.6495910134954053,
      "learning_rate": 1.1836905009149157e-05,
      "loss": 0.5198,
      "step": 5087
    },
    {
      "epoch": 0.4580791825159243,
      "grad_norm": 0.914100639124402,
      "learning_rate": 1.1834038379778465e-05,
      "loss": 0.5376,
      "step": 5088
    },
    {
      "epoch": 0.45816921380179615,
      "grad_norm": 0.8259172877183976,
      "learning_rate": 1.183117159444001e-05,
      "loss": 0.58,
      "step": 5089
    },
    {
      "epoch": 0.45825924508766797,
      "grad_norm": 0.8523799518461145,
      "learning_rate": 1.1828304653377582e-05,
      "loss": 0.6258,
      "step": 5090
    },
    {
      "epoch": 0.4583492763735398,
      "grad_norm": 0.8762974161844095,
      "learning_rate": 1.1825437556834994e-05,
      "loss": 0.5179,
      "step": 5091
    },
    {
      "epoch": 0.45843930765941165,
      "grad_norm": 0.7114605883656084,
      "learning_rate": 1.1822570305056065e-05,
      "loss": 0.5277,
      "step": 5092
    },
    {
      "epoch": 0.45852933894528347,
      "grad_norm": 0.6684413617116484,
      "learning_rate": 1.1819702898284622e-05,
      "loss": 0.5784,
      "step": 5093
    },
    {
      "epoch": 0.45861937023115534,
      "grad_norm": 0.7237855807249994,
      "learning_rate": 1.1816835336764514e-05,
      "loss": 0.5005,
      "step": 5094
    },
    {
      "epoch": 0.45870940151702716,
      "grad_norm": 0.6104984985212463,
      "learning_rate": 1.1813967620739603e-05,
      "loss": 0.4804,
      "step": 5095
    },
    {
      "epoch": 0.45879943280289903,
      "grad_norm": 0.7114095996628726,
      "learning_rate": 1.1811099750453758e-05,
      "loss": 0.5381,
      "step": 5096
    },
    {
      "epoch": 0.45888946408877085,
      "grad_norm": 0.7513221947949136,
      "learning_rate": 1.1808231726150864e-05,
      "loss": 0.5454,
      "step": 5097
    },
    {
      "epoch": 0.45897949537464267,
      "grad_norm": 0.7680790028096917,
      "learning_rate": 1.1805363548074824e-05,
      "loss": 0.5863,
      "step": 5098
    },
    {
      "epoch": 0.45906952666051454,
      "grad_norm": 0.6177410130454354,
      "learning_rate": 1.1802495216469546e-05,
      "loss": 0.5364,
      "step": 5099
    },
    {
      "epoch": 0.45915955794638635,
      "grad_norm": 0.6669907209012312,
      "learning_rate": 1.1799626731578957e-05,
      "loss": 0.4901,
      "step": 5100
    },
    {
      "epoch": 0.4592495892322582,
      "grad_norm": 0.7376000633592914,
      "learning_rate": 1.1796758093646989e-05,
      "loss": 0.5171,
      "step": 5101
    },
    {
      "epoch": 0.45933962051813004,
      "grad_norm": 0.7355202521194348,
      "learning_rate": 1.1793889302917602e-05,
      "loss": 0.4742,
      "step": 5102
    },
    {
      "epoch": 0.4594296518040019,
      "grad_norm": 0.6797887302763036,
      "learning_rate": 1.1791020359634752e-05,
      "loss": 0.5484,
      "step": 5103
    },
    {
      "epoch": 0.45951968308987373,
      "grad_norm": 0.7904980163125663,
      "learning_rate": 1.178815126404242e-05,
      "loss": 0.5495,
      "step": 5104
    },
    {
      "epoch": 0.45960971437574555,
      "grad_norm": 0.6897036041120417,
      "learning_rate": 1.1785282016384592e-05,
      "loss": 0.5117,
      "step": 5105
    },
    {
      "epoch": 0.4596997456616174,
      "grad_norm": 1.1779498340519543,
      "learning_rate": 1.1782412616905275e-05,
      "loss": 0.6731,
      "step": 5106
    },
    {
      "epoch": 0.45978977694748924,
      "grad_norm": 0.6803281157386215,
      "learning_rate": 1.1779543065848483e-05,
      "loss": 0.4777,
      "step": 5107
    },
    {
      "epoch": 0.4598798082333611,
      "grad_norm": 0.8480194993356573,
      "learning_rate": 1.177667336345824e-05,
      "loss": 0.4905,
      "step": 5108
    },
    {
      "epoch": 0.4599698395192329,
      "grad_norm": 0.7468195089364568,
      "learning_rate": 1.1773803509978594e-05,
      "loss": 0.5602,
      "step": 5109
    },
    {
      "epoch": 0.4600598708051048,
      "grad_norm": 0.8953584454119422,
      "learning_rate": 1.1770933505653595e-05,
      "loss": 0.5994,
      "step": 5110
    },
    {
      "epoch": 0.4601499020909766,
      "grad_norm": 0.7439766808110225,
      "learning_rate": 1.176806335072731e-05,
      "loss": 0.4507,
      "step": 5111
    },
    {
      "epoch": 0.46023993337684843,
      "grad_norm": 0.7276998263071948,
      "learning_rate": 1.1765193045443822e-05,
      "loss": 0.5369,
      "step": 5112
    },
    {
      "epoch": 0.4603299646627203,
      "grad_norm": 0.9055226965087455,
      "learning_rate": 1.176232259004722e-05,
      "loss": 0.6115,
      "step": 5113
    },
    {
      "epoch": 0.4604199959485921,
      "grad_norm": 0.6629661509490851,
      "learning_rate": 1.1759451984781612e-05,
      "loss": 0.4507,
      "step": 5114
    },
    {
      "epoch": 0.460510027234464,
      "grad_norm": 0.6100299511203519,
      "learning_rate": 1.1756581229891111e-05,
      "loss": 0.545,
      "step": 5115
    },
    {
      "epoch": 0.4606000585203358,
      "grad_norm": 0.9015069320911594,
      "learning_rate": 1.1753710325619856e-05,
      "loss": 0.5705,
      "step": 5116
    },
    {
      "epoch": 0.4606900898062077,
      "grad_norm": 0.762822006646212,
      "learning_rate": 1.1750839272211982e-05,
      "loss": 0.4767,
      "step": 5117
    },
    {
      "epoch": 0.4607801210920795,
      "grad_norm": 0.7236855051697585,
      "learning_rate": 1.1747968069911652e-05,
      "loss": 0.5605,
      "step": 5118
    },
    {
      "epoch": 0.4608701523779513,
      "grad_norm": 0.7042692849984905,
      "learning_rate": 1.1745096718963029e-05,
      "loss": 0.5255,
      "step": 5119
    },
    {
      "epoch": 0.4609601836638232,
      "grad_norm": 0.6249853133128176,
      "learning_rate": 1.1742225219610301e-05,
      "loss": 0.5423,
      "step": 5120
    },
    {
      "epoch": 0.461050214949695,
      "grad_norm": 0.9040702590974508,
      "learning_rate": 1.1739353572097658e-05,
      "loss": 0.5477,
      "step": 5121
    },
    {
      "epoch": 0.4611402462355669,
      "grad_norm": 1.3628774849603178,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 0.4754,
      "step": 5122
    },
    {
      "epoch": 0.4612302775214387,
      "grad_norm": 0.7313705323825149,
      "learning_rate": 1.1733609833569466e-05,
      "loss": 0.4983,
      "step": 5123
    },
    {
      "epoch": 0.46132030880731056,
      "grad_norm": 0.7365256433403643,
      "learning_rate": 1.1730737743042369e-05,
      "loss": 0.5526,
      "step": 5124
    },
    {
      "epoch": 0.4614103400931824,
      "grad_norm": 0.8693999962944199,
      "learning_rate": 1.172786550533226e-05,
      "loss": 0.5565,
      "step": 5125
    },
    {
      "epoch": 0.4615003713790542,
      "grad_norm": 0.8538767955084552,
      "learning_rate": 1.1724993120683392e-05,
      "loss": 0.5865,
      "step": 5126
    },
    {
      "epoch": 0.46159040266492607,
      "grad_norm": 0.6100407664191347,
      "learning_rate": 1.1722120589340045e-05,
      "loss": 0.5547,
      "step": 5127
    },
    {
      "epoch": 0.4616804339507979,
      "grad_norm": 0.7757241661876402,
      "learning_rate": 1.1719247911546488e-05,
      "loss": 0.5561,
      "step": 5128
    },
    {
      "epoch": 0.46177046523666976,
      "grad_norm": 0.7952157328816438,
      "learning_rate": 1.1716375087547023e-05,
      "loss": 0.5691,
      "step": 5129
    },
    {
      "epoch": 0.4618604965225416,
      "grad_norm": 0.8608405409769939,
      "learning_rate": 1.1713502117585955e-05,
      "loss": 0.5655,
      "step": 5130
    },
    {
      "epoch": 0.46195052780841345,
      "grad_norm": 0.706991693534674,
      "learning_rate": 1.1710629001907603e-05,
      "loss": 0.4732,
      "step": 5131
    },
    {
      "epoch": 0.46204055909428526,
      "grad_norm": 0.7526851753463781,
      "learning_rate": 1.1707755740756296e-05,
      "loss": 0.5212,
      "step": 5132
    },
    {
      "epoch": 0.4621305903801571,
      "grad_norm": 0.7418220711560432,
      "learning_rate": 1.1704882334376383e-05,
      "loss": 0.5992,
      "step": 5133
    },
    {
      "epoch": 0.46222062166602895,
      "grad_norm": 0.859844777971899,
      "learning_rate": 1.1702008783012218e-05,
      "loss": 0.6104,
      "step": 5134
    },
    {
      "epoch": 0.46231065295190077,
      "grad_norm": 0.926164904099634,
      "learning_rate": 1.1699135086908166e-05,
      "loss": 0.5334,
      "step": 5135
    },
    {
      "epoch": 0.46240068423777264,
      "grad_norm": 0.5815215802233095,
      "learning_rate": 1.1696261246308613e-05,
      "loss": 0.4583,
      "step": 5136
    },
    {
      "epoch": 0.46249071552364446,
      "grad_norm": 0.7798096435679318,
      "learning_rate": 1.1693387261457954e-05,
      "loss": 0.6212,
      "step": 5137
    },
    {
      "epoch": 0.46258074680951633,
      "grad_norm": 0.6761900590320273,
      "learning_rate": 1.1690513132600585e-05,
      "loss": 0.5021,
      "step": 5138
    },
    {
      "epoch": 0.46267077809538815,
      "grad_norm": 0.8292412162331927,
      "learning_rate": 1.1687638859980932e-05,
      "loss": 0.5449,
      "step": 5139
    },
    {
      "epoch": 0.46276080938125996,
      "grad_norm": 0.6155028257717071,
      "learning_rate": 1.1684764443843421e-05,
      "loss": 0.4969,
      "step": 5140
    },
    {
      "epoch": 0.46285084066713184,
      "grad_norm": 0.8290878574825059,
      "learning_rate": 1.16818898844325e-05,
      "loss": 0.5629,
      "step": 5141
    },
    {
      "epoch": 0.46294087195300365,
      "grad_norm": 0.6024898357287727,
      "learning_rate": 1.1679015181992615e-05,
      "loss": 0.5234,
      "step": 5142
    },
    {
      "epoch": 0.4630309032388755,
      "grad_norm": 0.6982352190466092,
      "learning_rate": 1.1676140336768236e-05,
      "loss": 0.6125,
      "step": 5143
    },
    {
      "epoch": 0.46312093452474734,
      "grad_norm": 0.7171476578270419,
      "learning_rate": 1.1673265349003846e-05,
      "loss": 0.4874,
      "step": 5144
    },
    {
      "epoch": 0.4632109658106192,
      "grad_norm": 0.7852747602588136,
      "learning_rate": 1.1670390218943928e-05,
      "loss": 0.6376,
      "step": 5145
    },
    {
      "epoch": 0.46330099709649103,
      "grad_norm": 0.8347842616554408,
      "learning_rate": 1.166751494683299e-05,
      "loss": 0.5655,
      "step": 5146
    },
    {
      "epoch": 0.46339102838236285,
      "grad_norm": 0.9838176443257374,
      "learning_rate": 1.1664639532915543e-05,
      "loss": 0.5741,
      "step": 5147
    },
    {
      "epoch": 0.4634810596682347,
      "grad_norm": 0.8322443305091395,
      "learning_rate": 1.166176397743612e-05,
      "loss": 0.5792,
      "step": 5148
    },
    {
      "epoch": 0.46357109095410653,
      "grad_norm": 0.95366246608238,
      "learning_rate": 1.1658888280639254e-05,
      "loss": 0.5687,
      "step": 5149
    },
    {
      "epoch": 0.4636611222399784,
      "grad_norm": 0.8582015144603682,
      "learning_rate": 1.16560124427695e-05,
      "loss": 0.5363,
      "step": 5150
    },
    {
      "epoch": 0.4637511535258502,
      "grad_norm": 0.7033651676466308,
      "learning_rate": 1.1653136464071419e-05,
      "loss": 0.5152,
      "step": 5151
    },
    {
      "epoch": 0.4638411848117221,
      "grad_norm": 0.841002106492512,
      "learning_rate": 1.1650260344789586e-05,
      "loss": 0.4451,
      "step": 5152
    },
    {
      "epoch": 0.4639312160975939,
      "grad_norm": 0.8009751366469122,
      "learning_rate": 1.164738408516859e-05,
      "loss": 0.4778,
      "step": 5153
    },
    {
      "epoch": 0.46402124738346573,
      "grad_norm": 0.6894255652580996,
      "learning_rate": 1.1644507685453026e-05,
      "loss": 0.4521,
      "step": 5154
    },
    {
      "epoch": 0.4641112786693376,
      "grad_norm": 0.6611934487842457,
      "learning_rate": 1.164163114588751e-05,
      "loss": 0.5202,
      "step": 5155
    },
    {
      "epoch": 0.4642013099552094,
      "grad_norm": 0.6962817295017362,
      "learning_rate": 1.1638754466716662e-05,
      "loss": 0.5244,
      "step": 5156
    },
    {
      "epoch": 0.4642913412410813,
      "grad_norm": 1.2251596241964637,
      "learning_rate": 1.1635877648185114e-05,
      "loss": 0.5559,
      "step": 5157
    },
    {
      "epoch": 0.4643813725269531,
      "grad_norm": 0.7409459283763637,
      "learning_rate": 1.1633000690537519e-05,
      "loss": 0.5867,
      "step": 5158
    },
    {
      "epoch": 0.464471403812825,
      "grad_norm": 1.1796464990385145,
      "learning_rate": 1.1630123594018528e-05,
      "loss": 0.594,
      "step": 5159
    },
    {
      "epoch": 0.4645614350986968,
      "grad_norm": 0.8024017147276304,
      "learning_rate": 1.1627246358872815e-05,
      "loss": 0.6056,
      "step": 5160
    },
    {
      "epoch": 0.4646514663845686,
      "grad_norm": 0.7807494742328429,
      "learning_rate": 1.162436898534506e-05,
      "loss": 0.5357,
      "step": 5161
    },
    {
      "epoch": 0.4647414976704405,
      "grad_norm": 0.7194936396264253,
      "learning_rate": 1.162149147367996e-05,
      "loss": 0.6258,
      "step": 5162
    },
    {
      "epoch": 0.4648315289563123,
      "grad_norm": 0.9678420986828465,
      "learning_rate": 1.1618613824122219e-05,
      "loss": 0.5273,
      "step": 5163
    },
    {
      "epoch": 0.4649215602421842,
      "grad_norm": 1.054531752270179,
      "learning_rate": 1.161573603691655e-05,
      "loss": 0.5814,
      "step": 5164
    },
    {
      "epoch": 0.465011591528056,
      "grad_norm": 0.7321399558612296,
      "learning_rate": 1.1612858112307687e-05,
      "loss": 0.5337,
      "step": 5165
    },
    {
      "epoch": 0.46510162281392786,
      "grad_norm": 2.05526930437508,
      "learning_rate": 1.160998005054037e-05,
      "loss": 0.5864,
      "step": 5166
    },
    {
      "epoch": 0.4651916540997997,
      "grad_norm": 0.7919875446446899,
      "learning_rate": 1.1607101851859347e-05,
      "loss": 0.6129,
      "step": 5167
    },
    {
      "epoch": 0.4652816853856715,
      "grad_norm": 0.6892108109604025,
      "learning_rate": 1.1604223516509386e-05,
      "loss": 0.5478,
      "step": 5168
    },
    {
      "epoch": 0.46537171667154337,
      "grad_norm": 0.674995130568391,
      "learning_rate": 1.1601345044735262e-05,
      "loss": 0.5487,
      "step": 5169
    },
    {
      "epoch": 0.4654617479574152,
      "grad_norm": 0.6924883118085308,
      "learning_rate": 1.159846643678176e-05,
      "loss": 0.4971,
      "step": 5170
    },
    {
      "epoch": 0.46555177924328706,
      "grad_norm": 0.7459915639357865,
      "learning_rate": 1.159558769289368e-05,
      "loss": 0.4849,
      "step": 5171
    },
    {
      "epoch": 0.4656418105291589,
      "grad_norm": 0.903194615746307,
      "learning_rate": 1.1592708813315832e-05,
      "loss": 0.5447,
      "step": 5172
    },
    {
      "epoch": 0.46573184181503074,
      "grad_norm": 0.7389870679203495,
      "learning_rate": 1.1589829798293035e-05,
      "loss": 0.5193,
      "step": 5173
    },
    {
      "epoch": 0.46582187310090256,
      "grad_norm": 0.8333503346766349,
      "learning_rate": 1.1586950648070128e-05,
      "loss": 0.5107,
      "step": 5174
    },
    {
      "epoch": 0.4659119043867744,
      "grad_norm": 0.7349623342265263,
      "learning_rate": 1.1584071362891951e-05,
      "loss": 0.6485,
      "step": 5175
    },
    {
      "epoch": 0.46600193567264625,
      "grad_norm": 0.7846085405713484,
      "learning_rate": 1.1581191943003363e-05,
      "loss": 0.6459,
      "step": 5176
    },
    {
      "epoch": 0.46609196695851807,
      "grad_norm": 0.9342416456904897,
      "learning_rate": 1.1578312388649228e-05,
      "loss": 0.5587,
      "step": 5177
    },
    {
      "epoch": 0.46618199824438994,
      "grad_norm": 0.7950616821500829,
      "learning_rate": 1.1575432700074429e-05,
      "loss": 0.5309,
      "step": 5178
    },
    {
      "epoch": 0.46627202953026176,
      "grad_norm": 1.0769168562746965,
      "learning_rate": 1.1572552877523855e-05,
      "loss": 0.5148,
      "step": 5179
    },
    {
      "epoch": 0.46636206081613363,
      "grad_norm": 0.9242877367505424,
      "learning_rate": 1.1569672921242407e-05,
      "loss": 0.4848,
      "step": 5180
    },
    {
      "epoch": 0.46645209210200544,
      "grad_norm": 0.6835292751164566,
      "learning_rate": 1.1566792831475e-05,
      "loss": 0.4953,
      "step": 5181
    },
    {
      "epoch": 0.46654212338787726,
      "grad_norm": 0.8492244774366519,
      "learning_rate": 1.1563912608466557e-05,
      "loss": 0.4929,
      "step": 5182
    },
    {
      "epoch": 0.46663215467374913,
      "grad_norm": 0.7019112677383498,
      "learning_rate": 1.1561032252462016e-05,
      "loss": 0.6094,
      "step": 5183
    },
    {
      "epoch": 0.46672218595962095,
      "grad_norm": 0.7070918714993797,
      "learning_rate": 1.1558151763706321e-05,
      "loss": 0.5439,
      "step": 5184
    },
    {
      "epoch": 0.4668122172454928,
      "grad_norm": 0.7510429279831116,
      "learning_rate": 1.1555271142444433e-05,
      "loss": 0.5203,
      "step": 5185
    },
    {
      "epoch": 0.46690224853136464,
      "grad_norm": 0.6455754067880839,
      "learning_rate": 1.1552390388921325e-05,
      "loss": 0.5185,
      "step": 5186
    },
    {
      "epoch": 0.4669922798172365,
      "grad_norm": 0.6927607804405648,
      "learning_rate": 1.154950950338197e-05,
      "loss": 0.5612,
      "step": 5187
    },
    {
      "epoch": 0.4670823111031083,
      "grad_norm": 0.8426238131407983,
      "learning_rate": 1.1546628486071365e-05,
      "loss": 0.5712,
      "step": 5188
    },
    {
      "epoch": 0.46717234238898014,
      "grad_norm": 0.7587841871005551,
      "learning_rate": 1.1543747337234512e-05,
      "loss": 0.6082,
      "step": 5189
    },
    {
      "epoch": 0.467262373674852,
      "grad_norm": 0.6734480644886368,
      "learning_rate": 1.1540866057116431e-05,
      "loss": 0.5446,
      "step": 5190
    },
    {
      "epoch": 0.46735240496072383,
      "grad_norm": 0.8168605689806535,
      "learning_rate": 1.1537984645962142e-05,
      "loss": 0.5453,
      "step": 5191
    },
    {
      "epoch": 0.4674424362465957,
      "grad_norm": 0.5232920769230831,
      "learning_rate": 1.1535103104016683e-05,
      "loss": 0.5354,
      "step": 5192
    },
    {
      "epoch": 0.4675324675324675,
      "grad_norm": 0.8349664154451656,
      "learning_rate": 1.1532221431525104e-05,
      "loss": 0.4949,
      "step": 5193
    },
    {
      "epoch": 0.4676224988183394,
      "grad_norm": 0.741462465124203,
      "learning_rate": 1.1529339628732462e-05,
      "loss": 0.5803,
      "step": 5194
    },
    {
      "epoch": 0.4677125301042112,
      "grad_norm": 0.7004205268675925,
      "learning_rate": 1.1526457695883829e-05,
      "loss": 0.5429,
      "step": 5195
    },
    {
      "epoch": 0.467802561390083,
      "grad_norm": 0.9273424442663145,
      "learning_rate": 1.1523575633224282e-05,
      "loss": 0.5716,
      "step": 5196
    },
    {
      "epoch": 0.4678925926759549,
      "grad_norm": 1.028326437849588,
      "learning_rate": 1.1520693440998922e-05,
      "loss": 0.5511,
      "step": 5197
    },
    {
      "epoch": 0.4679826239618267,
      "grad_norm": 1.1474276208606267,
      "learning_rate": 1.1517811119452847e-05,
      "loss": 0.5942,
      "step": 5198
    },
    {
      "epoch": 0.4680726552476986,
      "grad_norm": 0.7654693996533808,
      "learning_rate": 1.151492866883117e-05,
      "loss": 0.557,
      "step": 5199
    },
    {
      "epoch": 0.4681626865335704,
      "grad_norm": 0.8347128994212103,
      "learning_rate": 1.1512046089379022e-05,
      "loss": 0.536,
      "step": 5200
    },
    {
      "epoch": 0.4682527178194423,
      "grad_norm": 0.7274915157513164,
      "learning_rate": 1.1509163381341531e-05,
      "loss": 0.4319,
      "step": 5201
    },
    {
      "epoch": 0.4683427491053141,
      "grad_norm": 0.826890114754696,
      "learning_rate": 1.150628054496385e-05,
      "loss": 0.6164,
      "step": 5202
    },
    {
      "epoch": 0.4684327803911859,
      "grad_norm": 0.8772270633437766,
      "learning_rate": 1.1503397580491136e-05,
      "loss": 0.552,
      "step": 5203
    },
    {
      "epoch": 0.4685228116770578,
      "grad_norm": 0.6438625611075953,
      "learning_rate": 1.150051448816856e-05,
      "loss": 0.5436,
      "step": 5204
    },
    {
      "epoch": 0.4686128429629296,
      "grad_norm": 0.7167006563628046,
      "learning_rate": 1.1497631268241299e-05,
      "loss": 0.4341,
      "step": 5205
    },
    {
      "epoch": 0.46870287424880147,
      "grad_norm": 0.7060243284416144,
      "learning_rate": 1.1494747920954545e-05,
      "loss": 0.5836,
      "step": 5206
    },
    {
      "epoch": 0.4687929055346733,
      "grad_norm": 0.850801245698225,
      "learning_rate": 1.14918644465535e-05,
      "loss": 0.4601,
      "step": 5207
    },
    {
      "epoch": 0.46888293682054516,
      "grad_norm": 0.679598661307078,
      "learning_rate": 1.1488980845283377e-05,
      "loss": 0.5276,
      "step": 5208
    },
    {
      "epoch": 0.468972968106417,
      "grad_norm": 0.7643054584548895,
      "learning_rate": 1.1486097117389397e-05,
      "loss": 0.5355,
      "step": 5209
    },
    {
      "epoch": 0.4690629993922888,
      "grad_norm": 0.9063862086647966,
      "learning_rate": 1.1483213263116795e-05,
      "loss": 0.6364,
      "step": 5210
    },
    {
      "epoch": 0.46915303067816067,
      "grad_norm": 0.7154329155553089,
      "learning_rate": 1.1480329282710817e-05,
      "loss": 0.5692,
      "step": 5211
    },
    {
      "epoch": 0.4692430619640325,
      "grad_norm": 0.7487004180804653,
      "learning_rate": 1.1477445176416716e-05,
      "loss": 0.5282,
      "step": 5212
    },
    {
      "epoch": 0.46933309324990435,
      "grad_norm": 0.5867604167848542,
      "learning_rate": 1.1474560944479762e-05,
      "loss": 0.5335,
      "step": 5213
    },
    {
      "epoch": 0.46942312453577617,
      "grad_norm": 0.6519970833345763,
      "learning_rate": 1.1471676587145231e-05,
      "loss": 0.5451,
      "step": 5214
    },
    {
      "epoch": 0.46951315582164804,
      "grad_norm": 0.7197936260244602,
      "learning_rate": 1.1468792104658406e-05,
      "loss": 0.5623,
      "step": 5215
    },
    {
      "epoch": 0.46960318710751986,
      "grad_norm": 0.814219994565264,
      "learning_rate": 1.146590749726459e-05,
      "loss": 0.5586,
      "step": 5216
    },
    {
      "epoch": 0.4696932183933917,
      "grad_norm": 0.7239435269944159,
      "learning_rate": 1.1463022765209089e-05,
      "loss": 0.5702,
      "step": 5217
    },
    {
      "epoch": 0.46978324967926355,
      "grad_norm": 0.746036616253305,
      "learning_rate": 1.1460137908737226e-05,
      "loss": 0.5389,
      "step": 5218
    },
    {
      "epoch": 0.46987328096513536,
      "grad_norm": 0.8606870531743938,
      "learning_rate": 1.1457252928094324e-05,
      "loss": 0.5999,
      "step": 5219
    },
    {
      "epoch": 0.46996331225100724,
      "grad_norm": 0.8452389535882449,
      "learning_rate": 1.1454367823525731e-05,
      "loss": 0.5499,
      "step": 5220
    },
    {
      "epoch": 0.47005334353687905,
      "grad_norm": 0.6639541476879267,
      "learning_rate": 1.1451482595276798e-05,
      "loss": 0.6216,
      "step": 5221
    },
    {
      "epoch": 0.4701433748227509,
      "grad_norm": 0.8741844181753905,
      "learning_rate": 1.144859724359288e-05,
      "loss": 0.6684,
      "step": 5222
    },
    {
      "epoch": 0.47023340610862274,
      "grad_norm": 0.7937627404342056,
      "learning_rate": 1.1445711768719354e-05,
      "loss": 0.5817,
      "step": 5223
    },
    {
      "epoch": 0.4703234373944946,
      "grad_norm": 0.730976638585916,
      "learning_rate": 1.14428261709016e-05,
      "loss": 0.4905,
      "step": 5224
    },
    {
      "epoch": 0.47041346868036643,
      "grad_norm": 0.717682410209891,
      "learning_rate": 1.1439940450385016e-05,
      "loss": 0.582,
      "step": 5225
    },
    {
      "epoch": 0.47050349996623825,
      "grad_norm": 0.7317290675130826,
      "learning_rate": 1.1437054607414999e-05,
      "loss": 0.5706,
      "step": 5226
    },
    {
      "epoch": 0.4705935312521101,
      "grad_norm": 0.678839360827599,
      "learning_rate": 1.1434168642236964e-05,
      "loss": 0.5769,
      "step": 5227
    },
    {
      "epoch": 0.47068356253798194,
      "grad_norm": 0.7137907277284024,
      "learning_rate": 1.1431282555096343e-05,
      "loss": 0.573,
      "step": 5228
    },
    {
      "epoch": 0.4707735938238538,
      "grad_norm": 0.9586616648040541,
      "learning_rate": 1.1428396346238562e-05,
      "loss": 0.486,
      "step": 5229
    },
    {
      "epoch": 0.4708636251097256,
      "grad_norm": 0.7205915999977802,
      "learning_rate": 1.1425510015909068e-05,
      "loss": 0.5641,
      "step": 5230
    },
    {
      "epoch": 0.4709536563955975,
      "grad_norm": 0.8086336198767947,
      "learning_rate": 1.1422623564353317e-05,
      "loss": 0.5539,
      "step": 5231
    },
    {
      "epoch": 0.4710436876814693,
      "grad_norm": 0.6719701802569303,
      "learning_rate": 1.1419736991816777e-05,
      "loss": 0.5151,
      "step": 5232
    },
    {
      "epoch": 0.47113371896734113,
      "grad_norm": 0.8071950700485185,
      "learning_rate": 1.1416850298544916e-05,
      "loss": 0.5696,
      "step": 5233
    },
    {
      "epoch": 0.471223750253213,
      "grad_norm": 0.7576954217112993,
      "learning_rate": 1.141396348478323e-05,
      "loss": 0.543,
      "step": 5234
    },
    {
      "epoch": 0.4713137815390848,
      "grad_norm": 0.7827714338882832,
      "learning_rate": 1.1411076550777213e-05,
      "loss": 0.616,
      "step": 5235
    },
    {
      "epoch": 0.4714038128249567,
      "grad_norm": 0.8875724431643748,
      "learning_rate": 1.1408189496772369e-05,
      "loss": 0.5193,
      "step": 5236
    },
    {
      "epoch": 0.4714938441108285,
      "grad_norm": 0.7808373180882636,
      "learning_rate": 1.1405302323014213e-05,
      "loss": 0.5819,
      "step": 5237
    },
    {
      "epoch": 0.4715838753967004,
      "grad_norm": 0.6033709990869549,
      "learning_rate": 1.1402415029748277e-05,
      "loss": 0.4533,
      "step": 5238
    },
    {
      "epoch": 0.4716739066825722,
      "grad_norm": 0.7000339051762917,
      "learning_rate": 1.1399527617220097e-05,
      "loss": 0.5503,
      "step": 5239
    },
    {
      "epoch": 0.471763937968444,
      "grad_norm": 0.6873986971732089,
      "learning_rate": 1.1396640085675217e-05,
      "loss": 0.604,
      "step": 5240
    },
    {
      "epoch": 0.4718539692543159,
      "grad_norm": 0.7148393737730752,
      "learning_rate": 1.1393752435359198e-05,
      "loss": 0.5393,
      "step": 5241
    },
    {
      "epoch": 0.4719440005401877,
      "grad_norm": 0.7362540573751662,
      "learning_rate": 1.1390864666517607e-05,
      "loss": 0.4834,
      "step": 5242
    },
    {
      "epoch": 0.4720340318260596,
      "grad_norm": 0.7346017871487137,
      "learning_rate": 1.1387976779396019e-05,
      "loss": 0.5527,
      "step": 5243
    },
    {
      "epoch": 0.4721240631119314,
      "grad_norm": 0.8467023540541186,
      "learning_rate": 1.1385088774240024e-05,
      "loss": 0.5362,
      "step": 5244
    },
    {
      "epoch": 0.47221409439780326,
      "grad_norm": 0.7964903053243689,
      "learning_rate": 1.1382200651295221e-05,
      "loss": 0.5808,
      "step": 5245
    },
    {
      "epoch": 0.4723041256836751,
      "grad_norm": 0.9757418902686358,
      "learning_rate": 1.1379312410807218e-05,
      "loss": 0.5114,
      "step": 5246
    },
    {
      "epoch": 0.4723941569695469,
      "grad_norm": 0.6953019036920224,
      "learning_rate": 1.1376424053021629e-05,
      "loss": 0.4989,
      "step": 5247
    },
    {
      "epoch": 0.47248418825541877,
      "grad_norm": 0.8142837967238578,
      "learning_rate": 1.1373535578184083e-05,
      "loss": 0.5644,
      "step": 5248
    },
    {
      "epoch": 0.4725742195412906,
      "grad_norm": 0.6952300237214109,
      "learning_rate": 1.137064698654022e-05,
      "loss": 0.6345,
      "step": 5249
    },
    {
      "epoch": 0.47266425082716246,
      "grad_norm": 0.9200419076332909,
      "learning_rate": 1.1367758278335687e-05,
      "loss": 0.6315,
      "step": 5250
    },
    {
      "epoch": 0.4727542821130343,
      "grad_norm": 0.8178776010745502,
      "learning_rate": 1.1364869453816139e-05,
      "loss": 0.5391,
      "step": 5251
    },
    {
      "epoch": 0.47284431339890615,
      "grad_norm": 0.81159624612209,
      "learning_rate": 1.1361980513227243e-05,
      "loss": 0.523,
      "step": 5252
    },
    {
      "epoch": 0.47293434468477796,
      "grad_norm": 0.8381740856593878,
      "learning_rate": 1.1359091456814685e-05,
      "loss": 0.6067,
      "step": 5253
    },
    {
      "epoch": 0.4730243759706498,
      "grad_norm": 0.7454448390569794,
      "learning_rate": 1.1356202284824141e-05,
      "loss": 0.5706,
      "step": 5254
    },
    {
      "epoch": 0.47311440725652165,
      "grad_norm": 0.9532093011639176,
      "learning_rate": 1.1353312997501313e-05,
      "loss": 0.5026,
      "step": 5255
    },
    {
      "epoch": 0.47320443854239347,
      "grad_norm": 0.9280508333603533,
      "learning_rate": 1.1350423595091908e-05,
      "loss": 0.5489,
      "step": 5256
    },
    {
      "epoch": 0.47329446982826534,
      "grad_norm": 0.9474536870602404,
      "learning_rate": 1.1347534077841642e-05,
      "loss": 0.5645,
      "step": 5257
    },
    {
      "epoch": 0.47338450111413716,
      "grad_norm": 0.736611967797623,
      "learning_rate": 1.134464444599624e-05,
      "loss": 0.5292,
      "step": 5258
    },
    {
      "epoch": 0.47347453240000903,
      "grad_norm": 0.671904257175722,
      "learning_rate": 1.134175469980144e-05,
      "loss": 0.5473,
      "step": 5259
    },
    {
      "epoch": 0.47356456368588085,
      "grad_norm": 0.6795861136191672,
      "learning_rate": 1.133886483950299e-05,
      "loss": 0.5232,
      "step": 5260
    },
    {
      "epoch": 0.47365459497175266,
      "grad_norm": 0.6350259989292775,
      "learning_rate": 1.133597486534664e-05,
      "loss": 0.4629,
      "step": 5261
    },
    {
      "epoch": 0.47374462625762453,
      "grad_norm": 0.7054694846835894,
      "learning_rate": 1.1333084777578156e-05,
      "loss": 0.5359,
      "step": 5262
    },
    {
      "epoch": 0.47383465754349635,
      "grad_norm": 0.6405106821635199,
      "learning_rate": 1.1330194576443318e-05,
      "loss": 0.4757,
      "step": 5263
    },
    {
      "epoch": 0.4739246888293682,
      "grad_norm": 0.6576724914831069,
      "learning_rate": 1.1327304262187905e-05,
      "loss": 0.5475,
      "step": 5264
    },
    {
      "epoch": 0.47401472011524004,
      "grad_norm": 0.7603974227270024,
      "learning_rate": 1.1324413835057714e-05,
      "loss": 0.6026,
      "step": 5265
    },
    {
      "epoch": 0.4741047514011119,
      "grad_norm": 0.7197352511539084,
      "learning_rate": 1.1321523295298548e-05,
      "loss": 0.5189,
      "step": 5266
    },
    {
      "epoch": 0.47419478268698373,
      "grad_norm": 1.0316915957301989,
      "learning_rate": 1.1318632643156223e-05,
      "loss": 0.6157,
      "step": 5267
    },
    {
      "epoch": 0.47428481397285555,
      "grad_norm": 0.8197624729131189,
      "learning_rate": 1.1315741878876556e-05,
      "loss": 0.581,
      "step": 5268
    },
    {
      "epoch": 0.4743748452587274,
      "grad_norm": 0.7804974437071441,
      "learning_rate": 1.1312851002705383e-05,
      "loss": 0.5833,
      "step": 5269
    },
    {
      "epoch": 0.47446487654459923,
      "grad_norm": 0.8702467164795386,
      "learning_rate": 1.1309960014888548e-05,
      "loss": 0.5639,
      "step": 5270
    },
    {
      "epoch": 0.4745549078304711,
      "grad_norm": 0.707331605107382,
      "learning_rate": 1.1307068915671898e-05,
      "loss": 0.5218,
      "step": 5271
    },
    {
      "epoch": 0.4746449391163429,
      "grad_norm": 1.0755102743039466,
      "learning_rate": 1.1304177705301296e-05,
      "loss": 0.5361,
      "step": 5272
    },
    {
      "epoch": 0.4747349704022148,
      "grad_norm": 0.6877035989299525,
      "learning_rate": 1.130128638402261e-05,
      "loss": 0.4344,
      "step": 5273
    },
    {
      "epoch": 0.4748250016880866,
      "grad_norm": 0.7986480922734611,
      "learning_rate": 1.1298394952081726e-05,
      "loss": 0.4911,
      "step": 5274
    },
    {
      "epoch": 0.47491503297395843,
      "grad_norm": 0.7348554181968575,
      "learning_rate": 1.1295503409724526e-05,
      "loss": 0.5796,
      "step": 5275
    },
    {
      "epoch": 0.4750050642598303,
      "grad_norm": 0.8609121783599162,
      "learning_rate": 1.1292611757196912e-05,
      "loss": 0.5357,
      "step": 5276
    },
    {
      "epoch": 0.4750950955457021,
      "grad_norm": 0.8480253265029607,
      "learning_rate": 1.1289719994744795e-05,
      "loss": 0.581,
      "step": 5277
    },
    {
      "epoch": 0.475185126831574,
      "grad_norm": 0.7418782032142491,
      "learning_rate": 1.1286828122614084e-05,
      "loss": 0.5965,
      "step": 5278
    },
    {
      "epoch": 0.4752751581174458,
      "grad_norm": 0.7218152086803606,
      "learning_rate": 1.1283936141050714e-05,
      "loss": 0.5463,
      "step": 5279
    },
    {
      "epoch": 0.4753651894033177,
      "grad_norm": 0.9215270635911665,
      "learning_rate": 1.1281044050300614e-05,
      "loss": 0.5502,
      "step": 5280
    },
    {
      "epoch": 0.4754552206891895,
      "grad_norm": 0.6468557300880952,
      "learning_rate": 1.1278151850609736e-05,
      "loss": 0.4098,
      "step": 5281
    },
    {
      "epoch": 0.4755452519750613,
      "grad_norm": 0.7037627179731684,
      "learning_rate": 1.1275259542224028e-05,
      "loss": 0.524,
      "step": 5282
    },
    {
      "epoch": 0.4756352832609332,
      "grad_norm": 0.8558998763248018,
      "learning_rate": 1.1272367125389456e-05,
      "loss": 0.5864,
      "step": 5283
    },
    {
      "epoch": 0.475725314546805,
      "grad_norm": 0.7512289181769333,
      "learning_rate": 1.1269474600351998e-05,
      "loss": 0.5239,
      "step": 5284
    },
    {
      "epoch": 0.4758153458326769,
      "grad_norm": 0.6849304505194812,
      "learning_rate": 1.1266581967357628e-05,
      "loss": 0.4742,
      "step": 5285
    },
    {
      "epoch": 0.4759053771185487,
      "grad_norm": 0.7679848723361552,
      "learning_rate": 1.126368922665234e-05,
      "loss": 0.4791,
      "step": 5286
    },
    {
      "epoch": 0.47599540840442056,
      "grad_norm": 0.7400779405389069,
      "learning_rate": 1.1260796378482139e-05,
      "loss": 0.5284,
      "step": 5287
    },
    {
      "epoch": 0.4760854396902924,
      "grad_norm": 0.8999316149920983,
      "learning_rate": 1.1257903423093031e-05,
      "loss": 0.5436,
      "step": 5288
    },
    {
      "epoch": 0.4761754709761642,
      "grad_norm": 0.7578422636400791,
      "learning_rate": 1.1255010360731033e-05,
      "loss": 0.5499,
      "step": 5289
    },
    {
      "epoch": 0.47626550226203607,
      "grad_norm": 0.7901140398355008,
      "learning_rate": 1.1252117191642175e-05,
      "loss": 0.4666,
      "step": 5290
    },
    {
      "epoch": 0.4763555335479079,
      "grad_norm": 0.9083887483019423,
      "learning_rate": 1.1249223916072496e-05,
      "loss": 0.5052,
      "step": 5291
    },
    {
      "epoch": 0.47644556483377976,
      "grad_norm": 0.7643251419537406,
      "learning_rate": 1.1246330534268035e-05,
      "loss": 0.5023,
      "step": 5292
    },
    {
      "epoch": 0.47653559611965157,
      "grad_norm": 0.8335940450069783,
      "learning_rate": 1.1243437046474854e-05,
      "loss": 0.6347,
      "step": 5293
    },
    {
      "epoch": 0.47662562740552344,
      "grad_norm": 0.7562044636229592,
      "learning_rate": 1.1240543452939013e-05,
      "loss": 0.4979,
      "step": 5294
    },
    {
      "epoch": 0.47671565869139526,
      "grad_norm": 0.714207381437759,
      "learning_rate": 1.123764975390659e-05,
      "loss": 0.5862,
      "step": 5295
    },
    {
      "epoch": 0.4768056899772671,
      "grad_norm": 0.7453692338885317,
      "learning_rate": 1.123475594962366e-05,
      "loss": 0.5421,
      "step": 5296
    },
    {
      "epoch": 0.47689572126313895,
      "grad_norm": 0.8540909541289627,
      "learning_rate": 1.1231862040336316e-05,
      "loss": 0.5552,
      "step": 5297
    },
    {
      "epoch": 0.47698575254901077,
      "grad_norm": 0.9048409612112077,
      "learning_rate": 1.1228968026290666e-05,
      "loss": 0.5457,
      "step": 5298
    },
    {
      "epoch": 0.47707578383488264,
      "grad_norm": 0.8040404118359542,
      "learning_rate": 1.1226073907732806e-05,
      "loss": 0.5507,
      "step": 5299
    },
    {
      "epoch": 0.47716581512075446,
      "grad_norm": 0.8119163254466178,
      "learning_rate": 1.122317968490886e-05,
      "loss": 0.5151,
      "step": 5300
    },
    {
      "epoch": 0.4772558464066263,
      "grad_norm": 0.7132473206346467,
      "learning_rate": 1.1220285358064954e-05,
      "loss": 0.5046,
      "step": 5301
    },
    {
      "epoch": 0.47734587769249814,
      "grad_norm": 0.7892198941478381,
      "learning_rate": 1.1217390927447228e-05,
      "loss": 0.5199,
      "step": 5302
    },
    {
      "epoch": 0.47743590897836996,
      "grad_norm": 0.8890312380470017,
      "learning_rate": 1.1214496393301816e-05,
      "loss": 0.5363,
      "step": 5303
    },
    {
      "epoch": 0.47752594026424183,
      "grad_norm": 0.7573742237051795,
      "learning_rate": 1.1211601755874878e-05,
      "loss": 0.5861,
      "step": 5304
    },
    {
      "epoch": 0.47761597155011365,
      "grad_norm": 0.7255093333050928,
      "learning_rate": 1.1208707015412577e-05,
      "loss": 0.5481,
      "step": 5305
    },
    {
      "epoch": 0.4777060028359855,
      "grad_norm": 0.7832424913271044,
      "learning_rate": 1.1205812172161076e-05,
      "loss": 0.5977,
      "step": 5306
    },
    {
      "epoch": 0.47779603412185734,
      "grad_norm": 0.6765116314399493,
      "learning_rate": 1.1202917226366562e-05,
      "loss": 0.5189,
      "step": 5307
    },
    {
      "epoch": 0.4778860654077292,
      "grad_norm": 0.5904592371759267,
      "learning_rate": 1.120002217827522e-05,
      "loss": 0.4644,
      "step": 5308
    },
    {
      "epoch": 0.477976096693601,
      "grad_norm": 0.7496463781546601,
      "learning_rate": 1.119712702813325e-05,
      "loss": 0.5927,
      "step": 5309
    },
    {
      "epoch": 0.47806612797947284,
      "grad_norm": 0.7052184416192633,
      "learning_rate": 1.119423177618685e-05,
      "loss": 0.4759,
      "step": 5310
    },
    {
      "epoch": 0.4781561592653447,
      "grad_norm": 0.6632877602579517,
      "learning_rate": 1.1191336422682237e-05,
      "loss": 0.5939,
      "step": 5311
    },
    {
      "epoch": 0.47824619055121653,
      "grad_norm": 0.7869588596202649,
      "learning_rate": 1.1188440967865642e-05,
      "loss": 0.535,
      "step": 5312
    },
    {
      "epoch": 0.4783362218370884,
      "grad_norm": 0.8547089613830091,
      "learning_rate": 1.1185545411983283e-05,
      "loss": 0.5862,
      "step": 5313
    },
    {
      "epoch": 0.4784262531229602,
      "grad_norm": 1.5459905803342469,
      "learning_rate": 1.1182649755281404e-05,
      "loss": 0.5988,
      "step": 5314
    },
    {
      "epoch": 0.4785162844088321,
      "grad_norm": 0.8132647520728844,
      "learning_rate": 1.1179753998006259e-05,
      "loss": 0.5951,
      "step": 5315
    },
    {
      "epoch": 0.4786063156947039,
      "grad_norm": 0.6821275090647094,
      "learning_rate": 1.1176858140404103e-05,
      "loss": 0.4994,
      "step": 5316
    },
    {
      "epoch": 0.4786963469805757,
      "grad_norm": 0.6651860908129924,
      "learning_rate": 1.1173962182721198e-05,
      "loss": 0.6254,
      "step": 5317
    },
    {
      "epoch": 0.4787863782664476,
      "grad_norm": 0.7711776895191282,
      "learning_rate": 1.117106612520382e-05,
      "loss": 0.4793,
      "step": 5318
    },
    {
      "epoch": 0.4788764095523194,
      "grad_norm": 0.789162178728671,
      "learning_rate": 1.1168169968098255e-05,
      "loss": 0.5501,
      "step": 5319
    },
    {
      "epoch": 0.4789664408381913,
      "grad_norm": 0.71006168933335,
      "learning_rate": 1.1165273711650786e-05,
      "loss": 0.5419,
      "step": 5320
    },
    {
      "epoch": 0.4790564721240631,
      "grad_norm": 0.9514541995565595,
      "learning_rate": 1.1162377356107717e-05,
      "loss": 0.527,
      "step": 5321
    },
    {
      "epoch": 0.479146503409935,
      "grad_norm": 0.5916615438027901,
      "learning_rate": 1.1159480901715355e-05,
      "loss": 0.4733,
      "step": 5322
    },
    {
      "epoch": 0.4792365346958068,
      "grad_norm": 0.666652342452046,
      "learning_rate": 1.1156584348720022e-05,
      "loss": 0.5484,
      "step": 5323
    },
    {
      "epoch": 0.4793265659816786,
      "grad_norm": 0.9507487752483859,
      "learning_rate": 1.1153687697368037e-05,
      "loss": 0.4684,
      "step": 5324
    },
    {
      "epoch": 0.4794165972675505,
      "grad_norm": 0.7440677984349442,
      "learning_rate": 1.115079094790573e-05,
      "loss": 0.6494,
      "step": 5325
    },
    {
      "epoch": 0.4795066285534223,
      "grad_norm": 0.6701696549401023,
      "learning_rate": 1.114789410057945e-05,
      "loss": 0.5041,
      "step": 5326
    },
    {
      "epoch": 0.47959665983929417,
      "grad_norm": 0.7357500202528965,
      "learning_rate": 1.1144997155635539e-05,
      "loss": 0.5388,
      "step": 5327
    },
    {
      "epoch": 0.479686691125166,
      "grad_norm": 0.8407627032689602,
      "learning_rate": 1.1142100113320362e-05,
      "loss": 0.563,
      "step": 5328
    },
    {
      "epoch": 0.47977672241103786,
      "grad_norm": 0.8337430942672757,
      "learning_rate": 1.113920297388028e-05,
      "loss": 0.623,
      "step": 5329
    },
    {
      "epoch": 0.4798667536969097,
      "grad_norm": 0.6530100154346083,
      "learning_rate": 1.1136305737561673e-05,
      "loss": 0.5449,
      "step": 5330
    },
    {
      "epoch": 0.4799567849827815,
      "grad_norm": 0.857188423775804,
      "learning_rate": 1.1133408404610916e-05,
      "loss": 0.4974,
      "step": 5331
    },
    {
      "epoch": 0.48004681626865336,
      "grad_norm": 0.8398657092596109,
      "learning_rate": 1.1130510975274408e-05,
      "loss": 0.5693,
      "step": 5332
    },
    {
      "epoch": 0.4801368475545252,
      "grad_norm": 0.8167584492984273,
      "learning_rate": 1.1127613449798545e-05,
      "loss": 0.4811,
      "step": 5333
    },
    {
      "epoch": 0.48022687884039705,
      "grad_norm": 0.7276519769675066,
      "learning_rate": 1.1124715828429733e-05,
      "loss": 0.4852,
      "step": 5334
    },
    {
      "epoch": 0.48031691012626887,
      "grad_norm": 0.7194254876654059,
      "learning_rate": 1.1121818111414386e-05,
      "loss": 0.5953,
      "step": 5335
    },
    {
      "epoch": 0.48040694141214074,
      "grad_norm": 0.9569903397817692,
      "learning_rate": 1.1118920298998931e-05,
      "loss": 0.5389,
      "step": 5336
    },
    {
      "epoch": 0.48049697269801256,
      "grad_norm": 0.8064184132987495,
      "learning_rate": 1.1116022391429804e-05,
      "loss": 0.5629,
      "step": 5337
    },
    {
      "epoch": 0.4805870039838844,
      "grad_norm": 0.8094612525786902,
      "learning_rate": 1.1113124388953436e-05,
      "loss": 0.5457,
      "step": 5338
    },
    {
      "epoch": 0.48067703526975625,
      "grad_norm": 0.737559857091864,
      "learning_rate": 1.1110226291816278e-05,
      "loss": 0.5394,
      "step": 5339
    },
    {
      "epoch": 0.48076706655562806,
      "grad_norm": 0.6844210513893434,
      "learning_rate": 1.1107328100264789e-05,
      "loss": 0.4628,
      "step": 5340
    },
    {
      "epoch": 0.48085709784149994,
      "grad_norm": 0.7324346856149269,
      "learning_rate": 1.110442981454543e-05,
      "loss": 0.5055,
      "step": 5341
    },
    {
      "epoch": 0.48094712912737175,
      "grad_norm": 0.6746685910992835,
      "learning_rate": 1.1101531434904673e-05,
      "loss": 0.474,
      "step": 5342
    },
    {
      "epoch": 0.4810371604132436,
      "grad_norm": 0.7986660941072496,
      "learning_rate": 1.1098632961588998e-05,
      "loss": 0.6454,
      "step": 5343
    },
    {
      "epoch": 0.48112719169911544,
      "grad_norm": 0.7962283118521385,
      "learning_rate": 1.1095734394844897e-05,
      "loss": 0.5246,
      "step": 5344
    },
    {
      "epoch": 0.48121722298498726,
      "grad_norm": 0.6942812186781744,
      "learning_rate": 1.1092835734918859e-05,
      "loss": 0.5489,
      "step": 5345
    },
    {
      "epoch": 0.48130725427085913,
      "grad_norm": 0.7917701629770765,
      "learning_rate": 1.1089936982057392e-05,
      "loss": 0.5052,
      "step": 5346
    },
    {
      "epoch": 0.48139728555673095,
      "grad_norm": 0.7179820333903871,
      "learning_rate": 1.1087038136507011e-05,
      "loss": 0.5713,
      "step": 5347
    },
    {
      "epoch": 0.4814873168426028,
      "grad_norm": 0.7819929225685843,
      "learning_rate": 1.1084139198514227e-05,
      "loss": 0.6226,
      "step": 5348
    },
    {
      "epoch": 0.48157734812847464,
      "grad_norm": 0.8448386032574041,
      "learning_rate": 1.1081240168325575e-05,
      "loss": 0.6392,
      "step": 5349
    },
    {
      "epoch": 0.4816673794143465,
      "grad_norm": 0.7194331495969566,
      "learning_rate": 1.1078341046187588e-05,
      "loss": 0.5174,
      "step": 5350
    },
    {
      "epoch": 0.4817574107002183,
      "grad_norm": 0.9522617888225674,
      "learning_rate": 1.107544183234681e-05,
      "loss": 0.554,
      "step": 5351
    },
    {
      "epoch": 0.48184744198609014,
      "grad_norm": 0.6849236202010395,
      "learning_rate": 1.1072542527049788e-05,
      "loss": 0.4843,
      "step": 5352
    },
    {
      "epoch": 0.481937473271962,
      "grad_norm": 0.6956048944291371,
      "learning_rate": 1.1069643130543084e-05,
      "loss": 0.5825,
      "step": 5353
    },
    {
      "epoch": 0.48202750455783383,
      "grad_norm": 0.7510813006948703,
      "learning_rate": 1.1066743643073266e-05,
      "loss": 0.5543,
      "step": 5354
    },
    {
      "epoch": 0.4821175358437057,
      "grad_norm": 2.6364422277592845,
      "learning_rate": 1.1063844064886904e-05,
      "loss": 0.5808,
      "step": 5355
    },
    {
      "epoch": 0.4822075671295775,
      "grad_norm": 0.8720602535741495,
      "learning_rate": 1.1060944396230583e-05,
      "loss": 0.5967,
      "step": 5356
    },
    {
      "epoch": 0.4822975984154494,
      "grad_norm": 0.665229256798501,
      "learning_rate": 1.1058044637350892e-05,
      "loss": 0.4631,
      "step": 5357
    },
    {
      "epoch": 0.4823876297013212,
      "grad_norm": 0.7348393647149197,
      "learning_rate": 1.1055144788494428e-05,
      "loss": 0.5317,
      "step": 5358
    },
    {
      "epoch": 0.482477660987193,
      "grad_norm": 0.7298597068026421,
      "learning_rate": 1.1052244849907793e-05,
      "loss": 0.5552,
      "step": 5359
    },
    {
      "epoch": 0.4825676922730649,
      "grad_norm": 0.8601067609957695,
      "learning_rate": 1.1049344821837606e-05,
      "loss": 0.5377,
      "step": 5360
    },
    {
      "epoch": 0.4826577235589367,
      "grad_norm": 0.9775486040644094,
      "learning_rate": 1.1046444704530486e-05,
      "loss": 0.5075,
      "step": 5361
    },
    {
      "epoch": 0.4827477548448086,
      "grad_norm": 0.8149754365596408,
      "learning_rate": 1.1043544498233054e-05,
      "loss": 0.5086,
      "step": 5362
    },
    {
      "epoch": 0.4828377861306804,
      "grad_norm": 0.7233944153370618,
      "learning_rate": 1.104064420319195e-05,
      "loss": 0.4589,
      "step": 5363
    },
    {
      "epoch": 0.4829278174165523,
      "grad_norm": 0.6076750190685899,
      "learning_rate": 1.1037743819653819e-05,
      "loss": 0.4657,
      "step": 5364
    },
    {
      "epoch": 0.4830178487024241,
      "grad_norm": 0.7442564659704958,
      "learning_rate": 1.1034843347865309e-05,
      "loss": 0.5538,
      "step": 5365
    },
    {
      "epoch": 0.4831078799882959,
      "grad_norm": 0.633209402799364,
      "learning_rate": 1.1031942788073078e-05,
      "loss": 0.5131,
      "step": 5366
    },
    {
      "epoch": 0.4831979112741678,
      "grad_norm": 0.6931648620362691,
      "learning_rate": 1.102904214052379e-05,
      "loss": 0.5512,
      "step": 5367
    },
    {
      "epoch": 0.4832879425600396,
      "grad_norm": 0.708030436705162,
      "learning_rate": 1.1026141405464121e-05,
      "loss": 0.5033,
      "step": 5368
    },
    {
      "epoch": 0.48337797384591147,
      "grad_norm": 0.6754162937751953,
      "learning_rate": 1.1023240583140748e-05,
      "loss": 0.5593,
      "step": 5369
    },
    {
      "epoch": 0.4834680051317833,
      "grad_norm": 0.8550420239884217,
      "learning_rate": 1.1020339673800363e-05,
      "loss": 0.6486,
      "step": 5370
    },
    {
      "epoch": 0.48355803641765516,
      "grad_norm": 0.6996805666199957,
      "learning_rate": 1.1017438677689656e-05,
      "loss": 0.52,
      "step": 5371
    },
    {
      "epoch": 0.483648067703527,
      "grad_norm": 0.8898435320930584,
      "learning_rate": 1.1014537595055335e-05,
      "loss": 0.5581,
      "step": 5372
    },
    {
      "epoch": 0.4837380989893988,
      "grad_norm": 0.7628630207113909,
      "learning_rate": 1.1011636426144104e-05,
      "loss": 0.5811,
      "step": 5373
    },
    {
      "epoch": 0.48382813027527066,
      "grad_norm": 0.7690164829132417,
      "learning_rate": 1.1008735171202685e-05,
      "loss": 0.4819,
      "step": 5374
    },
    {
      "epoch": 0.4839181615611425,
      "grad_norm": 0.9346347267621281,
      "learning_rate": 1.1005833830477801e-05,
      "loss": 0.6168,
      "step": 5375
    },
    {
      "epoch": 0.48400819284701435,
      "grad_norm": 0.7680523303117451,
      "learning_rate": 1.1002932404216183e-05,
      "loss": 0.5128,
      "step": 5376
    },
    {
      "epoch": 0.48409822413288617,
      "grad_norm": 0.8436083579118329,
      "learning_rate": 1.1000030892664569e-05,
      "loss": 0.5937,
      "step": 5377
    },
    {
      "epoch": 0.48418825541875804,
      "grad_norm": 0.7157567354479011,
      "learning_rate": 1.0997129296069705e-05,
      "loss": 0.561,
      "step": 5378
    },
    {
      "epoch": 0.48427828670462986,
      "grad_norm": 0.61313220088427,
      "learning_rate": 1.0994227614678352e-05,
      "loss": 0.4962,
      "step": 5379
    },
    {
      "epoch": 0.4843683179905017,
      "grad_norm": 0.7244299831135702,
      "learning_rate": 1.0991325848737262e-05,
      "loss": 0.485,
      "step": 5380
    },
    {
      "epoch": 0.48445834927637355,
      "grad_norm": 0.6328803203001798,
      "learning_rate": 1.0988423998493207e-05,
      "loss": 0.4768,
      "step": 5381
    },
    {
      "epoch": 0.48454838056224536,
      "grad_norm": 0.8019892811318426,
      "learning_rate": 1.0985522064192963e-05,
      "loss": 0.6042,
      "step": 5382
    },
    {
      "epoch": 0.48463841184811723,
      "grad_norm": 0.8108758583349694,
      "learning_rate": 1.0982620046083307e-05,
      "loss": 0.6143,
      "step": 5383
    },
    {
      "epoch": 0.48472844313398905,
      "grad_norm": 0.7603096470245033,
      "learning_rate": 1.0979717944411035e-05,
      "loss": 0.5874,
      "step": 5384
    },
    {
      "epoch": 0.4848184744198609,
      "grad_norm": 0.6811532521273852,
      "learning_rate": 1.0976815759422941e-05,
      "loss": 0.732,
      "step": 5385
    },
    {
      "epoch": 0.48490850570573274,
      "grad_norm": 0.7390665674698597,
      "learning_rate": 1.097391349136583e-05,
      "loss": 0.526,
      "step": 5386
    },
    {
      "epoch": 0.48499853699160456,
      "grad_norm": 0.6362207092951406,
      "learning_rate": 1.0971011140486505e-05,
      "loss": 0.4624,
      "step": 5387
    },
    {
      "epoch": 0.48508856827747643,
      "grad_norm": 0.6013035461194128,
      "learning_rate": 1.0968108707031792e-05,
      "loss": 0.4883,
      "step": 5388
    },
    {
      "epoch": 0.48517859956334825,
      "grad_norm": 0.8079277743940537,
      "learning_rate": 1.0965206191248516e-05,
      "loss": 0.5339,
      "step": 5389
    },
    {
      "epoch": 0.4852686308492201,
      "grad_norm": 0.7384358685200655,
      "learning_rate": 1.0962303593383505e-05,
      "loss": 0.5081,
      "step": 5390
    },
    {
      "epoch": 0.48535866213509193,
      "grad_norm": 1.1318760087387796,
      "learning_rate": 1.0959400913683597e-05,
      "loss": 0.5311,
      "step": 5391
    },
    {
      "epoch": 0.4854486934209638,
      "grad_norm": 0.6072420200216233,
      "learning_rate": 1.095649815239564e-05,
      "loss": 0.4895,
      "step": 5392
    },
    {
      "epoch": 0.4855387247068356,
      "grad_norm": 0.6738304209464892,
      "learning_rate": 1.0953595309766489e-05,
      "loss": 0.5547,
      "step": 5393
    },
    {
      "epoch": 0.48562875599270744,
      "grad_norm": 0.6812740728282263,
      "learning_rate": 1.0950692386042999e-05,
      "loss": 0.461,
      "step": 5394
    },
    {
      "epoch": 0.4857187872785793,
      "grad_norm": 0.7270031842423027,
      "learning_rate": 1.0947789381472035e-05,
      "loss": 0.5856,
      "step": 5395
    },
    {
      "epoch": 0.48580881856445113,
      "grad_norm": 0.747639093761508,
      "learning_rate": 1.094488629630048e-05,
      "loss": 0.5629,
      "step": 5396
    },
    {
      "epoch": 0.485898849850323,
      "grad_norm": 0.6786900178662116,
      "learning_rate": 1.0941983130775204e-05,
      "loss": 0.4064,
      "step": 5397
    },
    {
      "epoch": 0.4859888811361948,
      "grad_norm": 0.6004726157927124,
      "learning_rate": 1.0939079885143095e-05,
      "loss": 0.5907,
      "step": 5398
    },
    {
      "epoch": 0.4860789124220667,
      "grad_norm": 1.1093211496807855,
      "learning_rate": 1.0936176559651053e-05,
      "loss": 0.5284,
      "step": 5399
    },
    {
      "epoch": 0.4861689437079385,
      "grad_norm": 0.7020035489595047,
      "learning_rate": 1.0933273154545976e-05,
      "loss": 0.5451,
      "step": 5400
    },
    {
      "epoch": 0.4862589749938103,
      "grad_norm": 0.7391716640742294,
      "learning_rate": 1.0930369670074767e-05,
      "loss": 0.4756,
      "step": 5401
    },
    {
      "epoch": 0.4863490062796822,
      "grad_norm": 0.6450055279150657,
      "learning_rate": 1.0927466106484346e-05,
      "loss": 0.5942,
      "step": 5402
    },
    {
      "epoch": 0.486439037565554,
      "grad_norm": 0.6549564454287461,
      "learning_rate": 1.0924562464021633e-05,
      "loss": 0.5664,
      "step": 5403
    },
    {
      "epoch": 0.4865290688514259,
      "grad_norm": 0.7676098608029216,
      "learning_rate": 1.092165874293355e-05,
      "loss": 0.5578,
      "step": 5404
    },
    {
      "epoch": 0.4866191001372977,
      "grad_norm": 0.7517300113906552,
      "learning_rate": 1.0918754943467035e-05,
      "loss": 0.582,
      "step": 5405
    },
    {
      "epoch": 0.48670913142316957,
      "grad_norm": 0.7333485504309888,
      "learning_rate": 1.0915851065869032e-05,
      "loss": 0.5706,
      "step": 5406
    },
    {
      "epoch": 0.4867991627090414,
      "grad_norm": 0.7159579363206257,
      "learning_rate": 1.0912947110386484e-05,
      "loss": 0.5713,
      "step": 5407
    },
    {
      "epoch": 0.4868891939949132,
      "grad_norm": 0.6755927460649328,
      "learning_rate": 1.0910043077266349e-05,
      "loss": 0.4584,
      "step": 5408
    },
    {
      "epoch": 0.4869792252807851,
      "grad_norm": 0.848397028953914,
      "learning_rate": 1.0907138966755584e-05,
      "loss": 0.5453,
      "step": 5409
    },
    {
      "epoch": 0.4870692565666569,
      "grad_norm": 0.7464967758351625,
      "learning_rate": 1.090423477910116e-05,
      "loss": 0.5208,
      "step": 5410
    },
    {
      "epoch": 0.48715928785252877,
      "grad_norm": 0.9594693127269339,
      "learning_rate": 1.0901330514550046e-05,
      "loss": 0.6,
      "step": 5411
    },
    {
      "epoch": 0.4872493191384006,
      "grad_norm": 0.6531069481697067,
      "learning_rate": 1.0898426173349226e-05,
      "loss": 0.5609,
      "step": 5412
    },
    {
      "epoch": 0.48733935042427245,
      "grad_norm": 0.5983316710877575,
      "learning_rate": 1.0895521755745686e-05,
      "loss": 0.5546,
      "step": 5413
    },
    {
      "epoch": 0.48742938171014427,
      "grad_norm": 0.8246754401655628,
      "learning_rate": 1.089261726198642e-05,
      "loss": 0.5652,
      "step": 5414
    },
    {
      "epoch": 0.4875194129960161,
      "grad_norm": 0.7560904133154619,
      "learning_rate": 1.0889712692318428e-05,
      "loss": 0.6095,
      "step": 5415
    },
    {
      "epoch": 0.48760944428188796,
      "grad_norm": 0.7700469683439388,
      "learning_rate": 1.0886808046988716e-05,
      "loss": 0.5158,
      "step": 5416
    },
    {
      "epoch": 0.4876994755677598,
      "grad_norm": 0.7442843851726302,
      "learning_rate": 1.08839033262443e-05,
      "loss": 0.5601,
      "step": 5417
    },
    {
      "epoch": 0.48778950685363165,
      "grad_norm": 0.6235040790455366,
      "learning_rate": 1.0880998530332191e-05,
      "loss": 0.5814,
      "step": 5418
    },
    {
      "epoch": 0.48787953813950347,
      "grad_norm": 0.5736312873472545,
      "learning_rate": 1.0878093659499425e-05,
      "loss": 0.4998,
      "step": 5419
    },
    {
      "epoch": 0.48796956942537534,
      "grad_norm": 0.6532296755407143,
      "learning_rate": 1.0875188713993026e-05,
      "loss": 0.5018,
      "step": 5420
    },
    {
      "epoch": 0.48805960071124715,
      "grad_norm": 0.7439075759303224,
      "learning_rate": 1.0872283694060038e-05,
      "loss": 0.57,
      "step": 5421
    },
    {
      "epoch": 0.48814963199711897,
      "grad_norm": 0.8126100862165051,
      "learning_rate": 1.08693785999475e-05,
      "loss": 0.5381,
      "step": 5422
    },
    {
      "epoch": 0.48823966328299084,
      "grad_norm": 0.8869472908218738,
      "learning_rate": 1.0866473431902465e-05,
      "loss": 0.5911,
      "step": 5423
    },
    {
      "epoch": 0.48832969456886266,
      "grad_norm": 0.6794047821772992,
      "learning_rate": 1.0863568190171996e-05,
      "loss": 0.5185,
      "step": 5424
    },
    {
      "epoch": 0.48841972585473453,
      "grad_norm": 0.6640600457906514,
      "learning_rate": 1.0860662875003148e-05,
      "loss": 0.5765,
      "step": 5425
    },
    {
      "epoch": 0.48850975714060635,
      "grad_norm": 0.8699559134085394,
      "learning_rate": 1.0857757486642995e-05,
      "loss": 0.5457,
      "step": 5426
    },
    {
      "epoch": 0.4885997884264782,
      "grad_norm": 0.6987088775397797,
      "learning_rate": 1.0854852025338613e-05,
      "loss": 0.6114,
      "step": 5427
    },
    {
      "epoch": 0.48868981971235004,
      "grad_norm": 0.6902146251484843,
      "learning_rate": 1.0851946491337087e-05,
      "loss": 0.5516,
      "step": 5428
    },
    {
      "epoch": 0.4887798509982219,
      "grad_norm": 1.1493122600190453,
      "learning_rate": 1.0849040884885498e-05,
      "loss": 0.5819,
      "step": 5429
    },
    {
      "epoch": 0.4888698822840937,
      "grad_norm": 0.747440374736483,
      "learning_rate": 1.0846135206230947e-05,
      "loss": 0.5274,
      "step": 5430
    },
    {
      "epoch": 0.48895991356996554,
      "grad_norm": 0.6330411795993439,
      "learning_rate": 1.0843229455620534e-05,
      "loss": 0.4829,
      "step": 5431
    },
    {
      "epoch": 0.4890499448558374,
      "grad_norm": 0.7520060552063996,
      "learning_rate": 1.0840323633301364e-05,
      "loss": 0.5538,
      "step": 5432
    },
    {
      "epoch": 0.48913997614170923,
      "grad_norm": 0.9361435703695947,
      "learning_rate": 1.0837417739520548e-05,
      "loss": 0.5464,
      "step": 5433
    },
    {
      "epoch": 0.4892300074275811,
      "grad_norm": 0.7088044266148793,
      "learning_rate": 1.0834511774525207e-05,
      "loss": 0.649,
      "step": 5434
    },
    {
      "epoch": 0.4893200387134529,
      "grad_norm": 0.7295873458739061,
      "learning_rate": 1.0831605738562472e-05,
      "loss": 0.5716,
      "step": 5435
    },
    {
      "epoch": 0.4894100699993248,
      "grad_norm": 0.8561571075783211,
      "learning_rate": 1.0828699631879464e-05,
      "loss": 0.4836,
      "step": 5436
    },
    {
      "epoch": 0.4895001012851966,
      "grad_norm": 0.7931371266412763,
      "learning_rate": 1.0825793454723325e-05,
      "loss": 0.6474,
      "step": 5437
    },
    {
      "epoch": 0.4895901325710684,
      "grad_norm": 0.8158093549558117,
      "learning_rate": 1.08228872073412e-05,
      "loss": 0.5588,
      "step": 5438
    },
    {
      "epoch": 0.4896801638569403,
      "grad_norm": 0.9582009120197994,
      "learning_rate": 1.0819980889980235e-05,
      "loss": 0.5496,
      "step": 5439
    },
    {
      "epoch": 0.4897701951428121,
      "grad_norm": 0.8110515154876544,
      "learning_rate": 1.0817074502887583e-05,
      "loss": 0.4852,
      "step": 5440
    },
    {
      "epoch": 0.489860226428684,
      "grad_norm": 0.7962475756676497,
      "learning_rate": 1.0814168046310411e-05,
      "loss": 0.6157,
      "step": 5441
    },
    {
      "epoch": 0.4899502577145558,
      "grad_norm": 0.7729884096647981,
      "learning_rate": 1.0811261520495884e-05,
      "loss": 0.5516,
      "step": 5442
    },
    {
      "epoch": 0.4900402890004277,
      "grad_norm": 0.6698129118672277,
      "learning_rate": 1.0808354925691172e-05,
      "loss": 0.4903,
      "step": 5443
    },
    {
      "epoch": 0.4901303202862995,
      "grad_norm": 0.8261844688159072,
      "learning_rate": 1.0805448262143453e-05,
      "loss": 0.4481,
      "step": 5444
    },
    {
      "epoch": 0.4902203515721713,
      "grad_norm": 0.681019762041484,
      "learning_rate": 1.0802541530099918e-05,
      "loss": 0.4667,
      "step": 5445
    },
    {
      "epoch": 0.4903103828580432,
      "grad_norm": 0.9703592828752234,
      "learning_rate": 1.0799634729807748e-05,
      "loss": 0.5669,
      "step": 5446
    },
    {
      "epoch": 0.490400414143915,
      "grad_norm": 0.7140455497907299,
      "learning_rate": 1.0796727861514146e-05,
      "loss": 0.6363,
      "step": 5447
    },
    {
      "epoch": 0.49049044542978687,
      "grad_norm": 0.649318807218858,
      "learning_rate": 1.0793820925466314e-05,
      "loss": 0.5824,
      "step": 5448
    },
    {
      "epoch": 0.4905804767156587,
      "grad_norm": 0.5445601840258935,
      "learning_rate": 1.0790913921911457e-05,
      "loss": 0.5344,
      "step": 5449
    },
    {
      "epoch": 0.49067050800153056,
      "grad_norm": 0.7342083560604663,
      "learning_rate": 1.0788006851096788e-05,
      "loss": 0.5396,
      "step": 5450
    },
    {
      "epoch": 0.4907605392874024,
      "grad_norm": 0.7165537017668704,
      "learning_rate": 1.0785099713269526e-05,
      "loss": 0.5466,
      "step": 5451
    },
    {
      "epoch": 0.4908505705732742,
      "grad_norm": 0.6734105021756728,
      "learning_rate": 1.0782192508676898e-05,
      "loss": 0.6371,
      "step": 5452
    },
    {
      "epoch": 0.49094060185914606,
      "grad_norm": 0.6899571003688835,
      "learning_rate": 1.0779285237566136e-05,
      "loss": 0.4791,
      "step": 5453
    },
    {
      "epoch": 0.4910306331450179,
      "grad_norm": 0.8338751932088594,
      "learning_rate": 1.0776377900184469e-05,
      "loss": 0.5833,
      "step": 5454
    },
    {
      "epoch": 0.49112066443088975,
      "grad_norm": 0.7129037748676977,
      "learning_rate": 1.0773470496779144e-05,
      "loss": 0.4664,
      "step": 5455
    },
    {
      "epoch": 0.49121069571676157,
      "grad_norm": 0.689482877502368,
      "learning_rate": 1.0770563027597411e-05,
      "loss": 0.6112,
      "step": 5456
    },
    {
      "epoch": 0.49130072700263344,
      "grad_norm": 0.7102235794095811,
      "learning_rate": 1.0767655492886518e-05,
      "loss": 0.4775,
      "step": 5457
    },
    {
      "epoch": 0.49139075828850526,
      "grad_norm": 0.7894924967131384,
      "learning_rate": 1.0764747892893724e-05,
      "loss": 0.5237,
      "step": 5458
    },
    {
      "epoch": 0.4914807895743771,
      "grad_norm": 0.9735981564902705,
      "learning_rate": 1.0761840227866297e-05,
      "loss": 0.5704,
      "step": 5459
    },
    {
      "epoch": 0.49157082086024895,
      "grad_norm": 0.7525022534744108,
      "learning_rate": 1.0758932498051499e-05,
      "loss": 0.4732,
      "step": 5460
    },
    {
      "epoch": 0.49166085214612076,
      "grad_norm": 0.6519636291028194,
      "learning_rate": 1.0756024703696613e-05,
      "loss": 0.5453,
      "step": 5461
    },
    {
      "epoch": 0.49175088343199264,
      "grad_norm": 0.6845754999603717,
      "learning_rate": 1.0753116845048916e-05,
      "loss": 0.6036,
      "step": 5462
    },
    {
      "epoch": 0.49184091471786445,
      "grad_norm": 0.6581823767626254,
      "learning_rate": 1.0750208922355699e-05,
      "loss": 0.5115,
      "step": 5463
    },
    {
      "epoch": 0.4919309460037363,
      "grad_norm": 0.9514843778611355,
      "learning_rate": 1.0747300935864245e-05,
      "loss": 0.4229,
      "step": 5464
    },
    {
      "epoch": 0.49202097728960814,
      "grad_norm": 0.9237957351280359,
      "learning_rate": 1.0744392885821855e-05,
      "loss": 0.6068,
      "step": 5465
    },
    {
      "epoch": 0.49211100857547996,
      "grad_norm": 0.7013275180259761,
      "learning_rate": 1.0741484772475837e-05,
      "loss": 0.6133,
      "step": 5466
    },
    {
      "epoch": 0.49220103986135183,
      "grad_norm": 0.9431924957562458,
      "learning_rate": 1.0738576596073487e-05,
      "loss": 0.5931,
      "step": 5467
    },
    {
      "epoch": 0.49229107114722365,
      "grad_norm": 0.7937436044442493,
      "learning_rate": 1.0735668356862128e-05,
      "loss": 0.5977,
      "step": 5468
    },
    {
      "epoch": 0.4923811024330955,
      "grad_norm": 0.6564823488074278,
      "learning_rate": 1.0732760055089074e-05,
      "loss": 0.5912,
      "step": 5469
    },
    {
      "epoch": 0.49247113371896734,
      "grad_norm": 0.6954687226110224,
      "learning_rate": 1.0729851691001651e-05,
      "loss": 0.555,
      "step": 5470
    },
    {
      "epoch": 0.4925611650048392,
      "grad_norm": 0.6367652181012031,
      "learning_rate": 1.0726943264847187e-05,
      "loss": 0.553,
      "step": 5471
    },
    {
      "epoch": 0.492651196290711,
      "grad_norm": 0.6895166883551438,
      "learning_rate": 1.0724034776873016e-05,
      "loss": 0.5514,
      "step": 5472
    },
    {
      "epoch": 0.49274122757658284,
      "grad_norm": 0.8058909261200254,
      "learning_rate": 1.072112622732648e-05,
      "loss": 0.5474,
      "step": 5473
    },
    {
      "epoch": 0.4928312588624547,
      "grad_norm": 0.6639320322122623,
      "learning_rate": 1.0718217616454917e-05,
      "loss": 0.442,
      "step": 5474
    },
    {
      "epoch": 0.49292129014832653,
      "grad_norm": 0.903513398766352,
      "learning_rate": 1.0715308944505685e-05,
      "loss": 0.5711,
      "step": 5475
    },
    {
      "epoch": 0.4930113214341984,
      "grad_norm": 0.8540586203215678,
      "learning_rate": 1.0712400211726135e-05,
      "loss": 0.5411,
      "step": 5476
    },
    {
      "epoch": 0.4931013527200702,
      "grad_norm": 1.066785438124513,
      "learning_rate": 1.0709491418363632e-05,
      "loss": 0.4895,
      "step": 5477
    },
    {
      "epoch": 0.4931913840059421,
      "grad_norm": 0.8460869515796603,
      "learning_rate": 1.0706582564665535e-05,
      "loss": 0.4968,
      "step": 5478
    },
    {
      "epoch": 0.4932814152918139,
      "grad_norm": 0.887600043739723,
      "learning_rate": 1.0703673650879219e-05,
      "loss": 0.6197,
      "step": 5479
    },
    {
      "epoch": 0.4933714465776857,
      "grad_norm": 1.0635204678240984,
      "learning_rate": 1.0700764677252058e-05,
      "loss": 0.669,
      "step": 5480
    },
    {
      "epoch": 0.4934614778635576,
      "grad_norm": 0.7759345087799508,
      "learning_rate": 1.0697855644031434e-05,
      "loss": 0.5664,
      "step": 5481
    },
    {
      "epoch": 0.4935515091494294,
      "grad_norm": 0.7497406570280828,
      "learning_rate": 1.0694946551464733e-05,
      "loss": 0.5665,
      "step": 5482
    },
    {
      "epoch": 0.4936415404353013,
      "grad_norm": 0.835535477763556,
      "learning_rate": 1.0692037399799344e-05,
      "loss": 0.5602,
      "step": 5483
    },
    {
      "epoch": 0.4937315717211731,
      "grad_norm": 0.7637003065722899,
      "learning_rate": 1.0689128189282669e-05,
      "loss": 0.6044,
      "step": 5484
    },
    {
      "epoch": 0.493821603007045,
      "grad_norm": 1.1261444689305096,
      "learning_rate": 1.0686218920162101e-05,
      "loss": 0.6292,
      "step": 5485
    },
    {
      "epoch": 0.4939116342929168,
      "grad_norm": 0.8265282703824454,
      "learning_rate": 1.0683309592685051e-05,
      "loss": 0.5615,
      "step": 5486
    },
    {
      "epoch": 0.4940016655787886,
      "grad_norm": 0.8451323228929989,
      "learning_rate": 1.068040020709893e-05,
      "loss": 0.5263,
      "step": 5487
    },
    {
      "epoch": 0.4940916968646605,
      "grad_norm": 0.7838467050531108,
      "learning_rate": 1.0677490763651153e-05,
      "loss": 0.4659,
      "step": 5488
    },
    {
      "epoch": 0.4941817281505323,
      "grad_norm": 0.7034106811203172,
      "learning_rate": 1.0674581262589139e-05,
      "loss": 0.548,
      "step": 5489
    },
    {
      "epoch": 0.49427175943640417,
      "grad_norm": 0.7509383251081574,
      "learning_rate": 1.0671671704160316e-05,
      "loss": 0.4864,
      "step": 5490
    },
    {
      "epoch": 0.494361790722276,
      "grad_norm": 0.7068594240940202,
      "learning_rate": 1.0668762088612114e-05,
      "loss": 0.593,
      "step": 5491
    },
    {
      "epoch": 0.49445182200814786,
      "grad_norm": 0.9612574539635704,
      "learning_rate": 1.066585241619197e-05,
      "loss": 0.5955,
      "step": 5492
    },
    {
      "epoch": 0.4945418532940197,
      "grad_norm": 0.6659760672835799,
      "learning_rate": 1.0662942687147322e-05,
      "loss": 0.5487,
      "step": 5493
    },
    {
      "epoch": 0.4946318845798915,
      "grad_norm": 0.7687397703355086,
      "learning_rate": 1.066003290172562e-05,
      "loss": 0.4738,
      "step": 5494
    },
    {
      "epoch": 0.49472191586576336,
      "grad_norm": 0.706095059254106,
      "learning_rate": 1.0657123060174304e-05,
      "loss": 0.5194,
      "step": 5495
    },
    {
      "epoch": 0.4948119471516352,
      "grad_norm": 0.8255095919073602,
      "learning_rate": 1.065421316274084e-05,
      "loss": 0.6087,
      "step": 5496
    },
    {
      "epoch": 0.49490197843750705,
      "grad_norm": 0.809905691016346,
      "learning_rate": 1.0651303209672676e-05,
      "loss": 0.5084,
      "step": 5497
    },
    {
      "epoch": 0.49499200972337887,
      "grad_norm": 0.7499836504588483,
      "learning_rate": 1.0648393201217288e-05,
      "loss": 0.4947,
      "step": 5498
    },
    {
      "epoch": 0.49508204100925074,
      "grad_norm": 0.6891662130202691,
      "learning_rate": 1.0645483137622139e-05,
      "loss": 0.4695,
      "step": 5499
    },
    {
      "epoch": 0.49517207229512256,
      "grad_norm": 0.7085438864572358,
      "learning_rate": 1.0642573019134703e-05,
      "loss": 0.4729,
      "step": 5500
    },
    {
      "epoch": 0.4952621035809944,
      "grad_norm": 0.7742252567857291,
      "learning_rate": 1.0639662846002459e-05,
      "loss": 0.6103,
      "step": 5501
    },
    {
      "epoch": 0.49535213486686624,
      "grad_norm": 0.6869543236458526,
      "learning_rate": 1.0636752618472887e-05,
      "loss": 0.4758,
      "step": 5502
    },
    {
      "epoch": 0.49544216615273806,
      "grad_norm": 0.7194760747411307,
      "learning_rate": 1.063384233679348e-05,
      "loss": 0.448,
      "step": 5503
    },
    {
      "epoch": 0.49553219743860993,
      "grad_norm": 0.7221759346159691,
      "learning_rate": 1.0630932001211722e-05,
      "loss": 0.5652,
      "step": 5504
    },
    {
      "epoch": 0.49562222872448175,
      "grad_norm": 0.729912216825652,
      "learning_rate": 1.062802161197512e-05,
      "loss": 0.5287,
      "step": 5505
    },
    {
      "epoch": 0.4957122600103536,
      "grad_norm": 0.7558865050226435,
      "learning_rate": 1.0625111169331167e-05,
      "loss": 0.5137,
      "step": 5506
    },
    {
      "epoch": 0.49580229129622544,
      "grad_norm": 0.701234910314903,
      "learning_rate": 1.0622200673527374e-05,
      "loss": 0.5163,
      "step": 5507
    },
    {
      "epoch": 0.49589232258209726,
      "grad_norm": 0.6904525956582857,
      "learning_rate": 1.061929012481125e-05,
      "loss": 0.5459,
      "step": 5508
    },
    {
      "epoch": 0.49598235386796913,
      "grad_norm": 0.6762664359973831,
      "learning_rate": 1.061637952343031e-05,
      "loss": 0.3948,
      "step": 5509
    },
    {
      "epoch": 0.49607238515384094,
      "grad_norm": 0.8617740922706968,
      "learning_rate": 1.061346886963207e-05,
      "loss": 0.5721,
      "step": 5510
    },
    {
      "epoch": 0.4961624164397128,
      "grad_norm": 1.2589053237245225,
      "learning_rate": 1.0610558163664055e-05,
      "loss": 0.5388,
      "step": 5511
    },
    {
      "epoch": 0.49625244772558463,
      "grad_norm": 0.748715441675668,
      "learning_rate": 1.0607647405773798e-05,
      "loss": 0.5138,
      "step": 5512
    },
    {
      "epoch": 0.4963424790114565,
      "grad_norm": 1.1014096747675548,
      "learning_rate": 1.0604736596208826e-05,
      "loss": 0.4797,
      "step": 5513
    },
    {
      "epoch": 0.4964325102973283,
      "grad_norm": 0.8230613777682374,
      "learning_rate": 1.060182573521668e-05,
      "loss": 0.5211,
      "step": 5514
    },
    {
      "epoch": 0.49652254158320014,
      "grad_norm": 0.6952341771155317,
      "learning_rate": 1.0598914823044899e-05,
      "loss": 0.5419,
      "step": 5515
    },
    {
      "epoch": 0.496612572869072,
      "grad_norm": 0.8113583058990895,
      "learning_rate": 1.0596003859941031e-05,
      "loss": 0.5649,
      "step": 5516
    },
    {
      "epoch": 0.4967026041549438,
      "grad_norm": 0.789882340754033,
      "learning_rate": 1.0593092846152621e-05,
      "loss": 0.5369,
      "step": 5517
    },
    {
      "epoch": 0.4967926354408157,
      "grad_norm": 0.6544335736864915,
      "learning_rate": 1.0590181781927229e-05,
      "loss": 0.5832,
      "step": 5518
    },
    {
      "epoch": 0.4968826667266875,
      "grad_norm": 0.6869792026406654,
      "learning_rate": 1.0587270667512414e-05,
      "loss": 0.5097,
      "step": 5519
    },
    {
      "epoch": 0.4969726980125594,
      "grad_norm": 0.8389425872396524,
      "learning_rate": 1.0584359503155734e-05,
      "loss": 0.5203,
      "step": 5520
    },
    {
      "epoch": 0.4970627292984312,
      "grad_norm": 0.6729165788130513,
      "learning_rate": 1.0581448289104759e-05,
      "loss": 0.533,
      "step": 5521
    },
    {
      "epoch": 0.497152760584303,
      "grad_norm": 1.1237270909025152,
      "learning_rate": 1.0578537025607063e-05,
      "loss": 0.4294,
      "step": 5522
    },
    {
      "epoch": 0.4972427918701749,
      "grad_norm": 0.7764755087985911,
      "learning_rate": 1.0575625712910217e-05,
      "loss": 0.6167,
      "step": 5523
    },
    {
      "epoch": 0.4973328231560467,
      "grad_norm": 0.7113081343923909,
      "learning_rate": 1.0572714351261803e-05,
      "loss": 0.5353,
      "step": 5524
    },
    {
      "epoch": 0.4974228544419186,
      "grad_norm": 0.9321023048310744,
      "learning_rate": 1.0569802940909406e-05,
      "loss": 0.5772,
      "step": 5525
    },
    {
      "epoch": 0.4975128857277904,
      "grad_norm": 0.6064097459366069,
      "learning_rate": 1.0566891482100614e-05,
      "loss": 0.5174,
      "step": 5526
    },
    {
      "epoch": 0.49760291701366227,
      "grad_norm": 0.6844735121769261,
      "learning_rate": 1.0563979975083016e-05,
      "loss": 0.584,
      "step": 5527
    },
    {
      "epoch": 0.4976929482995341,
      "grad_norm": 0.8100443073355756,
      "learning_rate": 1.056106842010421e-05,
      "loss": 0.5924,
      "step": 5528
    },
    {
      "epoch": 0.4977829795854059,
      "grad_norm": 0.853155337808918,
      "learning_rate": 1.0558156817411802e-05,
      "loss": 0.5425,
      "step": 5529
    },
    {
      "epoch": 0.4978730108712778,
      "grad_norm": 0.7576363636617259,
      "learning_rate": 1.0555245167253388e-05,
      "loss": 0.5531,
      "step": 5530
    },
    {
      "epoch": 0.4979630421571496,
      "grad_norm": 0.9461576265604374,
      "learning_rate": 1.0552333469876578e-05,
      "loss": 0.5848,
      "step": 5531
    },
    {
      "epoch": 0.49805307344302147,
      "grad_norm": 0.8318689836092079,
      "learning_rate": 1.054942172552899e-05,
      "loss": 0.6244,
      "step": 5532
    },
    {
      "epoch": 0.4981431047288933,
      "grad_norm": 1.0181159332175662,
      "learning_rate": 1.0546509934458238e-05,
      "loss": 0.5701,
      "step": 5533
    },
    {
      "epoch": 0.49823313601476515,
      "grad_norm": 0.6549435517697191,
      "learning_rate": 1.054359809691194e-05,
      "loss": 0.4655,
      "step": 5534
    },
    {
      "epoch": 0.49832316730063697,
      "grad_norm": 0.5919231526676065,
      "learning_rate": 1.0540686213137724e-05,
      "loss": 0.5587,
      "step": 5535
    },
    {
      "epoch": 0.4984131985865088,
      "grad_norm": 0.7936940881036207,
      "learning_rate": 1.0537774283383216e-05,
      "loss": 0.5113,
      "step": 5536
    },
    {
      "epoch": 0.49850322987238066,
      "grad_norm": 0.71676622562015,
      "learning_rate": 1.0534862307896049e-05,
      "loss": 0.5122,
      "step": 5537
    },
    {
      "epoch": 0.4985932611582525,
      "grad_norm": 0.7475063833580293,
      "learning_rate": 1.0531950286923856e-05,
      "loss": 0.5148,
      "step": 5538
    },
    {
      "epoch": 0.49868329244412435,
      "grad_norm": 1.0101931995800761,
      "learning_rate": 1.0529038220714283e-05,
      "loss": 0.5868,
      "step": 5539
    },
    {
      "epoch": 0.49877332372999617,
      "grad_norm": 0.7207214732749287,
      "learning_rate": 1.0526126109514972e-05,
      "loss": 0.5591,
      "step": 5540
    },
    {
      "epoch": 0.49886335501586804,
      "grad_norm": 0.6963776693445216,
      "learning_rate": 1.0523213953573568e-05,
      "loss": 0.5566,
      "step": 5541
    },
    {
      "epoch": 0.49895338630173985,
      "grad_norm": 0.7313907953857834,
      "learning_rate": 1.0520301753137725e-05,
      "loss": 0.4865,
      "step": 5542
    },
    {
      "epoch": 0.49904341758761167,
      "grad_norm": 0.6912253774728983,
      "learning_rate": 1.05173895084551e-05,
      "loss": 0.4321,
      "step": 5543
    },
    {
      "epoch": 0.49913344887348354,
      "grad_norm": 0.7375973954427402,
      "learning_rate": 1.051447721977335e-05,
      "loss": 0.5445,
      "step": 5544
    },
    {
      "epoch": 0.49922348015935536,
      "grad_norm": 0.6138616498741379,
      "learning_rate": 1.0511564887340136e-05,
      "loss": 0.5791,
      "step": 5545
    },
    {
      "epoch": 0.49931351144522723,
      "grad_norm": 0.6174663302018981,
      "learning_rate": 1.0508652511403123e-05,
      "loss": 0.5085,
      "step": 5546
    },
    {
      "epoch": 0.49940354273109905,
      "grad_norm": 0.8017162542079163,
      "learning_rate": 1.050574009220999e-05,
      "loss": 0.5124,
      "step": 5547
    },
    {
      "epoch": 0.4994935740169709,
      "grad_norm": 0.7550690551487119,
      "learning_rate": 1.0502827630008403e-05,
      "loss": 0.5151,
      "step": 5548
    },
    {
      "epoch": 0.49958360530284274,
      "grad_norm": 0.7290847112379123,
      "learning_rate": 1.0499915125046043e-05,
      "loss": 0.6097,
      "step": 5549
    },
    {
      "epoch": 0.49967363658871455,
      "grad_norm": 0.7261671378685332,
      "learning_rate": 1.0497002577570593e-05,
      "loss": 0.5108,
      "step": 5550
    },
    {
      "epoch": 0.4997636678745864,
      "grad_norm": 0.9689925283172666,
      "learning_rate": 1.049408998782973e-05,
      "loss": 0.5003,
      "step": 5551
    },
    {
      "epoch": 0.49985369916045824,
      "grad_norm": 0.7689904509836125,
      "learning_rate": 1.0491177356071149e-05,
      "loss": 0.5853,
      "step": 5552
    },
    {
      "epoch": 0.4999437304463301,
      "grad_norm": 0.6851535618371409,
      "learning_rate": 1.0488264682542541e-05,
      "loss": 0.5043,
      "step": 5553
    },
    {
      "epoch": 0.5000337617322019,
      "grad_norm": 0.6780352938298038,
      "learning_rate": 1.0485351967491604e-05,
      "loss": 0.4737,
      "step": 5554
    },
    {
      "epoch": 0.5001237930180737,
      "grad_norm": 0.8785534647135983,
      "learning_rate": 1.0482439211166032e-05,
      "loss": 0.618,
      "step": 5555
    },
    {
      "epoch": 0.5002138243039457,
      "grad_norm": 0.8306838451854802,
      "learning_rate": 1.047952641381353e-05,
      "loss": 0.6026,
      "step": 5556
    },
    {
      "epoch": 0.5003038555898175,
      "grad_norm": 0.6861029769009871,
      "learning_rate": 1.0476613575681805e-05,
      "loss": 0.5638,
      "step": 5557
    },
    {
      "epoch": 0.5003938868756893,
      "grad_norm": 0.8197406414559725,
      "learning_rate": 1.0473700697018563e-05,
      "loss": 0.5264,
      "step": 5558
    },
    {
      "epoch": 0.5004839181615611,
      "grad_norm": 0.6719188937917416,
      "learning_rate": 1.047078777807152e-05,
      "loss": 0.5388,
      "step": 5559
    },
    {
      "epoch": 0.5005739494474329,
      "grad_norm": 0.7622313339500686,
      "learning_rate": 1.0467874819088394e-05,
      "loss": 0.5444,
      "step": 5560
    },
    {
      "epoch": 0.5006639807333049,
      "grad_norm": 0.7738994795296584,
      "learning_rate": 1.0464961820316904e-05,
      "loss": 0.5999,
      "step": 5561
    },
    {
      "epoch": 0.5007540120191767,
      "grad_norm": 0.6469537435491098,
      "learning_rate": 1.046204878200477e-05,
      "loss": 0.5444,
      "step": 5562
    },
    {
      "epoch": 0.5008440433050485,
      "grad_norm": 0.6774169228491664,
      "learning_rate": 1.045913570439972e-05,
      "loss": 0.5532,
      "step": 5563
    },
    {
      "epoch": 0.5009340745909203,
      "grad_norm": 0.7053684404343038,
      "learning_rate": 1.0456222587749488e-05,
      "loss": 0.4805,
      "step": 5564
    },
    {
      "epoch": 0.5010241058767921,
      "grad_norm": 0.6865723713972723,
      "learning_rate": 1.04533094323018e-05,
      "loss": 0.5448,
      "step": 5565
    },
    {
      "epoch": 0.5011141371626641,
      "grad_norm": 0.8129485338696599,
      "learning_rate": 1.0450396238304397e-05,
      "loss": 0.6041,
      "step": 5566
    },
    {
      "epoch": 0.5012041684485359,
      "grad_norm": 0.8808629485621357,
      "learning_rate": 1.0447483006005018e-05,
      "loss": 0.5682,
      "step": 5567
    },
    {
      "epoch": 0.5012941997344077,
      "grad_norm": 0.9281016179290648,
      "learning_rate": 1.0444569735651408e-05,
      "loss": 0.5524,
      "step": 5568
    },
    {
      "epoch": 0.5013842310202795,
      "grad_norm": 0.7887078270941289,
      "learning_rate": 1.0441656427491311e-05,
      "loss": 0.5519,
      "step": 5569
    },
    {
      "epoch": 0.5014742623061514,
      "grad_norm": 0.7026771769043105,
      "learning_rate": 1.0438743081772476e-05,
      "loss": 0.6051,
      "step": 5570
    },
    {
      "epoch": 0.5015642935920233,
      "grad_norm": 0.6998715519807869,
      "learning_rate": 1.0435829698742659e-05,
      "loss": 0.5367,
      "step": 5571
    },
    {
      "epoch": 0.5016543248778951,
      "grad_norm": 0.849725570557021,
      "learning_rate": 1.0432916278649611e-05,
      "loss": 0.6238,
      "step": 5572
    },
    {
      "epoch": 0.5017443561637669,
      "grad_norm": 1.1902820212437981,
      "learning_rate": 1.0430002821741096e-05,
      "loss": 0.544,
      "step": 5573
    },
    {
      "epoch": 0.5018343874496387,
      "grad_norm": 0.627420701115973,
      "learning_rate": 1.0427089328264875e-05,
      "loss": 0.4473,
      "step": 5574
    },
    {
      "epoch": 0.5019244187355106,
      "grad_norm": 0.7311303853855851,
      "learning_rate": 1.0424175798468712e-05,
      "loss": 0.5251,
      "step": 5575
    },
    {
      "epoch": 0.5020144500213825,
      "grad_norm": 0.7100512409104942,
      "learning_rate": 1.0421262232600373e-05,
      "loss": 0.5529,
      "step": 5576
    },
    {
      "epoch": 0.5021044813072543,
      "grad_norm": 0.7109547694173324,
      "learning_rate": 1.0418348630907633e-05,
      "loss": 0.6456,
      "step": 5577
    },
    {
      "epoch": 0.5021945125931261,
      "grad_norm": 0.8743886241989389,
      "learning_rate": 1.0415434993638269e-05,
      "loss": 0.483,
      "step": 5578
    },
    {
      "epoch": 0.5022845438789979,
      "grad_norm": 0.7831685354545889,
      "learning_rate": 1.0412521321040052e-05,
      "loss": 0.6191,
      "step": 5579
    },
    {
      "epoch": 0.5023745751648698,
      "grad_norm": 0.6288167395971651,
      "learning_rate": 1.0409607613360766e-05,
      "loss": 0.4811,
      "step": 5580
    },
    {
      "epoch": 0.5024646064507416,
      "grad_norm": 0.7290454029664307,
      "learning_rate": 1.0406693870848192e-05,
      "loss": 0.5552,
      "step": 5581
    },
    {
      "epoch": 0.5025546377366135,
      "grad_norm": 0.9452867971948845,
      "learning_rate": 1.0403780093750124e-05,
      "loss": 0.564,
      "step": 5582
    },
    {
      "epoch": 0.5026446690224853,
      "grad_norm": 0.6669738090316334,
      "learning_rate": 1.040086628231434e-05,
      "loss": 0.5742,
      "step": 5583
    },
    {
      "epoch": 0.5027347003083572,
      "grad_norm": 0.6262827770311719,
      "learning_rate": 1.0397952436788643e-05,
      "loss": 0.4975,
      "step": 5584
    },
    {
      "epoch": 0.502824731594229,
      "grad_norm": 0.9201291394422815,
      "learning_rate": 1.0395038557420824e-05,
      "loss": 0.5398,
      "step": 5585
    },
    {
      "epoch": 0.5029147628801008,
      "grad_norm": 0.6823977101328929,
      "learning_rate": 1.039212464445868e-05,
      "loss": 0.5837,
      "step": 5586
    },
    {
      "epoch": 0.5030047941659727,
      "grad_norm": 0.7513935627706281,
      "learning_rate": 1.038921069815001e-05,
      "loss": 0.5141,
      "step": 5587
    },
    {
      "epoch": 0.5030948254518445,
      "grad_norm": 0.7512909771865006,
      "learning_rate": 1.0386296718742626e-05,
      "loss": 0.5222,
      "step": 5588
    },
    {
      "epoch": 0.5031848567377164,
      "grad_norm": 0.7736646566978725,
      "learning_rate": 1.0383382706484323e-05,
      "loss": 0.5701,
      "step": 5589
    },
    {
      "epoch": 0.5032748880235882,
      "grad_norm": 0.7625996399427026,
      "learning_rate": 1.038046866162292e-05,
      "loss": 0.4743,
      "step": 5590
    },
    {
      "epoch": 0.50336491930946,
      "grad_norm": 0.5950662164386593,
      "learning_rate": 1.0377554584406224e-05,
      "loss": 0.532,
      "step": 5591
    },
    {
      "epoch": 0.5034549505953319,
      "grad_norm": 0.8192137490084714,
      "learning_rate": 1.0374640475082054e-05,
      "loss": 0.6456,
      "step": 5592
    },
    {
      "epoch": 0.5035449818812037,
      "grad_norm": 0.7785158112529037,
      "learning_rate": 1.0371726333898225e-05,
      "loss": 0.4822,
      "step": 5593
    },
    {
      "epoch": 0.5036350131670756,
      "grad_norm": 0.7272963193714818,
      "learning_rate": 1.0368812161102555e-05,
      "loss": 0.5668,
      "step": 5594
    },
    {
      "epoch": 0.5037250444529474,
      "grad_norm": 0.8802123516908359,
      "learning_rate": 1.0365897956942874e-05,
      "loss": 0.513,
      "step": 5595
    },
    {
      "epoch": 0.5038150757388192,
      "grad_norm": 0.7542113373172823,
      "learning_rate": 1.0362983721667e-05,
      "loss": 0.5442,
      "step": 5596
    },
    {
      "epoch": 0.503905107024691,
      "grad_norm": 0.7475464578869202,
      "learning_rate": 1.0360069455522765e-05,
      "loss": 0.558,
      "step": 5597
    },
    {
      "epoch": 0.503995138310563,
      "grad_norm": 0.7756202679870302,
      "learning_rate": 1.0357155158758001e-05,
      "loss": 0.6359,
      "step": 5598
    },
    {
      "epoch": 0.5040851695964348,
      "grad_norm": 0.7529696655486952,
      "learning_rate": 1.0354240831620542e-05,
      "loss": 0.627,
      "step": 5599
    },
    {
      "epoch": 0.5041752008823066,
      "grad_norm": 0.7357590187647776,
      "learning_rate": 1.0351326474358222e-05,
      "loss": 0.5242,
      "step": 5600
    },
    {
      "epoch": 0.5042652321681784,
      "grad_norm": 0.8767249488797849,
      "learning_rate": 1.0348412087218878e-05,
      "loss": 0.5984,
      "step": 5601
    },
    {
      "epoch": 0.5043552634540502,
      "grad_norm": 0.7990271464304797,
      "learning_rate": 1.0345497670450357e-05,
      "loss": 0.5695,
      "step": 5602
    },
    {
      "epoch": 0.5044452947399222,
      "grad_norm": 0.6129446702831565,
      "learning_rate": 1.0342583224300498e-05,
      "loss": 0.5022,
      "step": 5603
    },
    {
      "epoch": 0.504535326025794,
      "grad_norm": 0.6994223770900497,
      "learning_rate": 1.0339668749017147e-05,
      "loss": 0.4709,
      "step": 5604
    },
    {
      "epoch": 0.5046253573116658,
      "grad_norm": 0.7418574217648121,
      "learning_rate": 1.0336754244848156e-05,
      "loss": 0.5714,
      "step": 5605
    },
    {
      "epoch": 0.5047153885975376,
      "grad_norm": 0.898460434618963,
      "learning_rate": 1.0333839712041379e-05,
      "loss": 0.5717,
      "step": 5606
    },
    {
      "epoch": 0.5048054198834094,
      "grad_norm": 0.6435004846008364,
      "learning_rate": 1.033092515084466e-05,
      "loss": 0.4646,
      "step": 5607
    },
    {
      "epoch": 0.5048954511692814,
      "grad_norm": 0.6953569184404476,
      "learning_rate": 1.0328010561505863e-05,
      "loss": 0.5431,
      "step": 5608
    },
    {
      "epoch": 0.5049854824551532,
      "grad_norm": 0.781479078608634,
      "learning_rate": 1.0325095944272845e-05,
      "loss": 0.5142,
      "step": 5609
    },
    {
      "epoch": 0.505075513741025,
      "grad_norm": 0.7796617304649205,
      "learning_rate": 1.0322181299393466e-05,
      "loss": 0.5991,
      "step": 5610
    },
    {
      "epoch": 0.5051655450268968,
      "grad_norm": 0.7428388409522717,
      "learning_rate": 1.0319266627115588e-05,
      "loss": 0.6261,
      "step": 5611
    },
    {
      "epoch": 0.5052555763127687,
      "grad_norm": 0.6629571535260104,
      "learning_rate": 1.0316351927687077e-05,
      "loss": 0.5455,
      "step": 5612
    },
    {
      "epoch": 0.5053456075986406,
      "grad_norm": 0.746382083640612,
      "learning_rate": 1.0313437201355803e-05,
      "loss": 0.491,
      "step": 5613
    },
    {
      "epoch": 0.5054356388845124,
      "grad_norm": 0.7649552266902037,
      "learning_rate": 1.0310522448369632e-05,
      "loss": 0.4964,
      "step": 5614
    },
    {
      "epoch": 0.5055256701703842,
      "grad_norm": 0.6288053406119007,
      "learning_rate": 1.0307607668976442e-05,
      "loss": 0.5242,
      "step": 5615
    },
    {
      "epoch": 0.505615701456256,
      "grad_norm": 0.6251404948714117,
      "learning_rate": 1.0304692863424107e-05,
      "loss": 0.5017,
      "step": 5616
    },
    {
      "epoch": 0.5057057327421279,
      "grad_norm": 0.8653035229759157,
      "learning_rate": 1.0301778031960498e-05,
      "loss": 0.4971,
      "step": 5617
    },
    {
      "epoch": 0.5057957640279998,
      "grad_norm": 0.811926753613876,
      "learning_rate": 1.0298863174833498e-05,
      "loss": 0.5334,
      "step": 5618
    },
    {
      "epoch": 0.5058857953138716,
      "grad_norm": 0.7051999169854766,
      "learning_rate": 1.0295948292290989e-05,
      "loss": 0.5494,
      "step": 5619
    },
    {
      "epoch": 0.5059758265997434,
      "grad_norm": 0.6573027378033197,
      "learning_rate": 1.0293033384580856e-05,
      "loss": 0.5757,
      "step": 5620
    },
    {
      "epoch": 0.5060658578856152,
      "grad_norm": 0.7275017768141072,
      "learning_rate": 1.029011845195098e-05,
      "loss": 0.464,
      "step": 5621
    },
    {
      "epoch": 0.5061558891714871,
      "grad_norm": 0.6200299716638566,
      "learning_rate": 1.0287203494649247e-05,
      "loss": 0.4976,
      "step": 5622
    },
    {
      "epoch": 0.5062459204573589,
      "grad_norm": 0.813022938043698,
      "learning_rate": 1.0284288512923555e-05,
      "loss": 0.5092,
      "step": 5623
    },
    {
      "epoch": 0.5063359517432308,
      "grad_norm": 0.6953219650924747,
      "learning_rate": 1.028137350702179e-05,
      "loss": 0.5584,
      "step": 5624
    },
    {
      "epoch": 0.5064259830291026,
      "grad_norm": 0.7554090017381706,
      "learning_rate": 1.0278458477191845e-05,
      "loss": 0.5396,
      "step": 5625
    },
    {
      "epoch": 0.5065160143149745,
      "grad_norm": 0.8723685597694603,
      "learning_rate": 1.0275543423681622e-05,
      "loss": 0.5246,
      "step": 5626
    },
    {
      "epoch": 0.5066060456008463,
      "grad_norm": 0.7067181810968937,
      "learning_rate": 1.0272628346739016e-05,
      "loss": 0.5649,
      "step": 5627
    },
    {
      "epoch": 0.5066960768867181,
      "grad_norm": 0.5458201421439863,
      "learning_rate": 1.0269713246611921e-05,
      "loss": 0.4845,
      "step": 5628
    },
    {
      "epoch": 0.50678610817259,
      "grad_norm": 0.901220268449215,
      "learning_rate": 1.0266798123548247e-05,
      "loss": 0.5196,
      "step": 5629
    },
    {
      "epoch": 0.5068761394584618,
      "grad_norm": 0.6633247186487163,
      "learning_rate": 1.0263882977795896e-05,
      "loss": 0.5427,
      "step": 5630
    },
    {
      "epoch": 0.5069661707443337,
      "grad_norm": 0.7489333222365983,
      "learning_rate": 1.0260967809602772e-05,
      "loss": 0.507,
      "step": 5631
    },
    {
      "epoch": 0.5070562020302055,
      "grad_norm": 0.6196277943863543,
      "learning_rate": 1.0258052619216781e-05,
      "loss": 0.5512,
      "step": 5632
    },
    {
      "epoch": 0.5071462333160773,
      "grad_norm": 0.8761287049698949,
      "learning_rate": 1.0255137406885836e-05,
      "loss": 0.5452,
      "step": 5633
    },
    {
      "epoch": 0.5072362646019491,
      "grad_norm": 0.6665747761437477,
      "learning_rate": 1.025222217285785e-05,
      "loss": 0.5031,
      "step": 5634
    },
    {
      "epoch": 0.507326295887821,
      "grad_norm": 0.6759405971295752,
      "learning_rate": 1.0249306917380731e-05,
      "loss": 0.4773,
      "step": 5635
    },
    {
      "epoch": 0.5074163271736929,
      "grad_norm": 0.7067624592229697,
      "learning_rate": 1.0246391640702397e-05,
      "loss": 0.4781,
      "step": 5636
    },
    {
      "epoch": 0.5075063584595647,
      "grad_norm": 0.7330883691788976,
      "learning_rate": 1.0243476343070768e-05,
      "loss": 0.5283,
      "step": 5637
    },
    {
      "epoch": 0.5075963897454365,
      "grad_norm": 0.6931363546147767,
      "learning_rate": 1.0240561024733757e-05,
      "loss": 0.4053,
      "step": 5638
    },
    {
      "epoch": 0.5076864210313083,
      "grad_norm": 0.6774803885880716,
      "learning_rate": 1.0237645685939286e-05,
      "loss": 0.4739,
      "step": 5639
    },
    {
      "epoch": 0.5077764523171803,
      "grad_norm": 0.7816643022331716,
      "learning_rate": 1.023473032693528e-05,
      "loss": 0.5085,
      "step": 5640
    },
    {
      "epoch": 0.5078664836030521,
      "grad_norm": 0.7406970491834347,
      "learning_rate": 1.0231814947969663e-05,
      "loss": 0.5524,
      "step": 5641
    },
    {
      "epoch": 0.5079565148889239,
      "grad_norm": 0.6679387859139664,
      "learning_rate": 1.0228899549290357e-05,
      "loss": 0.4821,
      "step": 5642
    },
    {
      "epoch": 0.5080465461747957,
      "grad_norm": 0.7498253214016752,
      "learning_rate": 1.0225984131145291e-05,
      "loss": 0.6199,
      "step": 5643
    },
    {
      "epoch": 0.5081365774606675,
      "grad_norm": 0.7076801636579277,
      "learning_rate": 1.0223068693782394e-05,
      "loss": 0.493,
      "step": 5644
    },
    {
      "epoch": 0.5082266087465395,
      "grad_norm": 0.8861613760798034,
      "learning_rate": 1.0220153237449597e-05,
      "loss": 0.6021,
      "step": 5645
    },
    {
      "epoch": 0.5083166400324113,
      "grad_norm": 0.7078791674943262,
      "learning_rate": 1.0217237762394832e-05,
      "loss": 0.54,
      "step": 5646
    },
    {
      "epoch": 0.5084066713182831,
      "grad_norm": 0.8090203181968678,
      "learning_rate": 1.0214322268866033e-05,
      "loss": 0.6005,
      "step": 5647
    },
    {
      "epoch": 0.5084967026041549,
      "grad_norm": 0.6713987271470432,
      "learning_rate": 1.0211406757111135e-05,
      "loss": 0.4752,
      "step": 5648
    },
    {
      "epoch": 0.5085867338900267,
      "grad_norm": 0.8054418654985124,
      "learning_rate": 1.0208491227378074e-05,
      "loss": 0.5245,
      "step": 5649
    },
    {
      "epoch": 0.5086767651758987,
      "grad_norm": 0.6899869259661485,
      "learning_rate": 1.0205575679914793e-05,
      "loss": 0.41,
      "step": 5650
    },
    {
      "epoch": 0.5087667964617705,
      "grad_norm": 0.6654969764038796,
      "learning_rate": 1.0202660114969227e-05,
      "loss": 0.5847,
      "step": 5651
    },
    {
      "epoch": 0.5088568277476423,
      "grad_norm": 0.5840057032493035,
      "learning_rate": 1.0199744532789318e-05,
      "loss": 0.4754,
      "step": 5652
    },
    {
      "epoch": 0.5089468590335141,
      "grad_norm": 0.6491040484064705,
      "learning_rate": 1.0196828933623008e-05,
      "loss": 0.5395,
      "step": 5653
    },
    {
      "epoch": 0.509036890319386,
      "grad_norm": 0.611242037703511,
      "learning_rate": 1.0193913317718245e-05,
      "loss": 0.4644,
      "step": 5654
    },
    {
      "epoch": 0.5091269216052579,
      "grad_norm": 0.6243057300857731,
      "learning_rate": 1.0190997685322975e-05,
      "loss": 0.5358,
      "step": 5655
    },
    {
      "epoch": 0.5092169528911297,
      "grad_norm": 0.8339649342020362,
      "learning_rate": 1.018808203668514e-05,
      "loss": 0.558,
      "step": 5656
    },
    {
      "epoch": 0.5093069841770015,
      "grad_norm": 0.6607907865386284,
      "learning_rate": 1.0185166372052696e-05,
      "loss": 0.5836,
      "step": 5657
    },
    {
      "epoch": 0.5093970154628733,
      "grad_norm": 0.7848065241675267,
      "learning_rate": 1.0182250691673585e-05,
      "loss": 0.5332,
      "step": 5658
    },
    {
      "epoch": 0.5094870467487452,
      "grad_norm": 0.7832880315154243,
      "learning_rate": 1.0179334995795764e-05,
      "loss": 0.5688,
      "step": 5659
    },
    {
      "epoch": 0.509577078034617,
      "grad_norm": 0.8513439421177929,
      "learning_rate": 1.0176419284667182e-05,
      "loss": 0.7239,
      "step": 5660
    },
    {
      "epoch": 0.5096671093204889,
      "grad_norm": 0.9348440890684271,
      "learning_rate": 1.0173503558535795e-05,
      "loss": 0.5296,
      "step": 5661
    },
    {
      "epoch": 0.5097571406063607,
      "grad_norm": 0.7344112138132952,
      "learning_rate": 1.017058781764956e-05,
      "loss": 0.4877,
      "step": 5662
    },
    {
      "epoch": 0.5098471718922325,
      "grad_norm": 1.1501919535962388,
      "learning_rate": 1.0167672062256428e-05,
      "loss": 0.4954,
      "step": 5663
    },
    {
      "epoch": 0.5099372031781044,
      "grad_norm": 0.7622943213586584,
      "learning_rate": 1.0164756292604361e-05,
      "loss": 0.5241,
      "step": 5664
    },
    {
      "epoch": 0.5100272344639762,
      "grad_norm": 0.7320250773104816,
      "learning_rate": 1.0161840508941316e-05,
      "loss": 0.5105,
      "step": 5665
    },
    {
      "epoch": 0.5101172657498481,
      "grad_norm": 1.158785104366584,
      "learning_rate": 1.0158924711515254e-05,
      "loss": 0.6449,
      "step": 5666
    },
    {
      "epoch": 0.5102072970357199,
      "grad_norm": 0.9146040276032517,
      "learning_rate": 1.0156008900574135e-05,
      "loss": 0.5859,
      "step": 5667
    },
    {
      "epoch": 0.5102973283215918,
      "grad_norm": 0.9856553559574395,
      "learning_rate": 1.0153093076365923e-05,
      "loss": 0.6104,
      "step": 5668
    },
    {
      "epoch": 0.5103873596074636,
      "grad_norm": 0.6900403143233579,
      "learning_rate": 1.0150177239138584e-05,
      "loss": 0.588,
      "step": 5669
    },
    {
      "epoch": 0.5104773908933354,
      "grad_norm": 0.7645340693526438,
      "learning_rate": 1.0147261389140074e-05,
      "loss": 0.5091,
      "step": 5670
    },
    {
      "epoch": 0.5105674221792073,
      "grad_norm": 0.8299424350327691,
      "learning_rate": 1.0144345526618364e-05,
      "loss": 0.5411,
      "step": 5671
    },
    {
      "epoch": 0.5106574534650791,
      "grad_norm": 0.8067096010865559,
      "learning_rate": 1.0141429651821426e-05,
      "loss": 0.5393,
      "step": 5672
    },
    {
      "epoch": 0.510747484750951,
      "grad_norm": 0.807773667912013,
      "learning_rate": 1.013851376499722e-05,
      "loss": 0.5776,
      "step": 5673
    },
    {
      "epoch": 0.5108375160368228,
      "grad_norm": 0.7311307998221548,
      "learning_rate": 1.0135597866393717e-05,
      "loss": 0.5664,
      "step": 5674
    },
    {
      "epoch": 0.5109275473226946,
      "grad_norm": 0.6610572774895188,
      "learning_rate": 1.0132681956258884e-05,
      "loss": 0.5646,
      "step": 5675
    },
    {
      "epoch": 0.5110175786085664,
      "grad_norm": 0.7729824374499356,
      "learning_rate": 1.01297660348407e-05,
      "loss": 0.5203,
      "step": 5676
    },
    {
      "epoch": 0.5111076098944383,
      "grad_norm": 0.8938519182337243,
      "learning_rate": 1.012685010238713e-05,
      "loss": 0.5247,
      "step": 5677
    },
    {
      "epoch": 0.5111976411803102,
      "grad_norm": 0.6770112692045457,
      "learning_rate": 1.0123934159146147e-05,
      "loss": 0.4493,
      "step": 5678
    },
    {
      "epoch": 0.511287672466182,
      "grad_norm": 0.9015310286514975,
      "learning_rate": 1.0121018205365731e-05,
      "loss": 0.4851,
      "step": 5679
    },
    {
      "epoch": 0.5113777037520538,
      "grad_norm": 0.8681278981203403,
      "learning_rate": 1.0118102241293848e-05,
      "loss": 0.6522,
      "step": 5680
    },
    {
      "epoch": 0.5114677350379256,
      "grad_norm": 0.7678442210752591,
      "learning_rate": 1.0115186267178475e-05,
      "loss": 0.5164,
      "step": 5681
    },
    {
      "epoch": 0.5115577663237976,
      "grad_norm": 0.89251781127447,
      "learning_rate": 1.0112270283267593e-05,
      "loss": 0.5479,
      "step": 5682
    },
    {
      "epoch": 0.5116477976096694,
      "grad_norm": 1.011793924413974,
      "learning_rate": 1.0109354289809176e-05,
      "loss": 0.5265,
      "step": 5683
    },
    {
      "epoch": 0.5117378288955412,
      "grad_norm": 0.7154307639600105,
      "learning_rate": 1.0106438287051203e-05,
      "loss": 0.5343,
      "step": 5684
    },
    {
      "epoch": 0.511827860181413,
      "grad_norm": 0.7284142031039604,
      "learning_rate": 1.0103522275241651e-05,
      "loss": 0.5369,
      "step": 5685
    },
    {
      "epoch": 0.5119178914672848,
      "grad_norm": 0.7450695336738568,
      "learning_rate": 1.0100606254628503e-05,
      "loss": 0.4915,
      "step": 5686
    },
    {
      "epoch": 0.5120079227531568,
      "grad_norm": 0.948303467987816,
      "learning_rate": 1.0097690225459733e-05,
      "loss": 0.5824,
      "step": 5687
    },
    {
      "epoch": 0.5120979540390286,
      "grad_norm": 0.6429445610476929,
      "learning_rate": 1.0094774187983327e-05,
      "loss": 0.4853,
      "step": 5688
    },
    {
      "epoch": 0.5121879853249004,
      "grad_norm": 0.7584380058320735,
      "learning_rate": 1.0091858142447266e-05,
      "loss": 0.5102,
      "step": 5689
    },
    {
      "epoch": 0.5122780166107722,
      "grad_norm": 0.8518758018751376,
      "learning_rate": 1.0088942089099532e-05,
      "loss": 0.5543,
      "step": 5690
    },
    {
      "epoch": 0.512368047896644,
      "grad_norm": 0.8164312023439005,
      "learning_rate": 1.0086026028188105e-05,
      "loss": 0.6145,
      "step": 5691
    },
    {
      "epoch": 0.512458079182516,
      "grad_norm": 0.7008425553090362,
      "learning_rate": 1.0083109959960974e-05,
      "loss": 0.5736,
      "step": 5692
    },
    {
      "epoch": 0.5125481104683878,
      "grad_norm": 0.7255167617988602,
      "learning_rate": 1.008019388466612e-05,
      "loss": 0.4906,
      "step": 5693
    },
    {
      "epoch": 0.5126381417542596,
      "grad_norm": 0.7416215564143068,
      "learning_rate": 1.0077277802551527e-05,
      "loss": 0.5351,
      "step": 5694
    },
    {
      "epoch": 0.5127281730401314,
      "grad_norm": 0.7582439516376626,
      "learning_rate": 1.007436171386518e-05,
      "loss": 0.5943,
      "step": 5695
    },
    {
      "epoch": 0.5128182043260033,
      "grad_norm": 0.8394746677119512,
      "learning_rate": 1.0071445618855066e-05,
      "loss": 0.5016,
      "step": 5696
    },
    {
      "epoch": 0.5129082356118752,
      "grad_norm": 1.050941640325497,
      "learning_rate": 1.0068529517769177e-05,
      "loss": 0.6362,
      "step": 5697
    },
    {
      "epoch": 0.512998266897747,
      "grad_norm": 0.923001793981197,
      "learning_rate": 1.0065613410855492e-05,
      "loss": 0.5352,
      "step": 5698
    },
    {
      "epoch": 0.5130882981836188,
      "grad_norm": 0.5842115360748482,
      "learning_rate": 1.0062697298362004e-05,
      "loss": 0.4822,
      "step": 5699
    },
    {
      "epoch": 0.5131783294694906,
      "grad_norm": 0.8310730475764716,
      "learning_rate": 1.0059781180536698e-05,
      "loss": 0.5564,
      "step": 5700
    },
    {
      "epoch": 0.5132683607553625,
      "grad_norm": 0.7634233554510776,
      "learning_rate": 1.0056865057627562e-05,
      "loss": 0.4667,
      "step": 5701
    },
    {
      "epoch": 0.5133583920412343,
      "grad_norm": 0.8619601734184107,
      "learning_rate": 1.0053948929882587e-05,
      "loss": 0.529,
      "step": 5702
    },
    {
      "epoch": 0.5134484233271062,
      "grad_norm": 0.8170257866462964,
      "learning_rate": 1.005103279754976e-05,
      "loss": 0.4925,
      "step": 5703
    },
    {
      "epoch": 0.513538454612978,
      "grad_norm": 0.6972037083226913,
      "learning_rate": 1.0048116660877076e-05,
      "loss": 0.4703,
      "step": 5704
    },
    {
      "epoch": 0.5136284858988498,
      "grad_norm": 0.6953434645094868,
      "learning_rate": 1.004520052011252e-05,
      "loss": 0.4959,
      "step": 5705
    },
    {
      "epoch": 0.5137185171847217,
      "grad_norm": 0.8010350072707685,
      "learning_rate": 1.0042284375504081e-05,
      "loss": 0.5122,
      "step": 5706
    },
    {
      "epoch": 0.5138085484705935,
      "grad_norm": 0.7699396600018164,
      "learning_rate": 1.0039368227299755e-05,
      "loss": 0.6692,
      "step": 5707
    },
    {
      "epoch": 0.5138985797564654,
      "grad_norm": 0.7072178193507359,
      "learning_rate": 1.0036452075747529e-05,
      "loss": 0.5789,
      "step": 5708
    },
    {
      "epoch": 0.5139886110423372,
      "grad_norm": 1.3788442653272635,
      "learning_rate": 1.0033535921095396e-05,
      "loss": 0.5365,
      "step": 5709
    },
    {
      "epoch": 0.5140786423282091,
      "grad_norm": 0.7243946216410674,
      "learning_rate": 1.0030619763591348e-05,
      "loss": 0.5632,
      "step": 5710
    },
    {
      "epoch": 0.5141686736140809,
      "grad_norm": 0.7381611439965161,
      "learning_rate": 1.0027703603483379e-05,
      "loss": 0.554,
      "step": 5711
    },
    {
      "epoch": 0.5142587048999527,
      "grad_norm": 0.7929077327191787,
      "learning_rate": 1.0024787441019475e-05,
      "loss": 0.4992,
      "step": 5712
    },
    {
      "epoch": 0.5143487361858246,
      "grad_norm": 0.8335561138416302,
      "learning_rate": 1.002187127644763e-05,
      "loss": 0.5228,
      "step": 5713
    },
    {
      "epoch": 0.5144387674716964,
      "grad_norm": 1.188746632784615,
      "learning_rate": 1.0018955110015843e-05,
      "loss": 0.6284,
      "step": 5714
    },
    {
      "epoch": 0.5145287987575683,
      "grad_norm": 0.8196413644027837,
      "learning_rate": 1.0016038941972097e-05,
      "loss": 0.6,
      "step": 5715
    },
    {
      "epoch": 0.5146188300434401,
      "grad_norm": 0.8009207717100797,
      "learning_rate": 1.0013122772564392e-05,
      "loss": 0.4499,
      "step": 5716
    },
    {
      "epoch": 0.5147088613293119,
      "grad_norm": 0.866850268900279,
      "learning_rate": 1.0010206602040718e-05,
      "loss": 0.5604,
      "step": 5717
    },
    {
      "epoch": 0.5147988926151837,
      "grad_norm": 0.8309230186214274,
      "learning_rate": 1.0007290430649068e-05,
      "loss": 0.4774,
      "step": 5718
    },
    {
      "epoch": 0.5148889239010556,
      "grad_norm": 0.645035669586614,
      "learning_rate": 1.0004374258637435e-05,
      "loss": 0.4712,
      "step": 5719
    },
    {
      "epoch": 0.5149789551869275,
      "grad_norm": 0.7568124588277891,
      "learning_rate": 1.000145808625381e-05,
      "loss": 0.5423,
      "step": 5720
    },
    {
      "epoch": 0.5150689864727993,
      "grad_norm": 0.7655553612073149,
      "learning_rate": 9.998541913746192e-06,
      "loss": 0.5848,
      "step": 5721
    },
    {
      "epoch": 0.5151590177586711,
      "grad_norm": 0.6741631581076278,
      "learning_rate": 9.99562574136257e-06,
      "loss": 0.5416,
      "step": 5722
    },
    {
      "epoch": 0.5152490490445429,
      "grad_norm": 0.7860890973306218,
      "learning_rate": 9.992709569350935e-06,
      "loss": 0.5467,
      "step": 5723
    },
    {
      "epoch": 0.5153390803304149,
      "grad_norm": 0.9130229093971818,
      "learning_rate": 9.989793397959287e-06,
      "loss": 0.5953,
      "step": 5724
    },
    {
      "epoch": 0.5154291116162867,
      "grad_norm": 0.7837071283121133,
      "learning_rate": 9.986877227435612e-06,
      "loss": 0.6296,
      "step": 5725
    },
    {
      "epoch": 0.5155191429021585,
      "grad_norm": 0.6897463140938974,
      "learning_rate": 9.983961058027908e-06,
      "loss": 0.494,
      "step": 5726
    },
    {
      "epoch": 0.5156091741880303,
      "grad_norm": 1.0906192857496992,
      "learning_rate": 9.98104488998416e-06,
      "loss": 0.5873,
      "step": 5727
    },
    {
      "epoch": 0.5156992054739021,
      "grad_norm": 0.8602301267775432,
      "learning_rate": 9.978128723552374e-06,
      "loss": 0.6124,
      "step": 5728
    },
    {
      "epoch": 0.5157892367597741,
      "grad_norm": 0.7392598226924545,
      "learning_rate": 9.975212558980529e-06,
      "loss": 0.4505,
      "step": 5729
    },
    {
      "epoch": 0.5158792680456459,
      "grad_norm": 0.7329920972647899,
      "learning_rate": 9.972296396516628e-06,
      "loss": 0.5036,
      "step": 5730
    },
    {
      "epoch": 0.5159692993315177,
      "grad_norm": 1.1758291213820697,
      "learning_rate": 9.969380236408656e-06,
      "loss": 0.5743,
      "step": 5731
    },
    {
      "epoch": 0.5160593306173895,
      "grad_norm": 0.8783661072353821,
      "learning_rate": 9.966464078904609e-06,
      "loss": 0.6165,
      "step": 5732
    },
    {
      "epoch": 0.5161493619032614,
      "grad_norm": 0.666847759206463,
      "learning_rate": 9.963547924252473e-06,
      "loss": 0.5086,
      "step": 5733
    },
    {
      "epoch": 0.5162393931891333,
      "grad_norm": 0.7341363773825996,
      "learning_rate": 9.96063177270025e-06,
      "loss": 0.6048,
      "step": 5734
    },
    {
      "epoch": 0.5163294244750051,
      "grad_norm": 0.8464887468700989,
      "learning_rate": 9.957715624495922e-06,
      "loss": 0.5539,
      "step": 5735
    },
    {
      "epoch": 0.5164194557608769,
      "grad_norm": 0.763105213702436,
      "learning_rate": 9.954799479887488e-06,
      "loss": 0.5132,
      "step": 5736
    },
    {
      "epoch": 0.5165094870467487,
      "grad_norm": 0.731390661185739,
      "learning_rate": 9.951883339122928e-06,
      "loss": 0.4883,
      "step": 5737
    },
    {
      "epoch": 0.5165995183326206,
      "grad_norm": 0.7781211230268683,
      "learning_rate": 9.948967202450243e-06,
      "loss": 0.5737,
      "step": 5738
    },
    {
      "epoch": 0.5166895496184924,
      "grad_norm": 1.2586359699663459,
      "learning_rate": 9.946051070117417e-06,
      "loss": 0.5796,
      "step": 5739
    },
    {
      "epoch": 0.5167795809043643,
      "grad_norm": 0.7845131478255778,
      "learning_rate": 9.943134942372443e-06,
      "loss": 0.5338,
      "step": 5740
    },
    {
      "epoch": 0.5168696121902361,
      "grad_norm": 0.7476038577469939,
      "learning_rate": 9.940218819463305e-06,
      "loss": 0.4867,
      "step": 5741
    },
    {
      "epoch": 0.5169596434761079,
      "grad_norm": 0.6072102515408246,
      "learning_rate": 9.937302701638001e-06,
      "loss": 0.5277,
      "step": 5742
    },
    {
      "epoch": 0.5170496747619798,
      "grad_norm": 0.839449787805337,
      "learning_rate": 9.93438658914451e-06,
      "loss": 0.5899,
      "step": 5743
    },
    {
      "epoch": 0.5171397060478516,
      "grad_norm": 0.9601932066340682,
      "learning_rate": 9.931470482230827e-06,
      "loss": 0.5429,
      "step": 5744
    },
    {
      "epoch": 0.5172297373337235,
      "grad_norm": 0.7602143551467895,
      "learning_rate": 9.928554381144936e-06,
      "loss": 0.4833,
      "step": 5745
    },
    {
      "epoch": 0.5173197686195953,
      "grad_norm": 0.7998665337429279,
      "learning_rate": 9.925638286134825e-06,
      "loss": 0.6231,
      "step": 5746
    },
    {
      "epoch": 0.5174097999054672,
      "grad_norm": 0.7736731882960022,
      "learning_rate": 9.922722197448478e-06,
      "loss": 0.5696,
      "step": 5747
    },
    {
      "epoch": 0.517499831191339,
      "grad_norm": 0.7761557939959591,
      "learning_rate": 9.919806115333886e-06,
      "loss": 0.5599,
      "step": 5748
    },
    {
      "epoch": 0.5175898624772108,
      "grad_norm": 0.6654548266381651,
      "learning_rate": 9.916890040039031e-06,
      "loss": 0.5723,
      "step": 5749
    },
    {
      "epoch": 0.5176798937630827,
      "grad_norm": 0.7917598770771802,
      "learning_rate": 9.9139739718119e-06,
      "loss": 0.5312,
      "step": 5750
    },
    {
      "epoch": 0.5177699250489545,
      "grad_norm": 0.6772970620489883,
      "learning_rate": 9.911057910900471e-06,
      "loss": 0.4851,
      "step": 5751
    },
    {
      "epoch": 0.5178599563348264,
      "grad_norm": 0.7128206607170515,
      "learning_rate": 9.908141857552737e-06,
      "loss": 0.5621,
      "step": 5752
    },
    {
      "epoch": 0.5179499876206982,
      "grad_norm": 0.7271626792938337,
      "learning_rate": 9.905225812016676e-06,
      "loss": 0.5366,
      "step": 5753
    },
    {
      "epoch": 0.51804001890657,
      "grad_norm": 0.8703657018493423,
      "learning_rate": 9.90230977454027e-06,
      "loss": 0.5695,
      "step": 5754
    },
    {
      "epoch": 0.5181300501924418,
      "grad_norm": 0.8037729182825458,
      "learning_rate": 9.899393745371499e-06,
      "loss": 0.5174,
      "step": 5755
    },
    {
      "epoch": 0.5182200814783137,
      "grad_norm": 0.6829405901639587,
      "learning_rate": 9.896477724758354e-06,
      "loss": 0.5985,
      "step": 5756
    },
    {
      "epoch": 0.5183101127641856,
      "grad_norm": 0.6736010149717387,
      "learning_rate": 9.893561712948798e-06,
      "loss": 0.5375,
      "step": 5757
    },
    {
      "epoch": 0.5184001440500574,
      "grad_norm": 0.6878413566949595,
      "learning_rate": 9.890645710190827e-06,
      "loss": 0.4443,
      "step": 5758
    },
    {
      "epoch": 0.5184901753359292,
      "grad_norm": 0.665890349162749,
      "learning_rate": 9.88772971673241e-06,
      "loss": 0.5131,
      "step": 5759
    },
    {
      "epoch": 0.518580206621801,
      "grad_norm": 0.9214332695123463,
      "learning_rate": 9.884813732821528e-06,
      "loss": 0.4934,
      "step": 5760
    },
    {
      "epoch": 0.518670237907673,
      "grad_norm": 0.8913804878149468,
      "learning_rate": 9.881897758706155e-06,
      "loss": 0.5345,
      "step": 5761
    },
    {
      "epoch": 0.5187602691935448,
      "grad_norm": 0.7198576632992214,
      "learning_rate": 9.878981794634274e-06,
      "loss": 0.49,
      "step": 5762
    },
    {
      "epoch": 0.5188503004794166,
      "grad_norm": 0.6483178480143523,
      "learning_rate": 9.876065840853855e-06,
      "loss": 0.5972,
      "step": 5763
    },
    {
      "epoch": 0.5189403317652884,
      "grad_norm": 0.5446367325639088,
      "learning_rate": 9.873149897612875e-06,
      "loss": 0.5036,
      "step": 5764
    },
    {
      "epoch": 0.5190303630511602,
      "grad_norm": 0.8005366434715866,
      "learning_rate": 9.870233965159303e-06,
      "loss": 0.5952,
      "step": 5765
    },
    {
      "epoch": 0.5191203943370322,
      "grad_norm": 0.6727971809152161,
      "learning_rate": 9.867318043741117e-06,
      "loss": 0.533,
      "step": 5766
    },
    {
      "epoch": 0.519210425622904,
      "grad_norm": 0.850985437677325,
      "learning_rate": 9.864402133606288e-06,
      "loss": 0.5299,
      "step": 5767
    },
    {
      "epoch": 0.5193004569087758,
      "grad_norm": 0.9104755538784195,
      "learning_rate": 9.861486235002786e-06,
      "loss": 0.6006,
      "step": 5768
    },
    {
      "epoch": 0.5193904881946476,
      "grad_norm": 0.637412682594504,
      "learning_rate": 9.858570348178577e-06,
      "loss": 0.5397,
      "step": 5769
    },
    {
      "epoch": 0.5194805194805194,
      "grad_norm": 0.7147290556606049,
      "learning_rate": 9.855654473381639e-06,
      "loss": 0.6033,
      "step": 5770
    },
    {
      "epoch": 0.5195705507663914,
      "grad_norm": 0.840863320651037,
      "learning_rate": 9.852738610859928e-06,
      "loss": 0.5452,
      "step": 5771
    },
    {
      "epoch": 0.5196605820522632,
      "grad_norm": 0.7258158963714089,
      "learning_rate": 9.849822760861423e-06,
      "loss": 0.5949,
      "step": 5772
    },
    {
      "epoch": 0.519750613338135,
      "grad_norm": 0.6754333350060403,
      "learning_rate": 9.846906923634079e-06,
      "loss": 0.5069,
      "step": 5773
    },
    {
      "epoch": 0.5198406446240068,
      "grad_norm": 0.6832401246212103,
      "learning_rate": 9.843991099425869e-06,
      "loss": 0.453,
      "step": 5774
    },
    {
      "epoch": 0.5199306759098787,
      "grad_norm": 0.8058081717912817,
      "learning_rate": 9.841075288484747e-06,
      "loss": 0.6024,
      "step": 5775
    },
    {
      "epoch": 0.5200207071957506,
      "grad_norm": 0.820315547379371,
      "learning_rate": 9.838159491058687e-06,
      "loss": 0.5972,
      "step": 5776
    },
    {
      "epoch": 0.5201107384816224,
      "grad_norm": 0.9250542087538768,
      "learning_rate": 9.835243707395644e-06,
      "loss": 0.5797,
      "step": 5777
    },
    {
      "epoch": 0.5202007697674942,
      "grad_norm": 0.7633896376313435,
      "learning_rate": 9.832327937743577e-06,
      "loss": 0.5498,
      "step": 5778
    },
    {
      "epoch": 0.520290801053366,
      "grad_norm": 0.6678706374161085,
      "learning_rate": 9.829412182350443e-06,
      "loss": 0.5167,
      "step": 5779
    },
    {
      "epoch": 0.5203808323392379,
      "grad_norm": 0.6957904347269529,
      "learning_rate": 9.826496441464208e-06,
      "loss": 0.56,
      "step": 5780
    },
    {
      "epoch": 0.5204708636251097,
      "grad_norm": 0.7867168072701788,
      "learning_rate": 9.823580715332821e-06,
      "loss": 0.5935,
      "step": 5781
    },
    {
      "epoch": 0.5205608949109816,
      "grad_norm": 0.8569878224796111,
      "learning_rate": 9.820665004204241e-06,
      "loss": 0.618,
      "step": 5782
    },
    {
      "epoch": 0.5206509261968534,
      "grad_norm": 0.795509197155632,
      "learning_rate": 9.817749308326417e-06,
      "loss": 0.5756,
      "step": 5783
    },
    {
      "epoch": 0.5207409574827252,
      "grad_norm": 0.7329412792752634,
      "learning_rate": 9.814833627947311e-06,
      "loss": 0.5582,
      "step": 5784
    },
    {
      "epoch": 0.5208309887685971,
      "grad_norm": 0.7692066393214446,
      "learning_rate": 9.811917963314862e-06,
      "loss": 0.4889,
      "step": 5785
    },
    {
      "epoch": 0.5209210200544689,
      "grad_norm": 0.6089221528893929,
      "learning_rate": 9.80900231467703e-06,
      "loss": 0.5452,
      "step": 5786
    },
    {
      "epoch": 0.5210110513403408,
      "grad_norm": 0.8289668792361352,
      "learning_rate": 9.806086682281759e-06,
      "loss": 0.4485,
      "step": 5787
    },
    {
      "epoch": 0.5211010826262126,
      "grad_norm": 0.6940420276233035,
      "learning_rate": 9.803171066376997e-06,
      "loss": 0.5758,
      "step": 5788
    },
    {
      "epoch": 0.5211911139120845,
      "grad_norm": 0.6558954140727886,
      "learning_rate": 9.800255467210687e-06,
      "loss": 0.5887,
      "step": 5789
    },
    {
      "epoch": 0.5212811451979563,
      "grad_norm": 0.8677381596709468,
      "learning_rate": 9.79733988503078e-06,
      "loss": 0.5355,
      "step": 5790
    },
    {
      "epoch": 0.5213711764838281,
      "grad_norm": 0.7946818844771827,
      "learning_rate": 9.794424320085212e-06,
      "loss": 0.5932,
      "step": 5791
    },
    {
      "epoch": 0.5214612077697,
      "grad_norm": 0.802866124624335,
      "learning_rate": 9.791508772621929e-06,
      "loss": 0.5195,
      "step": 5792
    },
    {
      "epoch": 0.5215512390555718,
      "grad_norm": 0.693816580771127,
      "learning_rate": 9.788593242888867e-06,
      "loss": 0.491,
      "step": 5793
    },
    {
      "epoch": 0.5216412703414437,
      "grad_norm": 0.7582278915905841,
      "learning_rate": 9.785677731133972e-06,
      "loss": 0.547,
      "step": 5794
    },
    {
      "epoch": 0.5217313016273155,
      "grad_norm": 0.7652787855938468,
      "learning_rate": 9.782762237605171e-06,
      "loss": 0.6652,
      "step": 5795
    },
    {
      "epoch": 0.5218213329131873,
      "grad_norm": 0.6606663501490104,
      "learning_rate": 9.779846762550408e-06,
      "loss": 0.5404,
      "step": 5796
    },
    {
      "epoch": 0.5219113641990591,
      "grad_norm": 0.808993269030791,
      "learning_rate": 9.776931306217609e-06,
      "loss": 0.5678,
      "step": 5797
    },
    {
      "epoch": 0.522001395484931,
      "grad_norm": 0.9006704737140449,
      "learning_rate": 9.774015868854714e-06,
      "loss": 0.6205,
      "step": 5798
    },
    {
      "epoch": 0.5220914267708029,
      "grad_norm": 0.7619075169960691,
      "learning_rate": 9.771100450709647e-06,
      "loss": 0.5074,
      "step": 5799
    },
    {
      "epoch": 0.5221814580566747,
      "grad_norm": 0.7994272724854157,
      "learning_rate": 9.768185052030342e-06,
      "loss": 0.5199,
      "step": 5800
    },
    {
      "epoch": 0.5222714893425465,
      "grad_norm": 0.6362270780749391,
      "learning_rate": 9.765269673064723e-06,
      "loss": 0.525,
      "step": 5801
    },
    {
      "epoch": 0.5223615206284183,
      "grad_norm": 1.0359947290971914,
      "learning_rate": 9.762354314060719e-06,
      "loss": 0.6192,
      "step": 5802
    },
    {
      "epoch": 0.5224515519142903,
      "grad_norm": 0.7628695301724949,
      "learning_rate": 9.759438975266245e-06,
      "loss": 0.4972,
      "step": 5803
    },
    {
      "epoch": 0.5225415832001621,
      "grad_norm": 0.758316372194829,
      "learning_rate": 9.756523656929237e-06,
      "loss": 0.4928,
      "step": 5804
    },
    {
      "epoch": 0.5226316144860339,
      "grad_norm": 0.8401144782409571,
      "learning_rate": 9.753608359297605e-06,
      "loss": 0.5439,
      "step": 5805
    },
    {
      "epoch": 0.5227216457719057,
      "grad_norm": 0.7074982986483479,
      "learning_rate": 9.750693082619274e-06,
      "loss": 0.4976,
      "step": 5806
    },
    {
      "epoch": 0.5228116770577775,
      "grad_norm": 0.7435749512368585,
      "learning_rate": 9.747777827142153e-06,
      "loss": 0.4829,
      "step": 5807
    },
    {
      "epoch": 0.5229017083436495,
      "grad_norm": 0.7157128051325017,
      "learning_rate": 9.744862593114167e-06,
      "loss": 0.5556,
      "step": 5808
    },
    {
      "epoch": 0.5229917396295213,
      "grad_norm": 0.7138229521547648,
      "learning_rate": 9.741947380783222e-06,
      "loss": 0.6134,
      "step": 5809
    },
    {
      "epoch": 0.5230817709153931,
      "grad_norm": 0.9568807379569748,
      "learning_rate": 9.739032190397233e-06,
      "loss": 0.5817,
      "step": 5810
    },
    {
      "epoch": 0.5231718022012649,
      "grad_norm": 0.8195403807365133,
      "learning_rate": 9.736117022204106e-06,
      "loss": 0.5816,
      "step": 5811
    },
    {
      "epoch": 0.5232618334871367,
      "grad_norm": 0.763850909624976,
      "learning_rate": 9.733201876451758e-06,
      "loss": 0.4821,
      "step": 5812
    },
    {
      "epoch": 0.5233518647730087,
      "grad_norm": 0.8295922199775008,
      "learning_rate": 9.73028675338808e-06,
      "loss": 0.5771,
      "step": 5813
    },
    {
      "epoch": 0.5234418960588805,
      "grad_norm": 0.7821739891668387,
      "learning_rate": 9.727371653260991e-06,
      "loss": 0.5252,
      "step": 5814
    },
    {
      "epoch": 0.5235319273447523,
      "grad_norm": 0.7123079322341473,
      "learning_rate": 9.724456576318383e-06,
      "loss": 0.5832,
      "step": 5815
    },
    {
      "epoch": 0.5236219586306241,
      "grad_norm": 1.1372817561349244,
      "learning_rate": 9.72154152280816e-06,
      "loss": 0.5972,
      "step": 5816
    },
    {
      "epoch": 0.523711989916496,
      "grad_norm": 0.7202841678690671,
      "learning_rate": 9.718626492978213e-06,
      "loss": 0.5117,
      "step": 5817
    },
    {
      "epoch": 0.5238020212023679,
      "grad_norm": 0.7284775793545462,
      "learning_rate": 9.71571148707645e-06,
      "loss": 0.5339,
      "step": 5818
    },
    {
      "epoch": 0.5238920524882397,
      "grad_norm": 0.8798061885587338,
      "learning_rate": 9.712796505350756e-06,
      "loss": 0.5742,
      "step": 5819
    },
    {
      "epoch": 0.5239820837741115,
      "grad_norm": 0.7983924842990543,
      "learning_rate": 9.709881548049028e-06,
      "loss": 0.5393,
      "step": 5820
    },
    {
      "epoch": 0.5240721150599833,
      "grad_norm": 0.777514844471569,
      "learning_rate": 9.706966615419148e-06,
      "loss": 0.6026,
      "step": 5821
    },
    {
      "epoch": 0.5241621463458552,
      "grad_norm": 0.7154960771844396,
      "learning_rate": 9.704051707709014e-06,
      "loss": 0.4739,
      "step": 5822
    },
    {
      "epoch": 0.524252177631727,
      "grad_norm": 0.8621525794634484,
      "learning_rate": 9.701136825166503e-06,
      "loss": 0.5614,
      "step": 5823
    },
    {
      "epoch": 0.5243422089175989,
      "grad_norm": 0.7211958045811632,
      "learning_rate": 9.698221968039505e-06,
      "loss": 0.4279,
      "step": 5824
    },
    {
      "epoch": 0.5244322402034707,
      "grad_norm": 0.7737224321955015,
      "learning_rate": 9.695307136575895e-06,
      "loss": 0.4816,
      "step": 5825
    },
    {
      "epoch": 0.5245222714893425,
      "grad_norm": 0.8718843429326117,
      "learning_rate": 9.692392331023563e-06,
      "loss": 0.516,
      "step": 5826
    },
    {
      "epoch": 0.5246123027752144,
      "grad_norm": 0.9242025651601289,
      "learning_rate": 9.68947755163037e-06,
      "loss": 0.5125,
      "step": 5827
    },
    {
      "epoch": 0.5247023340610862,
      "grad_norm": 0.6997272407386049,
      "learning_rate": 9.686562798644202e-06,
      "loss": 0.4886,
      "step": 5828
    },
    {
      "epoch": 0.5247923653469581,
      "grad_norm": 0.7713381642871303,
      "learning_rate": 9.683648072312927e-06,
      "loss": 0.5482,
      "step": 5829
    },
    {
      "epoch": 0.5248823966328299,
      "grad_norm": 0.8744102039847407,
      "learning_rate": 9.680733372884419e-06,
      "loss": 0.4403,
      "step": 5830
    },
    {
      "epoch": 0.5249724279187018,
      "grad_norm": 0.8488478196506712,
      "learning_rate": 9.677818700606538e-06,
      "loss": 0.5626,
      "step": 5831
    },
    {
      "epoch": 0.5250624592045736,
      "grad_norm": 0.7245794633038526,
      "learning_rate": 9.67490405572716e-06,
      "loss": 0.5433,
      "step": 5832
    },
    {
      "epoch": 0.5251524904904454,
      "grad_norm": 0.6666569213457632,
      "learning_rate": 9.67198943849414e-06,
      "loss": 0.5835,
      "step": 5833
    },
    {
      "epoch": 0.5252425217763173,
      "grad_norm": 0.6948869610262035,
      "learning_rate": 9.669074849155345e-06,
      "loss": 0.6073,
      "step": 5834
    },
    {
      "epoch": 0.5253325530621891,
      "grad_norm": 1.0319415615251064,
      "learning_rate": 9.666160287958625e-06,
      "loss": 0.4924,
      "step": 5835
    },
    {
      "epoch": 0.525422584348061,
      "grad_norm": 0.7591720661679711,
      "learning_rate": 9.663245755151847e-06,
      "loss": 0.5851,
      "step": 5836
    },
    {
      "epoch": 0.5255126156339328,
      "grad_norm": 0.7155332328119115,
      "learning_rate": 9.660331250982855e-06,
      "loss": 0.541,
      "step": 5837
    },
    {
      "epoch": 0.5256026469198046,
      "grad_norm": 0.6966046459484563,
      "learning_rate": 9.657416775699507e-06,
      "loss": 0.504,
      "step": 5838
    },
    {
      "epoch": 0.5256926782056764,
      "grad_norm": 0.6380991519307079,
      "learning_rate": 9.654502329549646e-06,
      "loss": 0.5494,
      "step": 5839
    },
    {
      "epoch": 0.5257827094915483,
      "grad_norm": 0.7761767215609243,
      "learning_rate": 9.651587912781127e-06,
      "loss": 0.4906,
      "step": 5840
    },
    {
      "epoch": 0.5258727407774202,
      "grad_norm": 0.8688854111297193,
      "learning_rate": 9.648673525641782e-06,
      "loss": 0.5515,
      "step": 5841
    },
    {
      "epoch": 0.525962772063292,
      "grad_norm": 0.6559491238475766,
      "learning_rate": 9.645759168379463e-06,
      "loss": 0.4509,
      "step": 5842
    },
    {
      "epoch": 0.5260528033491638,
      "grad_norm": 0.7747528412264952,
      "learning_rate": 9.642844841242002e-06,
      "loss": 0.494,
      "step": 5843
    },
    {
      "epoch": 0.5261428346350356,
      "grad_norm": 0.7460416377271353,
      "learning_rate": 9.63993054447724e-06,
      "loss": 0.5596,
      "step": 5844
    },
    {
      "epoch": 0.5262328659209076,
      "grad_norm": 0.9346284120343319,
      "learning_rate": 9.637016278333004e-06,
      "loss": 0.5476,
      "step": 5845
    },
    {
      "epoch": 0.5263228972067794,
      "grad_norm": 0.8489142577844907,
      "learning_rate": 9.634102043057131e-06,
      "loss": 0.6074,
      "step": 5846
    },
    {
      "epoch": 0.5264129284926512,
      "grad_norm": 0.6203030060738165,
      "learning_rate": 9.631187838897448e-06,
      "loss": 0.4973,
      "step": 5847
    },
    {
      "epoch": 0.526502959778523,
      "grad_norm": 0.8391855844141773,
      "learning_rate": 9.628273666101782e-06,
      "loss": 0.6322,
      "step": 5848
    },
    {
      "epoch": 0.5265929910643948,
      "grad_norm": 1.1552566426608017,
      "learning_rate": 9.62535952491795e-06,
      "loss": 0.5492,
      "step": 5849
    },
    {
      "epoch": 0.5266830223502668,
      "grad_norm": 0.6761975188185937,
      "learning_rate": 9.622445415593779e-06,
      "loss": 0.5039,
      "step": 5850
    },
    {
      "epoch": 0.5267730536361386,
      "grad_norm": 0.8350783497588166,
      "learning_rate": 9.619531338377084e-06,
      "loss": 0.5593,
      "step": 5851
    },
    {
      "epoch": 0.5268630849220104,
      "grad_norm": 0.783805891345084,
      "learning_rate": 9.616617293515682e-06,
      "loss": 0.5665,
      "step": 5852
    },
    {
      "epoch": 0.5269531162078822,
      "grad_norm": 0.8594690851883593,
      "learning_rate": 9.613703281257379e-06,
      "loss": 0.5833,
      "step": 5853
    },
    {
      "epoch": 0.527043147493754,
      "grad_norm": 0.7910308116945849,
      "learning_rate": 9.610789301849994e-06,
      "loss": 0.5399,
      "step": 5854
    },
    {
      "epoch": 0.527133178779626,
      "grad_norm": 0.9747108119143464,
      "learning_rate": 9.607875355541324e-06,
      "loss": 0.6034,
      "step": 5855
    },
    {
      "epoch": 0.5272232100654978,
      "grad_norm": 0.7472236873321876,
      "learning_rate": 9.60496144257918e-06,
      "loss": 0.5159,
      "step": 5856
    },
    {
      "epoch": 0.5273132413513696,
      "grad_norm": 0.7895170737536775,
      "learning_rate": 9.602047563211359e-06,
      "loss": 0.5715,
      "step": 5857
    },
    {
      "epoch": 0.5274032726372414,
      "grad_norm": 0.7275976726566611,
      "learning_rate": 9.599133717685663e-06,
      "loss": 0.4975,
      "step": 5858
    },
    {
      "epoch": 0.5274933039231133,
      "grad_norm": 0.8363772774773445,
      "learning_rate": 9.59621990624988e-06,
      "loss": 0.5929,
      "step": 5859
    },
    {
      "epoch": 0.5275833352089851,
      "grad_norm": 0.8092339293745311,
      "learning_rate": 9.593306129151811e-06,
      "loss": 0.5153,
      "step": 5860
    },
    {
      "epoch": 0.527673366494857,
      "grad_norm": 0.7501602851989855,
      "learning_rate": 9.590392386639238e-06,
      "loss": 0.5218,
      "step": 5861
    },
    {
      "epoch": 0.5277633977807288,
      "grad_norm": 0.6472369338706856,
      "learning_rate": 9.587478678959954e-06,
      "loss": 0.5125,
      "step": 5862
    },
    {
      "epoch": 0.5278534290666006,
      "grad_norm": 0.7562097265002182,
      "learning_rate": 9.584565006361735e-06,
      "loss": 0.58,
      "step": 5863
    },
    {
      "epoch": 0.5279434603524725,
      "grad_norm": 0.716676830785701,
      "learning_rate": 9.58165136909237e-06,
      "loss": 0.4946,
      "step": 5864
    },
    {
      "epoch": 0.5280334916383443,
      "grad_norm": 0.9245590322201059,
      "learning_rate": 9.578737767399629e-06,
      "loss": 0.6082,
      "step": 5865
    },
    {
      "epoch": 0.5281235229242162,
      "grad_norm": 0.6557964103652345,
      "learning_rate": 9.575824201531295e-06,
      "loss": 0.5199,
      "step": 5866
    },
    {
      "epoch": 0.528213554210088,
      "grad_norm": 0.7704768259415178,
      "learning_rate": 9.57291067173513e-06,
      "loss": 0.6217,
      "step": 5867
    },
    {
      "epoch": 0.5283035854959598,
      "grad_norm": 0.7872463703811012,
      "learning_rate": 9.569997178258909e-06,
      "loss": 0.567,
      "step": 5868
    },
    {
      "epoch": 0.5283936167818317,
      "grad_norm": 0.7857658177753828,
      "learning_rate": 9.56708372135039e-06,
      "loss": 0.5128,
      "step": 5869
    },
    {
      "epoch": 0.5284836480677035,
      "grad_norm": 0.9225300111525527,
      "learning_rate": 9.564170301257344e-06,
      "loss": 0.6141,
      "step": 5870
    },
    {
      "epoch": 0.5285736793535754,
      "grad_norm": 0.9418868701255484,
      "learning_rate": 9.561256918227527e-06,
      "loss": 0.565,
      "step": 5871
    },
    {
      "epoch": 0.5286637106394472,
      "grad_norm": 0.7774628311277351,
      "learning_rate": 9.558343572508695e-06,
      "loss": 0.5541,
      "step": 5872
    },
    {
      "epoch": 0.5287537419253191,
      "grad_norm": 0.9234332964718228,
      "learning_rate": 9.555430264348594e-06,
      "loss": 0.5843,
      "step": 5873
    },
    {
      "epoch": 0.5288437732111909,
      "grad_norm": 0.8751246542103605,
      "learning_rate": 9.552516993994984e-06,
      "loss": 0.4975,
      "step": 5874
    },
    {
      "epoch": 0.5289338044970627,
      "grad_norm": 0.9543684620234322,
      "learning_rate": 9.549603761695607e-06,
      "loss": 0.5036,
      "step": 5875
    },
    {
      "epoch": 0.5290238357829345,
      "grad_norm": 0.6252152316632331,
      "learning_rate": 9.546690567698206e-06,
      "loss": 0.5116,
      "step": 5876
    },
    {
      "epoch": 0.5291138670688064,
      "grad_norm": 0.9548452397260787,
      "learning_rate": 9.543777412250517e-06,
      "loss": 0.5647,
      "step": 5877
    },
    {
      "epoch": 0.5292038983546783,
      "grad_norm": 0.6720985039374094,
      "learning_rate": 9.540864295600282e-06,
      "loss": 0.5665,
      "step": 5878
    },
    {
      "epoch": 0.5292939296405501,
      "grad_norm": 0.6765305338532823,
      "learning_rate": 9.537951217995234e-06,
      "loss": 0.4542,
      "step": 5879
    },
    {
      "epoch": 0.5293839609264219,
      "grad_norm": 0.7594410908990333,
      "learning_rate": 9.535038179683101e-06,
      "loss": 0.5698,
      "step": 5880
    },
    {
      "epoch": 0.5294739922122937,
      "grad_norm": 0.7480015611027507,
      "learning_rate": 9.532125180911609e-06,
      "loss": 0.567,
      "step": 5881
    },
    {
      "epoch": 0.5295640234981656,
      "grad_norm": 0.6920056110001133,
      "learning_rate": 9.529212221928483e-06,
      "loss": 0.5032,
      "step": 5882
    },
    {
      "epoch": 0.5296540547840375,
      "grad_norm": 0.7391227938484941,
      "learning_rate": 9.526299302981438e-06,
      "loss": 0.5399,
      "step": 5883
    },
    {
      "epoch": 0.5297440860699093,
      "grad_norm": 0.7055671949644119,
      "learning_rate": 9.5233864243182e-06,
      "loss": 0.6506,
      "step": 5884
    },
    {
      "epoch": 0.5298341173557811,
      "grad_norm": 0.736163415199402,
      "learning_rate": 9.520473586186474e-06,
      "loss": 0.553,
      "step": 5885
    },
    {
      "epoch": 0.5299241486416529,
      "grad_norm": 0.6371120610935515,
      "learning_rate": 9.517560788833973e-06,
      "loss": 0.5766,
      "step": 5886
    },
    {
      "epoch": 0.5300141799275249,
      "grad_norm": 0.8681026619964938,
      "learning_rate": 9.5146480325084e-06,
      "loss": 0.5168,
      "step": 5887
    },
    {
      "epoch": 0.5301042112133967,
      "grad_norm": 0.7716364038113482,
      "learning_rate": 9.51173531745746e-06,
      "loss": 0.4962,
      "step": 5888
    },
    {
      "epoch": 0.5301942424992685,
      "grad_norm": 0.7540492138392609,
      "learning_rate": 9.508822643928853e-06,
      "loss": 0.5451,
      "step": 5889
    },
    {
      "epoch": 0.5302842737851403,
      "grad_norm": 0.6575001482557543,
      "learning_rate": 9.505910012170273e-06,
      "loss": 0.5067,
      "step": 5890
    },
    {
      "epoch": 0.5303743050710121,
      "grad_norm": 0.8021633724592181,
      "learning_rate": 9.50299742242941e-06,
      "loss": 0.5865,
      "step": 5891
    },
    {
      "epoch": 0.5304643363568841,
      "grad_norm": 0.6078451222128186,
      "learning_rate": 9.50008487495396e-06,
      "loss": 0.5003,
      "step": 5892
    },
    {
      "epoch": 0.5305543676427559,
      "grad_norm": 0.9208798554855068,
      "learning_rate": 9.497172369991599e-06,
      "loss": 0.582,
      "step": 5893
    },
    {
      "epoch": 0.5306443989286277,
      "grad_norm": 0.7233980997978637,
      "learning_rate": 9.494259907790015e-06,
      "loss": 0.5909,
      "step": 5894
    },
    {
      "epoch": 0.5307344302144995,
      "grad_norm": 1.1555686456970418,
      "learning_rate": 9.491347488596878e-06,
      "loss": 0.6009,
      "step": 5895
    },
    {
      "epoch": 0.5308244615003713,
      "grad_norm": 0.6713830774977951,
      "learning_rate": 9.48843511265987e-06,
      "loss": 0.5679,
      "step": 5896
    },
    {
      "epoch": 0.5309144927862433,
      "grad_norm": 1.614028230381321,
      "learning_rate": 9.485522780226653e-06,
      "loss": 0.6311,
      "step": 5897
    },
    {
      "epoch": 0.5310045240721151,
      "grad_norm": 0.6454263072961326,
      "learning_rate": 9.482610491544904e-06,
      "loss": 0.4496,
      "step": 5898
    },
    {
      "epoch": 0.5310945553579869,
      "grad_norm": 0.6393026174108076,
      "learning_rate": 9.479698246862277e-06,
      "loss": 0.5794,
      "step": 5899
    },
    {
      "epoch": 0.5311845866438587,
      "grad_norm": 0.811952463791622,
      "learning_rate": 9.476786046426435e-06,
      "loss": 0.5107,
      "step": 5900
    },
    {
      "epoch": 0.5312746179297306,
      "grad_norm": 0.6865055626461573,
      "learning_rate": 9.47387389048503e-06,
      "loss": 0.4959,
      "step": 5901
    },
    {
      "epoch": 0.5313646492156024,
      "grad_norm": 0.6750047213056025,
      "learning_rate": 9.47096177928572e-06,
      "loss": 0.5409,
      "step": 5902
    },
    {
      "epoch": 0.5314546805014743,
      "grad_norm": 0.6985881394592146,
      "learning_rate": 9.468049713076147e-06,
      "loss": 0.5898,
      "step": 5903
    },
    {
      "epoch": 0.5315447117873461,
      "grad_norm": 0.7508044733676339,
      "learning_rate": 9.465137692103958e-06,
      "loss": 0.583,
      "step": 5904
    },
    {
      "epoch": 0.5316347430732179,
      "grad_norm": 0.934123193940098,
      "learning_rate": 9.462225716616788e-06,
      "loss": 0.5351,
      "step": 5905
    },
    {
      "epoch": 0.5317247743590898,
      "grad_norm": 0.7398026031192527,
      "learning_rate": 9.459313786862281e-06,
      "loss": 0.5246,
      "step": 5906
    },
    {
      "epoch": 0.5318148056449616,
      "grad_norm": 0.9110219537657706,
      "learning_rate": 9.456401903088062e-06,
      "loss": 0.5498,
      "step": 5907
    },
    {
      "epoch": 0.5319048369308335,
      "grad_norm": 0.8141178368993789,
      "learning_rate": 9.453490065541767e-06,
      "loss": 0.5437,
      "step": 5908
    },
    {
      "epoch": 0.5319948682167053,
      "grad_norm": 0.7524673359317157,
      "learning_rate": 9.450578274471013e-06,
      "loss": 0.5303,
      "step": 5909
    },
    {
      "epoch": 0.5320848995025771,
      "grad_norm": 0.8010432752704314,
      "learning_rate": 9.447666530123427e-06,
      "loss": 0.4915,
      "step": 5910
    },
    {
      "epoch": 0.532174930788449,
      "grad_norm": 0.7383466141199685,
      "learning_rate": 9.444754832746616e-06,
      "loss": 0.5264,
      "step": 5911
    },
    {
      "epoch": 0.5322649620743208,
      "grad_norm": 0.8024692532657635,
      "learning_rate": 9.441843182588204e-06,
      "loss": 0.6053,
      "step": 5912
    },
    {
      "epoch": 0.5323549933601927,
      "grad_norm": 0.6544838502567706,
      "learning_rate": 9.438931579895793e-06,
      "loss": 0.5586,
      "step": 5913
    },
    {
      "epoch": 0.5324450246460645,
      "grad_norm": 0.5982532779578532,
      "learning_rate": 9.436020024916989e-06,
      "loss": 0.4881,
      "step": 5914
    },
    {
      "epoch": 0.5325350559319364,
      "grad_norm": 0.9572037156137675,
      "learning_rate": 9.43310851789939e-06,
      "loss": 0.5569,
      "step": 5915
    },
    {
      "epoch": 0.5326250872178082,
      "grad_norm": 0.9939689726340967,
      "learning_rate": 9.430197059090597e-06,
      "loss": 0.5607,
      "step": 5916
    },
    {
      "epoch": 0.53271511850368,
      "grad_norm": 0.9831487131753561,
      "learning_rate": 9.427285648738198e-06,
      "loss": 0.5868,
      "step": 5917
    },
    {
      "epoch": 0.5328051497895518,
      "grad_norm": 1.0529669445903678,
      "learning_rate": 9.424374287089788e-06,
      "loss": 0.6573,
      "step": 5918
    },
    {
      "epoch": 0.5328951810754237,
      "grad_norm": 0.6401336584124421,
      "learning_rate": 9.421462974392939e-06,
      "loss": 0.4741,
      "step": 5919
    },
    {
      "epoch": 0.5329852123612956,
      "grad_norm": 0.7060388633302999,
      "learning_rate": 9.418551710895243e-06,
      "loss": 0.4513,
      "step": 5920
    },
    {
      "epoch": 0.5330752436471674,
      "grad_norm": 0.8444168571671906,
      "learning_rate": 9.415640496844268e-06,
      "loss": 0.5142,
      "step": 5921
    },
    {
      "epoch": 0.5331652749330392,
      "grad_norm": 0.8284390029743991,
      "learning_rate": 9.412729332487591e-06,
      "loss": 0.6101,
      "step": 5922
    },
    {
      "epoch": 0.533255306218911,
      "grad_norm": 0.7433517406327522,
      "learning_rate": 9.409818218072774e-06,
      "loss": 0.5658,
      "step": 5923
    },
    {
      "epoch": 0.5333453375047829,
      "grad_norm": 1.0312768425121213,
      "learning_rate": 9.406907153847382e-06,
      "loss": 0.5691,
      "step": 5924
    },
    {
      "epoch": 0.5334353687906548,
      "grad_norm": 0.7049530086823088,
      "learning_rate": 9.403996140058972e-06,
      "loss": 0.4749,
      "step": 5925
    },
    {
      "epoch": 0.5335254000765266,
      "grad_norm": 0.7450000141476023,
      "learning_rate": 9.401085176955104e-06,
      "loss": 0.563,
      "step": 5926
    },
    {
      "epoch": 0.5336154313623984,
      "grad_norm": 0.7216942058940686,
      "learning_rate": 9.398174264783324e-06,
      "loss": 0.4547,
      "step": 5927
    },
    {
      "epoch": 0.5337054626482702,
      "grad_norm": 0.768587540033583,
      "learning_rate": 9.395263403791179e-06,
      "loss": 0.5025,
      "step": 5928
    },
    {
      "epoch": 0.5337954939341422,
      "grad_norm": 0.7148172380117493,
      "learning_rate": 9.392352594226205e-06,
      "loss": 0.6309,
      "step": 5929
    },
    {
      "epoch": 0.533885525220014,
      "grad_norm": 0.6982039618803408,
      "learning_rate": 9.389441836335948e-06,
      "loss": 0.6034,
      "step": 5930
    },
    {
      "epoch": 0.5339755565058858,
      "grad_norm": 0.7591408543820644,
      "learning_rate": 9.386531130367934e-06,
      "loss": 0.5216,
      "step": 5931
    },
    {
      "epoch": 0.5340655877917576,
      "grad_norm": 0.8566761121885875,
      "learning_rate": 9.383620476569697e-06,
      "loss": 0.562,
      "step": 5932
    },
    {
      "epoch": 0.5341556190776294,
      "grad_norm": 0.6816427655228267,
      "learning_rate": 9.380709875188752e-06,
      "loss": 0.4644,
      "step": 5933
    },
    {
      "epoch": 0.5342456503635014,
      "grad_norm": 0.7072377133238517,
      "learning_rate": 9.377799326472628e-06,
      "loss": 0.546,
      "step": 5934
    },
    {
      "epoch": 0.5343356816493732,
      "grad_norm": 0.7111932746792218,
      "learning_rate": 9.374888830668834e-06,
      "loss": 0.5378,
      "step": 5935
    },
    {
      "epoch": 0.534425712935245,
      "grad_norm": 0.7757708520189598,
      "learning_rate": 9.371978388024884e-06,
      "loss": 0.4672,
      "step": 5936
    },
    {
      "epoch": 0.5345157442211168,
      "grad_norm": 0.7033639898524525,
      "learning_rate": 9.36906799878828e-06,
      "loss": 0.4913,
      "step": 5937
    },
    {
      "epoch": 0.5346057755069887,
      "grad_norm": 0.6568317880988247,
      "learning_rate": 9.366157663206526e-06,
      "loss": 0.5764,
      "step": 5938
    },
    {
      "epoch": 0.5346958067928606,
      "grad_norm": 0.7835238356304606,
      "learning_rate": 9.363247381527115e-06,
      "loss": 0.6159,
      "step": 5939
    },
    {
      "epoch": 0.5347858380787324,
      "grad_norm": 0.6864046418205003,
      "learning_rate": 9.360337153997546e-06,
      "loss": 0.5788,
      "step": 5940
    },
    {
      "epoch": 0.5348758693646042,
      "grad_norm": 0.6779406301268471,
      "learning_rate": 9.3574269808653e-06,
      "loss": 0.4476,
      "step": 5941
    },
    {
      "epoch": 0.534965900650476,
      "grad_norm": 0.8363090140201441,
      "learning_rate": 9.354516862377866e-06,
      "loss": 0.5569,
      "step": 5942
    },
    {
      "epoch": 0.5350559319363479,
      "grad_norm": 0.6458974875214372,
      "learning_rate": 9.351606798782713e-06,
      "loss": 0.517,
      "step": 5943
    },
    {
      "epoch": 0.5351459632222197,
      "grad_norm": 0.8167819316428533,
      "learning_rate": 9.348696790327327e-06,
      "loss": 0.5642,
      "step": 5944
    },
    {
      "epoch": 0.5352359945080916,
      "grad_norm": 0.8508717089070545,
      "learning_rate": 9.345786837259166e-06,
      "loss": 0.6233,
      "step": 5945
    },
    {
      "epoch": 0.5353260257939634,
      "grad_norm": 0.7395134244113637,
      "learning_rate": 9.3428769398257e-06,
      "loss": 0.5187,
      "step": 5946
    },
    {
      "epoch": 0.5354160570798352,
      "grad_norm": 0.7903327731072669,
      "learning_rate": 9.339967098274386e-06,
      "loss": 0.5112,
      "step": 5947
    },
    {
      "epoch": 0.5355060883657071,
      "grad_norm": 0.8286166344962131,
      "learning_rate": 9.337057312852681e-06,
      "loss": 0.4821,
      "step": 5948
    },
    {
      "epoch": 0.5355961196515789,
      "grad_norm": 0.8169763012252572,
      "learning_rate": 9.334147583808033e-06,
      "loss": 0.5613,
      "step": 5949
    },
    {
      "epoch": 0.5356861509374508,
      "grad_norm": 0.8393156255753851,
      "learning_rate": 9.331237911387889e-06,
      "loss": 0.5342,
      "step": 5950
    },
    {
      "epoch": 0.5357761822233226,
      "grad_norm": 0.6544645693043442,
      "learning_rate": 9.328328295839687e-06,
      "loss": 0.4944,
      "step": 5951
    },
    {
      "epoch": 0.5358662135091945,
      "grad_norm": 0.8004522935892203,
      "learning_rate": 9.325418737410868e-06,
      "loss": 0.5336,
      "step": 5952
    },
    {
      "epoch": 0.5359562447950663,
      "grad_norm": 0.6850557695652266,
      "learning_rate": 9.32250923634885e-06,
      "loss": 0.5165,
      "step": 5953
    },
    {
      "epoch": 0.5360462760809381,
      "grad_norm": 0.6766542531497819,
      "learning_rate": 9.319599792901074e-06,
      "loss": 0.4147,
      "step": 5954
    },
    {
      "epoch": 0.53613630736681,
      "grad_norm": 0.6708572463025633,
      "learning_rate": 9.316690407314952e-06,
      "loss": 0.522,
      "step": 5955
    },
    {
      "epoch": 0.5362263386526818,
      "grad_norm": 0.6834665742788386,
      "learning_rate": 9.313781079837904e-06,
      "loss": 0.4691,
      "step": 5956
    },
    {
      "epoch": 0.5363163699385537,
      "grad_norm": 0.8082118656796852,
      "learning_rate": 9.310871810717335e-06,
      "loss": 0.5824,
      "step": 5957
    },
    {
      "epoch": 0.5364064012244255,
      "grad_norm": 0.8139811968413919,
      "learning_rate": 9.307962600200659e-06,
      "loss": 0.5549,
      "step": 5958
    },
    {
      "epoch": 0.5364964325102973,
      "grad_norm": 0.8053257245503141,
      "learning_rate": 9.30505344853527e-06,
      "loss": 0.5776,
      "step": 5959
    },
    {
      "epoch": 0.5365864637961691,
      "grad_norm": 1.0559431336115737,
      "learning_rate": 9.302144355968571e-06,
      "loss": 0.583,
      "step": 5960
    },
    {
      "epoch": 0.536676495082041,
      "grad_norm": 0.9122303119860653,
      "learning_rate": 9.299235322747945e-06,
      "loss": 0.5212,
      "step": 5961
    },
    {
      "epoch": 0.5367665263679129,
      "grad_norm": 0.7533645777311425,
      "learning_rate": 9.296326349120786e-06,
      "loss": 0.4869,
      "step": 5962
    },
    {
      "epoch": 0.5368565576537847,
      "grad_norm": 0.9254046376043442,
      "learning_rate": 9.293417435334468e-06,
      "loss": 0.512,
      "step": 5963
    },
    {
      "epoch": 0.5369465889396565,
      "grad_norm": 0.9336512132503884,
      "learning_rate": 9.290508581636373e-06,
      "loss": 0.4972,
      "step": 5964
    },
    {
      "epoch": 0.5370366202255283,
      "grad_norm": 0.8309750544095108,
      "learning_rate": 9.287599788273868e-06,
      "loss": 0.6532,
      "step": 5965
    },
    {
      "epoch": 0.5371266515114003,
      "grad_norm": 0.797684223815863,
      "learning_rate": 9.28469105549432e-06,
      "loss": 0.4757,
      "step": 5966
    },
    {
      "epoch": 0.5372166827972721,
      "grad_norm": 0.729067152597995,
      "learning_rate": 9.281782383545084e-06,
      "loss": 0.5825,
      "step": 5967
    },
    {
      "epoch": 0.5373067140831439,
      "grad_norm": 0.7365218077549174,
      "learning_rate": 9.278873772673527e-06,
      "loss": 0.5299,
      "step": 5968
    },
    {
      "epoch": 0.5373967453690157,
      "grad_norm": 0.7909826766782216,
      "learning_rate": 9.275965223126988e-06,
      "loss": 0.5639,
      "step": 5969
    },
    {
      "epoch": 0.5374867766548875,
      "grad_norm": 0.8926461291815478,
      "learning_rate": 9.273056735152818e-06,
      "loss": 0.4098,
      "step": 5970
    },
    {
      "epoch": 0.5375768079407595,
      "grad_norm": 0.7232481001950244,
      "learning_rate": 9.270148308998352e-06,
      "loss": 0.539,
      "step": 5971
    },
    {
      "epoch": 0.5376668392266313,
      "grad_norm": 0.8269666668416368,
      "learning_rate": 9.26723994491093e-06,
      "loss": 0.6114,
      "step": 5972
    },
    {
      "epoch": 0.5377568705125031,
      "grad_norm": 0.9298242844129393,
      "learning_rate": 9.264331643137875e-06,
      "loss": 0.5872,
      "step": 5973
    },
    {
      "epoch": 0.5378469017983749,
      "grad_norm": 0.8551166533088808,
      "learning_rate": 9.261423403926518e-06,
      "loss": 0.5352,
      "step": 5974
    },
    {
      "epoch": 0.5379369330842467,
      "grad_norm": 0.6572973925707923,
      "learning_rate": 9.258515227524167e-06,
      "loss": 0.4667,
      "step": 5975
    },
    {
      "epoch": 0.5380269643701187,
      "grad_norm": 0.7285994524571167,
      "learning_rate": 9.255607114178146e-06,
      "loss": 0.5077,
      "step": 5976
    },
    {
      "epoch": 0.5381169956559905,
      "grad_norm": 0.8529858371805654,
      "learning_rate": 9.252699064135759e-06,
      "loss": 0.4847,
      "step": 5977
    },
    {
      "epoch": 0.5382070269418623,
      "grad_norm": 0.6657257406866056,
      "learning_rate": 9.249791077644306e-06,
      "loss": 0.5469,
      "step": 5978
    },
    {
      "epoch": 0.5382970582277341,
      "grad_norm": 0.7425219832451997,
      "learning_rate": 9.246883154951086e-06,
      "loss": 0.5962,
      "step": 5979
    },
    {
      "epoch": 0.538387089513606,
      "grad_norm": 0.7533570616304848,
      "learning_rate": 9.243975296303392e-06,
      "loss": 0.5566,
      "step": 5980
    },
    {
      "epoch": 0.5384771207994778,
      "grad_norm": 0.6856321594682957,
      "learning_rate": 9.241067501948502e-06,
      "loss": 0.5783,
      "step": 5981
    },
    {
      "epoch": 0.5385671520853497,
      "grad_norm": 0.6625732312373235,
      "learning_rate": 9.238159772133708e-06,
      "loss": 0.5109,
      "step": 5982
    },
    {
      "epoch": 0.5386571833712215,
      "grad_norm": 0.7569713786762421,
      "learning_rate": 9.23525210710628e-06,
      "loss": 0.5492,
      "step": 5983
    },
    {
      "epoch": 0.5387472146570933,
      "grad_norm": 0.6238240756074998,
      "learning_rate": 9.23234450711349e-06,
      "loss": 0.4941,
      "step": 5984
    },
    {
      "epoch": 0.5388372459429652,
      "grad_norm": 0.8655208689094679,
      "learning_rate": 9.229436972402592e-06,
      "loss": 0.5703,
      "step": 5985
    },
    {
      "epoch": 0.538927277228837,
      "grad_norm": 0.8618016742029337,
      "learning_rate": 9.226529503220858e-06,
      "loss": 0.5439,
      "step": 5986
    },
    {
      "epoch": 0.5390173085147089,
      "grad_norm": 0.7568315058720727,
      "learning_rate": 9.223622099815534e-06,
      "loss": 0.5359,
      "step": 5987
    },
    {
      "epoch": 0.5391073398005807,
      "grad_norm": 0.7480659463270765,
      "learning_rate": 9.220714762433871e-06,
      "loss": 0.5577,
      "step": 5988
    },
    {
      "epoch": 0.5391973710864525,
      "grad_norm": 0.7871680011026191,
      "learning_rate": 9.217807491323104e-06,
      "loss": 0.5651,
      "step": 5989
    },
    {
      "epoch": 0.5392874023723244,
      "grad_norm": 0.7487976099109446,
      "learning_rate": 9.214900286730477e-06,
      "loss": 0.5208,
      "step": 5990
    },
    {
      "epoch": 0.5393774336581962,
      "grad_norm": 0.8238708781716316,
      "learning_rate": 9.211993148903215e-06,
      "loss": 0.5254,
      "step": 5991
    },
    {
      "epoch": 0.539467464944068,
      "grad_norm": 0.6146680734973938,
      "learning_rate": 9.209086078088548e-06,
      "loss": 0.4931,
      "step": 5992
    },
    {
      "epoch": 0.5395574962299399,
      "grad_norm": 0.8308295914704653,
      "learning_rate": 9.20617907453369e-06,
      "loss": 0.5347,
      "step": 5993
    },
    {
      "epoch": 0.5396475275158118,
      "grad_norm": 0.6963126145050527,
      "learning_rate": 9.203272138485859e-06,
      "loss": 0.5171,
      "step": 5994
    },
    {
      "epoch": 0.5397375588016836,
      "grad_norm": 0.7044925311668799,
      "learning_rate": 9.200365270192253e-06,
      "loss": 0.4868,
      "step": 5995
    },
    {
      "epoch": 0.5398275900875554,
      "grad_norm": 0.6464321958537008,
      "learning_rate": 9.197458469900087e-06,
      "loss": 0.519,
      "step": 5996
    },
    {
      "epoch": 0.5399176213734272,
      "grad_norm": 0.7262189904025688,
      "learning_rate": 9.194551737856549e-06,
      "loss": 0.4696,
      "step": 5997
    },
    {
      "epoch": 0.5400076526592991,
      "grad_norm": 0.6302433697159038,
      "learning_rate": 9.191645074308835e-06,
      "loss": 0.5135,
      "step": 5998
    },
    {
      "epoch": 0.540097683945171,
      "grad_norm": 0.7972228595595462,
      "learning_rate": 9.18873847950412e-06,
      "loss": 0.5262,
      "step": 5999
    },
    {
      "epoch": 0.5401877152310428,
      "grad_norm": 0.7685313540101198,
      "learning_rate": 9.185831953689592e-06,
      "loss": 0.5335,
      "step": 6000
    },
    {
      "epoch": 0.5402777465169146,
      "grad_norm": 0.962734481652779,
      "learning_rate": 9.182925497112418e-06,
      "loss": 0.5142,
      "step": 6001
    },
    {
      "epoch": 0.5403677778027864,
      "grad_norm": 0.6303457270136851,
      "learning_rate": 9.180019110019772e-06,
      "loss": 0.5328,
      "step": 6002
    },
    {
      "epoch": 0.5404578090886583,
      "grad_norm": 0.9167008470671888,
      "learning_rate": 9.177112792658802e-06,
      "loss": 0.5566,
      "step": 6003
    },
    {
      "epoch": 0.5405478403745302,
      "grad_norm": 0.7272017039406774,
      "learning_rate": 9.174206545276678e-06,
      "loss": 0.5106,
      "step": 6004
    },
    {
      "epoch": 0.540637871660402,
      "grad_norm": 0.7142161086026664,
      "learning_rate": 9.171300368120539e-06,
      "loss": 0.4684,
      "step": 6005
    },
    {
      "epoch": 0.5407279029462738,
      "grad_norm": 0.8538140712714581,
      "learning_rate": 9.168394261437534e-06,
      "loss": 0.5518,
      "step": 6006
    },
    {
      "epoch": 0.5408179342321456,
      "grad_norm": 0.8381998065093608,
      "learning_rate": 9.165488225474795e-06,
      "loss": 0.4508,
      "step": 6007
    },
    {
      "epoch": 0.5409079655180176,
      "grad_norm": 0.8221286226016093,
      "learning_rate": 9.162582260479459e-06,
      "loss": 0.5452,
      "step": 6008
    },
    {
      "epoch": 0.5409979968038894,
      "grad_norm": 0.7239907570823112,
      "learning_rate": 9.15967636669864e-06,
      "loss": 0.5415,
      "step": 6009
    },
    {
      "epoch": 0.5410880280897612,
      "grad_norm": 0.7055060897627622,
      "learning_rate": 9.15677054437947e-06,
      "loss": 0.4919,
      "step": 6010
    },
    {
      "epoch": 0.541178059375633,
      "grad_norm": 0.8261161653771895,
      "learning_rate": 9.153864793769056e-06,
      "loss": 0.4865,
      "step": 6011
    },
    {
      "epoch": 0.5412680906615048,
      "grad_norm": 0.662992000520675,
      "learning_rate": 9.150959115114505e-06,
      "loss": 0.4767,
      "step": 6012
    },
    {
      "epoch": 0.5413581219473768,
      "grad_norm": 0.7503604062933967,
      "learning_rate": 9.148053508662917e-06,
      "loss": 0.5645,
      "step": 6013
    },
    {
      "epoch": 0.5414481532332486,
      "grad_norm": 1.000461641662583,
      "learning_rate": 9.145147974661388e-06,
      "loss": 0.5883,
      "step": 6014
    },
    {
      "epoch": 0.5415381845191204,
      "grad_norm": 0.856593919294798,
      "learning_rate": 9.142242513357006e-06,
      "loss": 0.5845,
      "step": 6015
    },
    {
      "epoch": 0.5416282158049922,
      "grad_norm": 0.8801664102508445,
      "learning_rate": 9.139337124996857e-06,
      "loss": 0.4997,
      "step": 6016
    },
    {
      "epoch": 0.541718247090864,
      "grad_norm": 0.9126664947382045,
      "learning_rate": 9.136431809828009e-06,
      "loss": 0.463,
      "step": 6017
    },
    {
      "epoch": 0.541808278376736,
      "grad_norm": 0.7111786664329454,
      "learning_rate": 9.133526568097536e-06,
      "loss": 0.4945,
      "step": 6018
    },
    {
      "epoch": 0.5418983096626078,
      "grad_norm": 0.9790524941412903,
      "learning_rate": 9.130621400052503e-06,
      "loss": 0.5413,
      "step": 6019
    },
    {
      "epoch": 0.5419883409484796,
      "grad_norm": 0.9843748970149356,
      "learning_rate": 9.127716305939969e-06,
      "loss": 0.5552,
      "step": 6020
    },
    {
      "epoch": 0.5420783722343514,
      "grad_norm": 0.6982783165772037,
      "learning_rate": 9.124811286006978e-06,
      "loss": 0.5285,
      "step": 6021
    },
    {
      "epoch": 0.5421684035202233,
      "grad_norm": 0.9673109933525272,
      "learning_rate": 9.121906340500582e-06,
      "loss": 0.5212,
      "step": 6022
    },
    {
      "epoch": 0.5422584348060951,
      "grad_norm": 1.2442959926307435,
      "learning_rate": 9.11900146966781e-06,
      "loss": 0.6228,
      "step": 6023
    },
    {
      "epoch": 0.542348466091967,
      "grad_norm": 0.7634814516620695,
      "learning_rate": 9.116096673755705e-06,
      "loss": 0.4977,
      "step": 6024
    },
    {
      "epoch": 0.5424384973778388,
      "grad_norm": 0.7460651533922321,
      "learning_rate": 9.113191953011287e-06,
      "loss": 0.5768,
      "step": 6025
    },
    {
      "epoch": 0.5425285286637106,
      "grad_norm": 0.8222656902677913,
      "learning_rate": 9.110287307681577e-06,
      "loss": 0.4882,
      "step": 6026
    },
    {
      "epoch": 0.5426185599495825,
      "grad_norm": 0.6676747071609269,
      "learning_rate": 9.107382738013582e-06,
      "loss": 0.5494,
      "step": 6027
    },
    {
      "epoch": 0.5427085912354543,
      "grad_norm": 0.8732318813328349,
      "learning_rate": 9.104478244254318e-06,
      "loss": 0.601,
      "step": 6028
    },
    {
      "epoch": 0.5427986225213262,
      "grad_norm": 0.6383123349261383,
      "learning_rate": 9.101573826650779e-06,
      "loss": 0.5396,
      "step": 6029
    },
    {
      "epoch": 0.542888653807198,
      "grad_norm": 0.728994822463507,
      "learning_rate": 9.09866948544996e-06,
      "loss": 0.4872,
      "step": 6030
    },
    {
      "epoch": 0.5429786850930698,
      "grad_norm": 0.7079426680080639,
      "learning_rate": 9.095765220898845e-06,
      "loss": 0.5366,
      "step": 6031
    },
    {
      "epoch": 0.5430687163789417,
      "grad_norm": 0.7480329287156633,
      "learning_rate": 9.092861033244421e-06,
      "loss": 0.5831,
      "step": 6032
    },
    {
      "epoch": 0.5431587476648135,
      "grad_norm": 0.8036879568197426,
      "learning_rate": 9.089956922733653e-06,
      "loss": 0.608,
      "step": 6033
    },
    {
      "epoch": 0.5432487789506854,
      "grad_norm": 0.7432272642332259,
      "learning_rate": 9.087052889613519e-06,
      "loss": 0.5379,
      "step": 6034
    },
    {
      "epoch": 0.5433388102365572,
      "grad_norm": 0.7226856722170419,
      "learning_rate": 9.084148934130971e-06,
      "loss": 0.5499,
      "step": 6035
    },
    {
      "epoch": 0.5434288415224291,
      "grad_norm": 0.7416310751257389,
      "learning_rate": 9.08124505653297e-06,
      "loss": 0.5104,
      "step": 6036
    },
    {
      "epoch": 0.5435188728083009,
      "grad_norm": 0.8133031328763688,
      "learning_rate": 9.078341257066452e-06,
      "loss": 0.5766,
      "step": 6037
    },
    {
      "epoch": 0.5436089040941727,
      "grad_norm": 0.792299557983586,
      "learning_rate": 9.075437535978374e-06,
      "loss": 0.5103,
      "step": 6038
    },
    {
      "epoch": 0.5436989353800445,
      "grad_norm": 0.7074559127719972,
      "learning_rate": 9.072533893515658e-06,
      "loss": 0.5983,
      "step": 6039
    },
    {
      "epoch": 0.5437889666659164,
      "grad_norm": 0.829587823697365,
      "learning_rate": 9.069630329925236e-06,
      "loss": 0.5625,
      "step": 6040
    },
    {
      "epoch": 0.5438789979517883,
      "grad_norm": 0.7479076455031909,
      "learning_rate": 9.066726845454027e-06,
      "loss": 0.5119,
      "step": 6041
    },
    {
      "epoch": 0.5439690292376601,
      "grad_norm": 0.9324931471969004,
      "learning_rate": 9.06382344034895e-06,
      "loss": 0.6083,
      "step": 6042
    },
    {
      "epoch": 0.5440590605235319,
      "grad_norm": 0.8132482955231441,
      "learning_rate": 9.060920114856907e-06,
      "loss": 0.4843,
      "step": 6043
    },
    {
      "epoch": 0.5441490918094037,
      "grad_norm": 0.672640611258945,
      "learning_rate": 9.058016869224801e-06,
      "loss": 0.4559,
      "step": 6044
    },
    {
      "epoch": 0.5442391230952756,
      "grad_norm": 0.7595044180763767,
      "learning_rate": 9.055113703699522e-06,
      "loss": 0.4879,
      "step": 6045
    },
    {
      "epoch": 0.5443291543811475,
      "grad_norm": 0.8168262719859841,
      "learning_rate": 9.052210618527966e-06,
      "loss": 0.5116,
      "step": 6046
    },
    {
      "epoch": 0.5444191856670193,
      "grad_norm": 0.8250588198362296,
      "learning_rate": 9.049307613957004e-06,
      "loss": 0.6103,
      "step": 6047
    },
    {
      "epoch": 0.5445092169528911,
      "grad_norm": 0.6973783632159354,
      "learning_rate": 9.046404690233516e-06,
      "loss": 0.5598,
      "step": 6048
    },
    {
      "epoch": 0.5445992482387629,
      "grad_norm": 0.6159347203305895,
      "learning_rate": 9.043501847604363e-06,
      "loss": 0.4625,
      "step": 6049
    },
    {
      "epoch": 0.5446892795246349,
      "grad_norm": 0.7600214150940885,
      "learning_rate": 9.040599086316408e-06,
      "loss": 0.5774,
      "step": 6050
    },
    {
      "epoch": 0.5447793108105067,
      "grad_norm": 0.7410388291121625,
      "learning_rate": 9.037696406616498e-06,
      "loss": 0.5289,
      "step": 6051
    },
    {
      "epoch": 0.5448693420963785,
      "grad_norm": 0.6854837089863285,
      "learning_rate": 9.03479380875149e-06,
      "loss": 0.5242,
      "step": 6052
    },
    {
      "epoch": 0.5449593733822503,
      "grad_norm": 0.7989293651346114,
      "learning_rate": 9.03189129296821e-06,
      "loss": 0.5548,
      "step": 6053
    },
    {
      "epoch": 0.5450494046681221,
      "grad_norm": 0.7676708044094279,
      "learning_rate": 9.0289888595135e-06,
      "loss": 0.5753,
      "step": 6054
    },
    {
      "epoch": 0.5451394359539941,
      "grad_norm": 0.8779726066599881,
      "learning_rate": 9.026086508634176e-06,
      "loss": 0.6605,
      "step": 6055
    },
    {
      "epoch": 0.5452294672398659,
      "grad_norm": 0.7643449599661387,
      "learning_rate": 9.023184240577064e-06,
      "loss": 0.5427,
      "step": 6056
    },
    {
      "epoch": 0.5453194985257377,
      "grad_norm": 0.7749792820669184,
      "learning_rate": 9.020282055588966e-06,
      "loss": 0.5047,
      "step": 6057
    },
    {
      "epoch": 0.5454095298116095,
      "grad_norm": 0.9609816927992127,
      "learning_rate": 9.017379953916696e-06,
      "loss": 0.5217,
      "step": 6058
    },
    {
      "epoch": 0.5454995610974813,
      "grad_norm": 0.7639431107607106,
      "learning_rate": 9.01447793580704e-06,
      "loss": 0.5443,
      "step": 6059
    },
    {
      "epoch": 0.5455895923833533,
      "grad_norm": 0.884381310408141,
      "learning_rate": 9.011576001506795e-06,
      "loss": 0.5284,
      "step": 6060
    },
    {
      "epoch": 0.5456796236692251,
      "grad_norm": 0.8357937647418622,
      "learning_rate": 9.00867415126274e-06,
      "loss": 0.5963,
      "step": 6061
    },
    {
      "epoch": 0.5457696549550969,
      "grad_norm": 0.7561303476126091,
      "learning_rate": 9.005772385321653e-06,
      "loss": 0.5624,
      "step": 6062
    },
    {
      "epoch": 0.5458596862409687,
      "grad_norm": 0.7890609625509097,
      "learning_rate": 9.002870703930296e-06,
      "loss": 0.5706,
      "step": 6063
    },
    {
      "epoch": 0.5459497175268406,
      "grad_norm": 0.7043863092767432,
      "learning_rate": 8.999969107335438e-06,
      "loss": 0.4564,
      "step": 6064
    },
    {
      "epoch": 0.5460397488127124,
      "grad_norm": 0.6580821794408906,
      "learning_rate": 8.997067595783822e-06,
      "loss": 0.5351,
      "step": 6065
    },
    {
      "epoch": 0.5461297800985843,
      "grad_norm": 0.7743958659201927,
      "learning_rate": 8.994166169522204e-06,
      "loss": 0.5514,
      "step": 6066
    },
    {
      "epoch": 0.5462198113844561,
      "grad_norm": 0.7460779614382789,
      "learning_rate": 8.991264828797319e-06,
      "loss": 0.5089,
      "step": 6067
    },
    {
      "epoch": 0.5463098426703279,
      "grad_norm": 1.003044216204371,
      "learning_rate": 8.9883635738559e-06,
      "loss": 0.5969,
      "step": 6068
    },
    {
      "epoch": 0.5463998739561998,
      "grad_norm": 0.9351969993192496,
      "learning_rate": 8.985462404944668e-06,
      "loss": 0.5414,
      "step": 6069
    },
    {
      "epoch": 0.5464899052420716,
      "grad_norm": 1.5163964644395898,
      "learning_rate": 8.982561322310347e-06,
      "loss": 0.5447,
      "step": 6070
    },
    {
      "epoch": 0.5465799365279435,
      "grad_norm": 0.8406904730189265,
      "learning_rate": 8.97966032619964e-06,
      "loss": 0.5713,
      "step": 6071
    },
    {
      "epoch": 0.5466699678138153,
      "grad_norm": 0.651968476320289,
      "learning_rate": 8.976759416859257e-06,
      "loss": 0.4544,
      "step": 6072
    },
    {
      "epoch": 0.5467599990996871,
      "grad_norm": 0.788371983968146,
      "learning_rate": 8.97385859453588e-06,
      "loss": 0.5289,
      "step": 6073
    },
    {
      "epoch": 0.546850030385559,
      "grad_norm": 0.8038855440735202,
      "learning_rate": 8.970957859476214e-06,
      "loss": 0.6592,
      "step": 6074
    },
    {
      "epoch": 0.5469400616714308,
      "grad_norm": 0.7202758309033211,
      "learning_rate": 8.968057211926926e-06,
      "loss": 0.5478,
      "step": 6075
    },
    {
      "epoch": 0.5470300929573026,
      "grad_norm": 0.7962076002257283,
      "learning_rate": 8.965156652134696e-06,
      "loss": 0.6312,
      "step": 6076
    },
    {
      "epoch": 0.5471201242431745,
      "grad_norm": 0.7598097942629091,
      "learning_rate": 8.962256180346184e-06,
      "loss": 0.4606,
      "step": 6077
    },
    {
      "epoch": 0.5472101555290464,
      "grad_norm": 0.7303377755779707,
      "learning_rate": 8.959355796808055e-06,
      "loss": 0.498,
      "step": 6078
    },
    {
      "epoch": 0.5473001868149182,
      "grad_norm": 0.9851553564259391,
      "learning_rate": 8.95645550176695e-06,
      "loss": 0.462,
      "step": 6079
    },
    {
      "epoch": 0.54739021810079,
      "grad_norm": 0.7276776456415125,
      "learning_rate": 8.95355529546952e-06,
      "loss": 0.4939,
      "step": 6080
    },
    {
      "epoch": 0.5474802493866618,
      "grad_norm": 0.6773993271330836,
      "learning_rate": 8.950655178162397e-06,
      "loss": 0.4991,
      "step": 6081
    },
    {
      "epoch": 0.5475702806725337,
      "grad_norm": 0.7619094841566082,
      "learning_rate": 8.947755150092212e-06,
      "loss": 0.5486,
      "step": 6082
    },
    {
      "epoch": 0.5476603119584056,
      "grad_norm": 0.8371889708109822,
      "learning_rate": 8.944855211505575e-06,
      "loss": 0.5078,
      "step": 6083
    },
    {
      "epoch": 0.5477503432442774,
      "grad_norm": 0.6789590233473329,
      "learning_rate": 8.941955362649113e-06,
      "loss": 0.5786,
      "step": 6084
    },
    {
      "epoch": 0.5478403745301492,
      "grad_norm": 0.7281795073761337,
      "learning_rate": 8.93905560376942e-06,
      "loss": 0.5408,
      "step": 6085
    },
    {
      "epoch": 0.547930405816021,
      "grad_norm": 0.8044213776142979,
      "learning_rate": 8.936155935113101e-06,
      "loss": 0.5242,
      "step": 6086
    },
    {
      "epoch": 0.5480204371018929,
      "grad_norm": 0.9219956321924793,
      "learning_rate": 8.933256356926737e-06,
      "loss": 0.5615,
      "step": 6087
    },
    {
      "epoch": 0.5481104683877648,
      "grad_norm": 0.8671011576688809,
      "learning_rate": 8.93035686945692e-06,
      "loss": 0.6008,
      "step": 6088
    },
    {
      "epoch": 0.5482004996736366,
      "grad_norm": 0.6462468000961331,
      "learning_rate": 8.927457472950216e-06,
      "loss": 0.5237,
      "step": 6089
    },
    {
      "epoch": 0.5482905309595084,
      "grad_norm": 0.9288856824583481,
      "learning_rate": 8.924558167653195e-06,
      "loss": 0.5038,
      "step": 6090
    },
    {
      "epoch": 0.5483805622453802,
      "grad_norm": 0.824514362500497,
      "learning_rate": 8.921658953812416e-06,
      "loss": 0.4342,
      "step": 6091
    },
    {
      "epoch": 0.5484705935312522,
      "grad_norm": 0.7697033495301814,
      "learning_rate": 8.91875983167443e-06,
      "loss": 0.4771,
      "step": 6092
    },
    {
      "epoch": 0.548560624817124,
      "grad_norm": 0.7820040805221823,
      "learning_rate": 8.915860801485774e-06,
      "loss": 0.6029,
      "step": 6093
    },
    {
      "epoch": 0.5486506561029958,
      "grad_norm": 0.7296774924590921,
      "learning_rate": 8.912961863492994e-06,
      "loss": 0.5124,
      "step": 6094
    },
    {
      "epoch": 0.5487406873888676,
      "grad_norm": 0.7103280468138304,
      "learning_rate": 8.91006301794261e-06,
      "loss": 0.5486,
      "step": 6095
    },
    {
      "epoch": 0.5488307186747394,
      "grad_norm": 0.7642010973742793,
      "learning_rate": 8.907164265081144e-06,
      "loss": 0.4686,
      "step": 6096
    },
    {
      "epoch": 0.5489207499606114,
      "grad_norm": 0.7117037084770688,
      "learning_rate": 8.904265605155106e-06,
      "loss": 0.4682,
      "step": 6097
    },
    {
      "epoch": 0.5490107812464832,
      "grad_norm": 0.6203867083707446,
      "learning_rate": 8.901367038411005e-06,
      "loss": 0.5081,
      "step": 6098
    },
    {
      "epoch": 0.549100812532355,
      "grad_norm": 0.8031983509107564,
      "learning_rate": 8.89846856509533e-06,
      "loss": 0.5645,
      "step": 6099
    },
    {
      "epoch": 0.5491908438182268,
      "grad_norm": 0.7412653378271762,
      "learning_rate": 8.895570185454575e-06,
      "loss": 0.5445,
      "step": 6100
    },
    {
      "epoch": 0.5492808751040986,
      "grad_norm": 0.7528554649941512,
      "learning_rate": 8.892671899735213e-06,
      "loss": 0.4938,
      "step": 6101
    },
    {
      "epoch": 0.5493709063899705,
      "grad_norm": 0.6834793479805729,
      "learning_rate": 8.889773708183724e-06,
      "loss": 0.5218,
      "step": 6102
    },
    {
      "epoch": 0.5494609376758424,
      "grad_norm": 0.7926794210752418,
      "learning_rate": 8.886875611046567e-06,
      "loss": 0.592,
      "step": 6103
    },
    {
      "epoch": 0.5495509689617142,
      "grad_norm": 1.0894066519609413,
      "learning_rate": 8.883977608570201e-06,
      "loss": 0.5609,
      "step": 6104
    },
    {
      "epoch": 0.549641000247586,
      "grad_norm": 0.9002876826856653,
      "learning_rate": 8.88107970100107e-06,
      "loss": 0.5542,
      "step": 6105
    },
    {
      "epoch": 0.5497310315334579,
      "grad_norm": 0.706782951295395,
      "learning_rate": 8.878181888585618e-06,
      "loss": 0.5419,
      "step": 6106
    },
    {
      "epoch": 0.5498210628193297,
      "grad_norm": 0.7011772061295717,
      "learning_rate": 8.875284171570272e-06,
      "loss": 0.5559,
      "step": 6107
    },
    {
      "epoch": 0.5499110941052016,
      "grad_norm": 0.8146150153779402,
      "learning_rate": 8.872386550201459e-06,
      "loss": 0.6202,
      "step": 6108
    },
    {
      "epoch": 0.5500011253910734,
      "grad_norm": 0.842134791476852,
      "learning_rate": 8.869489024725595e-06,
      "loss": 0.5999,
      "step": 6109
    },
    {
      "epoch": 0.5500911566769452,
      "grad_norm": 0.7050079241729312,
      "learning_rate": 8.866591595389087e-06,
      "loss": 0.544,
      "step": 6110
    },
    {
      "epoch": 0.5501811879628171,
      "grad_norm": 0.8347214462422611,
      "learning_rate": 8.86369426243833e-06,
      "loss": 0.5607,
      "step": 6111
    },
    {
      "epoch": 0.5502712192486889,
      "grad_norm": 0.6672345688478627,
      "learning_rate": 8.860797026119723e-06,
      "loss": 0.5465,
      "step": 6112
    },
    {
      "epoch": 0.5503612505345608,
      "grad_norm": 0.7896127764841303,
      "learning_rate": 8.857899886679642e-06,
      "loss": 0.5476,
      "step": 6113
    },
    {
      "epoch": 0.5504512818204326,
      "grad_norm": 0.7037458676353132,
      "learning_rate": 8.855002844364466e-06,
      "loss": 0.4824,
      "step": 6114
    },
    {
      "epoch": 0.5505413131063044,
      "grad_norm": 0.715805440529235,
      "learning_rate": 8.852105899420553e-06,
      "loss": 0.5293,
      "step": 6115
    },
    {
      "epoch": 0.5506313443921763,
      "grad_norm": 0.5979967917479143,
      "learning_rate": 8.849209052094272e-06,
      "loss": 0.522,
      "step": 6116
    },
    {
      "epoch": 0.5507213756780481,
      "grad_norm": 1.0636208946578127,
      "learning_rate": 8.846312302631968e-06,
      "loss": 0.6432,
      "step": 6117
    },
    {
      "epoch": 0.55081140696392,
      "grad_norm": 0.8837290449651669,
      "learning_rate": 8.843415651279983e-06,
      "loss": 0.5904,
      "step": 6118
    },
    {
      "epoch": 0.5509014382497918,
      "grad_norm": 0.8908824910669975,
      "learning_rate": 8.840519098284646e-06,
      "loss": 0.5251,
      "step": 6119
    },
    {
      "epoch": 0.5509914695356637,
      "grad_norm": 0.736522140833676,
      "learning_rate": 8.837622643892288e-06,
      "loss": 0.587,
      "step": 6120
    },
    {
      "epoch": 0.5510815008215355,
      "grad_norm": 0.7054958696605879,
      "learning_rate": 8.834726288349217e-06,
      "loss": 0.5193,
      "step": 6121
    },
    {
      "epoch": 0.5511715321074073,
      "grad_norm": 0.6685120042332138,
      "learning_rate": 8.831830031901752e-06,
      "loss": 0.6067,
      "step": 6122
    },
    {
      "epoch": 0.5512615633932791,
      "grad_norm": 0.8229836457204625,
      "learning_rate": 8.828933874796183e-06,
      "loss": 0.5619,
      "step": 6123
    },
    {
      "epoch": 0.551351594679151,
      "grad_norm": 0.6249618221579363,
      "learning_rate": 8.826037817278807e-06,
      "loss": 0.5458,
      "step": 6124
    },
    {
      "epoch": 0.5514416259650229,
      "grad_norm": 0.7609413952128498,
      "learning_rate": 8.823141859595898e-06,
      "loss": 0.4888,
      "step": 6125
    },
    {
      "epoch": 0.5515316572508947,
      "grad_norm": 0.9035198715033531,
      "learning_rate": 8.820246001993743e-06,
      "loss": 0.6127,
      "step": 6126
    },
    {
      "epoch": 0.5516216885367665,
      "grad_norm": 0.7146699350731608,
      "learning_rate": 8.817350244718597e-06,
      "loss": 0.5878,
      "step": 6127
    },
    {
      "epoch": 0.5517117198226383,
      "grad_norm": 0.661684169547097,
      "learning_rate": 8.814454588016724e-06,
      "loss": 0.5036,
      "step": 6128
    },
    {
      "epoch": 0.5518017511085102,
      "grad_norm": 0.7443459212086986,
      "learning_rate": 8.811559032134363e-06,
      "loss": 0.4915,
      "step": 6129
    },
    {
      "epoch": 0.5518917823943821,
      "grad_norm": 0.9775114021891899,
      "learning_rate": 8.808663577317765e-06,
      "loss": 0.5718,
      "step": 6130
    },
    {
      "epoch": 0.5519818136802539,
      "grad_norm": 0.923998257880902,
      "learning_rate": 8.805768223813154e-06,
      "loss": 0.6643,
      "step": 6131
    },
    {
      "epoch": 0.5520718449661257,
      "grad_norm": 0.786570282673392,
      "learning_rate": 8.802872971866756e-06,
      "loss": 0.5196,
      "step": 6132
    },
    {
      "epoch": 0.5521618762519975,
      "grad_norm": 0.7624252657303664,
      "learning_rate": 8.799977821724783e-06,
      "loss": 0.5721,
      "step": 6133
    },
    {
      "epoch": 0.5522519075378695,
      "grad_norm": 0.8313990073349331,
      "learning_rate": 8.797082773633443e-06,
      "loss": 0.5778,
      "step": 6134
    },
    {
      "epoch": 0.5523419388237413,
      "grad_norm": 0.7207197383715808,
      "learning_rate": 8.794187827838926e-06,
      "loss": 0.5328,
      "step": 6135
    },
    {
      "epoch": 0.5524319701096131,
      "grad_norm": 0.8491803119072838,
      "learning_rate": 8.791292984587428e-06,
      "loss": 0.5808,
      "step": 6136
    },
    {
      "epoch": 0.5525220013954849,
      "grad_norm": 0.7807403762398891,
      "learning_rate": 8.788398244125124e-06,
      "loss": 0.4871,
      "step": 6137
    },
    {
      "epoch": 0.5526120326813567,
      "grad_norm": 0.6289095669342025,
      "learning_rate": 8.78550360669819e-06,
      "loss": 0.5176,
      "step": 6138
    },
    {
      "epoch": 0.5527020639672287,
      "grad_norm": 0.8252639267481162,
      "learning_rate": 8.782609072552777e-06,
      "loss": 0.5661,
      "step": 6139
    },
    {
      "epoch": 0.5527920952531005,
      "grad_norm": 0.7034327056098677,
      "learning_rate": 8.779714641935049e-06,
      "loss": 0.5003,
      "step": 6140
    },
    {
      "epoch": 0.5528821265389723,
      "grad_norm": 0.8272782740668877,
      "learning_rate": 8.776820315091144e-06,
      "loss": 0.5746,
      "step": 6141
    },
    {
      "epoch": 0.5529721578248441,
      "grad_norm": 0.8028353742639046,
      "learning_rate": 8.7739260922672e-06,
      "loss": 0.4775,
      "step": 6142
    },
    {
      "epoch": 0.553062189110716,
      "grad_norm": 0.7711583662200531,
      "learning_rate": 8.771031973709338e-06,
      "loss": 0.5798,
      "step": 6143
    },
    {
      "epoch": 0.5531522203965878,
      "grad_norm": 0.7508890706029835,
      "learning_rate": 8.768137959663687e-06,
      "loss": 0.5217,
      "step": 6144
    },
    {
      "epoch": 0.5532422516824597,
      "grad_norm": 0.8409169472423332,
      "learning_rate": 8.765244050376342e-06,
      "loss": 0.5797,
      "step": 6145
    },
    {
      "epoch": 0.5533322829683315,
      "grad_norm": 0.9666508014582892,
      "learning_rate": 8.762350246093416e-06,
      "loss": 0.5164,
      "step": 6146
    },
    {
      "epoch": 0.5534223142542033,
      "grad_norm": 0.7709468327405303,
      "learning_rate": 8.759456547060989e-06,
      "loss": 0.5027,
      "step": 6147
    },
    {
      "epoch": 0.5535123455400752,
      "grad_norm": 0.6741866372408544,
      "learning_rate": 8.756562953525151e-06,
      "loss": 0.5018,
      "step": 6148
    },
    {
      "epoch": 0.553602376825947,
      "grad_norm": 0.7292828425660133,
      "learning_rate": 8.753669465731968e-06,
      "loss": 0.5528,
      "step": 6149
    },
    {
      "epoch": 0.5536924081118189,
      "grad_norm": 0.8707052774583749,
      "learning_rate": 8.75077608392751e-06,
      "loss": 0.626,
      "step": 6150
    },
    {
      "epoch": 0.5537824393976907,
      "grad_norm": 0.7820851417797673,
      "learning_rate": 8.747882808357828e-06,
      "loss": 0.5573,
      "step": 6151
    },
    {
      "epoch": 0.5538724706835625,
      "grad_norm": 0.894282344001057,
      "learning_rate": 8.744989639268972e-06,
      "loss": 0.5432,
      "step": 6152
    },
    {
      "epoch": 0.5539625019694344,
      "grad_norm": 0.9642118917340871,
      "learning_rate": 8.74209657690697e-06,
      "loss": 0.6294,
      "step": 6153
    },
    {
      "epoch": 0.5540525332553062,
      "grad_norm": 0.9434397101718691,
      "learning_rate": 8.739203621517865e-06,
      "loss": 0.531,
      "step": 6154
    },
    {
      "epoch": 0.554142564541178,
      "grad_norm": 0.7348421329404938,
      "learning_rate": 8.736310773347661e-06,
      "loss": 0.5607,
      "step": 6155
    },
    {
      "epoch": 0.5542325958270499,
      "grad_norm": 0.8524294310901843,
      "learning_rate": 8.733418032642377e-06,
      "loss": 0.6051,
      "step": 6156
    },
    {
      "epoch": 0.5543226271129218,
      "grad_norm": 0.921786893826479,
      "learning_rate": 8.730525399648004e-06,
      "loss": 0.597,
      "step": 6157
    },
    {
      "epoch": 0.5544126583987936,
      "grad_norm": 0.7392425108421468,
      "learning_rate": 8.727632874610548e-06,
      "loss": 0.5656,
      "step": 6158
    },
    {
      "epoch": 0.5545026896846654,
      "grad_norm": 0.9884311841746956,
      "learning_rate": 8.724740457775976e-06,
      "loss": 0.5864,
      "step": 6159
    },
    {
      "epoch": 0.5545927209705372,
      "grad_norm": 0.9147140976114991,
      "learning_rate": 8.721848149390269e-06,
      "loss": 0.5484,
      "step": 6160
    },
    {
      "epoch": 0.5546827522564091,
      "grad_norm": 0.768064566366468,
      "learning_rate": 8.71895594969939e-06,
      "loss": 0.455,
      "step": 6161
    },
    {
      "epoch": 0.554772783542281,
      "grad_norm": 0.8228471836818597,
      "learning_rate": 8.716063858949293e-06,
      "loss": 0.5087,
      "step": 6162
    },
    {
      "epoch": 0.5548628148281528,
      "grad_norm": 0.8040347667350443,
      "learning_rate": 8.713171877385918e-06,
      "loss": 0.6082,
      "step": 6163
    },
    {
      "epoch": 0.5549528461140246,
      "grad_norm": 0.6710002635599279,
      "learning_rate": 8.71028000525521e-06,
      "loss": 0.4794,
      "step": 6164
    },
    {
      "epoch": 0.5550428773998964,
      "grad_norm": 0.8462577902366565,
      "learning_rate": 8.70738824280309e-06,
      "loss": 0.5089,
      "step": 6165
    },
    {
      "epoch": 0.5551329086857683,
      "grad_norm": 0.9615320354152227,
      "learning_rate": 8.704496590275479e-06,
      "loss": 0.5002,
      "step": 6166
    },
    {
      "epoch": 0.5552229399716402,
      "grad_norm": 0.7394321835415685,
      "learning_rate": 8.701605047918277e-06,
      "loss": 0.5136,
      "step": 6167
    },
    {
      "epoch": 0.555312971257512,
      "grad_norm": 0.8493195153111963,
      "learning_rate": 8.698713615977392e-06,
      "loss": 0.5728,
      "step": 6168
    },
    {
      "epoch": 0.5554030025433838,
      "grad_norm": 0.9515470807596698,
      "learning_rate": 8.695822294698708e-06,
      "loss": 0.5227,
      "step": 6169
    },
    {
      "epoch": 0.5554930338292556,
      "grad_norm": 0.7808555712554022,
      "learning_rate": 8.692931084328107e-06,
      "loss": 0.6617,
      "step": 6170
    },
    {
      "epoch": 0.5555830651151276,
      "grad_norm": 0.9008438081223301,
      "learning_rate": 8.690039985111455e-06,
      "loss": 0.6064,
      "step": 6171
    },
    {
      "epoch": 0.5556730964009994,
      "grad_norm": 1.1255640729081962,
      "learning_rate": 8.687148997294622e-06,
      "loss": 0.5485,
      "step": 6172
    },
    {
      "epoch": 0.5557631276868712,
      "grad_norm": 0.6148569135325788,
      "learning_rate": 8.684258121123447e-06,
      "loss": 0.4815,
      "step": 6173
    },
    {
      "epoch": 0.555853158972743,
      "grad_norm": 0.6741168826343711,
      "learning_rate": 8.681367356843782e-06,
      "loss": 0.5541,
      "step": 6174
    },
    {
      "epoch": 0.5559431902586148,
      "grad_norm": 0.6916550107322179,
      "learning_rate": 8.678476704701455e-06,
      "loss": 0.5355,
      "step": 6175
    },
    {
      "epoch": 0.5560332215444868,
      "grad_norm": 0.7525890558599054,
      "learning_rate": 8.675586164942291e-06,
      "loss": 0.5304,
      "step": 6176
    },
    {
      "epoch": 0.5561232528303586,
      "grad_norm": 0.7742670511933735,
      "learning_rate": 8.672695737812096e-06,
      "loss": 0.5554,
      "step": 6177
    },
    {
      "epoch": 0.5562132841162304,
      "grad_norm": 0.7990396886096123,
      "learning_rate": 8.669805423556686e-06,
      "loss": 0.5406,
      "step": 6178
    },
    {
      "epoch": 0.5563033154021022,
      "grad_norm": 0.6107968471754694,
      "learning_rate": 8.666915222421847e-06,
      "loss": 0.4937,
      "step": 6179
    },
    {
      "epoch": 0.556393346687974,
      "grad_norm": 0.7962352367074127,
      "learning_rate": 8.664025134653367e-06,
      "loss": 0.5205,
      "step": 6180
    },
    {
      "epoch": 0.556483377973846,
      "grad_norm": 0.6623891354618963,
      "learning_rate": 8.661135160497014e-06,
      "loss": 0.4075,
      "step": 6181
    },
    {
      "epoch": 0.5565734092597178,
      "grad_norm": 1.033170310755791,
      "learning_rate": 8.658245300198562e-06,
      "loss": 0.537,
      "step": 6182
    },
    {
      "epoch": 0.5566634405455896,
      "grad_norm": 0.7930412311930946,
      "learning_rate": 8.655355554003763e-06,
      "loss": 0.489,
      "step": 6183
    },
    {
      "epoch": 0.5567534718314614,
      "grad_norm": 0.7707943811375051,
      "learning_rate": 8.652465922158363e-06,
      "loss": 0.548,
      "step": 6184
    },
    {
      "epoch": 0.5568435031173333,
      "grad_norm": 1.1249600151795136,
      "learning_rate": 8.649576404908094e-06,
      "loss": 0.4978,
      "step": 6185
    },
    {
      "epoch": 0.5569335344032051,
      "grad_norm": 0.8500852399144023,
      "learning_rate": 8.646687002498692e-06,
      "loss": 0.4317,
      "step": 6186
    },
    {
      "epoch": 0.557023565689077,
      "grad_norm": 1.1433475562547337,
      "learning_rate": 8.643797715175862e-06,
      "loss": 0.5441,
      "step": 6187
    },
    {
      "epoch": 0.5571135969749488,
      "grad_norm": 0.763739017099984,
      "learning_rate": 8.64090854318532e-06,
      "loss": 0.5622,
      "step": 6188
    },
    {
      "epoch": 0.5572036282608206,
      "grad_norm": 0.7117642040026163,
      "learning_rate": 8.638019486772758e-06,
      "loss": 0.5581,
      "step": 6189
    },
    {
      "epoch": 0.5572936595466925,
      "grad_norm": 0.7314504218496019,
      "learning_rate": 8.635130546183866e-06,
      "loss": 0.5691,
      "step": 6190
    },
    {
      "epoch": 0.5573836908325643,
      "grad_norm": 0.6864262735111356,
      "learning_rate": 8.632241721664316e-06,
      "loss": 0.5444,
      "step": 6191
    },
    {
      "epoch": 0.5574737221184362,
      "grad_norm": 0.7421288396088056,
      "learning_rate": 8.629353013459784e-06,
      "loss": 0.5073,
      "step": 6192
    },
    {
      "epoch": 0.557563753404308,
      "grad_norm": 0.6511455804993106,
      "learning_rate": 8.626464421815919e-06,
      "loss": 0.5456,
      "step": 6193
    },
    {
      "epoch": 0.5576537846901798,
      "grad_norm": 0.8255121343781853,
      "learning_rate": 8.623575946978376e-06,
      "loss": 0.5483,
      "step": 6194
    },
    {
      "epoch": 0.5577438159760517,
      "grad_norm": 0.8183230547118637,
      "learning_rate": 8.620687589192784e-06,
      "loss": 0.5682,
      "step": 6195
    },
    {
      "epoch": 0.5578338472619235,
      "grad_norm": 0.7683206924958654,
      "learning_rate": 8.617799348704782e-06,
      "loss": 0.4845,
      "step": 6196
    },
    {
      "epoch": 0.5579238785477953,
      "grad_norm": 1.1825408883986268,
      "learning_rate": 8.614911225759977e-06,
      "loss": 0.5881,
      "step": 6197
    },
    {
      "epoch": 0.5580139098336672,
      "grad_norm": 0.6700718178366565,
      "learning_rate": 8.612023220603986e-06,
      "loss": 0.4782,
      "step": 6198
    },
    {
      "epoch": 0.5581039411195391,
      "grad_norm": 0.7189265317634143,
      "learning_rate": 8.609135333482395e-06,
      "loss": 0.5023,
      "step": 6199
    },
    {
      "epoch": 0.5581939724054109,
      "grad_norm": 0.8784100747601941,
      "learning_rate": 8.606247564640808e-06,
      "loss": 0.5485,
      "step": 6200
    },
    {
      "epoch": 0.5582840036912827,
      "grad_norm": 0.795164738927987,
      "learning_rate": 8.603359914324786e-06,
      "loss": 0.4987,
      "step": 6201
    },
    {
      "epoch": 0.5583740349771545,
      "grad_norm": 0.6412328847106585,
      "learning_rate": 8.600472382779908e-06,
      "loss": 0.5816,
      "step": 6202
    },
    {
      "epoch": 0.5584640662630264,
      "grad_norm": 0.7406529456519761,
      "learning_rate": 8.597584970251726e-06,
      "loss": 0.5268,
      "step": 6203
    },
    {
      "epoch": 0.5585540975488983,
      "grad_norm": 0.7394195016138042,
      "learning_rate": 8.594697676985792e-06,
      "loss": 0.5201,
      "step": 6204
    },
    {
      "epoch": 0.5586441288347701,
      "grad_norm": 0.7351567131547851,
      "learning_rate": 8.591810503227634e-06,
      "loss": 0.5051,
      "step": 6205
    },
    {
      "epoch": 0.5587341601206419,
      "grad_norm": 0.9321831406526518,
      "learning_rate": 8.588923449222792e-06,
      "loss": 0.6317,
      "step": 6206
    },
    {
      "epoch": 0.5588241914065137,
      "grad_norm": 0.8747883939561759,
      "learning_rate": 8.586036515216771e-06,
      "loss": 0.5499,
      "step": 6207
    },
    {
      "epoch": 0.5589142226923856,
      "grad_norm": 0.7936131144657332,
      "learning_rate": 8.583149701455087e-06,
      "loss": 0.5232,
      "step": 6208
    },
    {
      "epoch": 0.5590042539782575,
      "grad_norm": 1.0272431886572104,
      "learning_rate": 8.580263008183228e-06,
      "loss": 0.4997,
      "step": 6209
    },
    {
      "epoch": 0.5590942852641293,
      "grad_norm": 0.704972566642431,
      "learning_rate": 8.577376435646686e-06,
      "loss": 0.5529,
      "step": 6210
    },
    {
      "epoch": 0.5591843165500011,
      "grad_norm": 0.8841893693360897,
      "learning_rate": 8.574489984090936e-06,
      "loss": 0.4423,
      "step": 6211
    },
    {
      "epoch": 0.5592743478358729,
      "grad_norm": 1.0026720807499703,
      "learning_rate": 8.571603653761443e-06,
      "loss": 0.5722,
      "step": 6212
    },
    {
      "epoch": 0.5593643791217449,
      "grad_norm": 0.7354531523417204,
      "learning_rate": 8.568717444903659e-06,
      "loss": 0.4599,
      "step": 6213
    },
    {
      "epoch": 0.5594544104076167,
      "grad_norm": 0.638159625648617,
      "learning_rate": 8.565831357763039e-06,
      "loss": 0.5495,
      "step": 6214
    },
    {
      "epoch": 0.5595444416934885,
      "grad_norm": 0.8674824686197393,
      "learning_rate": 8.562945392585004e-06,
      "loss": 0.4688,
      "step": 6215
    },
    {
      "epoch": 0.5596344729793603,
      "grad_norm": 0.9873411198721458,
      "learning_rate": 8.56005954961499e-06,
      "loss": 0.6106,
      "step": 6216
    },
    {
      "epoch": 0.5597245042652321,
      "grad_norm": 0.7261340452841716,
      "learning_rate": 8.557173829098403e-06,
      "loss": 0.5077,
      "step": 6217
    },
    {
      "epoch": 0.559814535551104,
      "grad_norm": 0.5839084088793629,
      "learning_rate": 8.554288231280651e-06,
      "loss": 0.4736,
      "step": 6218
    },
    {
      "epoch": 0.5599045668369759,
      "grad_norm": 0.6342783730592059,
      "learning_rate": 8.551402756407123e-06,
      "loss": 0.5302,
      "step": 6219
    },
    {
      "epoch": 0.5599945981228477,
      "grad_norm": 0.8798797183701494,
      "learning_rate": 8.548517404723207e-06,
      "loss": 0.4176,
      "step": 6220
    },
    {
      "epoch": 0.5600846294087195,
      "grad_norm": 0.7257708021382342,
      "learning_rate": 8.54563217647427e-06,
      "loss": 0.6008,
      "step": 6221
    },
    {
      "epoch": 0.5601746606945913,
      "grad_norm": 0.7157721844513173,
      "learning_rate": 8.54274707190568e-06,
      "loss": 0.4684,
      "step": 6222
    },
    {
      "epoch": 0.5602646919804632,
      "grad_norm": 0.8250255314919414,
      "learning_rate": 8.539862091262778e-06,
      "loss": 0.5483,
      "step": 6223
    },
    {
      "epoch": 0.5603547232663351,
      "grad_norm": 0.7934814589206469,
      "learning_rate": 8.536977234790914e-06,
      "loss": 0.5157,
      "step": 6224
    },
    {
      "epoch": 0.5604447545522069,
      "grad_norm": 0.7515399784988624,
      "learning_rate": 8.534092502735415e-06,
      "loss": 0.4713,
      "step": 6225
    },
    {
      "epoch": 0.5605347858380787,
      "grad_norm": 0.7646232002304737,
      "learning_rate": 8.5312078953416e-06,
      "loss": 0.5293,
      "step": 6226
    },
    {
      "epoch": 0.5606248171239506,
      "grad_norm": 1.0485231520667397,
      "learning_rate": 8.528323412854772e-06,
      "loss": 0.5652,
      "step": 6227
    },
    {
      "epoch": 0.5607148484098224,
      "grad_norm": 0.7901507541051341,
      "learning_rate": 8.525439055520243e-06,
      "loss": 0.5132,
      "step": 6228
    },
    {
      "epoch": 0.5608048796956943,
      "grad_norm": 1.3152961762986162,
      "learning_rate": 8.522554823583285e-06,
      "loss": 0.5078,
      "step": 6229
    },
    {
      "epoch": 0.5608949109815661,
      "grad_norm": 0.7671553573649155,
      "learning_rate": 8.519670717289188e-06,
      "loss": 0.4946,
      "step": 6230
    },
    {
      "epoch": 0.5609849422674379,
      "grad_norm": 0.8255400826906082,
      "learning_rate": 8.516786736883209e-06,
      "loss": 0.5669,
      "step": 6231
    },
    {
      "epoch": 0.5610749735533098,
      "grad_norm": 0.7519366648133438,
      "learning_rate": 8.513902882610608e-06,
      "loss": 0.565,
      "step": 6232
    },
    {
      "epoch": 0.5611650048391816,
      "grad_norm": 0.6912465364243341,
      "learning_rate": 8.511019154716626e-06,
      "loss": 0.5797,
      "step": 6233
    },
    {
      "epoch": 0.5612550361250535,
      "grad_norm": 0.8331436818415857,
      "learning_rate": 8.508135553446505e-06,
      "loss": 0.5521,
      "step": 6234
    },
    {
      "epoch": 0.5613450674109253,
      "grad_norm": 1.217876572266747,
      "learning_rate": 8.505252079045459e-06,
      "loss": 0.5819,
      "step": 6235
    },
    {
      "epoch": 0.5614350986967971,
      "grad_norm": 0.7401026492422557,
      "learning_rate": 8.502368731758706e-06,
      "loss": 0.541,
      "step": 6236
    },
    {
      "epoch": 0.561525129982669,
      "grad_norm": 0.8340155575483805,
      "learning_rate": 8.499485511831442e-06,
      "loss": 0.608,
      "step": 6237
    },
    {
      "epoch": 0.5616151612685408,
      "grad_norm": 0.8316554001228317,
      "learning_rate": 8.496602419508867e-06,
      "loss": 0.5809,
      "step": 6238
    },
    {
      "epoch": 0.5617051925544126,
      "grad_norm": 0.8273552495883549,
      "learning_rate": 8.493719455036154e-06,
      "loss": 0.5921,
      "step": 6239
    },
    {
      "epoch": 0.5617952238402845,
      "grad_norm": 0.8362806409856307,
      "learning_rate": 8.490836618658474e-06,
      "loss": 0.5183,
      "step": 6240
    },
    {
      "epoch": 0.5618852551261564,
      "grad_norm": 1.0386049833144155,
      "learning_rate": 8.487953910620983e-06,
      "loss": 0.6862,
      "step": 6241
    },
    {
      "epoch": 0.5619752864120282,
      "grad_norm": 1.1238783117229356,
      "learning_rate": 8.485071331168835e-06,
      "loss": 0.4791,
      "step": 6242
    },
    {
      "epoch": 0.5620653176979,
      "grad_norm": 0.652637283133856,
      "learning_rate": 8.482188880547156e-06,
      "loss": 0.4996,
      "step": 6243
    },
    {
      "epoch": 0.5621553489837718,
      "grad_norm": 0.9195203774278434,
      "learning_rate": 8.479306559001081e-06,
      "loss": 0.4869,
      "step": 6244
    },
    {
      "epoch": 0.5622453802696437,
      "grad_norm": 0.9340812181899465,
      "learning_rate": 8.47642436677572e-06,
      "loss": 0.5594,
      "step": 6245
    },
    {
      "epoch": 0.5623354115555156,
      "grad_norm": 0.7588302286027239,
      "learning_rate": 8.473542304116178e-06,
      "loss": 0.5375,
      "step": 6246
    },
    {
      "epoch": 0.5624254428413874,
      "grad_norm": 0.7927807521353397,
      "learning_rate": 8.47066037126754e-06,
      "loss": 0.5473,
      "step": 6247
    },
    {
      "epoch": 0.5625154741272592,
      "grad_norm": 0.7725068562109825,
      "learning_rate": 8.4677785684749e-06,
      "loss": 0.5504,
      "step": 6248
    },
    {
      "epoch": 0.562605505413131,
      "grad_norm": 0.7253431234833916,
      "learning_rate": 8.464896895983322e-06,
      "loss": 0.468,
      "step": 6249
    },
    {
      "epoch": 0.5626955366990029,
      "grad_norm": 0.826478386996489,
      "learning_rate": 8.462015354037863e-06,
      "loss": 0.5517,
      "step": 6250
    },
    {
      "epoch": 0.5627855679848748,
      "grad_norm": 0.8422847985356718,
      "learning_rate": 8.45913394288357e-06,
      "loss": 0.5697,
      "step": 6251
    },
    {
      "epoch": 0.5628755992707466,
      "grad_norm": 0.8227445711368931,
      "learning_rate": 8.45625266276549e-06,
      "loss": 0.5206,
      "step": 6252
    },
    {
      "epoch": 0.5629656305566184,
      "grad_norm": 0.9055072614407237,
      "learning_rate": 8.453371513928638e-06,
      "loss": 0.6081,
      "step": 6253
    },
    {
      "epoch": 0.5630556618424902,
      "grad_norm": 0.7615284572291046,
      "learning_rate": 8.450490496618035e-06,
      "loss": 0.6191,
      "step": 6254
    },
    {
      "epoch": 0.5631456931283622,
      "grad_norm": 0.8214153617640045,
      "learning_rate": 8.447609611078679e-06,
      "loss": 0.5164,
      "step": 6255
    },
    {
      "epoch": 0.563235724414234,
      "grad_norm": 0.9202360178426492,
      "learning_rate": 8.444728857555572e-06,
      "loss": 0.5123,
      "step": 6256
    },
    {
      "epoch": 0.5633257557001058,
      "grad_norm": 1.4493782140562959,
      "learning_rate": 8.441848236293682e-06,
      "loss": 0.5171,
      "step": 6257
    },
    {
      "epoch": 0.5634157869859776,
      "grad_norm": 0.8975500421826919,
      "learning_rate": 8.438967747537989e-06,
      "loss": 0.4976,
      "step": 6258
    },
    {
      "epoch": 0.5635058182718494,
      "grad_norm": 0.8782883781796418,
      "learning_rate": 8.436087391533446e-06,
      "loss": 0.5373,
      "step": 6259
    },
    {
      "epoch": 0.5635958495577214,
      "grad_norm": 0.8383328548167865,
      "learning_rate": 8.433207168525004e-06,
      "loss": 0.5425,
      "step": 6260
    },
    {
      "epoch": 0.5636858808435932,
      "grad_norm": 0.7418768215299015,
      "learning_rate": 8.430327078757594e-06,
      "loss": 0.5137,
      "step": 6261
    },
    {
      "epoch": 0.563775912129465,
      "grad_norm": 0.984979649156553,
      "learning_rate": 8.427447122476148e-06,
      "loss": 0.478,
      "step": 6262
    },
    {
      "epoch": 0.5638659434153368,
      "grad_norm": 0.657715534656855,
      "learning_rate": 8.424567299925575e-06,
      "loss": 0.4612,
      "step": 6263
    },
    {
      "epoch": 0.5639559747012086,
      "grad_norm": 0.6866329633308625,
      "learning_rate": 8.421687611350777e-06,
      "loss": 0.5412,
      "step": 6264
    },
    {
      "epoch": 0.5640460059870805,
      "grad_norm": 0.7164836929485542,
      "learning_rate": 8.41880805699664e-06,
      "loss": 0.5484,
      "step": 6265
    },
    {
      "epoch": 0.5641360372729524,
      "grad_norm": 0.7110156119556468,
      "learning_rate": 8.415928637108052e-06,
      "loss": 0.5147,
      "step": 6266
    },
    {
      "epoch": 0.5642260685588242,
      "grad_norm": 0.8594667198974189,
      "learning_rate": 8.413049351929877e-06,
      "loss": 0.5376,
      "step": 6267
    },
    {
      "epoch": 0.564316099844696,
      "grad_norm": 0.9031728586704161,
      "learning_rate": 8.410170201706969e-06,
      "loss": 0.5377,
      "step": 6268
    },
    {
      "epoch": 0.5644061311305679,
      "grad_norm": 0.8244999751143539,
      "learning_rate": 8.407291186684171e-06,
      "loss": 0.5114,
      "step": 6269
    },
    {
      "epoch": 0.5644961624164397,
      "grad_norm": 0.7066545925943147,
      "learning_rate": 8.404412307106327e-06,
      "loss": 0.5307,
      "step": 6270
    },
    {
      "epoch": 0.5645861937023116,
      "grad_norm": 0.638674760726383,
      "learning_rate": 8.401533563218244e-06,
      "loss": 0.4959,
      "step": 6271
    },
    {
      "epoch": 0.5646762249881834,
      "grad_norm": 0.754179186431768,
      "learning_rate": 8.398654955264743e-06,
      "loss": 0.5841,
      "step": 6272
    },
    {
      "epoch": 0.5647662562740552,
      "grad_norm": 0.6874489970901075,
      "learning_rate": 8.395776483490617e-06,
      "loss": 0.5269,
      "step": 6273
    },
    {
      "epoch": 0.5648562875599271,
      "grad_norm": 0.6886074962651612,
      "learning_rate": 8.392898148140658e-06,
      "loss": 0.5764,
      "step": 6274
    },
    {
      "epoch": 0.5649463188457989,
      "grad_norm": 0.6202742056201598,
      "learning_rate": 8.390019949459634e-06,
      "loss": 0.4387,
      "step": 6275
    },
    {
      "epoch": 0.5650363501316708,
      "grad_norm": 0.9240776958835142,
      "learning_rate": 8.387141887692316e-06,
      "loss": 0.6073,
      "step": 6276
    },
    {
      "epoch": 0.5651263814175426,
      "grad_norm": 0.752513420144943,
      "learning_rate": 8.384263963083453e-06,
      "loss": 0.5056,
      "step": 6277
    },
    {
      "epoch": 0.5652164127034144,
      "grad_norm": 0.75811552565377,
      "learning_rate": 8.381386175877788e-06,
      "loss": 0.4812,
      "step": 6278
    },
    {
      "epoch": 0.5653064439892863,
      "grad_norm": 0.7573688547804637,
      "learning_rate": 8.378508526320042e-06,
      "loss": 0.5498,
      "step": 6279
    },
    {
      "epoch": 0.5653964752751581,
      "grad_norm": 0.6709648994927124,
      "learning_rate": 8.375631014654944e-06,
      "loss": 0.505,
      "step": 6280
    },
    {
      "epoch": 0.56548650656103,
      "grad_norm": 0.8021653891372842,
      "learning_rate": 8.37275364112719e-06,
      "loss": 0.5649,
      "step": 6281
    },
    {
      "epoch": 0.5655765378469018,
      "grad_norm": 0.8877544258971541,
      "learning_rate": 8.369876405981477e-06,
      "loss": 0.4745,
      "step": 6282
    },
    {
      "epoch": 0.5656665691327737,
      "grad_norm": 1.0513529242392998,
      "learning_rate": 8.366999309462485e-06,
      "loss": 0.5844,
      "step": 6283
    },
    {
      "epoch": 0.5657566004186455,
      "grad_norm": 0.7865118094873592,
      "learning_rate": 8.364122351814891e-06,
      "loss": 0.4618,
      "step": 6284
    },
    {
      "epoch": 0.5658466317045173,
      "grad_norm": 0.6713064284215988,
      "learning_rate": 8.361245533283341e-06,
      "loss": 0.494,
      "step": 6285
    },
    {
      "epoch": 0.5659366629903891,
      "grad_norm": 0.825567893107402,
      "learning_rate": 8.358368854112495e-06,
      "loss": 0.4391,
      "step": 6286
    },
    {
      "epoch": 0.566026694276261,
      "grad_norm": 1.226954321536568,
      "learning_rate": 8.355492314546977e-06,
      "loss": 0.5149,
      "step": 6287
    },
    {
      "epoch": 0.5661167255621329,
      "grad_norm": 0.7712937530833704,
      "learning_rate": 8.352615914831416e-06,
      "loss": 0.509,
      "step": 6288
    },
    {
      "epoch": 0.5662067568480047,
      "grad_norm": 0.8234338652304749,
      "learning_rate": 8.349739655210416e-06,
      "loss": 0.5023,
      "step": 6289
    },
    {
      "epoch": 0.5662967881338765,
      "grad_norm": 0.7993682424320074,
      "learning_rate": 8.346863535928586e-06,
      "loss": 0.4919,
      "step": 6290
    },
    {
      "epoch": 0.5663868194197483,
      "grad_norm": 0.875372065380722,
      "learning_rate": 8.343987557230504e-06,
      "loss": 0.4907,
      "step": 6291
    },
    {
      "epoch": 0.5664768507056201,
      "grad_norm": 0.735114710085396,
      "learning_rate": 8.34111171936075e-06,
      "loss": 0.5222,
      "step": 6292
    },
    {
      "epoch": 0.5665668819914921,
      "grad_norm": 0.8519530212575765,
      "learning_rate": 8.338236022563883e-06,
      "loss": 0.516,
      "step": 6293
    },
    {
      "epoch": 0.5666569132773639,
      "grad_norm": 0.7478933420929075,
      "learning_rate": 8.33536046708446e-06,
      "loss": 0.5411,
      "step": 6294
    },
    {
      "epoch": 0.5667469445632357,
      "grad_norm": 0.824548788772043,
      "learning_rate": 8.332485053167014e-06,
      "loss": 0.5131,
      "step": 6295
    },
    {
      "epoch": 0.5668369758491075,
      "grad_norm": 0.8857239503117242,
      "learning_rate": 8.329609781056077e-06,
      "loss": 0.5678,
      "step": 6296
    },
    {
      "epoch": 0.5669270071349795,
      "grad_norm": 0.9478447444422654,
      "learning_rate": 8.326734650996158e-06,
      "loss": 0.4863,
      "step": 6297
    },
    {
      "epoch": 0.5670170384208513,
      "grad_norm": 0.7620094262226669,
      "learning_rate": 8.323859663231768e-06,
      "loss": 0.4188,
      "step": 6298
    },
    {
      "epoch": 0.5671070697067231,
      "grad_norm": 0.769116950866476,
      "learning_rate": 8.320984818007386e-06,
      "loss": 0.5465,
      "step": 6299
    },
    {
      "epoch": 0.5671971009925949,
      "grad_norm": 0.8280732642583895,
      "learning_rate": 8.318110115567505e-06,
      "loss": 0.5788,
      "step": 6300
    },
    {
      "epoch": 0.5672871322784667,
      "grad_norm": 0.7748299267696523,
      "learning_rate": 8.31523555615658e-06,
      "loss": 0.562,
      "step": 6301
    },
    {
      "epoch": 0.5673771635643386,
      "grad_norm": 0.7308520710434641,
      "learning_rate": 8.312361140019073e-06,
      "loss": 0.4854,
      "step": 6302
    },
    {
      "epoch": 0.5674671948502105,
      "grad_norm": 0.7703721777566208,
      "learning_rate": 8.309486867399416e-06,
      "loss": 0.5645,
      "step": 6303
    },
    {
      "epoch": 0.5675572261360823,
      "grad_norm": 0.7988110907144451,
      "learning_rate": 8.306612738542052e-06,
      "loss": 0.5356,
      "step": 6304
    },
    {
      "epoch": 0.5676472574219541,
      "grad_norm": 0.8326925211073011,
      "learning_rate": 8.303738753691389e-06,
      "loss": 0.532,
      "step": 6305
    },
    {
      "epoch": 0.5677372887078259,
      "grad_norm": 0.6983094500088216,
      "learning_rate": 8.300864913091838e-06,
      "loss": 0.5804,
      "step": 6306
    },
    {
      "epoch": 0.5678273199936978,
      "grad_norm": 0.8149814639794608,
      "learning_rate": 8.297991216987786e-06,
      "loss": 0.5597,
      "step": 6307
    },
    {
      "epoch": 0.5679173512795697,
      "grad_norm": 0.8557082499444235,
      "learning_rate": 8.29511766562362e-06,
      "loss": 0.5353,
      "step": 6308
    },
    {
      "epoch": 0.5680073825654415,
      "grad_norm": 0.8873020627772786,
      "learning_rate": 8.292244259243707e-06,
      "loss": 0.5956,
      "step": 6309
    },
    {
      "epoch": 0.5680974138513133,
      "grad_norm": 0.8749544236780396,
      "learning_rate": 8.289370998092403e-06,
      "loss": 0.599,
      "step": 6310
    },
    {
      "epoch": 0.5681874451371852,
      "grad_norm": 0.9566390183720618,
      "learning_rate": 8.286497882414048e-06,
      "loss": 0.4685,
      "step": 6311
    },
    {
      "epoch": 0.568277476423057,
      "grad_norm": 0.7777311654101143,
      "learning_rate": 8.283624912452983e-06,
      "loss": 0.5578,
      "step": 6312
    },
    {
      "epoch": 0.5683675077089289,
      "grad_norm": 0.8386240488831394,
      "learning_rate": 8.280752088453515e-06,
      "loss": 0.5141,
      "step": 6313
    },
    {
      "epoch": 0.5684575389948007,
      "grad_norm": 0.7711846150471127,
      "learning_rate": 8.277879410659962e-06,
      "loss": 0.5131,
      "step": 6314
    },
    {
      "epoch": 0.5685475702806725,
      "grad_norm": 0.7480390220954645,
      "learning_rate": 8.27500687931661e-06,
      "loss": 0.6157,
      "step": 6315
    },
    {
      "epoch": 0.5686376015665444,
      "grad_norm": 0.6931232354755671,
      "learning_rate": 8.272134494667746e-06,
      "loss": 0.535,
      "step": 6316
    },
    {
      "epoch": 0.5687276328524162,
      "grad_norm": 0.7255044277515624,
      "learning_rate": 8.269262256957634e-06,
      "loss": 0.5246,
      "step": 6317
    },
    {
      "epoch": 0.568817664138288,
      "grad_norm": 0.6601213812987772,
      "learning_rate": 8.266390166430538e-06,
      "loss": 0.5344,
      "step": 6318
    },
    {
      "epoch": 0.5689076954241599,
      "grad_norm": 0.7726204811886337,
      "learning_rate": 8.263518223330698e-06,
      "loss": 0.5087,
      "step": 6319
    },
    {
      "epoch": 0.5689977267100317,
      "grad_norm": 0.883257073209446,
      "learning_rate": 8.260646427902349e-06,
      "loss": 0.5066,
      "step": 6320
    },
    {
      "epoch": 0.5690877579959036,
      "grad_norm": 0.7676194711109533,
      "learning_rate": 8.2577747803897e-06,
      "loss": 0.5716,
      "step": 6321
    },
    {
      "epoch": 0.5691777892817754,
      "grad_norm": 0.7453110956673242,
      "learning_rate": 8.254903281036973e-06,
      "loss": 0.5434,
      "step": 6322
    },
    {
      "epoch": 0.5692678205676472,
      "grad_norm": 0.9336493591189078,
      "learning_rate": 8.252031930088351e-06,
      "loss": 0.5366,
      "step": 6323
    },
    {
      "epoch": 0.5693578518535191,
      "grad_norm": 1.0766640065815256,
      "learning_rate": 8.249160727788021e-06,
      "loss": 0.5757,
      "step": 6324
    },
    {
      "epoch": 0.569447883139391,
      "grad_norm": 0.6853511404757564,
      "learning_rate": 8.246289674380147e-06,
      "loss": 0.4927,
      "step": 6325
    },
    {
      "epoch": 0.5695379144252628,
      "grad_norm": 0.8125728027775989,
      "learning_rate": 8.243418770108894e-06,
      "loss": 0.4989,
      "step": 6326
    },
    {
      "epoch": 0.5696279457111346,
      "grad_norm": 0.8708145456561383,
      "learning_rate": 8.240548015218392e-06,
      "loss": 0.5929,
      "step": 6327
    },
    {
      "epoch": 0.5697179769970064,
      "grad_norm": 0.8986945569283309,
      "learning_rate": 8.237677409952784e-06,
      "loss": 0.5442,
      "step": 6328
    },
    {
      "epoch": 0.5698080082828783,
      "grad_norm": 0.6592651191347891,
      "learning_rate": 8.234806954556183e-06,
      "loss": 0.4993,
      "step": 6329
    },
    {
      "epoch": 0.5698980395687502,
      "grad_norm": 0.9619904436064064,
      "learning_rate": 8.231936649272696e-06,
      "loss": 0.5183,
      "step": 6330
    },
    {
      "epoch": 0.569988070854622,
      "grad_norm": 0.7331233814792453,
      "learning_rate": 8.229066494346408e-06,
      "loss": 0.6103,
      "step": 6331
    },
    {
      "epoch": 0.5700781021404938,
      "grad_norm": 0.8854455922232916,
      "learning_rate": 8.226196490021411e-06,
      "loss": 0.5161,
      "step": 6332
    },
    {
      "epoch": 0.5701681334263656,
      "grad_norm": 0.6425211724007294,
      "learning_rate": 8.223326636541762e-06,
      "loss": 0.6196,
      "step": 6333
    },
    {
      "epoch": 0.5702581647122374,
      "grad_norm": 0.8034537024736434,
      "learning_rate": 8.220456934151524e-06,
      "loss": 0.4772,
      "step": 6334
    },
    {
      "epoch": 0.5703481959981094,
      "grad_norm": 0.7016485325947037,
      "learning_rate": 8.217587383094727e-06,
      "loss": 0.5464,
      "step": 6335
    },
    {
      "epoch": 0.5704382272839812,
      "grad_norm": 0.7203282943772921,
      "learning_rate": 8.214717983615412e-06,
      "loss": 0.5402,
      "step": 6336
    },
    {
      "epoch": 0.570528258569853,
      "grad_norm": 0.8439541334771539,
      "learning_rate": 8.211848735957584e-06,
      "loss": 0.5664,
      "step": 6337
    },
    {
      "epoch": 0.5706182898557248,
      "grad_norm": 0.7767721480026771,
      "learning_rate": 8.208979640365253e-06,
      "loss": 0.5465,
      "step": 6338
    },
    {
      "epoch": 0.5707083211415968,
      "grad_norm": 0.8828777169406458,
      "learning_rate": 8.2061106970824e-06,
      "loss": 0.6062,
      "step": 6339
    },
    {
      "epoch": 0.5707983524274686,
      "grad_norm": 0.725982025024251,
      "learning_rate": 8.203241906353014e-06,
      "loss": 0.5375,
      "step": 6340
    },
    {
      "epoch": 0.5708883837133404,
      "grad_norm": 0.7991496856492434,
      "learning_rate": 8.200373268421048e-06,
      "loss": 0.4798,
      "step": 6341
    },
    {
      "epoch": 0.5709784149992122,
      "grad_norm": 0.8984171367979631,
      "learning_rate": 8.197504783530459e-06,
      "loss": 0.6041,
      "step": 6342
    },
    {
      "epoch": 0.571068446285084,
      "grad_norm": 0.8024857120797033,
      "learning_rate": 8.194636451925179e-06,
      "loss": 0.5693,
      "step": 6343
    },
    {
      "epoch": 0.571158477570956,
      "grad_norm": 0.7428392495324315,
      "learning_rate": 8.19176827384914e-06,
      "loss": 0.49,
      "step": 6344
    },
    {
      "epoch": 0.5712485088568278,
      "grad_norm": 0.83449211540505,
      "learning_rate": 8.188900249546244e-06,
      "loss": 0.5467,
      "step": 6345
    },
    {
      "epoch": 0.5713385401426996,
      "grad_norm": 0.9629989708013617,
      "learning_rate": 8.186032379260402e-06,
      "loss": 0.5373,
      "step": 6346
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.8259709592000635,
      "learning_rate": 8.18316466323549e-06,
      "loss": 0.4798,
      "step": 6347
    },
    {
      "epoch": 0.5715186027144433,
      "grad_norm": 0.8076194632302398,
      "learning_rate": 8.180297101715384e-06,
      "loss": 0.4729,
      "step": 6348
    },
    {
      "epoch": 0.5716086340003151,
      "grad_norm": 0.7910762634755357,
      "learning_rate": 8.17742969494394e-06,
      "loss": 0.5102,
      "step": 6349
    },
    {
      "epoch": 0.571698665286187,
      "grad_norm": 0.9414698386969165,
      "learning_rate": 8.174562443165008e-06,
      "loss": 0.6037,
      "step": 6350
    },
    {
      "epoch": 0.5717886965720588,
      "grad_norm": 0.7714283379026805,
      "learning_rate": 8.17169534662242e-06,
      "loss": 0.537,
      "step": 6351
    },
    {
      "epoch": 0.5718787278579306,
      "grad_norm": 0.7864391071917057,
      "learning_rate": 8.168828405559996e-06,
      "loss": 0.4709,
      "step": 6352
    },
    {
      "epoch": 0.5719687591438025,
      "grad_norm": 0.8936336788259562,
      "learning_rate": 8.165961620221537e-06,
      "loss": 0.6034,
      "step": 6353
    },
    {
      "epoch": 0.5720587904296743,
      "grad_norm": 1.0367004760396585,
      "learning_rate": 8.163094990850848e-06,
      "loss": 0.5153,
      "step": 6354
    },
    {
      "epoch": 0.5721488217155462,
      "grad_norm": 0.5906436588831384,
      "learning_rate": 8.160228517691692e-06,
      "loss": 0.4817,
      "step": 6355
    },
    {
      "epoch": 0.572238853001418,
      "grad_norm": 0.8772112216437812,
      "learning_rate": 8.157362200987852e-06,
      "loss": 0.5537,
      "step": 6356
    },
    {
      "epoch": 0.5723288842872898,
      "grad_norm": 0.9292730344322533,
      "learning_rate": 8.154496040983073e-06,
      "loss": 0.4383,
      "step": 6357
    },
    {
      "epoch": 0.5724189155731617,
      "grad_norm": 0.6973312473667943,
      "learning_rate": 8.1516300379211e-06,
      "loss": 0.556,
      "step": 6358
    },
    {
      "epoch": 0.5725089468590335,
      "grad_norm": 0.8687604523264905,
      "learning_rate": 8.14876419204565e-06,
      "loss": 0.6345,
      "step": 6359
    },
    {
      "epoch": 0.5725989781449053,
      "grad_norm": 0.8606975508342898,
      "learning_rate": 8.145898503600446e-06,
      "loss": 0.4839,
      "step": 6360
    },
    {
      "epoch": 0.5726890094307772,
      "grad_norm": 1.0943115480128633,
      "learning_rate": 8.143032972829184e-06,
      "loss": 0.6221,
      "step": 6361
    },
    {
      "epoch": 0.5727790407166491,
      "grad_norm": 0.7866549960007844,
      "learning_rate": 8.140167599975553e-06,
      "loss": 0.4784,
      "step": 6362
    },
    {
      "epoch": 0.5728690720025209,
      "grad_norm": 0.7615038605808919,
      "learning_rate": 8.137302385283219e-06,
      "loss": 0.4486,
      "step": 6363
    },
    {
      "epoch": 0.5729591032883927,
      "grad_norm": 0.6346839887536563,
      "learning_rate": 8.134437328995853e-06,
      "loss": 0.5041,
      "step": 6364
    },
    {
      "epoch": 0.5730491345742645,
      "grad_norm": 0.925518595800069,
      "learning_rate": 8.13157243135709e-06,
      "loss": 0.5335,
      "step": 6365
    },
    {
      "epoch": 0.5731391658601364,
      "grad_norm": 0.8220322178321724,
      "learning_rate": 8.128707692610572e-06,
      "loss": 0.5067,
      "step": 6366
    },
    {
      "epoch": 0.5732291971460083,
      "grad_norm": 0.7898074243247267,
      "learning_rate": 8.125843112999908e-06,
      "loss": 0.4906,
      "step": 6367
    },
    {
      "epoch": 0.5733192284318801,
      "grad_norm": 0.8988703816796647,
      "learning_rate": 8.122978692768716e-06,
      "loss": 0.5543,
      "step": 6368
    },
    {
      "epoch": 0.5734092597177519,
      "grad_norm": 1.0683828487156957,
      "learning_rate": 8.120114432160576e-06,
      "loss": 0.5412,
      "step": 6369
    },
    {
      "epoch": 0.5734992910036237,
      "grad_norm": 0.7326982440969821,
      "learning_rate": 8.117250331419075e-06,
      "loss": 0.4808,
      "step": 6370
    },
    {
      "epoch": 0.5735893222894956,
      "grad_norm": 0.6589363980239191,
      "learning_rate": 8.114386390787774e-06,
      "loss": 0.5062,
      "step": 6371
    },
    {
      "epoch": 0.5736793535753675,
      "grad_norm": 0.9550104830039103,
      "learning_rate": 8.111522610510227e-06,
      "loss": 0.4616,
      "step": 6372
    },
    {
      "epoch": 0.5737693848612393,
      "grad_norm": 0.8513273868326244,
      "learning_rate": 8.108658990829966e-06,
      "loss": 0.657,
      "step": 6373
    },
    {
      "epoch": 0.5738594161471111,
      "grad_norm": 0.706441415032656,
      "learning_rate": 8.105795531990522e-06,
      "loss": 0.5499,
      "step": 6374
    },
    {
      "epoch": 0.5739494474329829,
      "grad_norm": 0.7422115498394265,
      "learning_rate": 8.1029322342354e-06,
      "loss": 0.5191,
      "step": 6375
    },
    {
      "epoch": 0.5740394787188549,
      "grad_norm": 0.8687200403373524,
      "learning_rate": 8.100069097808103e-06,
      "loss": 0.5225,
      "step": 6376
    },
    {
      "epoch": 0.5741295100047267,
      "grad_norm": 0.8288586788210095,
      "learning_rate": 8.097206122952103e-06,
      "loss": 0.5273,
      "step": 6377
    },
    {
      "epoch": 0.5742195412905985,
      "grad_norm": 0.6672209185979316,
      "learning_rate": 8.09434330991088e-06,
      "loss": 0.4623,
      "step": 6378
    },
    {
      "epoch": 0.5743095725764703,
      "grad_norm": 0.7413680666161543,
      "learning_rate": 8.091480658927886e-06,
      "loss": 0.433,
      "step": 6379
    },
    {
      "epoch": 0.5743996038623421,
      "grad_norm": 0.9916192480294974,
      "learning_rate": 8.088618170246562e-06,
      "loss": 0.5464,
      "step": 6380
    },
    {
      "epoch": 0.574489635148214,
      "grad_norm": 0.8027297311176315,
      "learning_rate": 8.085755844110331e-06,
      "loss": 0.5216,
      "step": 6381
    },
    {
      "epoch": 0.5745796664340859,
      "grad_norm": 0.7934939119222427,
      "learning_rate": 8.082893680762619e-06,
      "loss": 0.5552,
      "step": 6382
    },
    {
      "epoch": 0.5746696977199577,
      "grad_norm": 0.7729597135016311,
      "learning_rate": 8.080031680446812e-06,
      "loss": 0.5874,
      "step": 6383
    },
    {
      "epoch": 0.5747597290058295,
      "grad_norm": 0.712106699442233,
      "learning_rate": 8.077169843406308e-06,
      "loss": 0.5094,
      "step": 6384
    },
    {
      "epoch": 0.5748497602917013,
      "grad_norm": 0.7622480685064127,
      "learning_rate": 8.07430816988447e-06,
      "loss": 0.5838,
      "step": 6385
    },
    {
      "epoch": 0.5749397915775732,
      "grad_norm": 0.7352187256770515,
      "learning_rate": 8.071446660124666e-06,
      "loss": 0.4995,
      "step": 6386
    },
    {
      "epoch": 0.5750298228634451,
      "grad_norm": 0.7360265802245143,
      "learning_rate": 8.06858531437023e-06,
      "loss": 0.5122,
      "step": 6387
    },
    {
      "epoch": 0.5751198541493169,
      "grad_norm": 0.8358158738364372,
      "learning_rate": 8.065724132864503e-06,
      "loss": 0.5358,
      "step": 6388
    },
    {
      "epoch": 0.5752098854351887,
      "grad_norm": 0.871014219005755,
      "learning_rate": 8.062863115850793e-06,
      "loss": 0.4794,
      "step": 6389
    },
    {
      "epoch": 0.5752999167210606,
      "grad_norm": 0.8123558037837944,
      "learning_rate": 8.06000226357241e-06,
      "loss": 0.6097,
      "step": 6390
    },
    {
      "epoch": 0.5753899480069324,
      "grad_norm": 0.7916477180208983,
      "learning_rate": 8.057141576272635e-06,
      "loss": 0.567,
      "step": 6391
    },
    {
      "epoch": 0.5754799792928043,
      "grad_norm": 0.75943086058629,
      "learning_rate": 8.05428105419475e-06,
      "loss": 0.5514,
      "step": 6392
    },
    {
      "epoch": 0.5755700105786761,
      "grad_norm": 0.6668248025723615,
      "learning_rate": 8.05142069758201e-06,
      "loss": 0.4658,
      "step": 6393
    },
    {
      "epoch": 0.5756600418645479,
      "grad_norm": 0.8552034489198327,
      "learning_rate": 8.048560506677667e-06,
      "loss": 0.489,
      "step": 6394
    },
    {
      "epoch": 0.5757500731504198,
      "grad_norm": 0.8749628540426105,
      "learning_rate": 8.045700481724947e-06,
      "loss": 0.4234,
      "step": 6395
    },
    {
      "epoch": 0.5758401044362916,
      "grad_norm": 0.874112568887791,
      "learning_rate": 8.042840622967076e-06,
      "loss": 0.5687,
      "step": 6396
    },
    {
      "epoch": 0.5759301357221634,
      "grad_norm": 0.8010727272134031,
      "learning_rate": 8.039980930647249e-06,
      "loss": 0.471,
      "step": 6397
    },
    {
      "epoch": 0.5760201670080353,
      "grad_norm": 0.8350077844431679,
      "learning_rate": 8.037121405008664e-06,
      "loss": 0.5346,
      "step": 6398
    },
    {
      "epoch": 0.5761101982939071,
      "grad_norm": 0.7710704798442033,
      "learning_rate": 8.03426204629449e-06,
      "loss": 0.5483,
      "step": 6399
    },
    {
      "epoch": 0.576200229579779,
      "grad_norm": 0.8368141788376674,
      "learning_rate": 8.031402854747896e-06,
      "loss": 0.6005,
      "step": 6400
    },
    {
      "epoch": 0.5762902608656508,
      "grad_norm": 0.7518829099853588,
      "learning_rate": 8.028543830612021e-06,
      "loss": 0.5542,
      "step": 6401
    },
    {
      "epoch": 0.5763802921515226,
      "grad_norm": 0.7684407931874565,
      "learning_rate": 8.025684974130009e-06,
      "loss": 0.5389,
      "step": 6402
    },
    {
      "epoch": 0.5764703234373945,
      "grad_norm": 0.8130288275246251,
      "learning_rate": 8.022826285544967e-06,
      "loss": 0.4775,
      "step": 6403
    },
    {
      "epoch": 0.5765603547232664,
      "grad_norm": 0.7340586863271708,
      "learning_rate": 8.01996776510001e-06,
      "loss": 0.5272,
      "step": 6404
    },
    {
      "epoch": 0.5766503860091382,
      "grad_norm": 0.9030231721706823,
      "learning_rate": 8.017109413038218e-06,
      "loss": 0.5669,
      "step": 6405
    },
    {
      "epoch": 0.57674041729501,
      "grad_norm": 0.7589759907762329,
      "learning_rate": 8.014251229602678e-06,
      "loss": 0.5082,
      "step": 6406
    },
    {
      "epoch": 0.5768304485808818,
      "grad_norm": 0.7889699730531224,
      "learning_rate": 8.011393215036444e-06,
      "loss": 0.4608,
      "step": 6407
    },
    {
      "epoch": 0.5769204798667537,
      "grad_norm": 0.8038848663624804,
      "learning_rate": 8.00853536958257e-06,
      "loss": 0.5107,
      "step": 6408
    },
    {
      "epoch": 0.5770105111526256,
      "grad_norm": 0.8204893131840596,
      "learning_rate": 8.005677693484077e-06,
      "loss": 0.4662,
      "step": 6409
    },
    {
      "epoch": 0.5771005424384974,
      "grad_norm": 0.8034608161125414,
      "learning_rate": 8.002820186983999e-06,
      "loss": 0.5362,
      "step": 6410
    },
    {
      "epoch": 0.5771905737243692,
      "grad_norm": 0.9760085739010544,
      "learning_rate": 7.999962850325326e-06,
      "loss": 0.5425,
      "step": 6411
    },
    {
      "epoch": 0.577280605010241,
      "grad_norm": 0.7000052718933157,
      "learning_rate": 7.99710568375106e-06,
      "loss": 0.5174,
      "step": 6412
    },
    {
      "epoch": 0.5773706362961128,
      "grad_norm": 0.8221540073199003,
      "learning_rate": 7.994248687504166e-06,
      "loss": 0.5007,
      "step": 6413
    },
    {
      "epoch": 0.5774606675819848,
      "grad_norm": 0.7414029599505603,
      "learning_rate": 7.991391861827612e-06,
      "loss": 0.4692,
      "step": 6414
    },
    {
      "epoch": 0.5775506988678566,
      "grad_norm": 0.6849740569974176,
      "learning_rate": 7.988535206964338e-06,
      "loss": 0.4784,
      "step": 6415
    },
    {
      "epoch": 0.5776407301537284,
      "grad_norm": 0.6939366005508322,
      "learning_rate": 7.985678723157283e-06,
      "loss": 0.56,
      "step": 6416
    },
    {
      "epoch": 0.5777307614396002,
      "grad_norm": 0.6995719268372058,
      "learning_rate": 7.982822410649356e-06,
      "loss": 0.566,
      "step": 6417
    },
    {
      "epoch": 0.5778207927254722,
      "grad_norm": 0.8679809124943699,
      "learning_rate": 7.979966269683468e-06,
      "loss": 0.483,
      "step": 6418
    },
    {
      "epoch": 0.577910824011344,
      "grad_norm": 0.8312704512434368,
      "learning_rate": 7.977110300502498e-06,
      "loss": 0.569,
      "step": 6419
    },
    {
      "epoch": 0.5780008552972158,
      "grad_norm": 0.6428972313104201,
      "learning_rate": 7.974254503349327e-06,
      "loss": 0.4598,
      "step": 6420
    },
    {
      "epoch": 0.5780908865830876,
      "grad_norm": 0.7095410036920281,
      "learning_rate": 7.97139887846681e-06,
      "loss": 0.5052,
      "step": 6421
    },
    {
      "epoch": 0.5781809178689594,
      "grad_norm": 1.2057426144263366,
      "learning_rate": 7.968543426097794e-06,
      "loss": 0.4666,
      "step": 6422
    },
    {
      "epoch": 0.5782709491548313,
      "grad_norm": 0.6980944655410362,
      "learning_rate": 7.965688146485105e-06,
      "loss": 0.429,
      "step": 6423
    },
    {
      "epoch": 0.5783609804407032,
      "grad_norm": 0.9639695356965378,
      "learning_rate": 7.962833039871562e-06,
      "loss": 0.4337,
      "step": 6424
    },
    {
      "epoch": 0.578451011726575,
      "grad_norm": 0.9391973666265718,
      "learning_rate": 7.959978106499956e-06,
      "loss": 0.5011,
      "step": 6425
    },
    {
      "epoch": 0.5785410430124468,
      "grad_norm": 0.8325765566438824,
      "learning_rate": 7.957123346613084e-06,
      "loss": 0.5092,
      "step": 6426
    },
    {
      "epoch": 0.5786310742983186,
      "grad_norm": 0.7576769452433888,
      "learning_rate": 7.954268760453709e-06,
      "loss": 0.565,
      "step": 6427
    },
    {
      "epoch": 0.5787211055841905,
      "grad_norm": 0.914839349703876,
      "learning_rate": 7.95141434826459e-06,
      "loss": 0.5323,
      "step": 6428
    },
    {
      "epoch": 0.5788111368700624,
      "grad_norm": 0.6498760351334483,
      "learning_rate": 7.948560110288463e-06,
      "loss": 0.5469,
      "step": 6429
    },
    {
      "epoch": 0.5789011681559342,
      "grad_norm": 0.7708688090186553,
      "learning_rate": 7.945706046768062e-06,
      "loss": 0.4467,
      "step": 6430
    },
    {
      "epoch": 0.578991199441806,
      "grad_norm": 0.9233947159332475,
      "learning_rate": 7.942852157946093e-06,
      "loss": 0.6169,
      "step": 6431
    },
    {
      "epoch": 0.5790812307276779,
      "grad_norm": 0.8596288934875936,
      "learning_rate": 7.939998444065255e-06,
      "loss": 0.4417,
      "step": 6432
    },
    {
      "epoch": 0.5791712620135497,
      "grad_norm": 0.9527572561260259,
      "learning_rate": 7.937144905368226e-06,
      "loss": 0.5357,
      "step": 6433
    },
    {
      "epoch": 0.5792612932994216,
      "grad_norm": 0.6811962581180375,
      "learning_rate": 7.934291542097678e-06,
      "loss": 0.5003,
      "step": 6434
    },
    {
      "epoch": 0.5793513245852934,
      "grad_norm": 0.9179023707024553,
      "learning_rate": 7.931438354496259e-06,
      "loss": 0.5677,
      "step": 6435
    },
    {
      "epoch": 0.5794413558711652,
      "grad_norm": 0.6966097123849266,
      "learning_rate": 7.928585342806607e-06,
      "loss": 0.4137,
      "step": 6436
    },
    {
      "epoch": 0.5795313871570371,
      "grad_norm": 0.6927183538371854,
      "learning_rate": 7.925732507271342e-06,
      "loss": 0.4673,
      "step": 6437
    },
    {
      "epoch": 0.5796214184429089,
      "grad_norm": 0.9304786089845322,
      "learning_rate": 7.922879848133076e-06,
      "loss": 0.4854,
      "step": 6438
    },
    {
      "epoch": 0.5797114497287807,
      "grad_norm": 1.0802857080769375,
      "learning_rate": 7.920027365634393e-06,
      "loss": 0.5877,
      "step": 6439
    },
    {
      "epoch": 0.5798014810146526,
      "grad_norm": 0.6581249262239647,
      "learning_rate": 7.917175060017879e-06,
      "loss": 0.4047,
      "step": 6440
    },
    {
      "epoch": 0.5798915123005244,
      "grad_norm": 1.0252382295048825,
      "learning_rate": 7.914322931526088e-06,
      "loss": 0.6175,
      "step": 6441
    },
    {
      "epoch": 0.5799815435863963,
      "grad_norm": 0.780399466919549,
      "learning_rate": 7.911470980401573e-06,
      "loss": 0.4829,
      "step": 6442
    },
    {
      "epoch": 0.5800715748722681,
      "grad_norm": 0.7176584804429441,
      "learning_rate": 7.908619206886857e-06,
      "loss": 0.4843,
      "step": 6443
    },
    {
      "epoch": 0.5801616061581399,
      "grad_norm": 0.9069122760314318,
      "learning_rate": 7.905767611224468e-06,
      "loss": 0.5333,
      "step": 6444
    },
    {
      "epoch": 0.5802516374440118,
      "grad_norm": 0.8554689745039951,
      "learning_rate": 7.902916193656898e-06,
      "loss": 0.6598,
      "step": 6445
    },
    {
      "epoch": 0.5803416687298837,
      "grad_norm": 0.8729145325654585,
      "learning_rate": 7.900064954426641e-06,
      "loss": 0.5091,
      "step": 6446
    },
    {
      "epoch": 0.5804317000157555,
      "grad_norm": 0.8552558996076267,
      "learning_rate": 7.897213893776159e-06,
      "loss": 0.6147,
      "step": 6447
    },
    {
      "epoch": 0.5805217313016273,
      "grad_norm": 0.725209694322078,
      "learning_rate": 7.894363011947917e-06,
      "loss": 0.4687,
      "step": 6448
    },
    {
      "epoch": 0.5806117625874991,
      "grad_norm": 0.7825975836876897,
      "learning_rate": 7.891512309184351e-06,
      "loss": 0.48,
      "step": 6449
    },
    {
      "epoch": 0.580701793873371,
      "grad_norm": 0.757333439889806,
      "learning_rate": 7.888661785727889e-06,
      "loss": 0.5281,
      "step": 6450
    },
    {
      "epoch": 0.5807918251592429,
      "grad_norm": 0.8439378515921735,
      "learning_rate": 7.885811441820937e-06,
      "loss": 0.5218,
      "step": 6451
    },
    {
      "epoch": 0.5808818564451147,
      "grad_norm": 0.7547567923644697,
      "learning_rate": 7.882961277705897e-06,
      "loss": 0.5481,
      "step": 6452
    },
    {
      "epoch": 0.5809718877309865,
      "grad_norm": 0.9251263058375214,
      "learning_rate": 7.880111293625138e-06,
      "loss": 0.6265,
      "step": 6453
    },
    {
      "epoch": 0.5810619190168583,
      "grad_norm": 0.7761959106673082,
      "learning_rate": 7.877261489821035e-06,
      "loss": 0.5155,
      "step": 6454
    },
    {
      "epoch": 0.5811519503027301,
      "grad_norm": 0.9291537704828661,
      "learning_rate": 7.874411866535932e-06,
      "loss": 0.5969,
      "step": 6455
    },
    {
      "epoch": 0.5812419815886021,
      "grad_norm": 0.7626815370057165,
      "learning_rate": 7.871562424012166e-06,
      "loss": 0.4698,
      "step": 6456
    },
    {
      "epoch": 0.5813320128744739,
      "grad_norm": 0.9133544030604605,
      "learning_rate": 7.868713162492048e-06,
      "loss": 0.5017,
      "step": 6457
    },
    {
      "epoch": 0.5814220441603457,
      "grad_norm": 0.8533804524782361,
      "learning_rate": 7.865864082217892e-06,
      "loss": 0.5292,
      "step": 6458
    },
    {
      "epoch": 0.5815120754462175,
      "grad_norm": 0.7935832115217862,
      "learning_rate": 7.863015183431976e-06,
      "loss": 0.484,
      "step": 6459
    },
    {
      "epoch": 0.5816021067320895,
      "grad_norm": 0.7950495240973238,
      "learning_rate": 7.860166466376578e-06,
      "loss": 0.5015,
      "step": 6460
    },
    {
      "epoch": 0.5816921380179613,
      "grad_norm": 0.9070417296379875,
      "learning_rate": 7.857317931293948e-06,
      "loss": 0.5497,
      "step": 6461
    },
    {
      "epoch": 0.5817821693038331,
      "grad_norm": 0.7592224904984082,
      "learning_rate": 7.854469578426336e-06,
      "loss": 0.4922,
      "step": 6462
    },
    {
      "epoch": 0.5818722005897049,
      "grad_norm": 1.062954424807152,
      "learning_rate": 7.85162140801596e-06,
      "loss": 0.5533,
      "step": 6463
    },
    {
      "epoch": 0.5819622318755767,
      "grad_norm": 0.7750627972452724,
      "learning_rate": 7.84877342030504e-06,
      "loss": 0.4416,
      "step": 6464
    },
    {
      "epoch": 0.5820522631614486,
      "grad_norm": 0.9163286967884773,
      "learning_rate": 7.845925615535758e-06,
      "loss": 0.5028,
      "step": 6465
    },
    {
      "epoch": 0.5821422944473205,
      "grad_norm": 0.7127864067645053,
      "learning_rate": 7.843077993950302e-06,
      "loss": 0.4703,
      "step": 6466
    },
    {
      "epoch": 0.5822323257331923,
      "grad_norm": 0.8107865960120015,
      "learning_rate": 7.840230555790828e-06,
      "loss": 0.5247,
      "step": 6467
    },
    {
      "epoch": 0.5823223570190641,
      "grad_norm": 0.8758546006928686,
      "learning_rate": 7.837383301299495e-06,
      "loss": 0.501,
      "step": 6468
    },
    {
      "epoch": 0.5824123883049359,
      "grad_norm": 0.8205863646268243,
      "learning_rate": 7.834536230718425e-06,
      "loss": 0.5443,
      "step": 6469
    },
    {
      "epoch": 0.5825024195908078,
      "grad_norm": 0.7175479513348587,
      "learning_rate": 7.831689344289741e-06,
      "loss": 0.5006,
      "step": 6470
    },
    {
      "epoch": 0.5825924508766797,
      "grad_norm": 0.8146206696762147,
      "learning_rate": 7.828842642255538e-06,
      "loss": 0.5737,
      "step": 6471
    },
    {
      "epoch": 0.5826824821625515,
      "grad_norm": 0.7531608442956401,
      "learning_rate": 7.825996124857908e-06,
      "loss": 0.4407,
      "step": 6472
    },
    {
      "epoch": 0.5827725134484233,
      "grad_norm": 0.7063820982243643,
      "learning_rate": 7.823149792338917e-06,
      "loss": 0.4705,
      "step": 6473
    },
    {
      "epoch": 0.5828625447342952,
      "grad_norm": 0.8704024581533719,
      "learning_rate": 7.82030364494062e-06,
      "loss": 0.5455,
      "step": 6474
    },
    {
      "epoch": 0.582952576020167,
      "grad_norm": 0.8174717788955858,
      "learning_rate": 7.81745768290505e-06,
      "loss": 0.478,
      "step": 6475
    },
    {
      "epoch": 0.5830426073060389,
      "grad_norm": 0.6474136551026689,
      "learning_rate": 7.814611906474238e-06,
      "loss": 0.5548,
      "step": 6476
    },
    {
      "epoch": 0.5831326385919107,
      "grad_norm": 0.8320846106501233,
      "learning_rate": 7.811766315890186e-06,
      "loss": 0.5126,
      "step": 6477
    },
    {
      "epoch": 0.5832226698777825,
      "grad_norm": 0.8342364693533363,
      "learning_rate": 7.808920911394887e-06,
      "loss": 0.5892,
      "step": 6478
    },
    {
      "epoch": 0.5833127011636544,
      "grad_norm": 0.8677330225960662,
      "learning_rate": 7.806075693230312e-06,
      "loss": 0.6345,
      "step": 6479
    },
    {
      "epoch": 0.5834027324495262,
      "grad_norm": 0.8175342509272918,
      "learning_rate": 7.803230661638423e-06,
      "loss": 0.5821,
      "step": 6480
    },
    {
      "epoch": 0.583492763735398,
      "grad_norm": 0.8034833625755154,
      "learning_rate": 7.80038581686116e-06,
      "loss": 0.5072,
      "step": 6481
    },
    {
      "epoch": 0.5835827950212699,
      "grad_norm": 1.1146958071200066,
      "learning_rate": 7.797541159140457e-06,
      "loss": 0.5449,
      "step": 6482
    },
    {
      "epoch": 0.5836728263071417,
      "grad_norm": 0.8449035896767972,
      "learning_rate": 7.794696688718218e-06,
      "loss": 0.5677,
      "step": 6483
    },
    {
      "epoch": 0.5837628575930136,
      "grad_norm": 0.7219257395226863,
      "learning_rate": 7.791852405836346e-06,
      "loss": 0.541,
      "step": 6484
    },
    {
      "epoch": 0.5838528888788854,
      "grad_norm": 0.6584308884208423,
      "learning_rate": 7.789008310736711e-06,
      "loss": 0.5174,
      "step": 6485
    },
    {
      "epoch": 0.5839429201647572,
      "grad_norm": 0.82359553373412,
      "learning_rate": 7.786164403661188e-06,
      "loss": 0.6332,
      "step": 6486
    },
    {
      "epoch": 0.5840329514506291,
      "grad_norm": 0.7600951903444573,
      "learning_rate": 7.783320684851613e-06,
      "loss": 0.5046,
      "step": 6487
    },
    {
      "epoch": 0.584122982736501,
      "grad_norm": 0.8893428852084297,
      "learning_rate": 7.78047715454983e-06,
      "loss": 0.5655,
      "step": 6488
    },
    {
      "epoch": 0.5842130140223728,
      "grad_norm": 0.7815851431280878,
      "learning_rate": 7.77763381299764e-06,
      "loss": 0.5829,
      "step": 6489
    },
    {
      "epoch": 0.5843030453082446,
      "grad_norm": 0.8933691753474398,
      "learning_rate": 7.774790660436857e-06,
      "loss": 0.4235,
      "step": 6490
    },
    {
      "epoch": 0.5843930765941164,
      "grad_norm": 0.7669631981720794,
      "learning_rate": 7.771947697109255e-06,
      "loss": 0.4995,
      "step": 6491
    },
    {
      "epoch": 0.5844831078799883,
      "grad_norm": 0.7590130709818432,
      "learning_rate": 7.769104923256608e-06,
      "loss": 0.582,
      "step": 6492
    },
    {
      "epoch": 0.5845731391658602,
      "grad_norm": 0.8572896393746047,
      "learning_rate": 7.766262339120659e-06,
      "loss": 0.5217,
      "step": 6493
    },
    {
      "epoch": 0.584663170451732,
      "grad_norm": 0.7160558079505102,
      "learning_rate": 7.763419944943152e-06,
      "loss": 0.5898,
      "step": 6494
    },
    {
      "epoch": 0.5847532017376038,
      "grad_norm": 0.8082997239866275,
      "learning_rate": 7.760577740965796e-06,
      "loss": 0.443,
      "step": 6495
    },
    {
      "epoch": 0.5848432330234756,
      "grad_norm": 0.9046772454324316,
      "learning_rate": 7.757735727430304e-06,
      "loss": 0.552,
      "step": 6496
    },
    {
      "epoch": 0.5849332643093474,
      "grad_norm": 0.8281127390475393,
      "learning_rate": 7.754893904578355e-06,
      "loss": 0.4858,
      "step": 6497
    },
    {
      "epoch": 0.5850232955952194,
      "grad_norm": 0.7506500983361962,
      "learning_rate": 7.752052272651627e-06,
      "loss": 0.5909,
      "step": 6498
    },
    {
      "epoch": 0.5851133268810912,
      "grad_norm": 0.6213443170615165,
      "learning_rate": 7.749210831891763e-06,
      "loss": 0.5138,
      "step": 6499
    },
    {
      "epoch": 0.585203358166963,
      "grad_norm": 0.8751332718400603,
      "learning_rate": 7.746369582540412e-06,
      "loss": 0.5109,
      "step": 6500
    },
    {
      "epoch": 0.5852933894528348,
      "grad_norm": 0.9982059303116628,
      "learning_rate": 7.74352852483919e-06,
      "loss": 0.6032,
      "step": 6501
    },
    {
      "epoch": 0.5853834207387068,
      "grad_norm": 0.7133372355726505,
      "learning_rate": 7.740687659029705e-06,
      "loss": 0.5043,
      "step": 6502
    },
    {
      "epoch": 0.5854734520245786,
      "grad_norm": 0.856857582773592,
      "learning_rate": 7.737846985353541e-06,
      "loss": 0.4437,
      "step": 6503
    },
    {
      "epoch": 0.5855634833104504,
      "grad_norm": 0.8118159868211912,
      "learning_rate": 7.735006504052278e-06,
      "loss": 0.5535,
      "step": 6504
    },
    {
      "epoch": 0.5856535145963222,
      "grad_norm": 0.8302614415803738,
      "learning_rate": 7.732166215367465e-06,
      "loss": 0.5739,
      "step": 6505
    },
    {
      "epoch": 0.585743545882194,
      "grad_norm": 1.2458393463776916,
      "learning_rate": 7.729326119540647e-06,
      "loss": 0.5216,
      "step": 6506
    },
    {
      "epoch": 0.5858335771680659,
      "grad_norm": 0.7192730533060171,
      "learning_rate": 7.726486216813346e-06,
      "loss": 0.4887,
      "step": 6507
    },
    {
      "epoch": 0.5859236084539378,
      "grad_norm": 0.7929549766012919,
      "learning_rate": 7.72364650742707e-06,
      "loss": 0.4858,
      "step": 6508
    },
    {
      "epoch": 0.5860136397398096,
      "grad_norm": 0.797234947588193,
      "learning_rate": 7.720806991623303e-06,
      "loss": 0.577,
      "step": 6509
    },
    {
      "epoch": 0.5861036710256814,
      "grad_norm": 0.719526375246575,
      "learning_rate": 7.71796766964353e-06,
      "loss": 0.5297,
      "step": 6510
    },
    {
      "epoch": 0.5861937023115532,
      "grad_norm": 0.7086940420315652,
      "learning_rate": 7.715128541729201e-06,
      "loss": 0.5058,
      "step": 6511
    },
    {
      "epoch": 0.5862837335974251,
      "grad_norm": 0.8966825748619696,
      "learning_rate": 7.712289608121762e-06,
      "loss": 0.5646,
      "step": 6512
    },
    {
      "epoch": 0.586373764883297,
      "grad_norm": 0.8703546531482814,
      "learning_rate": 7.70945086906263e-06,
      "loss": 0.6206,
      "step": 6513
    },
    {
      "epoch": 0.5864637961691688,
      "grad_norm": 0.86035702218526,
      "learning_rate": 7.706612324793225e-06,
      "loss": 0.5229,
      "step": 6514
    },
    {
      "epoch": 0.5865538274550406,
      "grad_norm": 0.8890227607089443,
      "learning_rate": 7.703773975554929e-06,
      "loss": 0.5833,
      "step": 6515
    },
    {
      "epoch": 0.5866438587409125,
      "grad_norm": 0.8932560337585753,
      "learning_rate": 7.70093582158912e-06,
      "loss": 0.5134,
      "step": 6516
    },
    {
      "epoch": 0.5867338900267843,
      "grad_norm": 0.7571809290307746,
      "learning_rate": 7.698097863137153e-06,
      "loss": 0.4967,
      "step": 6517
    },
    {
      "epoch": 0.5868239213126561,
      "grad_norm": 0.6737337597585554,
      "learning_rate": 7.695260100440379e-06,
      "loss": 0.5218,
      "step": 6518
    },
    {
      "epoch": 0.586913952598528,
      "grad_norm": 0.8410622086913719,
      "learning_rate": 7.692422533740113e-06,
      "loss": 0.6221,
      "step": 6519
    },
    {
      "epoch": 0.5870039838843998,
      "grad_norm": 0.8118584501791098,
      "learning_rate": 7.689585163277671e-06,
      "loss": 0.6159,
      "step": 6520
    },
    {
      "epoch": 0.5870940151702717,
      "grad_norm": 0.6890945748164344,
      "learning_rate": 7.686747989294339e-06,
      "loss": 0.4907,
      "step": 6521
    },
    {
      "epoch": 0.5871840464561435,
      "grad_norm": 0.7230248603852637,
      "learning_rate": 7.683911012031396e-06,
      "loss": 0.5333,
      "step": 6522
    },
    {
      "epoch": 0.5872740777420153,
      "grad_norm": 0.7623005748852437,
      "learning_rate": 7.681074231730095e-06,
      "loss": 0.5804,
      "step": 6523
    },
    {
      "epoch": 0.5873641090278872,
      "grad_norm": 0.8507638471844842,
      "learning_rate": 7.678237648631685e-06,
      "loss": 0.5411,
      "step": 6524
    },
    {
      "epoch": 0.587454140313759,
      "grad_norm": 1.0371190997300201,
      "learning_rate": 7.675401262977384e-06,
      "loss": 0.4856,
      "step": 6525
    },
    {
      "epoch": 0.5875441715996309,
      "grad_norm": 0.8293139575871954,
      "learning_rate": 7.672565075008407e-06,
      "loss": 0.6229,
      "step": 6526
    },
    {
      "epoch": 0.5876342028855027,
      "grad_norm": 0.719704238809128,
      "learning_rate": 7.669729084965936e-06,
      "loss": 0.4806,
      "step": 6527
    },
    {
      "epoch": 0.5877242341713745,
      "grad_norm": 0.7401569191447399,
      "learning_rate": 7.666893293091154e-06,
      "loss": 0.5081,
      "step": 6528
    },
    {
      "epoch": 0.5878142654572464,
      "grad_norm": 0.7764070425753711,
      "learning_rate": 7.664057699625215e-06,
      "loss": 0.485,
      "step": 6529
    },
    {
      "epoch": 0.5879042967431183,
      "grad_norm": 0.919032586262087,
      "learning_rate": 7.66122230480926e-06,
      "loss": 0.5394,
      "step": 6530
    },
    {
      "epoch": 0.5879943280289901,
      "grad_norm": 0.8111740162861296,
      "learning_rate": 7.65838710888441e-06,
      "loss": 0.4798,
      "step": 6531
    },
    {
      "epoch": 0.5880843593148619,
      "grad_norm": 0.8278604545078252,
      "learning_rate": 7.655552112091777e-06,
      "loss": 0.5496,
      "step": 6532
    },
    {
      "epoch": 0.5881743906007337,
      "grad_norm": 0.7918474520200841,
      "learning_rate": 7.652717314672447e-06,
      "loss": 0.4923,
      "step": 6533
    },
    {
      "epoch": 0.5882644218866055,
      "grad_norm": 0.8831071121799567,
      "learning_rate": 7.649882716867495e-06,
      "loss": 0.5366,
      "step": 6534
    },
    {
      "epoch": 0.5883544531724775,
      "grad_norm": 1.3714022400278756,
      "learning_rate": 7.647048318917975e-06,
      "loss": 0.5142,
      "step": 6535
    },
    {
      "epoch": 0.5884444844583493,
      "grad_norm": 0.8219543424087566,
      "learning_rate": 7.644214121064929e-06,
      "loss": 0.5001,
      "step": 6536
    },
    {
      "epoch": 0.5885345157442211,
      "grad_norm": 0.745828430617271,
      "learning_rate": 7.641380123549372e-06,
      "loss": 0.5386,
      "step": 6537
    },
    {
      "epoch": 0.5886245470300929,
      "grad_norm": 1.0351434430756155,
      "learning_rate": 7.638546326612319e-06,
      "loss": 0.4439,
      "step": 6538
    },
    {
      "epoch": 0.5887145783159647,
      "grad_norm": 0.944925109094997,
      "learning_rate": 7.635712730494751e-06,
      "loss": 0.509,
      "step": 6539
    },
    {
      "epoch": 0.5888046096018367,
      "grad_norm": 0.7712949332348048,
      "learning_rate": 7.632879335437641e-06,
      "loss": 0.5832,
      "step": 6540
    },
    {
      "epoch": 0.5888946408877085,
      "grad_norm": 0.9523260246391531,
      "learning_rate": 7.63004614168194e-06,
      "loss": 0.5532,
      "step": 6541
    },
    {
      "epoch": 0.5889846721735803,
      "grad_norm": 0.9729814112777361,
      "learning_rate": 7.6272131494685885e-06,
      "loss": 0.4553,
      "step": 6542
    },
    {
      "epoch": 0.5890747034594521,
      "grad_norm": 0.6353830345549194,
      "learning_rate": 7.624380359038503e-06,
      "loss": 0.4991,
      "step": 6543
    },
    {
      "epoch": 0.589164734745324,
      "grad_norm": 0.7602107135718617,
      "learning_rate": 7.6215477706325905e-06,
      "loss": 0.5138,
      "step": 6544
    },
    {
      "epoch": 0.5892547660311959,
      "grad_norm": 1.2955211506995263,
      "learning_rate": 7.618715384491726e-06,
      "loss": 0.6137,
      "step": 6545
    },
    {
      "epoch": 0.5893447973170677,
      "grad_norm": 0.703116839352411,
      "learning_rate": 7.615883200856789e-06,
      "loss": 0.5128,
      "step": 6546
    },
    {
      "epoch": 0.5894348286029395,
      "grad_norm": 0.7371083479440537,
      "learning_rate": 7.613051219968624e-06,
      "loss": 0.6218,
      "step": 6547
    },
    {
      "epoch": 0.5895248598888113,
      "grad_norm": 0.7771344691832529,
      "learning_rate": 7.6102194420680655e-06,
      "loss": 0.6258,
      "step": 6548
    },
    {
      "epoch": 0.5896148911746832,
      "grad_norm": 0.9375002142507338,
      "learning_rate": 7.607387867395929e-06,
      "loss": 0.5298,
      "step": 6549
    },
    {
      "epoch": 0.5897049224605551,
      "grad_norm": 0.9187104729482024,
      "learning_rate": 7.604556496193015e-06,
      "loss": 0.5638,
      "step": 6550
    },
    {
      "epoch": 0.5897949537464269,
      "grad_norm": 0.7583398531318021,
      "learning_rate": 7.601725328700101e-06,
      "loss": 0.5661,
      "step": 6551
    },
    {
      "epoch": 0.5898849850322987,
      "grad_norm": 0.7009955750563921,
      "learning_rate": 7.598894365157957e-06,
      "loss": 0.4334,
      "step": 6552
    },
    {
      "epoch": 0.5899750163181706,
      "grad_norm": 0.8373513160490992,
      "learning_rate": 7.596063605807326e-06,
      "loss": 0.5464,
      "step": 6553
    },
    {
      "epoch": 0.5900650476040424,
      "grad_norm": 0.7555831800503967,
      "learning_rate": 7.59323305088894e-06,
      "loss": 0.4956,
      "step": 6554
    },
    {
      "epoch": 0.5901550788899143,
      "grad_norm": 0.7666659188154977,
      "learning_rate": 7.5904027006435064e-06,
      "loss": 0.4993,
      "step": 6555
    },
    {
      "epoch": 0.5902451101757861,
      "grad_norm": 0.8096981555462491,
      "learning_rate": 7.587572555311727e-06,
      "loss": 0.4801,
      "step": 6556
    },
    {
      "epoch": 0.5903351414616579,
      "grad_norm": 0.7857969441021644,
      "learning_rate": 7.584742615134274e-06,
      "loss": 0.5564,
      "step": 6557
    },
    {
      "epoch": 0.5904251727475298,
      "grad_norm": 0.8651983524672943,
      "learning_rate": 7.58191288035181e-06,
      "loss": 0.5501,
      "step": 6558
    },
    {
      "epoch": 0.5905152040334016,
      "grad_norm": 0.8063042366882028,
      "learning_rate": 7.579083351204972e-06,
      "loss": 0.5726,
      "step": 6559
    },
    {
      "epoch": 0.5906052353192734,
      "grad_norm": 0.7993974370240297,
      "learning_rate": 7.576254027934394e-06,
      "loss": 0.5188,
      "step": 6560
    },
    {
      "epoch": 0.5906952666051453,
      "grad_norm": 0.9041937609224423,
      "learning_rate": 7.573424910780676e-06,
      "loss": 0.5883,
      "step": 6561
    },
    {
      "epoch": 0.5907852978910171,
      "grad_norm": 0.8532757194834159,
      "learning_rate": 7.570595999984413e-06,
      "loss": 0.6148,
      "step": 6562
    },
    {
      "epoch": 0.590875329176889,
      "grad_norm": 0.7598710495684439,
      "learning_rate": 7.567767295786172e-06,
      "loss": 0.5241,
      "step": 6563
    },
    {
      "epoch": 0.5909653604627608,
      "grad_norm": 0.7933883097764399,
      "learning_rate": 7.564938798426512e-06,
      "loss": 0.4787,
      "step": 6564
    },
    {
      "epoch": 0.5910553917486326,
      "grad_norm": 0.9021883334620135,
      "learning_rate": 7.562110508145965e-06,
      "loss": 0.6208,
      "step": 6565
    },
    {
      "epoch": 0.5911454230345045,
      "grad_norm": 0.7030803596710439,
      "learning_rate": 7.55928242518506e-06,
      "loss": 0.5093,
      "step": 6566
    },
    {
      "epoch": 0.5912354543203764,
      "grad_norm": 0.9470641683751446,
      "learning_rate": 7.556454549784289e-06,
      "loss": 0.5466,
      "step": 6567
    },
    {
      "epoch": 0.5913254856062482,
      "grad_norm": 0.7529387826809331,
      "learning_rate": 7.5536268821841415e-06,
      "loss": 0.4989,
      "step": 6568
    },
    {
      "epoch": 0.59141551689212,
      "grad_norm": 0.7898269321909817,
      "learning_rate": 7.550799422625081e-06,
      "loss": 0.532,
      "step": 6569
    },
    {
      "epoch": 0.5915055481779918,
      "grad_norm": 0.9077381657395205,
      "learning_rate": 7.547972171347562e-06,
      "loss": 0.4659,
      "step": 6570
    },
    {
      "epoch": 0.5915955794638637,
      "grad_norm": 0.9161206280206897,
      "learning_rate": 7.545145128592009e-06,
      "loss": 0.5004,
      "step": 6571
    },
    {
      "epoch": 0.5916856107497356,
      "grad_norm": 0.8190221391612439,
      "learning_rate": 7.542318294598842e-06,
      "loss": 0.4581,
      "step": 6572
    },
    {
      "epoch": 0.5917756420356074,
      "grad_norm": 0.8448348318432545,
      "learning_rate": 7.539491669608449e-06,
      "loss": 0.4879,
      "step": 6573
    },
    {
      "epoch": 0.5918656733214792,
      "grad_norm": 0.7788282485726937,
      "learning_rate": 7.536665253861215e-06,
      "loss": 0.5865,
      "step": 6574
    },
    {
      "epoch": 0.591955704607351,
      "grad_norm": 0.8919806207276805,
      "learning_rate": 7.5338390475974975e-06,
      "loss": 0.4961,
      "step": 6575
    },
    {
      "epoch": 0.5920457358932228,
      "grad_norm": 0.732314736825202,
      "learning_rate": 7.531013051057639e-06,
      "loss": 0.5564,
      "step": 6576
    },
    {
      "epoch": 0.5921357671790948,
      "grad_norm": 0.8376891811983184,
      "learning_rate": 7.528187264481963e-06,
      "loss": 0.5182,
      "step": 6577
    },
    {
      "epoch": 0.5922257984649666,
      "grad_norm": 0.8489910775741107,
      "learning_rate": 7.5253616881107774e-06,
      "loss": 0.5256,
      "step": 6578
    },
    {
      "epoch": 0.5923158297508384,
      "grad_norm": 0.8687304466896344,
      "learning_rate": 7.5225363221843675e-06,
      "loss": 0.4921,
      "step": 6579
    },
    {
      "epoch": 0.5924058610367102,
      "grad_norm": 0.8438414190954524,
      "learning_rate": 7.519711166943011e-06,
      "loss": 0.5127,
      "step": 6580
    },
    {
      "epoch": 0.5924958923225822,
      "grad_norm": 0.7798059616137935,
      "learning_rate": 7.5168862226269545e-06,
      "loss": 0.5014,
      "step": 6581
    },
    {
      "epoch": 0.592585923608454,
      "grad_norm": 0.9710178560680239,
      "learning_rate": 7.514061489476438e-06,
      "loss": 0.6049,
      "step": 6582
    },
    {
      "epoch": 0.5926759548943258,
      "grad_norm": 0.8645090237622983,
      "learning_rate": 7.511236967731672e-06,
      "loss": 0.638,
      "step": 6583
    },
    {
      "epoch": 0.5927659861801976,
      "grad_norm": 0.9072615421812517,
      "learning_rate": 7.5084126576328645e-06,
      "loss": 0.5059,
      "step": 6584
    },
    {
      "epoch": 0.5928560174660694,
      "grad_norm": 0.8162578901573672,
      "learning_rate": 7.505588559420188e-06,
      "loss": 0.523,
      "step": 6585
    },
    {
      "epoch": 0.5929460487519413,
      "grad_norm": 0.9118212034645816,
      "learning_rate": 7.502764673333812e-06,
      "loss": 0.5545,
      "step": 6586
    },
    {
      "epoch": 0.5930360800378132,
      "grad_norm": 0.7002544546438383,
      "learning_rate": 7.499940999613875e-06,
      "loss": 0.5431,
      "step": 6587
    },
    {
      "epoch": 0.593126111323685,
      "grad_norm": 1.1050857962391674,
      "learning_rate": 7.497117538500511e-06,
      "loss": 0.5323,
      "step": 6588
    },
    {
      "epoch": 0.5932161426095568,
      "grad_norm": 0.7410899235075076,
      "learning_rate": 7.494294290233826e-06,
      "loss": 0.543,
      "step": 6589
    },
    {
      "epoch": 0.5933061738954286,
      "grad_norm": 0.7519033808595471,
      "learning_rate": 7.49147125505391e-06,
      "loss": 0.5415,
      "step": 6590
    },
    {
      "epoch": 0.5933962051813005,
      "grad_norm": 0.9317728972323155,
      "learning_rate": 7.4886484332008345e-06,
      "loss": 0.5537,
      "step": 6591
    },
    {
      "epoch": 0.5934862364671724,
      "grad_norm": 0.8142846655577508,
      "learning_rate": 7.485825824914658e-06,
      "loss": 0.5095,
      "step": 6592
    },
    {
      "epoch": 0.5935762677530442,
      "grad_norm": 0.7457168269800828,
      "learning_rate": 7.4830034304354115e-06,
      "loss": 0.5167,
      "step": 6593
    },
    {
      "epoch": 0.593666299038916,
      "grad_norm": 0.9004402891116727,
      "learning_rate": 7.480181250003119e-06,
      "loss": 0.5731,
      "step": 6594
    },
    {
      "epoch": 0.5937563303247879,
      "grad_norm": 1.0892019818677867,
      "learning_rate": 7.477359283857775e-06,
      "loss": 0.6727,
      "step": 6595
    },
    {
      "epoch": 0.5938463616106597,
      "grad_norm": 0.8730723868819951,
      "learning_rate": 7.474537532239368e-06,
      "loss": 0.5403,
      "step": 6596
    },
    {
      "epoch": 0.5939363928965316,
      "grad_norm": 0.8513586770139151,
      "learning_rate": 7.471715995387852e-06,
      "loss": 0.568,
      "step": 6597
    },
    {
      "epoch": 0.5940264241824034,
      "grad_norm": 0.8980769281448803,
      "learning_rate": 7.468894673543182e-06,
      "loss": 0.5088,
      "step": 6598
    },
    {
      "epoch": 0.5941164554682752,
      "grad_norm": 0.8170581615605716,
      "learning_rate": 7.466073566945277e-06,
      "loss": 0.5596,
      "step": 6599
    },
    {
      "epoch": 0.5942064867541471,
      "grad_norm": 0.9158668775701347,
      "learning_rate": 7.463252675834054e-06,
      "loss": 0.6627,
      "step": 6600
    },
    {
      "epoch": 0.5942965180400189,
      "grad_norm": 0.8389160088666152,
      "learning_rate": 7.460432000449393e-06,
      "loss": 0.5306,
      "step": 6601
    },
    {
      "epoch": 0.5943865493258907,
      "grad_norm": 0.8205140529499171,
      "learning_rate": 7.4576115410311765e-06,
      "loss": 0.5319,
      "step": 6602
    },
    {
      "epoch": 0.5944765806117626,
      "grad_norm": 0.747835832171843,
      "learning_rate": 7.4547912978192505e-06,
      "loss": 0.4423,
      "step": 6603
    },
    {
      "epoch": 0.5945666118976344,
      "grad_norm": 0.8147832096767225,
      "learning_rate": 7.451971271053455e-06,
      "loss": 0.5057,
      "step": 6604
    },
    {
      "epoch": 0.5946566431835063,
      "grad_norm": 0.8493647293030109,
      "learning_rate": 7.4491514609736045e-06,
      "loss": 0.5267,
      "step": 6605
    },
    {
      "epoch": 0.5947466744693781,
      "grad_norm": 0.8638088221998146,
      "learning_rate": 7.446331867819498e-06,
      "loss": 0.5133,
      "step": 6606
    },
    {
      "epoch": 0.5948367057552499,
      "grad_norm": 0.9141430006073858,
      "learning_rate": 7.443512491830911e-06,
      "loss": 0.5355,
      "step": 6607
    },
    {
      "epoch": 0.5949267370411218,
      "grad_norm": 0.7519626004048576,
      "learning_rate": 7.440693333247614e-06,
      "loss": 0.5394,
      "step": 6608
    },
    {
      "epoch": 0.5950167683269937,
      "grad_norm": 0.8479272885148348,
      "learning_rate": 7.437874392309342e-06,
      "loss": 0.5076,
      "step": 6609
    },
    {
      "epoch": 0.5951067996128655,
      "grad_norm": 0.923960200617319,
      "learning_rate": 7.435055669255826e-06,
      "loss": 0.559,
      "step": 6610
    },
    {
      "epoch": 0.5951968308987373,
      "grad_norm": 0.8675587015101027,
      "learning_rate": 7.432237164326764e-06,
      "loss": 0.5449,
      "step": 6611
    },
    {
      "epoch": 0.5952868621846091,
      "grad_norm": 0.9082829232691004,
      "learning_rate": 7.429418877761851e-06,
      "loss": 0.5346,
      "step": 6612
    },
    {
      "epoch": 0.595376893470481,
      "grad_norm": 0.9749785255980291,
      "learning_rate": 7.426600809800753e-06,
      "loss": 0.6945,
      "step": 6613
    },
    {
      "epoch": 0.5954669247563529,
      "grad_norm": 0.8683394787972754,
      "learning_rate": 7.423782960683119e-06,
      "loss": 0.4947,
      "step": 6614
    },
    {
      "epoch": 0.5955569560422247,
      "grad_norm": 0.7531242860709624,
      "learning_rate": 7.42096533064858e-06,
      "loss": 0.5107,
      "step": 6615
    },
    {
      "epoch": 0.5956469873280965,
      "grad_norm": 0.8624376633265737,
      "learning_rate": 7.418147919936753e-06,
      "loss": 0.5062,
      "step": 6616
    },
    {
      "epoch": 0.5957370186139683,
      "grad_norm": 0.9182160634693727,
      "learning_rate": 7.41533072878723e-06,
      "loss": 0.4818,
      "step": 6617
    },
    {
      "epoch": 0.5958270498998401,
      "grad_norm": 0.6989651282165777,
      "learning_rate": 7.412513757439588e-06,
      "loss": 0.5459,
      "step": 6618
    },
    {
      "epoch": 0.5959170811857121,
      "grad_norm": 0.9031652991733894,
      "learning_rate": 7.4096970061333805e-06,
      "loss": 0.5439,
      "step": 6619
    },
    {
      "epoch": 0.5960071124715839,
      "grad_norm": 0.8136648129452978,
      "learning_rate": 7.4068804751081515e-06,
      "loss": 0.5001,
      "step": 6620
    },
    {
      "epoch": 0.5960971437574557,
      "grad_norm": 0.9754775440117489,
      "learning_rate": 7.40406416460341e-06,
      "loss": 0.5108,
      "step": 6621
    },
    {
      "epoch": 0.5961871750433275,
      "grad_norm": 0.7138345289828152,
      "learning_rate": 7.401248074858671e-06,
      "loss": 0.4226,
      "step": 6622
    },
    {
      "epoch": 0.5962772063291994,
      "grad_norm": 0.7321163029806791,
      "learning_rate": 7.398432206113406e-06,
      "loss": 0.536,
      "step": 6623
    },
    {
      "epoch": 0.5963672376150713,
      "grad_norm": 0.7308593886173198,
      "learning_rate": 7.395616558607084e-06,
      "loss": 0.5084,
      "step": 6624
    },
    {
      "epoch": 0.5964572689009431,
      "grad_norm": 0.720240510062953,
      "learning_rate": 7.392801132579142e-06,
      "loss": 0.5623,
      "step": 6625
    },
    {
      "epoch": 0.5965473001868149,
      "grad_norm": 0.8234093722202637,
      "learning_rate": 7.3899859282690144e-06,
      "loss": 0.4893,
      "step": 6626
    },
    {
      "epoch": 0.5966373314726867,
      "grad_norm": 0.7057670979627111,
      "learning_rate": 7.3871709459161035e-06,
      "loss": 0.5196,
      "step": 6627
    },
    {
      "epoch": 0.5967273627585586,
      "grad_norm": 0.8677986160007457,
      "learning_rate": 7.3843561857598e-06,
      "loss": 0.5539,
      "step": 6628
    },
    {
      "epoch": 0.5968173940444305,
      "grad_norm": 0.930517752055322,
      "learning_rate": 7.381541648039466e-06,
      "loss": 0.5036,
      "step": 6629
    },
    {
      "epoch": 0.5969074253303023,
      "grad_norm": 0.7691123878940669,
      "learning_rate": 7.37872733299446e-06,
      "loss": 0.5129,
      "step": 6630
    },
    {
      "epoch": 0.5969974566161741,
      "grad_norm": 0.7688967204300997,
      "learning_rate": 7.375913240864106e-06,
      "loss": 0.4967,
      "step": 6631
    },
    {
      "epoch": 0.5970874879020459,
      "grad_norm": 0.9499536220501292,
      "learning_rate": 7.3730993718877245e-06,
      "loss": 0.4468,
      "step": 6632
    },
    {
      "epoch": 0.5971775191879178,
      "grad_norm": 0.8779234894735538,
      "learning_rate": 7.370285726304598e-06,
      "loss": 0.4915,
      "step": 6633
    },
    {
      "epoch": 0.5972675504737897,
      "grad_norm": 1.0047198752504116,
      "learning_rate": 7.367472304354011e-06,
      "loss": 0.5235,
      "step": 6634
    },
    {
      "epoch": 0.5973575817596615,
      "grad_norm": 0.848411566164689,
      "learning_rate": 7.364659106275209e-06,
      "loss": 0.4829,
      "step": 6635
    },
    {
      "epoch": 0.5974476130455333,
      "grad_norm": 0.7752932625072205,
      "learning_rate": 7.361846132307436e-06,
      "loss": 0.4437,
      "step": 6636
    },
    {
      "epoch": 0.5975376443314052,
      "grad_norm": 0.8248195623118894,
      "learning_rate": 7.359033382689905e-06,
      "loss": 0.5014,
      "step": 6637
    },
    {
      "epoch": 0.597627675617277,
      "grad_norm": 0.9750960897464496,
      "learning_rate": 7.356220857661818e-06,
      "loss": 0.5484,
      "step": 6638
    },
    {
      "epoch": 0.5977177069031488,
      "grad_norm": 0.7663018428767002,
      "learning_rate": 7.353408557462345e-06,
      "loss": 0.5572,
      "step": 6639
    },
    {
      "epoch": 0.5978077381890207,
      "grad_norm": 0.7705917093643645,
      "learning_rate": 7.350596482330656e-06,
      "loss": 0.5626,
      "step": 6640
    },
    {
      "epoch": 0.5978977694748925,
      "grad_norm": 0.7050954652288406,
      "learning_rate": 7.3477846325058865e-06,
      "loss": 0.4942,
      "step": 6641
    },
    {
      "epoch": 0.5979878007607644,
      "grad_norm": 0.9651113306999792,
      "learning_rate": 7.344973008227161e-06,
      "loss": 0.4943,
      "step": 6642
    },
    {
      "epoch": 0.5980778320466362,
      "grad_norm": 1.0200398795135242,
      "learning_rate": 7.3421616097335735e-06,
      "loss": 0.5472,
      "step": 6643
    },
    {
      "epoch": 0.598167863332508,
      "grad_norm": 1.0412732414422303,
      "learning_rate": 7.3393504372642185e-06,
      "loss": 0.4669,
      "step": 6644
    },
    {
      "epoch": 0.5982578946183799,
      "grad_norm": 1.0519475352497205,
      "learning_rate": 7.336539491058152e-06,
      "loss": 0.6406,
      "step": 6645
    },
    {
      "epoch": 0.5983479259042517,
      "grad_norm": 0.8567569693457266,
      "learning_rate": 7.333728771354422e-06,
      "loss": 0.5514,
      "step": 6646
    },
    {
      "epoch": 0.5984379571901236,
      "grad_norm": 0.8974786860748959,
      "learning_rate": 7.330918278392052e-06,
      "loss": 0.5013,
      "step": 6647
    },
    {
      "epoch": 0.5985279884759954,
      "grad_norm": 0.977891362190345,
      "learning_rate": 7.328108012410051e-06,
      "loss": 0.5565,
      "step": 6648
    },
    {
      "epoch": 0.5986180197618672,
      "grad_norm": 0.863642187262498,
      "learning_rate": 7.325297973647401e-06,
      "loss": 0.5347,
      "step": 6649
    },
    {
      "epoch": 0.598708051047739,
      "grad_norm": 0.7819743322472223,
      "learning_rate": 7.3224881623430736e-06,
      "loss": 0.5611,
      "step": 6650
    },
    {
      "epoch": 0.598798082333611,
      "grad_norm": 0.9517740496140715,
      "learning_rate": 7.3196785787360144e-06,
      "loss": 0.5747,
      "step": 6651
    },
    {
      "epoch": 0.5988881136194828,
      "grad_norm": 0.7561174980101839,
      "learning_rate": 7.316869223065156e-06,
      "loss": 0.4926,
      "step": 6652
    },
    {
      "epoch": 0.5989781449053546,
      "grad_norm": 0.9402830651079076,
      "learning_rate": 7.3140600955694e-06,
      "loss": 0.5689,
      "step": 6653
    },
    {
      "epoch": 0.5990681761912264,
      "grad_norm": 0.7570939681671247,
      "learning_rate": 7.3112511964876455e-06,
      "loss": 0.592,
      "step": 6654
    },
    {
      "epoch": 0.5991582074770982,
      "grad_norm": 0.8313558117833819,
      "learning_rate": 7.308442526058757e-06,
      "loss": 0.5217,
      "step": 6655
    },
    {
      "epoch": 0.5992482387629702,
      "grad_norm": 0.804583829252799,
      "learning_rate": 7.305634084521589e-06,
      "loss": 0.5653,
      "step": 6656
    },
    {
      "epoch": 0.599338270048842,
      "grad_norm": 0.8728081280868438,
      "learning_rate": 7.302825872114968e-06,
      "loss": 0.5463,
      "step": 6657
    },
    {
      "epoch": 0.5994283013347138,
      "grad_norm": 0.8423508901964621,
      "learning_rate": 7.300017889077713e-06,
      "loss": 0.5665,
      "step": 6658
    },
    {
      "epoch": 0.5995183326205856,
      "grad_norm": 0.7599291053561361,
      "learning_rate": 7.297210135648612e-06,
      "loss": 0.4774,
      "step": 6659
    },
    {
      "epoch": 0.5996083639064574,
      "grad_norm": 0.7531242640631318,
      "learning_rate": 7.294402612066441e-06,
      "loss": 0.5071,
      "step": 6660
    },
    {
      "epoch": 0.5996983951923294,
      "grad_norm": 0.7787471415993753,
      "learning_rate": 7.291595318569951e-06,
      "loss": 0.5761,
      "step": 6661
    },
    {
      "epoch": 0.5997884264782012,
      "grad_norm": 0.9400074212932082,
      "learning_rate": 7.2887882553978775e-06,
      "loss": 0.482,
      "step": 6662
    },
    {
      "epoch": 0.599878457764073,
      "grad_norm": 0.7418552921592573,
      "learning_rate": 7.285981422788931e-06,
      "loss": 0.5028,
      "step": 6663
    },
    {
      "epoch": 0.5999684890499448,
      "grad_norm": 0.7534949854101857,
      "learning_rate": 7.283174820981812e-06,
      "loss": 0.5256,
      "step": 6664
    },
    {
      "epoch": 0.6000585203358167,
      "grad_norm": 0.6800822274335043,
      "learning_rate": 7.2803684502151924e-06,
      "loss": 0.3709,
      "step": 6665
    },
    {
      "epoch": 0.6001485516216886,
      "grad_norm": 0.8094252246605864,
      "learning_rate": 7.27756231072773e-06,
      "loss": 0.4797,
      "step": 6666
    },
    {
      "epoch": 0.6002385829075604,
      "grad_norm": 0.8047225255835218,
      "learning_rate": 7.274756402758057e-06,
      "loss": 0.6188,
      "step": 6667
    },
    {
      "epoch": 0.6003286141934322,
      "grad_norm": 2.1753287759422952,
      "learning_rate": 7.271950726544794e-06,
      "loss": 0.5659,
      "step": 6668
    },
    {
      "epoch": 0.600418645479304,
      "grad_norm": 0.9149842824423742,
      "learning_rate": 7.269145282326533e-06,
      "loss": 0.4745,
      "step": 6669
    },
    {
      "epoch": 0.6005086767651759,
      "grad_norm": 0.6812174845062425,
      "learning_rate": 7.266340070341855e-06,
      "loss": 0.543,
      "step": 6670
    },
    {
      "epoch": 0.6005987080510478,
      "grad_norm": 0.665558844840933,
      "learning_rate": 7.263535090829312e-06,
      "loss": 0.5523,
      "step": 6671
    },
    {
      "epoch": 0.6006887393369196,
      "grad_norm": 0.852185825869138,
      "learning_rate": 7.260730344027447e-06,
      "loss": 0.5413,
      "step": 6672
    },
    {
      "epoch": 0.6007787706227914,
      "grad_norm": 0.8826516833780899,
      "learning_rate": 7.257925830174772e-06,
      "loss": 0.5961,
      "step": 6673
    },
    {
      "epoch": 0.6008688019086632,
      "grad_norm": 0.7084683728681515,
      "learning_rate": 7.255121549509791e-06,
      "loss": 0.5926,
      "step": 6674
    },
    {
      "epoch": 0.6009588331945351,
      "grad_norm": 0.8687378002606392,
      "learning_rate": 7.2523175022709745e-06,
      "loss": 0.4635,
      "step": 6675
    },
    {
      "epoch": 0.601048864480407,
      "grad_norm": 0.7485693884112801,
      "learning_rate": 7.249513688696786e-06,
      "loss": 0.5299,
      "step": 6676
    },
    {
      "epoch": 0.6011388957662788,
      "grad_norm": 1.0427621706985062,
      "learning_rate": 7.246710109025656e-06,
      "loss": 0.5609,
      "step": 6677
    },
    {
      "epoch": 0.6012289270521506,
      "grad_norm": 0.7394425444185583,
      "learning_rate": 7.2439067634960135e-06,
      "loss": 0.4654,
      "step": 6678
    },
    {
      "epoch": 0.6013189583380225,
      "grad_norm": 0.7632365313733025,
      "learning_rate": 7.241103652346249e-06,
      "loss": 0.4432,
      "step": 6679
    },
    {
      "epoch": 0.6014089896238943,
      "grad_norm": 0.8149542478159754,
      "learning_rate": 7.238300775814746e-06,
      "loss": 0.591,
      "step": 6680
    },
    {
      "epoch": 0.6014990209097661,
      "grad_norm": 0.6674054724428349,
      "learning_rate": 7.235498134139854e-06,
      "loss": 0.5688,
      "step": 6681
    },
    {
      "epoch": 0.601589052195638,
      "grad_norm": 0.7421038055266619,
      "learning_rate": 7.2326957275599215e-06,
      "loss": 0.5507,
      "step": 6682
    },
    {
      "epoch": 0.6016790834815098,
      "grad_norm": 0.9068346907780707,
      "learning_rate": 7.229893556313262e-06,
      "loss": 0.536,
      "step": 6683
    },
    {
      "epoch": 0.6017691147673817,
      "grad_norm": 0.9186210837465716,
      "learning_rate": 7.227091620638176e-06,
      "loss": 0.517,
      "step": 6684
    },
    {
      "epoch": 0.6018591460532535,
      "grad_norm": 0.7599402503430571,
      "learning_rate": 7.224289920772936e-06,
      "loss": 0.4992,
      "step": 6685
    },
    {
      "epoch": 0.6019491773391253,
      "grad_norm": 0.9733215537592499,
      "learning_rate": 7.22148845695581e-06,
      "loss": 0.5756,
      "step": 6686
    },
    {
      "epoch": 0.6020392086249972,
      "grad_norm": 0.6947912728314153,
      "learning_rate": 7.218687229425028e-06,
      "loss": 0.4795,
      "step": 6687
    },
    {
      "epoch": 0.602129239910869,
      "grad_norm": 0.694708567867882,
      "learning_rate": 7.215886238418812e-06,
      "loss": 0.5223,
      "step": 6688
    },
    {
      "epoch": 0.6022192711967409,
      "grad_norm": 0.7430761321153145,
      "learning_rate": 7.213085484175357e-06,
      "loss": 0.503,
      "step": 6689
    },
    {
      "epoch": 0.6023093024826127,
      "grad_norm": 0.7430524470447797,
      "learning_rate": 7.210284966932847e-06,
      "loss": 0.4324,
      "step": 6690
    },
    {
      "epoch": 0.6023993337684845,
      "grad_norm": 0.799457079038522,
      "learning_rate": 7.20748468692943e-06,
      "loss": 0.5228,
      "step": 6691
    },
    {
      "epoch": 0.6024893650543564,
      "grad_norm": 0.9657611967146192,
      "learning_rate": 7.204684644403255e-06,
      "loss": 0.5186,
      "step": 6692
    },
    {
      "epoch": 0.6025793963402283,
      "grad_norm": 0.797235582015966,
      "learning_rate": 7.20188483959243e-06,
      "loss": 0.5511,
      "step": 6693
    },
    {
      "epoch": 0.6026694276261001,
      "grad_norm": 0.746023586372884,
      "learning_rate": 7.199085272735058e-06,
      "loss": 0.5413,
      "step": 6694
    },
    {
      "epoch": 0.6027594589119719,
      "grad_norm": 0.8238225517635119,
      "learning_rate": 7.196285944069209e-06,
      "loss": 0.5401,
      "step": 6695
    },
    {
      "epoch": 0.6028494901978437,
      "grad_norm": 1.2601103596865186,
      "learning_rate": 7.19348685383295e-06,
      "loss": 0.4826,
      "step": 6696
    },
    {
      "epoch": 0.6029395214837155,
      "grad_norm": 0.7985873743523654,
      "learning_rate": 7.190688002264308e-06,
      "loss": 0.5237,
      "step": 6697
    },
    {
      "epoch": 0.6030295527695875,
      "grad_norm": 0.735549193325415,
      "learning_rate": 7.187889389601305e-06,
      "loss": 0.5841,
      "step": 6698
    },
    {
      "epoch": 0.6031195840554593,
      "grad_norm": 0.8550352439617861,
      "learning_rate": 7.185091016081931e-06,
      "loss": 0.5426,
      "step": 6699
    },
    {
      "epoch": 0.6032096153413311,
      "grad_norm": 1.0487604836177775,
      "learning_rate": 7.18229288194417e-06,
      "loss": 0.4559,
      "step": 6700
    },
    {
      "epoch": 0.6032996466272029,
      "grad_norm": 0.844521857215504,
      "learning_rate": 7.179494987425965e-06,
      "loss": 0.5145,
      "step": 6701
    },
    {
      "epoch": 0.6033896779130747,
      "grad_norm": 0.8259224701387202,
      "learning_rate": 7.176697332765264e-06,
      "loss": 0.5539,
      "step": 6702
    },
    {
      "epoch": 0.6034797091989467,
      "grad_norm": 0.7982835267301805,
      "learning_rate": 7.173899918199973e-06,
      "loss": 0.4819,
      "step": 6703
    },
    {
      "epoch": 0.6035697404848185,
      "grad_norm": 0.7725428650708679,
      "learning_rate": 7.171102743967988e-06,
      "loss": 0.5428,
      "step": 6704
    },
    {
      "epoch": 0.6036597717706903,
      "grad_norm": 1.1005180040373583,
      "learning_rate": 7.168305810307179e-06,
      "loss": 0.483,
      "step": 6705
    },
    {
      "epoch": 0.6037498030565621,
      "grad_norm": 0.9557621194777225,
      "learning_rate": 7.165509117455407e-06,
      "loss": 0.5339,
      "step": 6706
    },
    {
      "epoch": 0.603839834342434,
      "grad_norm": 0.8288616957715822,
      "learning_rate": 7.162712665650497e-06,
      "loss": 0.6393,
      "step": 6707
    },
    {
      "epoch": 0.6039298656283059,
      "grad_norm": 0.736110227170018,
      "learning_rate": 7.159916455130266e-06,
      "loss": 0.4792,
      "step": 6708
    },
    {
      "epoch": 0.6040198969141777,
      "grad_norm": 1.0617087599371264,
      "learning_rate": 7.1571204861324984e-06,
      "loss": 0.5683,
      "step": 6709
    },
    {
      "epoch": 0.6041099282000495,
      "grad_norm": 0.9510133527841055,
      "learning_rate": 7.154324758894975e-06,
      "loss": 0.5765,
      "step": 6710
    },
    {
      "epoch": 0.6041999594859213,
      "grad_norm": 0.7863072632508223,
      "learning_rate": 7.151529273655437e-06,
      "loss": 0.5053,
      "step": 6711
    },
    {
      "epoch": 0.6042899907717932,
      "grad_norm": 0.9841812991895267,
      "learning_rate": 7.148734030651623e-06,
      "loss": 0.4274,
      "step": 6712
    },
    {
      "epoch": 0.6043800220576651,
      "grad_norm": 0.7912086424618315,
      "learning_rate": 7.1459390301212316e-06,
      "loss": 0.574,
      "step": 6713
    },
    {
      "epoch": 0.6044700533435369,
      "grad_norm": 0.84656849063788,
      "learning_rate": 7.143144272301965e-06,
      "loss": 0.5577,
      "step": 6714
    },
    {
      "epoch": 0.6045600846294087,
      "grad_norm": 0.8723250521610613,
      "learning_rate": 7.140349757431476e-06,
      "loss": 0.5537,
      "step": 6715
    },
    {
      "epoch": 0.6046501159152805,
      "grad_norm": 0.6900668538590583,
      "learning_rate": 7.137555485747425e-06,
      "loss": 0.4875,
      "step": 6716
    },
    {
      "epoch": 0.6047401472011524,
      "grad_norm": 0.829682513842155,
      "learning_rate": 7.1347614574874295e-06,
      "loss": 0.5302,
      "step": 6717
    },
    {
      "epoch": 0.6048301784870243,
      "grad_norm": 0.7648477253515893,
      "learning_rate": 7.131967672889101e-06,
      "loss": 0.4508,
      "step": 6718
    },
    {
      "epoch": 0.6049202097728961,
      "grad_norm": 0.8234054082727783,
      "learning_rate": 7.1291741321900175e-06,
      "loss": 0.5382,
      "step": 6719
    },
    {
      "epoch": 0.6050102410587679,
      "grad_norm": 0.8026030626145629,
      "learning_rate": 7.126380835627753e-06,
      "loss": 0.6026,
      "step": 6720
    },
    {
      "epoch": 0.6051002723446398,
      "grad_norm": 0.7552326634086197,
      "learning_rate": 7.123587783439846e-06,
      "loss": 0.498,
      "step": 6721
    },
    {
      "epoch": 0.6051903036305116,
      "grad_norm": 0.9192502063295892,
      "learning_rate": 7.120794975863821e-06,
      "loss": 0.5031,
      "step": 6722
    },
    {
      "epoch": 0.6052803349163834,
      "grad_norm": 0.9414381555582695,
      "learning_rate": 7.118002413137175e-06,
      "loss": 0.5763,
      "step": 6723
    },
    {
      "epoch": 0.6053703662022553,
      "grad_norm": 0.7651489839144383,
      "learning_rate": 7.115210095497397e-06,
      "loss": 0.505,
      "step": 6724
    },
    {
      "epoch": 0.6054603974881271,
      "grad_norm": 0.7859984133064956,
      "learning_rate": 7.1124180231819415e-06,
      "loss": 0.4499,
      "step": 6725
    },
    {
      "epoch": 0.605550428773999,
      "grad_norm": 1.0337984241792757,
      "learning_rate": 7.109626196428254e-06,
      "loss": 0.5659,
      "step": 6726
    },
    {
      "epoch": 0.6056404600598708,
      "grad_norm": 0.7224565659010077,
      "learning_rate": 7.106834615473743e-06,
      "loss": 0.512,
      "step": 6727
    },
    {
      "epoch": 0.6057304913457426,
      "grad_norm": 0.9617173394618257,
      "learning_rate": 7.104043280555821e-06,
      "loss": 0.5608,
      "step": 6728
    },
    {
      "epoch": 0.6058205226316145,
      "grad_norm": 2.1447540592615715,
      "learning_rate": 7.10125219191185e-06,
      "loss": 0.5125,
      "step": 6729
    },
    {
      "epoch": 0.6059105539174863,
      "grad_norm": 0.7007127397320371,
      "learning_rate": 7.098461349779197e-06,
      "loss": 0.4256,
      "step": 6730
    },
    {
      "epoch": 0.6060005852033582,
      "grad_norm": 0.9217230327726498,
      "learning_rate": 7.09567075439519e-06,
      "loss": 0.501,
      "step": 6731
    },
    {
      "epoch": 0.60609061648923,
      "grad_norm": 0.9115837050679547,
      "learning_rate": 7.092880405997147e-06,
      "loss": 0.481,
      "step": 6732
    },
    {
      "epoch": 0.6061806477751018,
      "grad_norm": 0.8586210965338165,
      "learning_rate": 7.090090304822356e-06,
      "loss": 0.5171,
      "step": 6733
    },
    {
      "epoch": 0.6062706790609736,
      "grad_norm": 0.7450541839083762,
      "learning_rate": 7.087300451108097e-06,
      "loss": 0.5768,
      "step": 6734
    },
    {
      "epoch": 0.6063607103468456,
      "grad_norm": 0.9742242773699088,
      "learning_rate": 7.0845108450916125e-06,
      "loss": 0.6212,
      "step": 6735
    },
    {
      "epoch": 0.6064507416327174,
      "grad_norm": 1.0015640186578982,
      "learning_rate": 7.081721487010139e-06,
      "loss": 0.5307,
      "step": 6736
    },
    {
      "epoch": 0.6065407729185892,
      "grad_norm": 0.8125428138751687,
      "learning_rate": 7.0789323771008775e-06,
      "loss": 0.5401,
      "step": 6737
    },
    {
      "epoch": 0.606630804204461,
      "grad_norm": 0.7354477044932716,
      "learning_rate": 7.076143515601026e-06,
      "loss": 0.4594,
      "step": 6738
    },
    {
      "epoch": 0.6067208354903328,
      "grad_norm": 0.8220850811949626,
      "learning_rate": 7.073354902747742e-06,
      "loss": 0.5223,
      "step": 6739
    },
    {
      "epoch": 0.6068108667762048,
      "grad_norm": 0.7871400964905498,
      "learning_rate": 7.0705665387781786e-06,
      "loss": 0.471,
      "step": 6740
    },
    {
      "epoch": 0.6069008980620766,
      "grad_norm": 0.8118928695016027,
      "learning_rate": 7.06777842392945e-06,
      "loss": 0.5365,
      "step": 6741
    },
    {
      "epoch": 0.6069909293479484,
      "grad_norm": 0.839618767153445,
      "learning_rate": 7.0649905584386715e-06,
      "loss": 0.6628,
      "step": 6742
    },
    {
      "epoch": 0.6070809606338202,
      "grad_norm": 0.9387975313602867,
      "learning_rate": 7.0622029425429115e-06,
      "loss": 0.507,
      "step": 6743
    },
    {
      "epoch": 0.607170991919692,
      "grad_norm": 0.8248089423319422,
      "learning_rate": 7.059415576479243e-06,
      "loss": 0.4685,
      "step": 6744
    },
    {
      "epoch": 0.607261023205564,
      "grad_norm": 1.0050394049576186,
      "learning_rate": 7.056628460484696e-06,
      "loss": 0.5614,
      "step": 6745
    },
    {
      "epoch": 0.6073510544914358,
      "grad_norm": 0.9496367617765632,
      "learning_rate": 7.053841594796296e-06,
      "loss": 0.6057,
      "step": 6746
    },
    {
      "epoch": 0.6074410857773076,
      "grad_norm": 0.6858905942774022,
      "learning_rate": 7.051054979651031e-06,
      "loss": 0.5648,
      "step": 6747
    },
    {
      "epoch": 0.6075311170631794,
      "grad_norm": 0.8155994518164564,
      "learning_rate": 7.048268615285887e-06,
      "loss": 0.4822,
      "step": 6748
    },
    {
      "epoch": 0.6076211483490513,
      "grad_norm": 1.7850991616463217,
      "learning_rate": 7.045482501937809e-06,
      "loss": 0.4491,
      "step": 6749
    },
    {
      "epoch": 0.6077111796349232,
      "grad_norm": 0.7542278909830824,
      "learning_rate": 7.042696639843737e-06,
      "loss": 0.4485,
      "step": 6750
    },
    {
      "epoch": 0.607801210920795,
      "grad_norm": 0.7903259790718755,
      "learning_rate": 7.039911029240574e-06,
      "loss": 0.5207,
      "step": 6751
    },
    {
      "epoch": 0.6078912422066668,
      "grad_norm": 0.8554184143443133,
      "learning_rate": 7.03712567036522e-06,
      "loss": 0.5922,
      "step": 6752
    },
    {
      "epoch": 0.6079812734925386,
      "grad_norm": 0.8455333071019612,
      "learning_rate": 7.034340563454537e-06,
      "loss": 0.4929,
      "step": 6753
    },
    {
      "epoch": 0.6080713047784105,
      "grad_norm": 0.9142699973583888,
      "learning_rate": 7.031555708745374e-06,
      "loss": 0.5589,
      "step": 6754
    },
    {
      "epoch": 0.6081613360642824,
      "grad_norm": 0.8734448951450938,
      "learning_rate": 7.028771106474556e-06,
      "loss": 0.5786,
      "step": 6755
    },
    {
      "epoch": 0.6082513673501542,
      "grad_norm": 0.9010198003864452,
      "learning_rate": 7.025986756878892e-06,
      "loss": 0.5064,
      "step": 6756
    },
    {
      "epoch": 0.608341398636026,
      "grad_norm": 0.7729835479379845,
      "learning_rate": 7.0232026601951545e-06,
      "loss": 0.4684,
      "step": 6757
    },
    {
      "epoch": 0.6084314299218979,
      "grad_norm": 0.778436824222565,
      "learning_rate": 7.0204188166601176e-06,
      "loss": 0.5,
      "step": 6758
    },
    {
      "epoch": 0.6085214612077697,
      "grad_norm": 0.8430815971070121,
      "learning_rate": 7.017635226510509e-06,
      "loss": 0.6032,
      "step": 6759
    },
    {
      "epoch": 0.6086114924936415,
      "grad_norm": 1.0097626078129853,
      "learning_rate": 7.014851889983058e-06,
      "loss": 0.5933,
      "step": 6760
    },
    {
      "epoch": 0.6087015237795134,
      "grad_norm": 1.2451397356628302,
      "learning_rate": 7.012068807314449e-06,
      "loss": 0.5034,
      "step": 6761
    },
    {
      "epoch": 0.6087915550653852,
      "grad_norm": 0.7872394371189001,
      "learning_rate": 7.00928597874137e-06,
      "loss": 0.5608,
      "step": 6762
    },
    {
      "epoch": 0.6088815863512571,
      "grad_norm": 0.6916204440182654,
      "learning_rate": 7.006503404500467e-06,
      "loss": 0.496,
      "step": 6763
    },
    {
      "epoch": 0.6089716176371289,
      "grad_norm": 0.8497326268040066,
      "learning_rate": 7.003721084828373e-06,
      "loss": 0.484,
      "step": 6764
    },
    {
      "epoch": 0.6090616489230007,
      "grad_norm": 0.8765589936601123,
      "learning_rate": 7.000939019961695e-06,
      "loss": 0.5717,
      "step": 6765
    },
    {
      "epoch": 0.6091516802088726,
      "grad_norm": 0.851323879017288,
      "learning_rate": 6.998157210137031e-06,
      "loss": 0.4721,
      "step": 6766
    },
    {
      "epoch": 0.6092417114947444,
      "grad_norm": 0.7523158266248563,
      "learning_rate": 6.995375655590938e-06,
      "loss": 0.521,
      "step": 6767
    },
    {
      "epoch": 0.6093317427806163,
      "grad_norm": 0.7357034991288514,
      "learning_rate": 6.992594356559968e-06,
      "loss": 0.4336,
      "step": 6768
    },
    {
      "epoch": 0.6094217740664881,
      "grad_norm": 0.9317161438434992,
      "learning_rate": 6.9898133132806354e-06,
      "loss": 0.5159,
      "step": 6769
    },
    {
      "epoch": 0.6095118053523599,
      "grad_norm": 0.9054824936917992,
      "learning_rate": 6.987032525989456e-06,
      "loss": 0.5499,
      "step": 6770
    },
    {
      "epoch": 0.6096018366382318,
      "grad_norm": 0.6919138925142404,
      "learning_rate": 6.984251994922895e-06,
      "loss": 0.5361,
      "step": 6771
    },
    {
      "epoch": 0.6096918679241037,
      "grad_norm": 1.0230846194573808,
      "learning_rate": 6.9814717203174196e-06,
      "loss": 0.565,
      "step": 6772
    },
    {
      "epoch": 0.6097818992099755,
      "grad_norm": 0.7933704765317345,
      "learning_rate": 6.978691702409463e-06,
      "loss": 0.6085,
      "step": 6773
    },
    {
      "epoch": 0.6098719304958473,
      "grad_norm": 0.9673222078982491,
      "learning_rate": 6.975911941435441e-06,
      "loss": 0.4625,
      "step": 6774
    },
    {
      "epoch": 0.6099619617817191,
      "grad_norm": 0.810855178446511,
      "learning_rate": 6.973132437631743e-06,
      "loss": 0.5233,
      "step": 6775
    },
    {
      "epoch": 0.610051993067591,
      "grad_norm": 0.7688019922373976,
      "learning_rate": 6.970353191234744e-06,
      "loss": 0.4338,
      "step": 6776
    },
    {
      "epoch": 0.6101420243534629,
      "grad_norm": 0.7612702978046011,
      "learning_rate": 6.967574202480789e-06,
      "loss": 0.5573,
      "step": 6777
    },
    {
      "epoch": 0.6102320556393347,
      "grad_norm": 0.8070812423562387,
      "learning_rate": 6.96479547160621e-06,
      "loss": 0.4597,
      "step": 6778
    },
    {
      "epoch": 0.6103220869252065,
      "grad_norm": 0.8966021719617133,
      "learning_rate": 6.962016998847303e-06,
      "loss": 0.4965,
      "step": 6779
    },
    {
      "epoch": 0.6104121182110783,
      "grad_norm": 0.7815051453785925,
      "learning_rate": 6.959238784440362e-06,
      "loss": 0.5669,
      "step": 6780
    },
    {
      "epoch": 0.6105021494969501,
      "grad_norm": 0.8327675710511049,
      "learning_rate": 6.956460828621641e-06,
      "loss": 0.5345,
      "step": 6781
    },
    {
      "epoch": 0.6105921807828221,
      "grad_norm": 0.8815019146951459,
      "learning_rate": 6.953683131627382e-06,
      "loss": 0.4894,
      "step": 6782
    },
    {
      "epoch": 0.6106822120686939,
      "grad_norm": 0.8094480249914172,
      "learning_rate": 6.950905693693799e-06,
      "loss": 0.5555,
      "step": 6783
    },
    {
      "epoch": 0.6107722433545657,
      "grad_norm": 1.1302947771327942,
      "learning_rate": 6.948128515057093e-06,
      "loss": 0.5874,
      "step": 6784
    },
    {
      "epoch": 0.6108622746404375,
      "grad_norm": 0.8275865191891763,
      "learning_rate": 6.945351595953429e-06,
      "loss": 0.4969,
      "step": 6785
    },
    {
      "epoch": 0.6109523059263094,
      "grad_norm": 1.1258642505752572,
      "learning_rate": 6.942574936618965e-06,
      "loss": 0.6558,
      "step": 6786
    },
    {
      "epoch": 0.6110423372121813,
      "grad_norm": 0.6976383844495456,
      "learning_rate": 6.939798537289826e-06,
      "loss": 0.4781,
      "step": 6787
    },
    {
      "epoch": 0.6111323684980531,
      "grad_norm": 0.6960086051774848,
      "learning_rate": 6.93702239820212e-06,
      "loss": 0.5022,
      "step": 6788
    },
    {
      "epoch": 0.6112223997839249,
      "grad_norm": 0.6824677925483372,
      "learning_rate": 6.934246519591927e-06,
      "loss": 0.5171,
      "step": 6789
    },
    {
      "epoch": 0.6113124310697967,
      "grad_norm": 1.1308993472117832,
      "learning_rate": 6.931470901695318e-06,
      "loss": 0.452,
      "step": 6790
    },
    {
      "epoch": 0.6114024623556686,
      "grad_norm": 0.9822336630669823,
      "learning_rate": 6.928695544748329e-06,
      "loss": 0.6123,
      "step": 6791
    },
    {
      "epoch": 0.6114924936415405,
      "grad_norm": 0.85112502555952,
      "learning_rate": 6.925920448986978e-06,
      "loss": 0.6356,
      "step": 6792
    },
    {
      "epoch": 0.6115825249274123,
      "grad_norm": 1.091339456345309,
      "learning_rate": 6.923145614647257e-06,
      "loss": 0.5205,
      "step": 6793
    },
    {
      "epoch": 0.6116725562132841,
      "grad_norm": 1.2930255927863548,
      "learning_rate": 6.9203710419651484e-06,
      "loss": 0.5605,
      "step": 6794
    },
    {
      "epoch": 0.6117625874991559,
      "grad_norm": 0.8126640846183842,
      "learning_rate": 6.917596731176596e-06,
      "loss": 0.5532,
      "step": 6795
    },
    {
      "epoch": 0.6118526187850278,
      "grad_norm": 1.1236277252367144,
      "learning_rate": 6.914822682517532e-06,
      "loss": 0.5203,
      "step": 6796
    },
    {
      "epoch": 0.6119426500708997,
      "grad_norm": 0.9068023851676774,
      "learning_rate": 6.912048896223862e-06,
      "loss": 0.6119,
      "step": 6797
    },
    {
      "epoch": 0.6120326813567715,
      "grad_norm": 0.6785319778897007,
      "learning_rate": 6.909275372531474e-06,
      "loss": 0.5927,
      "step": 6798
    },
    {
      "epoch": 0.6121227126426433,
      "grad_norm": 0.780876342518186,
      "learning_rate": 6.906502111676224e-06,
      "loss": 0.4993,
      "step": 6799
    },
    {
      "epoch": 0.6122127439285152,
      "grad_norm": 0.7856002876509071,
      "learning_rate": 6.903729113893957e-06,
      "loss": 0.5422,
      "step": 6800
    },
    {
      "epoch": 0.612302775214387,
      "grad_norm": 0.8241958598168762,
      "learning_rate": 6.900956379420489e-06,
      "loss": 0.4616,
      "step": 6801
    },
    {
      "epoch": 0.6123928065002588,
      "grad_norm": 0.9953213842145318,
      "learning_rate": 6.898183908491617e-06,
      "loss": 0.5671,
      "step": 6802
    },
    {
      "epoch": 0.6124828377861307,
      "grad_norm": 0.7590880585527278,
      "learning_rate": 6.895411701343107e-06,
      "loss": 0.4537,
      "step": 6803
    },
    {
      "epoch": 0.6125728690720025,
      "grad_norm": 0.7496533681901122,
      "learning_rate": 6.892639758210717e-06,
      "loss": 0.4716,
      "step": 6804
    },
    {
      "epoch": 0.6126629003578744,
      "grad_norm": 0.8545033143460076,
      "learning_rate": 6.889868079330169e-06,
      "loss": 0.5808,
      "step": 6805
    },
    {
      "epoch": 0.6127529316437462,
      "grad_norm": 0.6578039126624704,
      "learning_rate": 6.887096664937174e-06,
      "loss": 0.5017,
      "step": 6806
    },
    {
      "epoch": 0.612842962929618,
      "grad_norm": 0.8928484512793409,
      "learning_rate": 6.884325515267406e-06,
      "loss": 0.4651,
      "step": 6807
    },
    {
      "epoch": 0.6129329942154899,
      "grad_norm": 0.8519380847774863,
      "learning_rate": 6.881554630556535e-06,
      "loss": 0.5198,
      "step": 6808
    },
    {
      "epoch": 0.6130230255013617,
      "grad_norm": 0.8227007733431135,
      "learning_rate": 6.878784011040194e-06,
      "loss": 0.5708,
      "step": 6809
    },
    {
      "epoch": 0.6131130567872336,
      "grad_norm": 0.7458735468405513,
      "learning_rate": 6.876013656954e-06,
      "loss": 0.491,
      "step": 6810
    },
    {
      "epoch": 0.6132030880731054,
      "grad_norm": 1.0737775846323,
      "learning_rate": 6.873243568533541e-06,
      "loss": 0.5463,
      "step": 6811
    },
    {
      "epoch": 0.6132931193589772,
      "grad_norm": 0.7851817077068701,
      "learning_rate": 6.870473746014397e-06,
      "loss": 0.5072,
      "step": 6812
    },
    {
      "epoch": 0.613383150644849,
      "grad_norm": 0.8259326363183246,
      "learning_rate": 6.8677041896321e-06,
      "loss": 0.5078,
      "step": 6813
    },
    {
      "epoch": 0.613473181930721,
      "grad_norm": 0.762370824721946,
      "learning_rate": 6.864934899622191e-06,
      "loss": 0.5581,
      "step": 6814
    },
    {
      "epoch": 0.6135632132165928,
      "grad_norm": 0.7366748784726507,
      "learning_rate": 6.86216587622016e-06,
      "loss": 0.5857,
      "step": 6815
    },
    {
      "epoch": 0.6136532445024646,
      "grad_norm": 0.7621003710217412,
      "learning_rate": 6.859397119661495e-06,
      "loss": 0.4407,
      "step": 6816
    },
    {
      "epoch": 0.6137432757883364,
      "grad_norm": 0.7814247985957878,
      "learning_rate": 6.856628630181645e-06,
      "loss": 0.5111,
      "step": 6817
    },
    {
      "epoch": 0.6138333070742082,
      "grad_norm": 0.8637719402143517,
      "learning_rate": 6.853860408016052e-06,
      "loss": 0.5422,
      "step": 6818
    },
    {
      "epoch": 0.6139233383600802,
      "grad_norm": 0.8154551929904308,
      "learning_rate": 6.851092453400121e-06,
      "loss": 0.5237,
      "step": 6819
    },
    {
      "epoch": 0.614013369645952,
      "grad_norm": 0.9706699187833709,
      "learning_rate": 6.8483247665692455e-06,
      "loss": 0.5418,
      "step": 6820
    },
    {
      "epoch": 0.6141034009318238,
      "grad_norm": 0.8758541876762471,
      "learning_rate": 6.8455573477587845e-06,
      "loss": 0.4646,
      "step": 6821
    },
    {
      "epoch": 0.6141934322176956,
      "grad_norm": 0.870384276312924,
      "learning_rate": 6.84279019720409e-06,
      "loss": 0.5589,
      "step": 6822
    },
    {
      "epoch": 0.6142834635035674,
      "grad_norm": 0.8841983389787779,
      "learning_rate": 6.840023315140476e-06,
      "loss": 0.6091,
      "step": 6823
    },
    {
      "epoch": 0.6143734947894394,
      "grad_norm": 0.7953504702771994,
      "learning_rate": 6.837256701803243e-06,
      "loss": 0.5278,
      "step": 6824
    },
    {
      "epoch": 0.6144635260753112,
      "grad_norm": 0.7786963958065077,
      "learning_rate": 6.8344903574276585e-06,
      "loss": 0.5025,
      "step": 6825
    },
    {
      "epoch": 0.614553557361183,
      "grad_norm": 1.0150028575021979,
      "learning_rate": 6.831724282248987e-06,
      "loss": 0.563,
      "step": 6826
    },
    {
      "epoch": 0.6146435886470548,
      "grad_norm": 0.7547155064303064,
      "learning_rate": 6.828958476502443e-06,
      "loss": 0.532,
      "step": 6827
    },
    {
      "epoch": 0.6147336199329267,
      "grad_norm": 1.0158549484608315,
      "learning_rate": 6.826192940423244e-06,
      "loss": 0.529,
      "step": 6828
    },
    {
      "epoch": 0.6148236512187986,
      "grad_norm": 0.756607785050984,
      "learning_rate": 6.823427674246566e-06,
      "loss": 0.4733,
      "step": 6829
    },
    {
      "epoch": 0.6149136825046704,
      "grad_norm": 0.7724811318416456,
      "learning_rate": 6.820662678207573e-06,
      "loss": 0.5527,
      "step": 6830
    },
    {
      "epoch": 0.6150037137905422,
      "grad_norm": 0.7583880470716998,
      "learning_rate": 6.817897952541397e-06,
      "loss": 0.5949,
      "step": 6831
    },
    {
      "epoch": 0.615093745076414,
      "grad_norm": 0.9177738944489571,
      "learning_rate": 6.815133497483157e-06,
      "loss": 0.5265,
      "step": 6832
    },
    {
      "epoch": 0.6151837763622859,
      "grad_norm": 0.8880338497823249,
      "learning_rate": 6.812369313267941e-06,
      "loss": 0.4994,
      "step": 6833
    },
    {
      "epoch": 0.6152738076481578,
      "grad_norm": 0.8455428506373991,
      "learning_rate": 6.809605400130821e-06,
      "loss": 0.5407,
      "step": 6834
    },
    {
      "epoch": 0.6153638389340296,
      "grad_norm": 1.0523379938966178,
      "learning_rate": 6.8068417583068325e-06,
      "loss": 0.5398,
      "step": 6835
    },
    {
      "epoch": 0.6154538702199014,
      "grad_norm": 0.8891892477096028,
      "learning_rate": 6.8040783880310105e-06,
      "loss": 0.4336,
      "step": 6836
    },
    {
      "epoch": 0.6155439015057732,
      "grad_norm": 0.9536074108916299,
      "learning_rate": 6.801315289538341e-06,
      "loss": 0.5807,
      "step": 6837
    },
    {
      "epoch": 0.6156339327916451,
      "grad_norm": 0.8889906415921882,
      "learning_rate": 6.798552463063811e-06,
      "loss": 0.4295,
      "step": 6838
    },
    {
      "epoch": 0.615723964077517,
      "grad_norm": 0.9201609705026068,
      "learning_rate": 6.795789908842362e-06,
      "loss": 0.4829,
      "step": 6839
    },
    {
      "epoch": 0.6158139953633888,
      "grad_norm": 0.9438442188318507,
      "learning_rate": 6.793027627108935e-06,
      "loss": 0.5618,
      "step": 6840
    },
    {
      "epoch": 0.6159040266492606,
      "grad_norm": 0.762837892009563,
      "learning_rate": 6.790265618098424e-06,
      "loss": 0.5298,
      "step": 6841
    },
    {
      "epoch": 0.6159940579351325,
      "grad_norm": 0.8964146989751504,
      "learning_rate": 6.787503882045722e-06,
      "loss": 0.4884,
      "step": 6842
    },
    {
      "epoch": 0.6160840892210043,
      "grad_norm": 0.8610071801367928,
      "learning_rate": 6.784742419185682e-06,
      "loss": 0.4689,
      "step": 6843
    },
    {
      "epoch": 0.6161741205068761,
      "grad_norm": 0.7907773542329853,
      "learning_rate": 6.781981229753145e-06,
      "loss": 0.5621,
      "step": 6844
    },
    {
      "epoch": 0.616264151792748,
      "grad_norm": 0.7206243627389106,
      "learning_rate": 6.77922031398292e-06,
      "loss": 0.4959,
      "step": 6845
    },
    {
      "epoch": 0.6163541830786198,
      "grad_norm": 2.015607333079873,
      "learning_rate": 6.776459672109802e-06,
      "loss": 0.6489,
      "step": 6846
    },
    {
      "epoch": 0.6164442143644917,
      "grad_norm": 0.9489219201741586,
      "learning_rate": 6.7736993043685526e-06,
      "loss": 0.5368,
      "step": 6847
    },
    {
      "epoch": 0.6165342456503635,
      "grad_norm": 0.8188392585806877,
      "learning_rate": 6.770939210993922e-06,
      "loss": 0.4493,
      "step": 6848
    },
    {
      "epoch": 0.6166242769362353,
      "grad_norm": 0.8131913254828859,
      "learning_rate": 6.7681793922206195e-06,
      "loss": 0.5001,
      "step": 6849
    },
    {
      "epoch": 0.6167143082221072,
      "grad_norm": 0.8559834760079192,
      "learning_rate": 6.765419848283354e-06,
      "loss": 0.4706,
      "step": 6850
    },
    {
      "epoch": 0.616804339507979,
      "grad_norm": 1.1085418841384815,
      "learning_rate": 6.762660579416791e-06,
      "loss": 0.5299,
      "step": 6851
    },
    {
      "epoch": 0.6168943707938509,
      "grad_norm": 1.041506938174306,
      "learning_rate": 6.759901585855584e-06,
      "loss": 0.5029,
      "step": 6852
    },
    {
      "epoch": 0.6169844020797227,
      "grad_norm": 0.9593218410891801,
      "learning_rate": 6.757142867834354e-06,
      "loss": 0.616,
      "step": 6853
    },
    {
      "epoch": 0.6170744333655945,
      "grad_norm": 0.9383072420307517,
      "learning_rate": 6.7543844255877145e-06,
      "loss": 0.5431,
      "step": 6854
    },
    {
      "epoch": 0.6171644646514663,
      "grad_norm": 0.9420615658178626,
      "learning_rate": 6.751626259350232e-06,
      "loss": 0.5952,
      "step": 6855
    },
    {
      "epoch": 0.6172544959373383,
      "grad_norm": 0.9668845829440098,
      "learning_rate": 6.748868369356474e-06,
      "loss": 0.5251,
      "step": 6856
    },
    {
      "epoch": 0.6173445272232101,
      "grad_norm": 1.1163671863458653,
      "learning_rate": 6.746110755840966e-06,
      "loss": 0.4832,
      "step": 6857
    },
    {
      "epoch": 0.6174345585090819,
      "grad_norm": 0.888605046805267,
      "learning_rate": 6.7433534190382234e-06,
      "loss": 0.5112,
      "step": 6858
    },
    {
      "epoch": 0.6175245897949537,
      "grad_norm": 1.113247123386564,
      "learning_rate": 6.740596359182723e-06,
      "loss": 0.584,
      "step": 6859
    },
    {
      "epoch": 0.6176146210808255,
      "grad_norm": 0.7915312417053844,
      "learning_rate": 6.737839576508935e-06,
      "loss": 0.5215,
      "step": 6860
    },
    {
      "epoch": 0.6177046523666975,
      "grad_norm": 0.8712121063681476,
      "learning_rate": 6.7350830712512946e-06,
      "loss": 0.5712,
      "step": 6861
    },
    {
      "epoch": 0.6177946836525693,
      "grad_norm": 1.040493164777712,
      "learning_rate": 6.732326843644219e-06,
      "loss": 0.5657,
      "step": 6862
    },
    {
      "epoch": 0.6178847149384411,
      "grad_norm": 1.0156608145315404,
      "learning_rate": 6.729570893922094e-06,
      "loss": 0.5676,
      "step": 6863
    },
    {
      "epoch": 0.6179747462243129,
      "grad_norm": 0.9600872853323874,
      "learning_rate": 6.726815222319294e-06,
      "loss": 0.4744,
      "step": 6864
    },
    {
      "epoch": 0.6180647775101847,
      "grad_norm": 0.7933751866604737,
      "learning_rate": 6.7240598290701585e-06,
      "loss": 0.5341,
      "step": 6865
    },
    {
      "epoch": 0.6181548087960567,
      "grad_norm": 1.0513190737305145,
      "learning_rate": 6.721304714409011e-06,
      "loss": 0.4922,
      "step": 6866
    },
    {
      "epoch": 0.6182448400819285,
      "grad_norm": 1.1981193532397165,
      "learning_rate": 6.718549878570143e-06,
      "loss": 0.6231,
      "step": 6867
    },
    {
      "epoch": 0.6183348713678003,
      "grad_norm": 0.9618683213072775,
      "learning_rate": 6.715795321787837e-06,
      "loss": 0.5779,
      "step": 6868
    },
    {
      "epoch": 0.6184249026536721,
      "grad_norm": 0.8929469161345098,
      "learning_rate": 6.713041044296329e-06,
      "loss": 0.4662,
      "step": 6869
    },
    {
      "epoch": 0.618514933939544,
      "grad_norm": 1.011532121776601,
      "learning_rate": 6.710287046329856e-06,
      "loss": 0.5828,
      "step": 6870
    },
    {
      "epoch": 0.6186049652254159,
      "grad_norm": 0.9278587480673804,
      "learning_rate": 6.707533328122614e-06,
      "loss": 0.4351,
      "step": 6871
    },
    {
      "epoch": 0.6186949965112877,
      "grad_norm": 0.7814210128623199,
      "learning_rate": 6.704779889908783e-06,
      "loss": 0.456,
      "step": 6872
    },
    {
      "epoch": 0.6187850277971595,
      "grad_norm": 0.8441525202459694,
      "learning_rate": 6.702026731922514e-06,
      "loss": 0.4753,
      "step": 6873
    },
    {
      "epoch": 0.6188750590830313,
      "grad_norm": 0.7159549639772036,
      "learning_rate": 6.699273854397943e-06,
      "loss": 0.5312,
      "step": 6874
    },
    {
      "epoch": 0.6189650903689032,
      "grad_norm": 0.8178859805862453,
      "learning_rate": 6.696521257569171e-06,
      "loss": 0.4188,
      "step": 6875
    },
    {
      "epoch": 0.619055121654775,
      "grad_norm": 0.9507303692882249,
      "learning_rate": 6.693768941670284e-06,
      "loss": 0.4883,
      "step": 6876
    },
    {
      "epoch": 0.6191451529406469,
      "grad_norm": 1.357236367448806,
      "learning_rate": 6.6910169069353345e-06,
      "loss": 0.6316,
      "step": 6877
    },
    {
      "epoch": 0.6192351842265187,
      "grad_norm": 0.7499257416159736,
      "learning_rate": 6.688265153598368e-06,
      "loss": 0.5477,
      "step": 6878
    },
    {
      "epoch": 0.6193252155123905,
      "grad_norm": 0.7850043261553049,
      "learning_rate": 6.685513681893385e-06,
      "loss": 0.5313,
      "step": 6879
    },
    {
      "epoch": 0.6194152467982624,
      "grad_norm": 1.0896403841188422,
      "learning_rate": 6.68276249205438e-06,
      "loss": 0.4825,
      "step": 6880
    },
    {
      "epoch": 0.6195052780841342,
      "grad_norm": 1.1068393832720718,
      "learning_rate": 6.680011584315308e-06,
      "loss": 0.554,
      "step": 6881
    },
    {
      "epoch": 0.6195953093700061,
      "grad_norm": 0.9898043320378668,
      "learning_rate": 6.677260958910119e-06,
      "loss": 0.4522,
      "step": 6882
    },
    {
      "epoch": 0.6196853406558779,
      "grad_norm": 0.8833369732956357,
      "learning_rate": 6.674510616072715e-06,
      "loss": 0.4732,
      "step": 6883
    },
    {
      "epoch": 0.6197753719417498,
      "grad_norm": 0.9430505906677237,
      "learning_rate": 6.6717605560369966e-06,
      "loss": 0.5578,
      "step": 6884
    },
    {
      "epoch": 0.6198654032276216,
      "grad_norm": 0.9418083760569875,
      "learning_rate": 6.669010779036825e-06,
      "loss": 0.5552,
      "step": 6885
    },
    {
      "epoch": 0.6199554345134934,
      "grad_norm": 0.8418186320304868,
      "learning_rate": 6.666261285306048e-06,
      "loss": 0.5325,
      "step": 6886
    },
    {
      "epoch": 0.6200454657993653,
      "grad_norm": 0.725686478746634,
      "learning_rate": 6.663512075078477e-06,
      "loss": 0.562,
      "step": 6887
    },
    {
      "epoch": 0.6201354970852371,
      "grad_norm": 0.7303675536769989,
      "learning_rate": 6.660763148587914e-06,
      "loss": 0.4381,
      "step": 6888
    },
    {
      "epoch": 0.620225528371109,
      "grad_norm": 0.828599163100507,
      "learning_rate": 6.6580145060681255e-06,
      "loss": 0.5393,
      "step": 6889
    },
    {
      "epoch": 0.6203155596569808,
      "grad_norm": 0.811360734328501,
      "learning_rate": 6.655266147752861e-06,
      "loss": 0.6018,
      "step": 6890
    },
    {
      "epoch": 0.6204055909428526,
      "grad_norm": 0.8775729335027153,
      "learning_rate": 6.6525180738758355e-06,
      "loss": 0.5293,
      "step": 6891
    },
    {
      "epoch": 0.6204956222287245,
      "grad_norm": 0.7824605486221327,
      "learning_rate": 6.649770284670756e-06,
      "loss": 0.6026,
      "step": 6892
    },
    {
      "epoch": 0.6205856535145963,
      "grad_norm": 0.7394376203067501,
      "learning_rate": 6.6470227803712885e-06,
      "loss": 0.5168,
      "step": 6893
    },
    {
      "epoch": 0.6206756848004682,
      "grad_norm": 0.8838201541726878,
      "learning_rate": 6.644275561211089e-06,
      "loss": 0.4528,
      "step": 6894
    },
    {
      "epoch": 0.62076571608634,
      "grad_norm": 0.7802436280061764,
      "learning_rate": 6.6415286274237744e-06,
      "loss": 0.4474,
      "step": 6895
    },
    {
      "epoch": 0.6208557473722118,
      "grad_norm": 0.8710246786170248,
      "learning_rate": 6.638781979242958e-06,
      "loss": 0.5935,
      "step": 6896
    },
    {
      "epoch": 0.6209457786580836,
      "grad_norm": 0.7978206061323456,
      "learning_rate": 6.636035616902202e-06,
      "loss": 0.5403,
      "step": 6897
    },
    {
      "epoch": 0.6210358099439556,
      "grad_norm": 0.7255525376837249,
      "learning_rate": 6.633289540635071e-06,
      "loss": 0.4629,
      "step": 6898
    },
    {
      "epoch": 0.6211258412298274,
      "grad_norm": 0.6831328532334754,
      "learning_rate": 6.630543750675084e-06,
      "loss": 0.5691,
      "step": 6899
    },
    {
      "epoch": 0.6212158725156992,
      "grad_norm": 0.9968156728622546,
      "learning_rate": 6.6277982472557535e-06,
      "loss": 0.6381,
      "step": 6900
    },
    {
      "epoch": 0.621305903801571,
      "grad_norm": 0.9337505652465414,
      "learning_rate": 6.6250530306105485e-06,
      "loss": 0.5207,
      "step": 6901
    },
    {
      "epoch": 0.6213959350874428,
      "grad_norm": 0.8883154474267217,
      "learning_rate": 6.622308100972934e-06,
      "loss": 0.5209,
      "step": 6902
    },
    {
      "epoch": 0.6214859663733148,
      "grad_norm": 0.8062737936505538,
      "learning_rate": 6.619563458576333e-06,
      "loss": 0.5899,
      "step": 6903
    },
    {
      "epoch": 0.6215759976591866,
      "grad_norm": 1.1708622060647225,
      "learning_rate": 6.616819103654157e-06,
      "loss": 0.5933,
      "step": 6904
    },
    {
      "epoch": 0.6216660289450584,
      "grad_norm": 0.7092960249619543,
      "learning_rate": 6.6140750364397825e-06,
      "loss": 0.5603,
      "step": 6905
    },
    {
      "epoch": 0.6217560602309302,
      "grad_norm": 0.7140069672038648,
      "learning_rate": 6.6113312571665715e-06,
      "loss": 0.5053,
      "step": 6906
    },
    {
      "epoch": 0.621846091516802,
      "grad_norm": 0.8022491516202582,
      "learning_rate": 6.608587766067853e-06,
      "loss": 0.4972,
      "step": 6907
    },
    {
      "epoch": 0.621936122802674,
      "grad_norm": 0.9173241970989333,
      "learning_rate": 6.605844563376939e-06,
      "loss": 0.5473,
      "step": 6908
    },
    {
      "epoch": 0.6220261540885458,
      "grad_norm": 0.8576117374055431,
      "learning_rate": 6.603101649327107e-06,
      "loss": 0.5534,
      "step": 6909
    },
    {
      "epoch": 0.6221161853744176,
      "grad_norm": 0.8686291627286267,
      "learning_rate": 6.600359024151626e-06,
      "loss": 0.56,
      "step": 6910
    },
    {
      "epoch": 0.6222062166602894,
      "grad_norm": 0.7954892339524722,
      "learning_rate": 6.597616688083719e-06,
      "loss": 0.5512,
      "step": 6911
    },
    {
      "epoch": 0.6222962479461613,
      "grad_norm": 0.8629849393344609,
      "learning_rate": 6.594874641356604e-06,
      "loss": 0.566,
      "step": 6912
    },
    {
      "epoch": 0.6223862792320332,
      "grad_norm": 0.7582115965664389,
      "learning_rate": 6.5921328842034635e-06,
      "loss": 0.5572,
      "step": 6913
    },
    {
      "epoch": 0.622476310517905,
      "grad_norm": 0.8150460951289256,
      "learning_rate": 6.58939141685746e-06,
      "loss": 0.4466,
      "step": 6914
    },
    {
      "epoch": 0.6225663418037768,
      "grad_norm": 0.8394547266673607,
      "learning_rate": 6.586650239551724e-06,
      "loss": 0.5027,
      "step": 6915
    },
    {
      "epoch": 0.6226563730896486,
      "grad_norm": 0.7029769040314673,
      "learning_rate": 6.583909352519375e-06,
      "loss": 0.5482,
      "step": 6916
    },
    {
      "epoch": 0.6227464043755205,
      "grad_norm": 0.6366717260438777,
      "learning_rate": 6.581168755993494e-06,
      "loss": 0.5326,
      "step": 6917
    },
    {
      "epoch": 0.6228364356613924,
      "grad_norm": 0.9532457176700345,
      "learning_rate": 6.578428450207148e-06,
      "loss": 0.5014,
      "step": 6918
    },
    {
      "epoch": 0.6229264669472642,
      "grad_norm": 0.8339185659704965,
      "learning_rate": 6.5756884353933656e-06,
      "loss": 0.5216,
      "step": 6919
    },
    {
      "epoch": 0.623016498233136,
      "grad_norm": 0.7957798615169858,
      "learning_rate": 6.5729487117851695e-06,
      "loss": 0.5008,
      "step": 6920
    },
    {
      "epoch": 0.6231065295190078,
      "grad_norm": 0.6499248134953767,
      "learning_rate": 6.570209279615541e-06,
      "loss": 0.4633,
      "step": 6921
    },
    {
      "epoch": 0.6231965608048797,
      "grad_norm": 0.8255496587141142,
      "learning_rate": 6.5674701391174475e-06,
      "loss": 0.5189,
      "step": 6922
    },
    {
      "epoch": 0.6232865920907515,
      "grad_norm": 0.818695946420395,
      "learning_rate": 6.56473129052382e-06,
      "loss": 0.5438,
      "step": 6923
    },
    {
      "epoch": 0.6233766233766234,
      "grad_norm": 0.9003157863511758,
      "learning_rate": 6.561992734067584e-06,
      "loss": 0.5138,
      "step": 6924
    },
    {
      "epoch": 0.6234666546624952,
      "grad_norm": 0.7042392313581582,
      "learning_rate": 6.559254469981613e-06,
      "loss": 0.4625,
      "step": 6925
    },
    {
      "epoch": 0.6235566859483671,
      "grad_norm": 0.8748816041185963,
      "learning_rate": 6.556516498498784e-06,
      "loss": 0.648,
      "step": 6926
    },
    {
      "epoch": 0.6236467172342389,
      "grad_norm": 0.7749674216651186,
      "learning_rate": 6.553778819851927e-06,
      "loss": 0.5313,
      "step": 6927
    },
    {
      "epoch": 0.6237367485201107,
      "grad_norm": 0.8150959239639655,
      "learning_rate": 6.551041434273862e-06,
      "loss": 0.6104,
      "step": 6928
    },
    {
      "epoch": 0.6238267798059826,
      "grad_norm": 0.880518940095203,
      "learning_rate": 6.5483043419973695e-06,
      "loss": 0.5278,
      "step": 6929
    },
    {
      "epoch": 0.6239168110918544,
      "grad_norm": 1.0131682912837185,
      "learning_rate": 6.545567543255225e-06,
      "loss": 0.5812,
      "step": 6930
    },
    {
      "epoch": 0.6240068423777263,
      "grad_norm": 0.7972130331034957,
      "learning_rate": 6.542831038280157e-06,
      "loss": 0.5282,
      "step": 6931
    },
    {
      "epoch": 0.6240968736635981,
      "grad_norm": 0.7667405837267964,
      "learning_rate": 6.540094827304888e-06,
      "loss": 0.4812,
      "step": 6932
    },
    {
      "epoch": 0.6241869049494699,
      "grad_norm": 0.6530385493979383,
      "learning_rate": 6.537358910562099e-06,
      "loss": 0.486,
      "step": 6933
    },
    {
      "epoch": 0.6242769362353418,
      "grad_norm": 0.7167058096403431,
      "learning_rate": 6.534623288284463e-06,
      "loss": 0.478,
      "step": 6934
    },
    {
      "epoch": 0.6243669675212136,
      "grad_norm": 0.9995729432493377,
      "learning_rate": 6.531887960704612e-06,
      "loss": 0.5034,
      "step": 6935
    },
    {
      "epoch": 0.6244569988070855,
      "grad_norm": 0.7451714413167396,
      "learning_rate": 6.529152928055164e-06,
      "loss": 0.5512,
      "step": 6936
    },
    {
      "epoch": 0.6245470300929573,
      "grad_norm": 0.7606916645256165,
      "learning_rate": 6.526418190568702e-06,
      "loss": 0.5006,
      "step": 6937
    },
    {
      "epoch": 0.6246370613788291,
      "grad_norm": 0.9708833216737932,
      "learning_rate": 6.5236837484778e-06,
      "loss": 0.5857,
      "step": 6938
    },
    {
      "epoch": 0.624727092664701,
      "grad_norm": 1.0254764026004364,
      "learning_rate": 6.5209496020149855e-06,
      "loss": 0.5762,
      "step": 6939
    },
    {
      "epoch": 0.6248171239505729,
      "grad_norm": 1.0566744990519295,
      "learning_rate": 6.51821575141278e-06,
      "loss": 0.5607,
      "step": 6940
    },
    {
      "epoch": 0.6249071552364447,
      "grad_norm": 0.9060019846793255,
      "learning_rate": 6.515482196903669e-06,
      "loss": 0.4791,
      "step": 6941
    },
    {
      "epoch": 0.6249971865223165,
      "grad_norm": 0.7957618913286229,
      "learning_rate": 6.512748938720115e-06,
      "loss": 0.644,
      "step": 6942
    },
    {
      "epoch": 0.6250872178081883,
      "grad_norm": 0.8684645870120246,
      "learning_rate": 6.510015977094554e-06,
      "loss": 0.5801,
      "step": 6943
    },
    {
      "epoch": 0.6251772490940601,
      "grad_norm": 0.7485202516744107,
      "learning_rate": 6.507283312259405e-06,
      "loss": 0.4641,
      "step": 6944
    },
    {
      "epoch": 0.6252672803799321,
      "grad_norm": 0.8279412872007809,
      "learning_rate": 6.50455094444705e-06,
      "loss": 0.5147,
      "step": 6945
    },
    {
      "epoch": 0.6253573116658039,
      "grad_norm": 0.9132055105728595,
      "learning_rate": 6.501818873889856e-06,
      "loss": 0.6469,
      "step": 6946
    },
    {
      "epoch": 0.6254473429516757,
      "grad_norm": 2.3527807748143137,
      "learning_rate": 6.499087100820152e-06,
      "loss": 0.5193,
      "step": 6947
    },
    {
      "epoch": 0.6255373742375475,
      "grad_norm": 0.7142070861593252,
      "learning_rate": 6.496355625470259e-06,
      "loss": 0.5268,
      "step": 6948
    },
    {
      "epoch": 0.6256274055234193,
      "grad_norm": 0.7128884213495978,
      "learning_rate": 6.4936244480724575e-06,
      "loss": 0.484,
      "step": 6949
    },
    {
      "epoch": 0.6257174368092913,
      "grad_norm": 0.7811926177118489,
      "learning_rate": 6.490893568859011e-06,
      "loss": 0.3822,
      "step": 6950
    },
    {
      "epoch": 0.6258074680951631,
      "grad_norm": 0.8849578563330933,
      "learning_rate": 6.488162988062152e-06,
      "loss": 0.4576,
      "step": 6951
    },
    {
      "epoch": 0.6258974993810349,
      "grad_norm": 0.7039137351729748,
      "learning_rate": 6.485432705914099e-06,
      "loss": 0.6647,
      "step": 6952
    },
    {
      "epoch": 0.6259875306669067,
      "grad_norm": 0.7285641229201465,
      "learning_rate": 6.482702722647024e-06,
      "loss": 0.5581,
      "step": 6953
    },
    {
      "epoch": 0.6260775619527786,
      "grad_norm": 0.7224188866874298,
      "learning_rate": 6.479973038493098e-06,
      "loss": 0.5525,
      "step": 6954
    },
    {
      "epoch": 0.6261675932386505,
      "grad_norm": 0.8155180717404997,
      "learning_rate": 6.477243653684449e-06,
      "loss": 0.518,
      "step": 6955
    },
    {
      "epoch": 0.6262576245245223,
      "grad_norm": 0.7218837462448597,
      "learning_rate": 6.474514568453189e-06,
      "loss": 0.4576,
      "step": 6956
    },
    {
      "epoch": 0.6263476558103941,
      "grad_norm": 0.7577511654281716,
      "learning_rate": 6.471785783031395e-06,
      "loss": 0.5074,
      "step": 6957
    },
    {
      "epoch": 0.6264376870962659,
      "grad_norm": 0.8139345475254071,
      "learning_rate": 6.469057297651132e-06,
      "loss": 0.5295,
      "step": 6958
    },
    {
      "epoch": 0.6265277183821378,
      "grad_norm": 0.7784071543897926,
      "learning_rate": 6.466329112544427e-06,
      "loss": 0.5152,
      "step": 6959
    },
    {
      "epoch": 0.6266177496680096,
      "grad_norm": 0.6918721938910571,
      "learning_rate": 6.463601227943293e-06,
      "loss": 0.5571,
      "step": 6960
    },
    {
      "epoch": 0.6267077809538815,
      "grad_norm": 0.9524459630239709,
      "learning_rate": 6.460873644079699e-06,
      "loss": 0.5257,
      "step": 6961
    },
    {
      "epoch": 0.6267978122397533,
      "grad_norm": 1.4042815487301181,
      "learning_rate": 6.458146361185613e-06,
      "loss": 0.5732,
      "step": 6962
    },
    {
      "epoch": 0.6268878435256252,
      "grad_norm": 0.8280480107756107,
      "learning_rate": 6.45541937949296e-06,
      "loss": 0.5088,
      "step": 6963
    },
    {
      "epoch": 0.626977874811497,
      "grad_norm": 0.8676696360128417,
      "learning_rate": 6.452692699233644e-06,
      "loss": 0.4723,
      "step": 6964
    },
    {
      "epoch": 0.6270679060973688,
      "grad_norm": 0.8318985904679468,
      "learning_rate": 6.449966320639541e-06,
      "loss": 0.477,
      "step": 6965
    },
    {
      "epoch": 0.6271579373832407,
      "grad_norm": 1.045855163451788,
      "learning_rate": 6.44724024394251e-06,
      "loss": 0.4769,
      "step": 6966
    },
    {
      "epoch": 0.6272479686691125,
      "grad_norm": 0.8039108700016073,
      "learning_rate": 6.44451446937437e-06,
      "loss": 0.5524,
      "step": 6967
    },
    {
      "epoch": 0.6273379999549844,
      "grad_norm": 1.1896211204378502,
      "learning_rate": 6.441788997166931e-06,
      "loss": 0.6147,
      "step": 6968
    },
    {
      "epoch": 0.6274280312408562,
      "grad_norm": 1.068834541173572,
      "learning_rate": 6.439063827551963e-06,
      "loss": 0.4589,
      "step": 6969
    },
    {
      "epoch": 0.627518062526728,
      "grad_norm": 0.8244647084940608,
      "learning_rate": 6.4363389607612204e-06,
      "loss": 0.4888,
      "step": 6970
    },
    {
      "epoch": 0.6276080938125999,
      "grad_norm": 0.9893058106677024,
      "learning_rate": 6.433614397026422e-06,
      "loss": 0.4264,
      "step": 6971
    },
    {
      "epoch": 0.6276981250984717,
      "grad_norm": 0.8704856548101545,
      "learning_rate": 6.4308901365792735e-06,
      "loss": 0.5441,
      "step": 6972
    },
    {
      "epoch": 0.6277881563843436,
      "grad_norm": 0.7594127245991099,
      "learning_rate": 6.42816617965144e-06,
      "loss": 0.449,
      "step": 6973
    },
    {
      "epoch": 0.6278781876702154,
      "grad_norm": 0.876739971039002,
      "learning_rate": 6.425442526474577e-06,
      "loss": 0.5603,
      "step": 6974
    },
    {
      "epoch": 0.6279682189560872,
      "grad_norm": 0.7820539657809946,
      "learning_rate": 6.422719177280297e-06,
      "loss": 0.5576,
      "step": 6975
    },
    {
      "epoch": 0.628058250241959,
      "grad_norm": 1.1758235807512765,
      "learning_rate": 6.419996132300203e-06,
      "loss": 0.604,
      "step": 6976
    },
    {
      "epoch": 0.628148281527831,
      "grad_norm": 0.8846190948139421,
      "learning_rate": 6.417273391765859e-06,
      "loss": 0.5543,
      "step": 6977
    },
    {
      "epoch": 0.6282383128137028,
      "grad_norm": 0.7208178743108548,
      "learning_rate": 6.414550955908812e-06,
      "loss": 0.4992,
      "step": 6978
    },
    {
      "epoch": 0.6283283440995746,
      "grad_norm": 0.8194823045181588,
      "learning_rate": 6.411828824960576e-06,
      "loss": 0.5608,
      "step": 6979
    },
    {
      "epoch": 0.6284183753854464,
      "grad_norm": 0.7261762024903193,
      "learning_rate": 6.409106999152648e-06,
      "loss": 0.4729,
      "step": 6980
    },
    {
      "epoch": 0.6285084066713182,
      "grad_norm": 0.9457097759572684,
      "learning_rate": 6.406385478716485e-06,
      "loss": 0.4873,
      "step": 6981
    },
    {
      "epoch": 0.6285984379571902,
      "grad_norm": 0.7555608954114793,
      "learning_rate": 6.4036642638835365e-06,
      "loss": 0.5165,
      "step": 6982
    },
    {
      "epoch": 0.628688469243062,
      "grad_norm": 0.8185591108979263,
      "learning_rate": 6.400943354885211e-06,
      "loss": 0.4835,
      "step": 6983
    },
    {
      "epoch": 0.6287785005289338,
      "grad_norm": 1.0437008896626543,
      "learning_rate": 6.3982227519528986e-06,
      "loss": 0.5833,
      "step": 6984
    },
    {
      "epoch": 0.6288685318148056,
      "grad_norm": 0.8530282007667831,
      "learning_rate": 6.395502455317957e-06,
      "loss": 0.5677,
      "step": 6985
    },
    {
      "epoch": 0.6289585631006774,
      "grad_norm": 0.9874959985626828,
      "learning_rate": 6.392782465211726e-06,
      "loss": 0.5291,
      "step": 6986
    },
    {
      "epoch": 0.6290485943865494,
      "grad_norm": 1.0532076950739342,
      "learning_rate": 6.390062781865516e-06,
      "loss": 0.4648,
      "step": 6987
    },
    {
      "epoch": 0.6291386256724212,
      "grad_norm": 0.9450144034054434,
      "learning_rate": 6.387343405510608e-06,
      "loss": 0.5254,
      "step": 6988
    },
    {
      "epoch": 0.629228656958293,
      "grad_norm": 0.7687033994343668,
      "learning_rate": 6.384624336378255e-06,
      "loss": 0.5189,
      "step": 6989
    },
    {
      "epoch": 0.6293186882441648,
      "grad_norm": 0.8408984782815152,
      "learning_rate": 6.381905574699701e-06,
      "loss": 0.5343,
      "step": 6990
    },
    {
      "epoch": 0.6294087195300367,
      "grad_norm": 0.8524178421785027,
      "learning_rate": 6.379187120706138e-06,
      "loss": 0.6084,
      "step": 6991
    },
    {
      "epoch": 0.6294987508159086,
      "grad_norm": 0.755783771120947,
      "learning_rate": 6.376468974628755e-06,
      "loss": 0.4854,
      "step": 6992
    },
    {
      "epoch": 0.6295887821017804,
      "grad_norm": 1.10275904388516,
      "learning_rate": 6.3737511366986985e-06,
      "loss": 0.5041,
      "step": 6993
    },
    {
      "epoch": 0.6296788133876522,
      "grad_norm": 0.8051878547327135,
      "learning_rate": 6.371033607147097e-06,
      "loss": 0.4361,
      "step": 6994
    },
    {
      "epoch": 0.629768844673524,
      "grad_norm": 1.0634671669000713,
      "learning_rate": 6.368316386205048e-06,
      "loss": 0.605,
      "step": 6995
    },
    {
      "epoch": 0.6298588759593959,
      "grad_norm": 0.7968072236417686,
      "learning_rate": 6.365599474103632e-06,
      "loss": 0.4927,
      "step": 6996
    },
    {
      "epoch": 0.6299489072452678,
      "grad_norm": 0.8557762461294082,
      "learning_rate": 6.3628828710738925e-06,
      "loss": 0.5342,
      "step": 6997
    },
    {
      "epoch": 0.6300389385311396,
      "grad_norm": 1.1399772450955625,
      "learning_rate": 6.360166577346853e-06,
      "loss": 0.5488,
      "step": 6998
    },
    {
      "epoch": 0.6301289698170114,
      "grad_norm": 0.7636954152039069,
      "learning_rate": 6.357450593153505e-06,
      "loss": 0.5687,
      "step": 6999
    },
    {
      "epoch": 0.6302190011028832,
      "grad_norm": 1.0494392905812302,
      "learning_rate": 6.354734918724824e-06,
      "loss": 0.5757,
      "step": 7000
    },
    {
      "epoch": 0.6303090323887551,
      "grad_norm": 0.8795852328679833,
      "learning_rate": 6.352019554291748e-06,
      "loss": 0.5242,
      "step": 7001
    },
    {
      "epoch": 0.630399063674627,
      "grad_norm": 0.7566241671112124,
      "learning_rate": 6.349304500085194e-06,
      "loss": 0.5569,
      "step": 7002
    },
    {
      "epoch": 0.6304890949604988,
      "grad_norm": 0.8630710966231282,
      "learning_rate": 6.34658975633605e-06,
      "loss": 0.5841,
      "step": 7003
    },
    {
      "epoch": 0.6305791262463706,
      "grad_norm": 0.7917577732530061,
      "learning_rate": 6.343875323275185e-06,
      "loss": 0.5026,
      "step": 7004
    },
    {
      "epoch": 0.6306691575322425,
      "grad_norm": 0.840705377959167,
      "learning_rate": 6.34116120113343e-06,
      "loss": 0.5706,
      "step": 7005
    },
    {
      "epoch": 0.6307591888181143,
      "grad_norm": 0.9227811182379949,
      "learning_rate": 6.338447390141602e-06,
      "loss": 0.5197,
      "step": 7006
    },
    {
      "epoch": 0.6308492201039861,
      "grad_norm": 0.7442439886137432,
      "learning_rate": 6.335733890530477e-06,
      "loss": 0.5501,
      "step": 7007
    },
    {
      "epoch": 0.630939251389858,
      "grad_norm": 0.8551513295732875,
      "learning_rate": 6.333020702530823e-06,
      "loss": 0.5228,
      "step": 7008
    },
    {
      "epoch": 0.6310292826757298,
      "grad_norm": 0.9232449778323146,
      "learning_rate": 6.330307826373359e-06,
      "loss": 0.6336,
      "step": 7009
    },
    {
      "epoch": 0.6311193139616017,
      "grad_norm": 0.8930612283218158,
      "learning_rate": 6.3275952622888e-06,
      "loss": 0.6744,
      "step": 7010
    },
    {
      "epoch": 0.6312093452474735,
      "grad_norm": 0.9377488756829957,
      "learning_rate": 6.3248830105078185e-06,
      "loss": 0.5658,
      "step": 7011
    },
    {
      "epoch": 0.6312993765333453,
      "grad_norm": 1.0581286586635044,
      "learning_rate": 6.322171071261071e-06,
      "loss": 0.5591,
      "step": 7012
    },
    {
      "epoch": 0.6313894078192172,
      "grad_norm": 0.813521007809359,
      "learning_rate": 6.319459444779175e-06,
      "loss": 0.5187,
      "step": 7013
    },
    {
      "epoch": 0.631479439105089,
      "grad_norm": 0.70209851655559,
      "learning_rate": 6.316748131292736e-06,
      "loss": 0.5548,
      "step": 7014
    },
    {
      "epoch": 0.6315694703909609,
      "grad_norm": 1.016967783954704,
      "learning_rate": 6.314037131032321e-06,
      "loss": 0.5393,
      "step": 7015
    },
    {
      "epoch": 0.6316595016768327,
      "grad_norm": 0.7634900200031765,
      "learning_rate": 6.31132644422848e-06,
      "loss": 0.5598,
      "step": 7016
    },
    {
      "epoch": 0.6317495329627045,
      "grad_norm": 0.6696472359796666,
      "learning_rate": 6.308616071111724e-06,
      "loss": 0.4887,
      "step": 7017
    },
    {
      "epoch": 0.6318395642485763,
      "grad_norm": 0.9035433609353891,
      "learning_rate": 6.305906011912555e-06,
      "loss": 0.4488,
      "step": 7018
    },
    {
      "epoch": 0.6319295955344483,
      "grad_norm": 0.9156199178304418,
      "learning_rate": 6.303196266861429e-06,
      "loss": 0.5585,
      "step": 7019
    },
    {
      "epoch": 0.6320196268203201,
      "grad_norm": 0.7312549361537777,
      "learning_rate": 6.30048683618879e-06,
      "loss": 0.501,
      "step": 7020
    },
    {
      "epoch": 0.6321096581061919,
      "grad_norm": 1.0345787255246848,
      "learning_rate": 6.297777720125046e-06,
      "loss": 0.4949,
      "step": 7021
    },
    {
      "epoch": 0.6321996893920637,
      "grad_norm": 0.8719197501760887,
      "learning_rate": 6.295068918900587e-06,
      "loss": 0.5304,
      "step": 7022
    },
    {
      "epoch": 0.6322897206779355,
      "grad_norm": 0.8461514001061088,
      "learning_rate": 6.292360432745761e-06,
      "loss": 0.5826,
      "step": 7023
    },
    {
      "epoch": 0.6323797519638075,
      "grad_norm": 0.7676404331932544,
      "learning_rate": 6.2896522618909106e-06,
      "loss": 0.5761,
      "step": 7024
    },
    {
      "epoch": 0.6324697832496793,
      "grad_norm": 0.8037197798511075,
      "learning_rate": 6.286944406566336e-06,
      "loss": 0.588,
      "step": 7025
    },
    {
      "epoch": 0.6325598145355511,
      "grad_norm": 0.9811112844873133,
      "learning_rate": 6.284236867002313e-06,
      "loss": 0.4494,
      "step": 7026
    },
    {
      "epoch": 0.6326498458214229,
      "grad_norm": 0.788457765631563,
      "learning_rate": 6.281529643429094e-06,
      "loss": 0.5086,
      "step": 7027
    },
    {
      "epoch": 0.6327398771072947,
      "grad_norm": 1.0613455884424776,
      "learning_rate": 6.278822736076904e-06,
      "loss": 0.5003,
      "step": 7028
    },
    {
      "epoch": 0.6328299083931667,
      "grad_norm": 0.9023893106096157,
      "learning_rate": 6.276116145175939e-06,
      "loss": 0.61,
      "step": 7029
    },
    {
      "epoch": 0.6329199396790385,
      "grad_norm": 1.2189707937885157,
      "learning_rate": 6.273409870956371e-06,
      "loss": 0.7137,
      "step": 7030
    },
    {
      "epoch": 0.6330099709649103,
      "grad_norm": 0.7348753293588797,
      "learning_rate": 6.270703913648337e-06,
      "loss": 0.4703,
      "step": 7031
    },
    {
      "epoch": 0.6331000022507821,
      "grad_norm": 0.8731853376358694,
      "learning_rate": 6.2679982734819625e-06,
      "loss": 0.5843,
      "step": 7032
    },
    {
      "epoch": 0.633190033536654,
      "grad_norm": 0.9594607792609386,
      "learning_rate": 6.265292950687329e-06,
      "loss": 0.4463,
      "step": 7033
    },
    {
      "epoch": 0.6332800648225259,
      "grad_norm": 0.8431583575126295,
      "learning_rate": 6.262587945494505e-06,
      "loss": 0.4821,
      "step": 7034
    },
    {
      "epoch": 0.6333700961083977,
      "grad_norm": 0.9077768846063321,
      "learning_rate": 6.259883258133521e-06,
      "loss": 0.5942,
      "step": 7035
    },
    {
      "epoch": 0.6334601273942695,
      "grad_norm": 1.025717196288207,
      "learning_rate": 6.25717888883439e-06,
      "loss": 0.5286,
      "step": 7036
    },
    {
      "epoch": 0.6335501586801413,
      "grad_norm": 0.7718263734591115,
      "learning_rate": 6.2544748378270846e-06,
      "loss": 0.4873,
      "step": 7037
    },
    {
      "epoch": 0.6336401899660132,
      "grad_norm": 0.8332085453527572,
      "learning_rate": 6.251771105341568e-06,
      "loss": 0.4848,
      "step": 7038
    },
    {
      "epoch": 0.633730221251885,
      "grad_norm": 0.8827362836331413,
      "learning_rate": 6.249067691607764e-06,
      "loss": 0.5273,
      "step": 7039
    },
    {
      "epoch": 0.6338202525377569,
      "grad_norm": 0.9951781263383563,
      "learning_rate": 6.246364596855574e-06,
      "loss": 0.5218,
      "step": 7040
    },
    {
      "epoch": 0.6339102838236287,
      "grad_norm": 0.7978098724211703,
      "learning_rate": 6.243661821314866e-06,
      "loss": 0.5432,
      "step": 7041
    },
    {
      "epoch": 0.6340003151095005,
      "grad_norm": 0.9076916003582645,
      "learning_rate": 6.2409593652154936e-06,
      "loss": 0.4888,
      "step": 7042
    },
    {
      "epoch": 0.6340903463953724,
      "grad_norm": 0.8689946956228709,
      "learning_rate": 6.238257228787267e-06,
      "loss": 0.5228,
      "step": 7043
    },
    {
      "epoch": 0.6341803776812442,
      "grad_norm": 0.9422737658397887,
      "learning_rate": 6.235555412259984e-06,
      "loss": 0.5256,
      "step": 7044
    },
    {
      "epoch": 0.6342704089671161,
      "grad_norm": 0.8867668183415336,
      "learning_rate": 6.2328539158634034e-06,
      "loss": 0.5613,
      "step": 7045
    },
    {
      "epoch": 0.6343604402529879,
      "grad_norm": 0.9218770987330329,
      "learning_rate": 6.230152739827269e-06,
      "loss": 0.5913,
      "step": 7046
    },
    {
      "epoch": 0.6344504715388598,
      "grad_norm": 1.0131766473577979,
      "learning_rate": 6.227451884381282e-06,
      "loss": 0.5736,
      "step": 7047
    },
    {
      "epoch": 0.6345405028247316,
      "grad_norm": 0.886556675443185,
      "learning_rate": 6.224751349755134e-06,
      "loss": 0.454,
      "step": 7048
    },
    {
      "epoch": 0.6346305341106034,
      "grad_norm": 1.045907392544748,
      "learning_rate": 6.222051136178473e-06,
      "loss": 0.521,
      "step": 7049
    },
    {
      "epoch": 0.6347205653964753,
      "grad_norm": 0.8670179639752731,
      "learning_rate": 6.21935124388093e-06,
      "loss": 0.5068,
      "step": 7050
    },
    {
      "epoch": 0.6348105966823471,
      "grad_norm": 0.7504382713546566,
      "learning_rate": 6.216651673092103e-06,
      "loss": 0.5,
      "step": 7051
    },
    {
      "epoch": 0.634900627968219,
      "grad_norm": 0.7993179419784002,
      "learning_rate": 6.213952424041567e-06,
      "loss": 0.5373,
      "step": 7052
    },
    {
      "epoch": 0.6349906592540908,
      "grad_norm": 0.7793516078159844,
      "learning_rate": 6.211253496958869e-06,
      "loss": 0.5808,
      "step": 7053
    },
    {
      "epoch": 0.6350806905399626,
      "grad_norm": 0.794274054865227,
      "learning_rate": 6.208554892073528e-06,
      "loss": 0.4943,
      "step": 7054
    },
    {
      "epoch": 0.6351707218258345,
      "grad_norm": 0.8135317381260657,
      "learning_rate": 6.205856609615029e-06,
      "loss": 0.5645,
      "step": 7055
    },
    {
      "epoch": 0.6352607531117063,
      "grad_norm": 0.6830141562344719,
      "learning_rate": 6.203158649812843e-06,
      "loss": 0.5739,
      "step": 7056
    },
    {
      "epoch": 0.6353507843975782,
      "grad_norm": 0.7188131027162793,
      "learning_rate": 6.200461012896401e-06,
      "loss": 0.5096,
      "step": 7057
    },
    {
      "epoch": 0.63544081568345,
      "grad_norm": 0.8166201953159548,
      "learning_rate": 6.197763699095117e-06,
      "loss": 0.4904,
      "step": 7058
    },
    {
      "epoch": 0.6355308469693218,
      "grad_norm": 0.8144417303203603,
      "learning_rate": 6.195066708638364e-06,
      "loss": 0.6444,
      "step": 7059
    },
    {
      "epoch": 0.6356208782551936,
      "grad_norm": 0.7523107734876407,
      "learning_rate": 6.192370041755505e-06,
      "loss": 0.5593,
      "step": 7060
    },
    {
      "epoch": 0.6357109095410656,
      "grad_norm": 0.7675195078688465,
      "learning_rate": 6.18967369867586e-06,
      "loss": 0.568,
      "step": 7061
    },
    {
      "epoch": 0.6358009408269374,
      "grad_norm": 0.7721717297750804,
      "learning_rate": 6.186977679628732e-06,
      "loss": 0.4981,
      "step": 7062
    },
    {
      "epoch": 0.6358909721128092,
      "grad_norm": 0.6540907363159052,
      "learning_rate": 6.184281984843387e-06,
      "loss": 0.4042,
      "step": 7063
    },
    {
      "epoch": 0.635981003398681,
      "grad_norm": 0.8880621771957508,
      "learning_rate": 6.181586614549074e-06,
      "loss": 0.52,
      "step": 7064
    },
    {
      "epoch": 0.6360710346845528,
      "grad_norm": 0.8509719425327422,
      "learning_rate": 6.178891568975003e-06,
      "loss": 0.5605,
      "step": 7065
    },
    {
      "epoch": 0.6361610659704248,
      "grad_norm": 0.8500906190126949,
      "learning_rate": 6.176196848350369e-06,
      "loss": 0.5005,
      "step": 7066
    },
    {
      "epoch": 0.6362510972562966,
      "grad_norm": 0.7771103894734329,
      "learning_rate": 6.1735024529043265e-06,
      "loss": 0.5998,
      "step": 7067
    },
    {
      "epoch": 0.6363411285421684,
      "grad_norm": 0.7790459229221737,
      "learning_rate": 6.170808382866014e-06,
      "loss": 0.5427,
      "step": 7068
    },
    {
      "epoch": 0.6364311598280402,
      "grad_norm": 0.8350851636315645,
      "learning_rate": 6.168114638464531e-06,
      "loss": 0.5225,
      "step": 7069
    },
    {
      "epoch": 0.636521191113912,
      "grad_norm": 0.8604508602052859,
      "learning_rate": 6.165421219928962e-06,
      "loss": 0.4828,
      "step": 7070
    },
    {
      "epoch": 0.636611222399784,
      "grad_norm": 0.8883215415348028,
      "learning_rate": 6.162728127488351e-06,
      "loss": 0.5952,
      "step": 7071
    },
    {
      "epoch": 0.6367012536856558,
      "grad_norm": 1.1548779272791718,
      "learning_rate": 6.160035361371726e-06,
      "loss": 0.5662,
      "step": 7072
    },
    {
      "epoch": 0.6367912849715276,
      "grad_norm": 0.8638700996920577,
      "learning_rate": 6.1573429218080725e-06,
      "loss": 0.6006,
      "step": 7073
    },
    {
      "epoch": 0.6368813162573994,
      "grad_norm": 0.9226567898007151,
      "learning_rate": 6.154650809026368e-06,
      "loss": 0.5569,
      "step": 7074
    },
    {
      "epoch": 0.6369713475432713,
      "grad_norm": 0.7793610083126108,
      "learning_rate": 6.151959023255545e-06,
      "loss": 0.5985,
      "step": 7075
    },
    {
      "epoch": 0.6370613788291432,
      "grad_norm": 0.8701504810910952,
      "learning_rate": 6.149267564724517e-06,
      "loss": 0.552,
      "step": 7076
    },
    {
      "epoch": 0.637151410115015,
      "grad_norm": 0.7248775237735523,
      "learning_rate": 6.1465764336621656e-06,
      "loss": 0.4888,
      "step": 7077
    },
    {
      "epoch": 0.6372414414008868,
      "grad_norm": 0.9204805621683283,
      "learning_rate": 6.143885630297348e-06,
      "loss": 0.5403,
      "step": 7078
    },
    {
      "epoch": 0.6373314726867586,
      "grad_norm": 0.7385339172024759,
      "learning_rate": 6.141195154858888e-06,
      "loss": 0.5369,
      "step": 7079
    },
    {
      "epoch": 0.6374215039726305,
      "grad_norm": 1.0178733900642427,
      "learning_rate": 6.13850500757559e-06,
      "loss": 0.531,
      "step": 7080
    },
    {
      "epoch": 0.6375115352585023,
      "grad_norm": 0.8281771645723482,
      "learning_rate": 6.135815188676224e-06,
      "loss": 0.4431,
      "step": 7081
    },
    {
      "epoch": 0.6376015665443742,
      "grad_norm": 0.7608262889932964,
      "learning_rate": 6.133125698389535e-06,
      "loss": 0.4872,
      "step": 7082
    },
    {
      "epoch": 0.637691597830246,
      "grad_norm": 0.8322938151451902,
      "learning_rate": 6.1304365369442335e-06,
      "loss": 0.4855,
      "step": 7083
    },
    {
      "epoch": 0.6377816291161178,
      "grad_norm": 0.8010763840000562,
      "learning_rate": 6.127747704569016e-06,
      "loss": 0.5714,
      "step": 7084
    },
    {
      "epoch": 0.6378716604019897,
      "grad_norm": 0.792632893365235,
      "learning_rate": 6.125059201492536e-06,
      "loss": 0.4891,
      "step": 7085
    },
    {
      "epoch": 0.6379616916878615,
      "grad_norm": 0.9344794747875054,
      "learning_rate": 6.1223710279434304e-06,
      "loss": 0.5703,
      "step": 7086
    },
    {
      "epoch": 0.6380517229737334,
      "grad_norm": 1.0999344443370234,
      "learning_rate": 6.119683184150295e-06,
      "loss": 0.5331,
      "step": 7087
    },
    {
      "epoch": 0.6381417542596052,
      "grad_norm": 0.8727683364485521,
      "learning_rate": 6.116995670341716e-06,
      "loss": 0.5104,
      "step": 7088
    },
    {
      "epoch": 0.6382317855454771,
      "grad_norm": 0.8989886650345766,
      "learning_rate": 6.114308486746233e-06,
      "loss": 0.5347,
      "step": 7089
    },
    {
      "epoch": 0.6383218168313489,
      "grad_norm": 0.8548349576937477,
      "learning_rate": 6.1116216335923726e-06,
      "loss": 0.5971,
      "step": 7090
    },
    {
      "epoch": 0.6384118481172207,
      "grad_norm": 0.8792616918522136,
      "learning_rate": 6.10893511110862e-06,
      "loss": 0.5697,
      "step": 7091
    },
    {
      "epoch": 0.6385018794030926,
      "grad_norm": 1.0121659653469648,
      "learning_rate": 6.1062489195234445e-06,
      "loss": 0.6097,
      "step": 7092
    },
    {
      "epoch": 0.6385919106889644,
      "grad_norm": 0.7398984523870389,
      "learning_rate": 6.103563059065274e-06,
      "loss": 0.6214,
      "step": 7093
    },
    {
      "epoch": 0.6386819419748363,
      "grad_norm": 0.8577362377024517,
      "learning_rate": 6.1008775299625255e-06,
      "loss": 0.5896,
      "step": 7094
    },
    {
      "epoch": 0.6387719732607081,
      "grad_norm": 1.032551260233324,
      "learning_rate": 6.098192332443569e-06,
      "loss": 0.579,
      "step": 7095
    },
    {
      "epoch": 0.6388620045465799,
      "grad_norm": 0.9375171896822627,
      "learning_rate": 6.095507466736763e-06,
      "loss": 0.5295,
      "step": 7096
    },
    {
      "epoch": 0.6389520358324517,
      "grad_norm": 0.9605383978171255,
      "learning_rate": 6.092822933070421e-06,
      "loss": 0.5798,
      "step": 7097
    },
    {
      "epoch": 0.6390420671183236,
      "grad_norm": 0.8981240895694522,
      "learning_rate": 6.090138731672849e-06,
      "loss": 0.6185,
      "step": 7098
    },
    {
      "epoch": 0.6391320984041955,
      "grad_norm": 0.6812682820401984,
      "learning_rate": 6.087454862772304e-06,
      "loss": 0.4592,
      "step": 7099
    },
    {
      "epoch": 0.6392221296900673,
      "grad_norm": 0.9753781291531974,
      "learning_rate": 6.084771326597029e-06,
      "loss": 0.5758,
      "step": 7100
    },
    {
      "epoch": 0.6393121609759391,
      "grad_norm": 0.7761211705920126,
      "learning_rate": 6.082088123375227e-06,
      "loss": 0.4733,
      "step": 7101
    },
    {
      "epoch": 0.6394021922618109,
      "grad_norm": 1.0362030609478718,
      "learning_rate": 6.079405253335089e-06,
      "loss": 0.5823,
      "step": 7102
    },
    {
      "epoch": 0.6394922235476829,
      "grad_norm": 0.9701010764107378,
      "learning_rate": 6.076722716704759e-06,
      "loss": 0.5191,
      "step": 7103
    },
    {
      "epoch": 0.6395822548335547,
      "grad_norm": 0.9481338537523112,
      "learning_rate": 6.07404051371237e-06,
      "loss": 0.6556,
      "step": 7104
    },
    {
      "epoch": 0.6396722861194265,
      "grad_norm": 0.8255185357446134,
      "learning_rate": 6.071358644586008e-06,
      "loss": 0.613,
      "step": 7105
    },
    {
      "epoch": 0.6397623174052983,
      "grad_norm": 0.8454175185685866,
      "learning_rate": 6.06867710955375e-06,
      "loss": 0.4977,
      "step": 7106
    },
    {
      "epoch": 0.6398523486911701,
      "grad_norm": 0.8511665430127363,
      "learning_rate": 6.065995908843628e-06,
      "loss": 0.5085,
      "step": 7107
    },
    {
      "epoch": 0.6399423799770421,
      "grad_norm": 0.8582362978499039,
      "learning_rate": 6.063315042683661e-06,
      "loss": 0.5755,
      "step": 7108
    },
    {
      "epoch": 0.6400324112629139,
      "grad_norm": 0.7686691170307844,
      "learning_rate": 6.060634511301824e-06,
      "loss": 0.5059,
      "step": 7109
    },
    {
      "epoch": 0.6401224425487857,
      "grad_norm": 0.8882827579077696,
      "learning_rate": 6.0579543149260776e-06,
      "loss": 0.5305,
      "step": 7110
    },
    {
      "epoch": 0.6402124738346575,
      "grad_norm": 0.7199416015240456,
      "learning_rate": 6.055274453784339e-06,
      "loss": 0.5678,
      "step": 7111
    },
    {
      "epoch": 0.6403025051205293,
      "grad_norm": 1.1195504060459824,
      "learning_rate": 6.052594928104515e-06,
      "loss": 0.6429,
      "step": 7112
    },
    {
      "epoch": 0.6403925364064013,
      "grad_norm": 0.854664460416745,
      "learning_rate": 6.049915738114467e-06,
      "loss": 0.5465,
      "step": 7113
    },
    {
      "epoch": 0.6404825676922731,
      "grad_norm": 0.9182992369518878,
      "learning_rate": 6.047236884042039e-06,
      "loss": 0.4864,
      "step": 7114
    },
    {
      "epoch": 0.6405725989781449,
      "grad_norm": 0.9540818853671174,
      "learning_rate": 6.044558366115038e-06,
      "loss": 0.5895,
      "step": 7115
    },
    {
      "epoch": 0.6406626302640167,
      "grad_norm": 0.8270405515108952,
      "learning_rate": 6.041880184561253e-06,
      "loss": 0.5674,
      "step": 7116
    },
    {
      "epoch": 0.6407526615498886,
      "grad_norm": 0.9828040926637466,
      "learning_rate": 6.039202339608432e-06,
      "loss": 0.4825,
      "step": 7117
    },
    {
      "epoch": 0.6408426928357605,
      "grad_norm": 0.8863794052868362,
      "learning_rate": 6.036524831484307e-06,
      "loss": 0.5553,
      "step": 7118
    },
    {
      "epoch": 0.6409327241216323,
      "grad_norm": 0.7485746785877729,
      "learning_rate": 6.0338476604165685e-06,
      "loss": 0.4582,
      "step": 7119
    },
    {
      "epoch": 0.6410227554075041,
      "grad_norm": 0.8609046743593928,
      "learning_rate": 6.031170826632891e-06,
      "loss": 0.4702,
      "step": 7120
    },
    {
      "epoch": 0.6411127866933759,
      "grad_norm": 0.7756413549871108,
      "learning_rate": 6.028494330360904e-06,
      "loss": 0.5199,
      "step": 7121
    },
    {
      "epoch": 0.6412028179792478,
      "grad_norm": 0.9556457484870844,
      "learning_rate": 6.025818171828231e-06,
      "loss": 0.5189,
      "step": 7122
    },
    {
      "epoch": 0.6412928492651196,
      "grad_norm": 0.7640187394761766,
      "learning_rate": 6.023142351262447e-06,
      "loss": 0.5287,
      "step": 7123
    },
    {
      "epoch": 0.6413828805509915,
      "grad_norm": 0.8938959256389903,
      "learning_rate": 6.020466868891108e-06,
      "loss": 0.5713,
      "step": 7124
    },
    {
      "epoch": 0.6414729118368633,
      "grad_norm": 0.8731541841761962,
      "learning_rate": 6.017791724941734e-06,
      "loss": 0.548,
      "step": 7125
    },
    {
      "epoch": 0.6415629431227351,
      "grad_norm": 1.1655622756685045,
      "learning_rate": 6.015116919641828e-06,
      "loss": 0.5107,
      "step": 7126
    },
    {
      "epoch": 0.641652974408607,
      "grad_norm": 1.0870766084520715,
      "learning_rate": 6.012442453218852e-06,
      "loss": 0.4946,
      "step": 7127
    },
    {
      "epoch": 0.6417430056944788,
      "grad_norm": 0.9885351624998614,
      "learning_rate": 6.009768325900247e-06,
      "loss": 0.5503,
      "step": 7128
    },
    {
      "epoch": 0.6418330369803507,
      "grad_norm": 1.0852634970696358,
      "learning_rate": 6.007094537913418e-06,
      "loss": 0.505,
      "step": 7129
    },
    {
      "epoch": 0.6419230682662225,
      "grad_norm": 0.8847125675286491,
      "learning_rate": 6.0044210894857524e-06,
      "loss": 0.4811,
      "step": 7130
    },
    {
      "epoch": 0.6420130995520944,
      "grad_norm": 1.215602681155735,
      "learning_rate": 6.0017479808445965e-06,
      "loss": 0.582,
      "step": 7131
    },
    {
      "epoch": 0.6421031308379662,
      "grad_norm": 0.9803186040208458,
      "learning_rate": 5.999075212217278e-06,
      "loss": 0.5529,
      "step": 7132
    },
    {
      "epoch": 0.642193162123838,
      "grad_norm": 0.9295548587534274,
      "learning_rate": 5.996402783831085e-06,
      "loss": 0.5629,
      "step": 7133
    },
    {
      "epoch": 0.6422831934097099,
      "grad_norm": 0.9164999087606733,
      "learning_rate": 5.993730695913287e-06,
      "loss": 0.4691,
      "step": 7134
    },
    {
      "epoch": 0.6423732246955817,
      "grad_norm": 0.7909749635150976,
      "learning_rate": 5.991058948691115e-06,
      "loss": 0.5281,
      "step": 7135
    },
    {
      "epoch": 0.6424632559814536,
      "grad_norm": 0.9504667791422308,
      "learning_rate": 5.9883875423917824e-06,
      "loss": 0.5654,
      "step": 7136
    },
    {
      "epoch": 0.6425532872673254,
      "grad_norm": 1.035837429184211,
      "learning_rate": 5.985716477242463e-06,
      "loss": 0.538,
      "step": 7137
    },
    {
      "epoch": 0.6426433185531972,
      "grad_norm": 1.0415598643844488,
      "learning_rate": 5.983045753470308e-06,
      "loss": 0.6305,
      "step": 7138
    },
    {
      "epoch": 0.642733349839069,
      "grad_norm": 0.8712086603012228,
      "learning_rate": 5.980375371302434e-06,
      "loss": 0.5181,
      "step": 7139
    },
    {
      "epoch": 0.6428233811249409,
      "grad_norm": 0.7867424409639574,
      "learning_rate": 5.977705330965937e-06,
      "loss": 0.5632,
      "step": 7140
    },
    {
      "epoch": 0.6429134124108128,
      "grad_norm": 0.6951975321385969,
      "learning_rate": 5.975035632687873e-06,
      "loss": 0.4258,
      "step": 7141
    },
    {
      "epoch": 0.6430034436966846,
      "grad_norm": 0.7976485438692239,
      "learning_rate": 5.972366276695283e-06,
      "loss": 0.4924,
      "step": 7142
    },
    {
      "epoch": 0.6430934749825564,
      "grad_norm": 0.9068953594891577,
      "learning_rate": 5.96969726321516e-06,
      "loss": 0.4467,
      "step": 7143
    },
    {
      "epoch": 0.6431835062684282,
      "grad_norm": 0.9219714202454375,
      "learning_rate": 5.9670285924744885e-06,
      "loss": 0.4367,
      "step": 7144
    },
    {
      "epoch": 0.6432735375543002,
      "grad_norm": 0.8781943300441403,
      "learning_rate": 5.964360264700209e-06,
      "loss": 0.6257,
      "step": 7145
    },
    {
      "epoch": 0.643363568840172,
      "grad_norm": 0.7330256219807754,
      "learning_rate": 5.961692280119239e-06,
      "loss": 0.5002,
      "step": 7146
    },
    {
      "epoch": 0.6434536001260438,
      "grad_norm": 0.8876864951407798,
      "learning_rate": 5.959024638958464e-06,
      "loss": 0.6248,
      "step": 7147
    },
    {
      "epoch": 0.6435436314119156,
      "grad_norm": 1.1216472926228611,
      "learning_rate": 5.956357341444744e-06,
      "loss": 0.5499,
      "step": 7148
    },
    {
      "epoch": 0.6436336626977874,
      "grad_norm": 0.9067505696656871,
      "learning_rate": 5.9536903878049035e-06,
      "loss": 0.5765,
      "step": 7149
    },
    {
      "epoch": 0.6437236939836594,
      "grad_norm": 0.7526128190537338,
      "learning_rate": 5.951023778265747e-06,
      "loss": 0.4304,
      "step": 7150
    },
    {
      "epoch": 0.6438137252695312,
      "grad_norm": 0.8588924721292138,
      "learning_rate": 5.948357513054044e-06,
      "loss": 0.6347,
      "step": 7151
    },
    {
      "epoch": 0.643903756555403,
      "grad_norm": 0.8007393812127873,
      "learning_rate": 5.945691592396533e-06,
      "loss": 0.4683,
      "step": 7152
    },
    {
      "epoch": 0.6439937878412748,
      "grad_norm": 0.9213235284183688,
      "learning_rate": 5.943026016519924e-06,
      "loss": 0.5953,
      "step": 7153
    },
    {
      "epoch": 0.6440838191271466,
      "grad_norm": 0.9947209439844809,
      "learning_rate": 5.940360785650905e-06,
      "loss": 0.5896,
      "step": 7154
    },
    {
      "epoch": 0.6441738504130186,
      "grad_norm": 0.7776901249679103,
      "learning_rate": 5.937695900016124e-06,
      "loss": 0.492,
      "step": 7155
    },
    {
      "epoch": 0.6442638816988904,
      "grad_norm": 0.7628593532285336,
      "learning_rate": 5.935031359842207e-06,
      "loss": 0.4887,
      "step": 7156
    },
    {
      "epoch": 0.6443539129847622,
      "grad_norm": 0.8418662879788755,
      "learning_rate": 5.932367165355745e-06,
      "loss": 0.4128,
      "step": 7157
    },
    {
      "epoch": 0.644443944270634,
      "grad_norm": 0.9225729140913727,
      "learning_rate": 5.929703316783309e-06,
      "loss": 0.5595,
      "step": 7158
    },
    {
      "epoch": 0.6445339755565059,
      "grad_norm": 0.7997972653045169,
      "learning_rate": 5.927039814351426e-06,
      "loss": 0.4244,
      "step": 7159
    },
    {
      "epoch": 0.6446240068423778,
      "grad_norm": 0.8499729612489498,
      "learning_rate": 5.92437665828661e-06,
      "loss": 0.556,
      "step": 7160
    },
    {
      "epoch": 0.6447140381282496,
      "grad_norm": 0.7084456989314685,
      "learning_rate": 5.921713848815332e-06,
      "loss": 0.4649,
      "step": 7161
    },
    {
      "epoch": 0.6448040694141214,
      "grad_norm": 0.711123879475645,
      "learning_rate": 5.919051386164041e-06,
      "loss": 0.5596,
      "step": 7162
    },
    {
      "epoch": 0.6448941006999932,
      "grad_norm": 0.8328490604734294,
      "learning_rate": 5.916389270559152e-06,
      "loss": 0.4884,
      "step": 7163
    },
    {
      "epoch": 0.6449841319858651,
      "grad_norm": 0.9010067314488869,
      "learning_rate": 5.913727502227058e-06,
      "loss": 0.4908,
      "step": 7164
    },
    {
      "epoch": 0.6450741632717369,
      "grad_norm": 0.7900707909661959,
      "learning_rate": 5.9110660813941136e-06,
      "loss": 0.537,
      "step": 7165
    },
    {
      "epoch": 0.6451641945576088,
      "grad_norm": 0.7679502839388063,
      "learning_rate": 5.90840500828665e-06,
      "loss": 0.484,
      "step": 7166
    },
    {
      "epoch": 0.6452542258434806,
      "grad_norm": 0.9281359536861374,
      "learning_rate": 5.9057442831309605e-06,
      "loss": 0.5067,
      "step": 7167
    },
    {
      "epoch": 0.6453442571293525,
      "grad_norm": 0.7173629008283698,
      "learning_rate": 5.903083906153324e-06,
      "loss": 0.5214,
      "step": 7168
    },
    {
      "epoch": 0.6454342884152243,
      "grad_norm": 0.7429676672359471,
      "learning_rate": 5.900423877579975e-06,
      "loss": 0.5058,
      "step": 7169
    },
    {
      "epoch": 0.6455243197010961,
      "grad_norm": 0.7483517238671298,
      "learning_rate": 5.897764197637126e-06,
      "loss": 0.4769,
      "step": 7170
    },
    {
      "epoch": 0.645614350986968,
      "grad_norm": 0.8300805403257039,
      "learning_rate": 5.895104866550954e-06,
      "loss": 0.5419,
      "step": 7171
    },
    {
      "epoch": 0.6457043822728398,
      "grad_norm": 0.8568337192947166,
      "learning_rate": 5.892445884547615e-06,
      "loss": 0.4945,
      "step": 7172
    },
    {
      "epoch": 0.6457944135587117,
      "grad_norm": 0.862760510922653,
      "learning_rate": 5.889787251853228e-06,
      "loss": 0.5822,
      "step": 7173
    },
    {
      "epoch": 0.6458844448445835,
      "grad_norm": 0.6557247772981553,
      "learning_rate": 5.887128968693887e-06,
      "loss": 0.5307,
      "step": 7174
    },
    {
      "epoch": 0.6459744761304553,
      "grad_norm": 1.0014541779200785,
      "learning_rate": 5.88447103529565e-06,
      "loss": 0.5204,
      "step": 7175
    },
    {
      "epoch": 0.6460645074163271,
      "grad_norm": 0.9173187160744808,
      "learning_rate": 5.881813451884554e-06,
      "loss": 0.4794,
      "step": 7176
    },
    {
      "epoch": 0.646154538702199,
      "grad_norm": 0.8503239628687458,
      "learning_rate": 5.8791562186865945e-06,
      "loss": 0.568,
      "step": 7177
    },
    {
      "epoch": 0.6462445699880709,
      "grad_norm": 0.7735042587424606,
      "learning_rate": 5.8764993359277546e-06,
      "loss": 0.5195,
      "step": 7178
    },
    {
      "epoch": 0.6463346012739427,
      "grad_norm": 0.8198070836111847,
      "learning_rate": 5.873842803833969e-06,
      "loss": 0.542,
      "step": 7179
    },
    {
      "epoch": 0.6464246325598145,
      "grad_norm": 0.8784264624867784,
      "learning_rate": 5.871186622631155e-06,
      "loss": 0.5057,
      "step": 7180
    },
    {
      "epoch": 0.6465146638456863,
      "grad_norm": 0.7628998880617802,
      "learning_rate": 5.868530792545191e-06,
      "loss": 0.5649,
      "step": 7181
    },
    {
      "epoch": 0.6466046951315583,
      "grad_norm": 0.852961493497302,
      "learning_rate": 5.865875313801937e-06,
      "loss": 0.5341,
      "step": 7182
    },
    {
      "epoch": 0.6466947264174301,
      "grad_norm": 0.8293696164374479,
      "learning_rate": 5.863220186627211e-06,
      "loss": 0.4786,
      "step": 7183
    },
    {
      "epoch": 0.6467847577033019,
      "grad_norm": 0.8430519371463028,
      "learning_rate": 5.860565411246812e-06,
      "loss": 0.5404,
      "step": 7184
    },
    {
      "epoch": 0.6468747889891737,
      "grad_norm": 0.7446208062767137,
      "learning_rate": 5.857910987886497e-06,
      "loss": 0.5299,
      "step": 7185
    },
    {
      "epoch": 0.6469648202750455,
      "grad_norm": 1.204050571652882,
      "learning_rate": 5.855256916772007e-06,
      "loss": 0.5289,
      "step": 7186
    },
    {
      "epoch": 0.6470548515609175,
      "grad_norm": 0.9472381773470339,
      "learning_rate": 5.85260319812904e-06,
      "loss": 0.5356,
      "step": 7187
    },
    {
      "epoch": 0.6471448828467893,
      "grad_norm": 0.7088245543887215,
      "learning_rate": 5.849949832183275e-06,
      "loss": 0.4438,
      "step": 7188
    },
    {
      "epoch": 0.6472349141326611,
      "grad_norm": 2.2021789224215755,
      "learning_rate": 5.8472968191603504e-06,
      "loss": 0.5239,
      "step": 7189
    },
    {
      "epoch": 0.6473249454185329,
      "grad_norm": 0.8744884550738601,
      "learning_rate": 5.844644159285886e-06,
      "loss": 0.5007,
      "step": 7190
    },
    {
      "epoch": 0.6474149767044047,
      "grad_norm": 0.8377883302816788,
      "learning_rate": 5.841991852785458e-06,
      "loss": 0.5956,
      "step": 7191
    },
    {
      "epoch": 0.6475050079902767,
      "grad_norm": 0.895800008125506,
      "learning_rate": 5.839339899884629e-06,
      "loss": 0.5288,
      "step": 7192
    },
    {
      "epoch": 0.6475950392761485,
      "grad_norm": 0.8597928615795704,
      "learning_rate": 5.836688300808915e-06,
      "loss": 0.6096,
      "step": 7193
    },
    {
      "epoch": 0.6476850705620203,
      "grad_norm": 1.2328177738633088,
      "learning_rate": 5.834037055783818e-06,
      "loss": 0.5343,
      "step": 7194
    },
    {
      "epoch": 0.6477751018478921,
      "grad_norm": 0.8565307261229678,
      "learning_rate": 5.831386165034792e-06,
      "loss": 0.5703,
      "step": 7195
    },
    {
      "epoch": 0.647865133133764,
      "grad_norm": 0.8699796476918371,
      "learning_rate": 5.828735628787276e-06,
      "loss": 0.4502,
      "step": 7196
    },
    {
      "epoch": 0.6479551644196359,
      "grad_norm": 0.9317514094837805,
      "learning_rate": 5.8260854472666725e-06,
      "loss": 0.5895,
      "step": 7197
    },
    {
      "epoch": 0.6480451957055077,
      "grad_norm": 1.004640930678469,
      "learning_rate": 5.823435620698354e-06,
      "loss": 0.525,
      "step": 7198
    },
    {
      "epoch": 0.6481352269913795,
      "grad_norm": 1.0916167741544334,
      "learning_rate": 5.820786149307664e-06,
      "loss": 0.5001,
      "step": 7199
    },
    {
      "epoch": 0.6482252582772513,
      "grad_norm": 0.8324468541532183,
      "learning_rate": 5.818137033319919e-06,
      "loss": 0.5912,
      "step": 7200
    },
    {
      "epoch": 0.6483152895631232,
      "grad_norm": 0.9447182092072808,
      "learning_rate": 5.815488272960388e-06,
      "loss": 0.6137,
      "step": 7201
    },
    {
      "epoch": 0.648405320848995,
      "grad_norm": 0.8897138452393019,
      "learning_rate": 5.8128398684543415e-06,
      "loss": 0.6008,
      "step": 7202
    },
    {
      "epoch": 0.6484953521348669,
      "grad_norm": 0.7735988510240757,
      "learning_rate": 5.810191820026989e-06,
      "loss": 0.4522,
      "step": 7203
    },
    {
      "epoch": 0.6485853834207387,
      "grad_norm": 1.0126268553149993,
      "learning_rate": 5.807544127903526e-06,
      "loss": 0.5824,
      "step": 7204
    },
    {
      "epoch": 0.6486754147066105,
      "grad_norm": 0.7748845029955842,
      "learning_rate": 5.804896792309112e-06,
      "loss": 0.549,
      "step": 7205
    },
    {
      "epoch": 0.6487654459924824,
      "grad_norm": 0.7044528586709685,
      "learning_rate": 5.8022498134688855e-06,
      "loss": 0.3947,
      "step": 7206
    },
    {
      "epoch": 0.6488554772783542,
      "grad_norm": 0.7776552451484529,
      "learning_rate": 5.799603191607932e-06,
      "loss": 0.4243,
      "step": 7207
    },
    {
      "epoch": 0.6489455085642261,
      "grad_norm": 0.7187510453177223,
      "learning_rate": 5.79695692695134e-06,
      "loss": 0.4231,
      "step": 7208
    },
    {
      "epoch": 0.6490355398500979,
      "grad_norm": 0.8216488373681018,
      "learning_rate": 5.794311019724137e-06,
      "loss": 0.5068,
      "step": 7209
    },
    {
      "epoch": 0.6491255711359698,
      "grad_norm": 1.5090981939223453,
      "learning_rate": 5.791665470151336e-06,
      "loss": 0.5756,
      "step": 7210
    },
    {
      "epoch": 0.6492156024218416,
      "grad_norm": 1.0642025738686263,
      "learning_rate": 5.789020278457917e-06,
      "loss": 0.6157,
      "step": 7211
    },
    {
      "epoch": 0.6493056337077134,
      "grad_norm": 1.0449017459137155,
      "learning_rate": 5.7863754448688284e-06,
      "loss": 0.5776,
      "step": 7212
    },
    {
      "epoch": 0.6493956649935853,
      "grad_norm": 0.9566946440192877,
      "learning_rate": 5.783730969608987e-06,
      "loss": 0.5651,
      "step": 7213
    },
    {
      "epoch": 0.6494856962794571,
      "grad_norm": 1.1885932669683013,
      "learning_rate": 5.781086852903287e-06,
      "loss": 0.4554,
      "step": 7214
    },
    {
      "epoch": 0.649575727565329,
      "grad_norm": 0.7905897966607925,
      "learning_rate": 5.778443094976573e-06,
      "loss": 0.5729,
      "step": 7215
    },
    {
      "epoch": 0.6496657588512008,
      "grad_norm": 1.1740037851169425,
      "learning_rate": 5.775799696053688e-06,
      "loss": 0.5253,
      "step": 7216
    },
    {
      "epoch": 0.6497557901370726,
      "grad_norm": 1.1702220841211204,
      "learning_rate": 5.773156656359414e-06,
      "loss": 0.5436,
      "step": 7217
    },
    {
      "epoch": 0.6498458214229444,
      "grad_norm": 0.9935107944134838,
      "learning_rate": 5.7705139761185236e-06,
      "loss": 0.587,
      "step": 7218
    },
    {
      "epoch": 0.6499358527088163,
      "grad_norm": 0.9065101824639886,
      "learning_rate": 5.7678716555557515e-06,
      "loss": 0.4407,
      "step": 7219
    },
    {
      "epoch": 0.6500258839946882,
      "grad_norm": 0.8153681397766609,
      "learning_rate": 5.765229694895804e-06,
      "loss": 0.4873,
      "step": 7220
    },
    {
      "epoch": 0.65011591528056,
      "grad_norm": 0.8366900552712649,
      "learning_rate": 5.762588094363346e-06,
      "loss": 0.5087,
      "step": 7221
    },
    {
      "epoch": 0.6502059465664318,
      "grad_norm": 0.7568228738372593,
      "learning_rate": 5.759946854183036e-06,
      "loss": 0.5369,
      "step": 7222
    },
    {
      "epoch": 0.6502959778523036,
      "grad_norm": 0.8595717988612721,
      "learning_rate": 5.7573059745794745e-06,
      "loss": 0.5292,
      "step": 7223
    },
    {
      "epoch": 0.6503860091381756,
      "grad_norm": 0.9927650546148329,
      "learning_rate": 5.754665455777247e-06,
      "loss": 0.6679,
      "step": 7224
    },
    {
      "epoch": 0.6504760404240474,
      "grad_norm": 0.9797313037509876,
      "learning_rate": 5.752025298000905e-06,
      "loss": 0.5914,
      "step": 7225
    },
    {
      "epoch": 0.6505660717099192,
      "grad_norm": 0.902943990658073,
      "learning_rate": 5.749385501474969e-06,
      "loss": 0.5925,
      "step": 7226
    },
    {
      "epoch": 0.650656102995791,
      "grad_norm": 0.9921516321246907,
      "learning_rate": 5.746746066423929e-06,
      "loss": 0.4791,
      "step": 7227
    },
    {
      "epoch": 0.6507461342816628,
      "grad_norm": 0.922396486197975,
      "learning_rate": 5.74410699307225e-06,
      "loss": 0.5442,
      "step": 7228
    },
    {
      "epoch": 0.6508361655675348,
      "grad_norm": 0.9224778515451033,
      "learning_rate": 5.741468281644347e-06,
      "loss": 0.5456,
      "step": 7229
    },
    {
      "epoch": 0.6509261968534066,
      "grad_norm": 0.7305215675472829,
      "learning_rate": 5.738829932364634e-06,
      "loss": 0.4281,
      "step": 7230
    },
    {
      "epoch": 0.6510162281392784,
      "grad_norm": 0.9331386811395619,
      "learning_rate": 5.736191945457463e-06,
      "loss": 0.5811,
      "step": 7231
    },
    {
      "epoch": 0.6511062594251502,
      "grad_norm": 0.8013822151060735,
      "learning_rate": 5.733554321147179e-06,
      "loss": 0.5096,
      "step": 7232
    },
    {
      "epoch": 0.651196290711022,
      "grad_norm": 0.9261353171676665,
      "learning_rate": 5.730917059658083e-06,
      "loss": 0.4901,
      "step": 7233
    },
    {
      "epoch": 0.651286321996894,
      "grad_norm": 0.7349838607123086,
      "learning_rate": 5.728280161214455e-06,
      "loss": 0.4773,
      "step": 7234
    },
    {
      "epoch": 0.6513763532827658,
      "grad_norm": 0.9367726611120529,
      "learning_rate": 5.725643626040528e-06,
      "loss": 0.5806,
      "step": 7235
    },
    {
      "epoch": 0.6514663845686376,
      "grad_norm": 0.9456172826282697,
      "learning_rate": 5.723007454360529e-06,
      "loss": 0.5341,
      "step": 7236
    },
    {
      "epoch": 0.6515564158545094,
      "grad_norm": 0.8840440863624435,
      "learning_rate": 5.720371646398627e-06,
      "loss": 0.5866,
      "step": 7237
    },
    {
      "epoch": 0.6516464471403813,
      "grad_norm": 1.0754004137961652,
      "learning_rate": 5.717736202378979e-06,
      "loss": 0.5768,
      "step": 7238
    },
    {
      "epoch": 0.6517364784262532,
      "grad_norm": 0.9410842409109142,
      "learning_rate": 5.715101122525701e-06,
      "loss": 0.4864,
      "step": 7239
    },
    {
      "epoch": 0.651826509712125,
      "grad_norm": 0.9591134467882293,
      "learning_rate": 5.712466407062887e-06,
      "loss": 0.4902,
      "step": 7240
    },
    {
      "epoch": 0.6519165409979968,
      "grad_norm": 0.9963258824367356,
      "learning_rate": 5.70983205621459e-06,
      "loss": 0.5692,
      "step": 7241
    },
    {
      "epoch": 0.6520065722838686,
      "grad_norm": 0.9076408615356704,
      "learning_rate": 5.707198070204843e-06,
      "loss": 0.6143,
      "step": 7242
    },
    {
      "epoch": 0.6520966035697405,
      "grad_norm": 0.9703005954006924,
      "learning_rate": 5.704564449257635e-06,
      "loss": 0.6286,
      "step": 7243
    },
    {
      "epoch": 0.6521866348556123,
      "grad_norm": 0.7321503300851249,
      "learning_rate": 5.7019311935969325e-06,
      "loss": 0.4785,
      "step": 7244
    },
    {
      "epoch": 0.6522766661414842,
      "grad_norm": 0.7825283006184662,
      "learning_rate": 5.699298303446669e-06,
      "loss": 0.53,
      "step": 7245
    },
    {
      "epoch": 0.652366697427356,
      "grad_norm": 0.8826688254719149,
      "learning_rate": 5.69666577903075e-06,
      "loss": 0.471,
      "step": 7246
    },
    {
      "epoch": 0.6524567287132278,
      "grad_norm": 1.1618655578732466,
      "learning_rate": 5.694033620573045e-06,
      "loss": 0.4956,
      "step": 7247
    },
    {
      "epoch": 0.6525467599990997,
      "grad_norm": 0.7056967645804126,
      "learning_rate": 5.691401828297399e-06,
      "loss": 0.526,
      "step": 7248
    },
    {
      "epoch": 0.6526367912849715,
      "grad_norm": 0.9755059892109484,
      "learning_rate": 5.688770402427608e-06,
      "loss": 0.4575,
      "step": 7249
    },
    {
      "epoch": 0.6527268225708434,
      "grad_norm": 0.8826163245179033,
      "learning_rate": 5.686139343187468e-06,
      "loss": 0.6147,
      "step": 7250
    },
    {
      "epoch": 0.6528168538567152,
      "grad_norm": 0.7567740485147841,
      "learning_rate": 5.6835086508007135e-06,
      "loss": 0.5981,
      "step": 7251
    },
    {
      "epoch": 0.6529068851425871,
      "grad_norm": 0.7420890052688924,
      "learning_rate": 5.6808783254910616e-06,
      "loss": 0.4821,
      "step": 7252
    },
    {
      "epoch": 0.6529969164284589,
      "grad_norm": 0.9388777581469302,
      "learning_rate": 5.6782483674822e-06,
      "loss": 0.5173,
      "step": 7253
    },
    {
      "epoch": 0.6530869477143307,
      "grad_norm": 0.988906231528752,
      "learning_rate": 5.67561877699778e-06,
      "loss": 0.5802,
      "step": 7254
    },
    {
      "epoch": 0.6531769790002026,
      "grad_norm": 0.7796833185355722,
      "learning_rate": 5.6729895542614254e-06,
      "loss": 0.5953,
      "step": 7255
    },
    {
      "epoch": 0.6532670102860744,
      "grad_norm": 0.8406664001202931,
      "learning_rate": 5.670360699496729e-06,
      "loss": 0.5185,
      "step": 7256
    },
    {
      "epoch": 0.6533570415719463,
      "grad_norm": 0.9888555910340346,
      "learning_rate": 5.667732212927244e-06,
      "loss": 0.5956,
      "step": 7257
    },
    {
      "epoch": 0.6534470728578181,
      "grad_norm": 0.7204403704583606,
      "learning_rate": 5.665104094776501e-06,
      "loss": 0.5217,
      "step": 7258
    },
    {
      "epoch": 0.6535371041436899,
      "grad_norm": 0.7637217429945997,
      "learning_rate": 5.6624763452679975e-06,
      "loss": 0.4531,
      "step": 7259
    },
    {
      "epoch": 0.6536271354295617,
      "grad_norm": 1.0150368342481,
      "learning_rate": 5.659848964625199e-06,
      "loss": 0.5369,
      "step": 7260
    },
    {
      "epoch": 0.6537171667154336,
      "grad_norm": 0.700062418523926,
      "learning_rate": 5.65722195307154e-06,
      "loss": 0.5078,
      "step": 7261
    },
    {
      "epoch": 0.6538071980013055,
      "grad_norm": 0.7211960083737006,
      "learning_rate": 5.654595310830426e-06,
      "loss": 0.4927,
      "step": 7262
    },
    {
      "epoch": 0.6538972292871773,
      "grad_norm": 0.8397336579781506,
      "learning_rate": 5.651969038125218e-06,
      "loss": 0.6471,
      "step": 7263
    },
    {
      "epoch": 0.6539872605730491,
      "grad_norm": 1.3848057521528094,
      "learning_rate": 5.649343135179271e-06,
      "loss": 0.6228,
      "step": 7264
    },
    {
      "epoch": 0.6540772918589209,
      "grad_norm": 0.8055792349081496,
      "learning_rate": 5.64671760221588e-06,
      "loss": 0.482,
      "step": 7265
    },
    {
      "epoch": 0.6541673231447929,
      "grad_norm": 1.0602184890374897,
      "learning_rate": 5.644092439458329e-06,
      "loss": 0.5265,
      "step": 7266
    },
    {
      "epoch": 0.6542573544306647,
      "grad_norm": 0.9953374524138335,
      "learning_rate": 5.641467647129859e-06,
      "loss": 0.483,
      "step": 7267
    },
    {
      "epoch": 0.6543473857165365,
      "grad_norm": 1.0357065815077329,
      "learning_rate": 5.638843225453689e-06,
      "loss": 0.5309,
      "step": 7268
    },
    {
      "epoch": 0.6544374170024083,
      "grad_norm": 0.9443681051998296,
      "learning_rate": 5.636219174652998e-06,
      "loss": 0.467,
      "step": 7269
    },
    {
      "epoch": 0.6545274482882801,
      "grad_norm": 1.1433636256791662,
      "learning_rate": 5.633595494950943e-06,
      "loss": 0.4243,
      "step": 7270
    },
    {
      "epoch": 0.6546174795741521,
      "grad_norm": 1.1190450498120657,
      "learning_rate": 5.630972186570634e-06,
      "loss": 0.4812,
      "step": 7271
    },
    {
      "epoch": 0.6547075108600239,
      "grad_norm": 0.7472548886241586,
      "learning_rate": 5.628349249735163e-06,
      "loss": 0.5393,
      "step": 7272
    },
    {
      "epoch": 0.6547975421458957,
      "grad_norm": 1.0337291270524156,
      "learning_rate": 5.6257266846675865e-06,
      "loss": 0.5901,
      "step": 7273
    },
    {
      "epoch": 0.6548875734317675,
      "grad_norm": 0.8696616073336285,
      "learning_rate": 5.623104491590928e-06,
      "loss": 0.5073,
      "step": 7274
    },
    {
      "epoch": 0.6549776047176393,
      "grad_norm": 0.887283085534361,
      "learning_rate": 5.620482670728182e-06,
      "loss": 0.5852,
      "step": 7275
    },
    {
      "epoch": 0.6550676360035113,
      "grad_norm": 0.7810724691688529,
      "learning_rate": 5.617861222302315e-06,
      "loss": 0.5495,
      "step": 7276
    },
    {
      "epoch": 0.6551576672893831,
      "grad_norm": 1.1288508855242072,
      "learning_rate": 5.61524014653624e-06,
      "loss": 0.505,
      "step": 7277
    },
    {
      "epoch": 0.6552476985752549,
      "grad_norm": 0.8774392572213048,
      "learning_rate": 5.612619443652873e-06,
      "loss": 0.4889,
      "step": 7278
    },
    {
      "epoch": 0.6553377298611267,
      "grad_norm": 0.8956296937949975,
      "learning_rate": 5.609999113875071e-06,
      "loss": 0.5015,
      "step": 7279
    },
    {
      "epoch": 0.6554277611469986,
      "grad_norm": 0.8949501021773395,
      "learning_rate": 5.60737915742567e-06,
      "loss": 0.5854,
      "step": 7280
    },
    {
      "epoch": 0.6555177924328704,
      "grad_norm": 0.8004004130016892,
      "learning_rate": 5.604759574527473e-06,
      "loss": 0.4258,
      "step": 7281
    },
    {
      "epoch": 0.6556078237187423,
      "grad_norm": 1.2412570181338884,
      "learning_rate": 5.60214036540325e-06,
      "loss": 0.5243,
      "step": 7282
    },
    {
      "epoch": 0.6556978550046141,
      "grad_norm": 0.8176826200816569,
      "learning_rate": 5.5995215302757426e-06,
      "loss": 0.5403,
      "step": 7283
    },
    {
      "epoch": 0.6557878862904859,
      "grad_norm": 0.9688688746511734,
      "learning_rate": 5.5969030693676595e-06,
      "loss": 0.608,
      "step": 7284
    },
    {
      "epoch": 0.6558779175763578,
      "grad_norm": 0.7652883206308013,
      "learning_rate": 5.59428498290167e-06,
      "loss": 0.4594,
      "step": 7285
    },
    {
      "epoch": 0.6559679488622296,
      "grad_norm": 0.8403739767640946,
      "learning_rate": 5.5916672711004206e-06,
      "loss": 0.4202,
      "step": 7286
    },
    {
      "epoch": 0.6560579801481015,
      "grad_norm": 1.0549655193693652,
      "learning_rate": 5.589049934186525e-06,
      "loss": 0.4825,
      "step": 7287
    },
    {
      "epoch": 0.6561480114339733,
      "grad_norm": 1.0007945864835595,
      "learning_rate": 5.586432972382561e-06,
      "loss": 0.5536,
      "step": 7288
    },
    {
      "epoch": 0.6562380427198451,
      "grad_norm": 0.7942681572868961,
      "learning_rate": 5.583816385911078e-06,
      "loss": 0.5267,
      "step": 7289
    },
    {
      "epoch": 0.656328074005717,
      "grad_norm": 0.8497505330638918,
      "learning_rate": 5.5812001749945954e-06,
      "loss": 0.5579,
      "step": 7290
    },
    {
      "epoch": 0.6564181052915888,
      "grad_norm": 0.9583418739855724,
      "learning_rate": 5.578584339855586e-06,
      "loss": 0.5044,
      "step": 7291
    },
    {
      "epoch": 0.6565081365774607,
      "grad_norm": 0.9842392885196521,
      "learning_rate": 5.575968880716518e-06,
      "loss": 0.4667,
      "step": 7292
    },
    {
      "epoch": 0.6565981678633325,
      "grad_norm": 0.9981289722246569,
      "learning_rate": 5.573353797799799e-06,
      "loss": 0.3911,
      "step": 7293
    },
    {
      "epoch": 0.6566881991492044,
      "grad_norm": 1.0915065818734673,
      "learning_rate": 5.5707390913278216e-06,
      "loss": 0.5214,
      "step": 7294
    },
    {
      "epoch": 0.6567782304350762,
      "grad_norm": 1.0224881451354928,
      "learning_rate": 5.568124761522942e-06,
      "loss": 0.4961,
      "step": 7295
    },
    {
      "epoch": 0.656868261720948,
      "grad_norm": 0.8571560686361785,
      "learning_rate": 5.565510808607486e-06,
      "loss": 0.4824,
      "step": 7296
    },
    {
      "epoch": 0.6569582930068198,
      "grad_norm": 0.9370407780796041,
      "learning_rate": 5.5628972328037435e-06,
      "loss": 0.5239,
      "step": 7297
    },
    {
      "epoch": 0.6570483242926917,
      "grad_norm": 0.9009833776205961,
      "learning_rate": 5.560284034333978e-06,
      "loss": 0.5361,
      "step": 7298
    },
    {
      "epoch": 0.6571383555785636,
      "grad_norm": 0.8423021606615467,
      "learning_rate": 5.557671213420414e-06,
      "loss": 0.5103,
      "step": 7299
    },
    {
      "epoch": 0.6572283868644354,
      "grad_norm": 0.8313634060414585,
      "learning_rate": 5.5550587702852465e-06,
      "loss": 0.5267,
      "step": 7300
    },
    {
      "epoch": 0.6573184181503072,
      "grad_norm": 0.8138849225479514,
      "learning_rate": 5.552446705150641e-06,
      "loss": 0.523,
      "step": 7301
    },
    {
      "epoch": 0.657408449436179,
      "grad_norm": 1.082901039879282,
      "learning_rate": 5.549835018238731e-06,
      "loss": 0.5491,
      "step": 7302
    },
    {
      "epoch": 0.6574984807220509,
      "grad_norm": 0.9348471524488928,
      "learning_rate": 5.547223709771613e-06,
      "loss": 0.5182,
      "step": 7303
    },
    {
      "epoch": 0.6575885120079228,
      "grad_norm": 1.14765167431596,
      "learning_rate": 5.544612779971359e-06,
      "loss": 0.4607,
      "step": 7304
    },
    {
      "epoch": 0.6576785432937946,
      "grad_norm": 0.7914222072400053,
      "learning_rate": 5.542002229059995e-06,
      "loss": 0.4714,
      "step": 7305
    },
    {
      "epoch": 0.6577685745796664,
      "grad_norm": 1.2016593997357912,
      "learning_rate": 5.539392057259536e-06,
      "loss": 0.5771,
      "step": 7306
    },
    {
      "epoch": 0.6578586058655382,
      "grad_norm": 0.8907211043145198,
      "learning_rate": 5.536782264791942e-06,
      "loss": 0.5235,
      "step": 7307
    },
    {
      "epoch": 0.6579486371514102,
      "grad_norm": 0.8638826557155491,
      "learning_rate": 5.534172851879156e-06,
      "loss": 0.5077,
      "step": 7308
    },
    {
      "epoch": 0.658038668437282,
      "grad_norm": 0.9517172430882277,
      "learning_rate": 5.531563818743085e-06,
      "loss": 0.6458,
      "step": 7309
    },
    {
      "epoch": 0.6581286997231538,
      "grad_norm": 0.8729253611001407,
      "learning_rate": 5.5289551656055996e-06,
      "loss": 0.4902,
      "step": 7310
    },
    {
      "epoch": 0.6582187310090256,
      "grad_norm": 0.7393255968218045,
      "learning_rate": 5.526346892688543e-06,
      "loss": 0.5268,
      "step": 7311
    },
    {
      "epoch": 0.6583087622948974,
      "grad_norm": 0.942393071098402,
      "learning_rate": 5.52373900021373e-06,
      "loss": 0.5278,
      "step": 7312
    },
    {
      "epoch": 0.6583987935807694,
      "grad_norm": 0.7570144839758115,
      "learning_rate": 5.521131488402928e-06,
      "loss": 0.5201,
      "step": 7313
    },
    {
      "epoch": 0.6584888248666412,
      "grad_norm": 0.9450697322363455,
      "learning_rate": 5.518524357477885e-06,
      "loss": 0.6055,
      "step": 7314
    },
    {
      "epoch": 0.658578856152513,
      "grad_norm": 0.7977281381986666,
      "learning_rate": 5.515917607660313e-06,
      "loss": 0.547,
      "step": 7315
    },
    {
      "epoch": 0.6586688874383848,
      "grad_norm": 0.7742135212851728,
      "learning_rate": 5.513311239171893e-06,
      "loss": 0.5703,
      "step": 7316
    },
    {
      "epoch": 0.6587589187242566,
      "grad_norm": 0.9077319033752573,
      "learning_rate": 5.5107052522342694e-06,
      "loss": 0.494,
      "step": 7317
    },
    {
      "epoch": 0.6588489500101286,
      "grad_norm": 0.8579665466157629,
      "learning_rate": 5.508099647069063e-06,
      "loss": 0.487,
      "step": 7318
    },
    {
      "epoch": 0.6589389812960004,
      "grad_norm": 0.9510924325977081,
      "learning_rate": 5.505494423897845e-06,
      "loss": 0.5445,
      "step": 7319
    },
    {
      "epoch": 0.6590290125818722,
      "grad_norm": 0.8405345041313629,
      "learning_rate": 5.502889582942179e-06,
      "loss": 0.4894,
      "step": 7320
    },
    {
      "epoch": 0.659119043867744,
      "grad_norm": 0.7769870071815641,
      "learning_rate": 5.500285124423572e-06,
      "loss": 0.4921,
      "step": 7321
    },
    {
      "epoch": 0.6592090751536159,
      "grad_norm": 0.7867795489251027,
      "learning_rate": 5.497681048563512e-06,
      "loss": 0.517,
      "step": 7322
    },
    {
      "epoch": 0.6592991064394877,
      "grad_norm": 1.346166565455405,
      "learning_rate": 5.495077355583451e-06,
      "loss": 0.5282,
      "step": 7323
    },
    {
      "epoch": 0.6593891377253596,
      "grad_norm": 1.0512059989181473,
      "learning_rate": 5.492474045704809e-06,
      "loss": 0.4933,
      "step": 7324
    },
    {
      "epoch": 0.6594791690112314,
      "grad_norm": 1.0544277966378286,
      "learning_rate": 5.4898711191489724e-06,
      "loss": 0.5195,
      "step": 7325
    },
    {
      "epoch": 0.6595692002971032,
      "grad_norm": 0.8522690679041678,
      "learning_rate": 5.487268576137301e-06,
      "loss": 0.4953,
      "step": 7326
    },
    {
      "epoch": 0.6596592315829751,
      "grad_norm": 0.9840961470567627,
      "learning_rate": 5.484666416891109e-06,
      "loss": 0.5297,
      "step": 7327
    },
    {
      "epoch": 0.6597492628688469,
      "grad_norm": 0.9214799991192882,
      "learning_rate": 5.482064641631688e-06,
      "loss": 0.4986,
      "step": 7328
    },
    {
      "epoch": 0.6598392941547188,
      "grad_norm": 0.7670315365246889,
      "learning_rate": 5.479463250580295e-06,
      "loss": 0.5743,
      "step": 7329
    },
    {
      "epoch": 0.6599293254405906,
      "grad_norm": 0.8537112333836235,
      "learning_rate": 5.476862243958154e-06,
      "loss": 0.4669,
      "step": 7330
    },
    {
      "epoch": 0.6600193567264624,
      "grad_norm": 0.995280033601792,
      "learning_rate": 5.474261621986456e-06,
      "loss": 0.5112,
      "step": 7331
    },
    {
      "epoch": 0.6601093880123343,
      "grad_norm": 0.7192431080365516,
      "learning_rate": 5.471661384886365e-06,
      "loss": 0.4404,
      "step": 7332
    },
    {
      "epoch": 0.6601994192982061,
      "grad_norm": 0.8569999474528248,
      "learning_rate": 5.4690615328789945e-06,
      "loss": 0.5366,
      "step": 7333
    },
    {
      "epoch": 0.660289450584078,
      "grad_norm": 0.9588347321081451,
      "learning_rate": 5.466462066185451e-06,
      "loss": 0.5085,
      "step": 7334
    },
    {
      "epoch": 0.6603794818699498,
      "grad_norm": 0.84702166334388,
      "learning_rate": 5.463862985026784e-06,
      "loss": 0.5386,
      "step": 7335
    },
    {
      "epoch": 0.6604695131558217,
      "grad_norm": 0.9315810321164871,
      "learning_rate": 5.461264289624026e-06,
      "loss": 0.5797,
      "step": 7336
    },
    {
      "epoch": 0.6605595444416935,
      "grad_norm": 1.041097407712385,
      "learning_rate": 5.4586659801981724e-06,
      "loss": 0.4764,
      "step": 7337
    },
    {
      "epoch": 0.6606495757275653,
      "grad_norm": 1.0568809128915995,
      "learning_rate": 5.4560680569701825e-06,
      "loss": 0.4891,
      "step": 7338
    },
    {
      "epoch": 0.6607396070134371,
      "grad_norm": 1.0131880447781125,
      "learning_rate": 5.453470520160986e-06,
      "loss": 0.4882,
      "step": 7339
    },
    {
      "epoch": 0.660829638299309,
      "grad_norm": 1.1099076307824054,
      "learning_rate": 5.450873369991485e-06,
      "loss": 0.4765,
      "step": 7340
    },
    {
      "epoch": 0.6609196695851809,
      "grad_norm": 0.7090876309988933,
      "learning_rate": 5.448276606682532e-06,
      "loss": 0.5166,
      "step": 7341
    },
    {
      "epoch": 0.6610097008710527,
      "grad_norm": 1.0646522268506204,
      "learning_rate": 5.445680230454964e-06,
      "loss": 0.5994,
      "step": 7342
    },
    {
      "epoch": 0.6610997321569245,
      "grad_norm": 0.7467849815913592,
      "learning_rate": 5.443084241529577e-06,
      "loss": 0.4986,
      "step": 7343
    },
    {
      "epoch": 0.6611897634427963,
      "grad_norm": 1.0231022257025284,
      "learning_rate": 5.440488640127135e-06,
      "loss": 0.4391,
      "step": 7344
    },
    {
      "epoch": 0.6612797947286682,
      "grad_norm": 1.0707233910631717,
      "learning_rate": 5.43789342646837e-06,
      "loss": 0.5379,
      "step": 7345
    },
    {
      "epoch": 0.6613698260145401,
      "grad_norm": 0.9974522366328892,
      "learning_rate": 5.435298600773986e-06,
      "loss": 0.5024,
      "step": 7346
    },
    {
      "epoch": 0.6614598573004119,
      "grad_norm": 1.035604951871427,
      "learning_rate": 5.432704163264635e-06,
      "loss": 0.5312,
      "step": 7347
    },
    {
      "epoch": 0.6615498885862837,
      "grad_norm": 0.7682884027200924,
      "learning_rate": 5.430110114160965e-06,
      "loss": 0.5424,
      "step": 7348
    },
    {
      "epoch": 0.6616399198721555,
      "grad_norm": 1.1363241111010818,
      "learning_rate": 5.427516453683565e-06,
      "loss": 0.54,
      "step": 7349
    },
    {
      "epoch": 0.6617299511580275,
      "grad_norm": 1.016862007075663,
      "learning_rate": 5.424923182053006e-06,
      "loss": 0.5306,
      "step": 7350
    },
    {
      "epoch": 0.6618199824438993,
      "grad_norm": 1.2167759768628241,
      "learning_rate": 5.422330299489819e-06,
      "loss": 0.4931,
      "step": 7351
    },
    {
      "epoch": 0.6619100137297711,
      "grad_norm": 0.8231592305356561,
      "learning_rate": 5.419737806214507e-06,
      "loss": 0.5787,
      "step": 7352
    },
    {
      "epoch": 0.6620000450156429,
      "grad_norm": 0.9726757397306443,
      "learning_rate": 5.417145702447536e-06,
      "loss": 0.5591,
      "step": 7353
    },
    {
      "epoch": 0.6620900763015147,
      "grad_norm": 0.6985515863187604,
      "learning_rate": 5.414553988409343e-06,
      "loss": 0.492,
      "step": 7354
    },
    {
      "epoch": 0.6621801075873867,
      "grad_norm": 0.7753348419415766,
      "learning_rate": 5.411962664320323e-06,
      "loss": 0.4336,
      "step": 7355
    },
    {
      "epoch": 0.6622701388732585,
      "grad_norm": 0.8464006193970061,
      "learning_rate": 5.409371730400847e-06,
      "loss": 0.4908,
      "step": 7356
    },
    {
      "epoch": 0.6623601701591303,
      "grad_norm": 0.8233488498278914,
      "learning_rate": 5.406781186871249e-06,
      "loss": 0.4669,
      "step": 7357
    },
    {
      "epoch": 0.6624502014450021,
      "grad_norm": 0.9226169717230852,
      "learning_rate": 5.404191033951832e-06,
      "loss": 0.5605,
      "step": 7358
    },
    {
      "epoch": 0.6625402327308739,
      "grad_norm": 0.9355562049002557,
      "learning_rate": 5.401601271862862e-06,
      "loss": 0.6093,
      "step": 7359
    },
    {
      "epoch": 0.6626302640167459,
      "grad_norm": 0.8318333630165538,
      "learning_rate": 5.39901190082458e-06,
      "loss": 0.5081,
      "step": 7360
    },
    {
      "epoch": 0.6627202953026177,
      "grad_norm": 0.7436996705725425,
      "learning_rate": 5.396422921057175e-06,
      "loss": 0.5507,
      "step": 7361
    },
    {
      "epoch": 0.6628103265884895,
      "grad_norm": 1.1380521084475705,
      "learning_rate": 5.393834332780831e-06,
      "loss": 0.6251,
      "step": 7362
    },
    {
      "epoch": 0.6629003578743613,
      "grad_norm": 0.7621995335712038,
      "learning_rate": 5.391246136215671e-06,
      "loss": 0.4912,
      "step": 7363
    },
    {
      "epoch": 0.6629903891602332,
      "grad_norm": 0.8114520288455673,
      "learning_rate": 5.388658331581802e-06,
      "loss": 0.5103,
      "step": 7364
    },
    {
      "epoch": 0.663080420446105,
      "grad_norm": 0.932865220841475,
      "learning_rate": 5.386070919099291e-06,
      "loss": 0.5124,
      "step": 7365
    },
    {
      "epoch": 0.6631704517319769,
      "grad_norm": 0.9239933143176229,
      "learning_rate": 5.383483898988176e-06,
      "loss": 0.5267,
      "step": 7366
    },
    {
      "epoch": 0.6632604830178487,
      "grad_norm": 0.7396445762972557,
      "learning_rate": 5.3808972714684545e-06,
      "loss": 0.4894,
      "step": 7367
    },
    {
      "epoch": 0.6633505143037205,
      "grad_norm": 0.8246230501876454,
      "learning_rate": 5.378311036760102e-06,
      "loss": 0.5983,
      "step": 7368
    },
    {
      "epoch": 0.6634405455895924,
      "grad_norm": 0.9174387371108895,
      "learning_rate": 5.375725195083046e-06,
      "loss": 0.4952,
      "step": 7369
    },
    {
      "epoch": 0.6635305768754642,
      "grad_norm": 0.7903236192609302,
      "learning_rate": 5.37313974665719e-06,
      "loss": 0.5557,
      "step": 7370
    },
    {
      "epoch": 0.6636206081613361,
      "grad_norm": 1.0682852843614241,
      "learning_rate": 5.370554691702402e-06,
      "loss": 0.5773,
      "step": 7371
    },
    {
      "epoch": 0.6637106394472079,
      "grad_norm": 1.0217678493900895,
      "learning_rate": 5.367970030438518e-06,
      "loss": 0.5799,
      "step": 7372
    },
    {
      "epoch": 0.6638006707330798,
      "grad_norm": 0.9025659192261887,
      "learning_rate": 5.36538576308534e-06,
      "loss": 0.4838,
      "step": 7373
    },
    {
      "epoch": 0.6638907020189516,
      "grad_norm": 0.7619644440300816,
      "learning_rate": 5.362801889862638e-06,
      "loss": 0.5263,
      "step": 7374
    },
    {
      "epoch": 0.6639807333048234,
      "grad_norm": 0.8393527722076137,
      "learning_rate": 5.3602184109901345e-06,
      "loss": 0.5456,
      "step": 7375
    },
    {
      "epoch": 0.6640707645906953,
      "grad_norm": 0.8413376957420244,
      "learning_rate": 5.357635326687546e-06,
      "loss": 0.4714,
      "step": 7376
    },
    {
      "epoch": 0.6641607958765671,
      "grad_norm": 1.094538279627873,
      "learning_rate": 5.355052637174528e-06,
      "loss": 0.5249,
      "step": 7377
    },
    {
      "epoch": 0.664250827162439,
      "grad_norm": 0.8602635278724234,
      "learning_rate": 5.352470342670719e-06,
      "loss": 0.4852,
      "step": 7378
    },
    {
      "epoch": 0.6643408584483108,
      "grad_norm": 0.9359428684905017,
      "learning_rate": 5.349888443395717e-06,
      "loss": 0.5727,
      "step": 7379
    },
    {
      "epoch": 0.6644308897341826,
      "grad_norm": 0.7746474969353542,
      "learning_rate": 5.347306939569089e-06,
      "loss": 0.532,
      "step": 7380
    },
    {
      "epoch": 0.6645209210200544,
      "grad_norm": 0.966648009096611,
      "learning_rate": 5.344725831410369e-06,
      "loss": 0.4908,
      "step": 7381
    },
    {
      "epoch": 0.6646109523059263,
      "grad_norm": 0.7301544938881652,
      "learning_rate": 5.342145119139057e-06,
      "loss": 0.466,
      "step": 7382
    },
    {
      "epoch": 0.6647009835917982,
      "grad_norm": 0.7509644935572718,
      "learning_rate": 5.339564802974615e-06,
      "loss": 0.4973,
      "step": 7383
    },
    {
      "epoch": 0.66479101487767,
      "grad_norm": 0.8131975655896831,
      "learning_rate": 5.336984883136473e-06,
      "loss": 0.4945,
      "step": 7384
    },
    {
      "epoch": 0.6648810461635418,
      "grad_norm": 0.8145465589978018,
      "learning_rate": 5.334405359844035e-06,
      "loss": 0.4081,
      "step": 7385
    },
    {
      "epoch": 0.6649710774494136,
      "grad_norm": 0.846642580681495,
      "learning_rate": 5.33182623331666e-06,
      "loss": 0.506,
      "step": 7386
    },
    {
      "epoch": 0.6650611087352856,
      "grad_norm": 0.6818405293435209,
      "learning_rate": 5.329247503773682e-06,
      "loss": 0.5323,
      "step": 7387
    },
    {
      "epoch": 0.6651511400211574,
      "grad_norm": 0.8731569730046412,
      "learning_rate": 5.3266691714344e-06,
      "loss": 0.5214,
      "step": 7388
    },
    {
      "epoch": 0.6652411713070292,
      "grad_norm": 0.9823508380852981,
      "learning_rate": 5.324091236518065e-06,
      "loss": 0.5499,
      "step": 7389
    },
    {
      "epoch": 0.665331202592901,
      "grad_norm": 0.7799078800489303,
      "learning_rate": 5.321513699243924e-06,
      "loss": 0.4435,
      "step": 7390
    },
    {
      "epoch": 0.6654212338787728,
      "grad_norm": 0.9035485003993017,
      "learning_rate": 5.318936559831158e-06,
      "loss": 0.4586,
      "step": 7391
    },
    {
      "epoch": 0.6655112651646448,
      "grad_norm": 1.0109994508422573,
      "learning_rate": 5.316359818498935e-06,
      "loss": 0.5327,
      "step": 7392
    },
    {
      "epoch": 0.6656012964505166,
      "grad_norm": 0.8380360158587844,
      "learning_rate": 5.313783475466381e-06,
      "loss": 0.4929,
      "step": 7393
    },
    {
      "epoch": 0.6656913277363884,
      "grad_norm": 0.7956196130102318,
      "learning_rate": 5.311207530952589e-06,
      "loss": 0.5239,
      "step": 7394
    },
    {
      "epoch": 0.6657813590222602,
      "grad_norm": 1.012428649189478,
      "learning_rate": 5.30863198517662e-06,
      "loss": 0.5263,
      "step": 7395
    },
    {
      "epoch": 0.665871390308132,
      "grad_norm": 0.851816468432923,
      "learning_rate": 5.306056838357505e-06,
      "loss": 0.4604,
      "step": 7396
    },
    {
      "epoch": 0.665961421594004,
      "grad_norm": 0.8351908262993682,
      "learning_rate": 5.303482090714227e-06,
      "loss": 0.5145,
      "step": 7397
    },
    {
      "epoch": 0.6660514528798758,
      "grad_norm": 0.9062169738808035,
      "learning_rate": 5.3009077424657485e-06,
      "loss": 0.4665,
      "step": 7398
    },
    {
      "epoch": 0.6661414841657476,
      "grad_norm": 0.8314887025152,
      "learning_rate": 5.298333793830992e-06,
      "loss": 0.4861,
      "step": 7399
    },
    {
      "epoch": 0.6662315154516194,
      "grad_norm": 0.9904867722810301,
      "learning_rate": 5.2957602450288515e-06,
      "loss": 0.5779,
      "step": 7400
    },
    {
      "epoch": 0.6663215467374913,
      "grad_norm": 0.9007930011919869,
      "learning_rate": 5.2931870962781785e-06,
      "loss": 0.5486,
      "step": 7401
    },
    {
      "epoch": 0.6664115780233631,
      "grad_norm": 0.8611921613452574,
      "learning_rate": 5.290614347797802e-06,
      "loss": 0.5083,
      "step": 7402
    },
    {
      "epoch": 0.666501609309235,
      "grad_norm": 1.080162603600589,
      "learning_rate": 5.2880419998064995e-06,
      "loss": 0.551,
      "step": 7403
    },
    {
      "epoch": 0.6665916405951068,
      "grad_norm": 0.7494171081797013,
      "learning_rate": 5.285470052523036e-06,
      "loss": 0.4721,
      "step": 7404
    },
    {
      "epoch": 0.6666816718809786,
      "grad_norm": 0.8777126491610564,
      "learning_rate": 5.282898506166125e-06,
      "loss": 0.5424,
      "step": 7405
    },
    {
      "epoch": 0.6667717031668505,
      "grad_norm": 0.9474005723319453,
      "learning_rate": 5.280327360954454e-06,
      "loss": 0.5652,
      "step": 7406
    },
    {
      "epoch": 0.6668617344527223,
      "grad_norm": 0.8736271853883238,
      "learning_rate": 5.277756617106674e-06,
      "loss": 0.5363,
      "step": 7407
    },
    {
      "epoch": 0.6669517657385942,
      "grad_norm": 1.0478933107363253,
      "learning_rate": 5.275186274841404e-06,
      "loss": 0.4672,
      "step": 7408
    },
    {
      "epoch": 0.667041797024466,
      "grad_norm": 0.873559874269649,
      "learning_rate": 5.272616334377227e-06,
      "loss": 0.4838,
      "step": 7409
    },
    {
      "epoch": 0.6671318283103378,
      "grad_norm": 0.7734156055099174,
      "learning_rate": 5.270046795932696e-06,
      "loss": 0.4448,
      "step": 7410
    },
    {
      "epoch": 0.6672218595962097,
      "grad_norm": 0.9271563780477521,
      "learning_rate": 5.267477659726319e-06,
      "loss": 0.4584,
      "step": 7411
    },
    {
      "epoch": 0.6673118908820815,
      "grad_norm": 1.074682461506694,
      "learning_rate": 5.264908925976579e-06,
      "loss": 0.5802,
      "step": 7412
    },
    {
      "epoch": 0.6674019221679534,
      "grad_norm": 1.0664416397320198,
      "learning_rate": 5.262340594901927e-06,
      "loss": 0.4526,
      "step": 7413
    },
    {
      "epoch": 0.6674919534538252,
      "grad_norm": 1.0774866775263054,
      "learning_rate": 5.259772666720771e-06,
      "loss": 0.5149,
      "step": 7414
    },
    {
      "epoch": 0.6675819847396971,
      "grad_norm": 0.9155631477189609,
      "learning_rate": 5.2572051416514905e-06,
      "loss": 0.5851,
      "step": 7415
    },
    {
      "epoch": 0.6676720160255689,
      "grad_norm": 0.9720551718969278,
      "learning_rate": 5.254638019912434e-06,
      "loss": 0.5813,
      "step": 7416
    },
    {
      "epoch": 0.6677620473114407,
      "grad_norm": 0.8961636770979093,
      "learning_rate": 5.252071301721899e-06,
      "loss": 0.5004,
      "step": 7417
    },
    {
      "epoch": 0.6678520785973125,
      "grad_norm": 0.8719221763938463,
      "learning_rate": 5.249504987298177e-06,
      "loss": 0.5497,
      "step": 7418
    },
    {
      "epoch": 0.6679421098831844,
      "grad_norm": 0.7483369075772801,
      "learning_rate": 5.246939076859497e-06,
      "loss": 0.5062,
      "step": 7419
    },
    {
      "epoch": 0.6680321411690563,
      "grad_norm": 0.9529328956800929,
      "learning_rate": 5.244373570624067e-06,
      "loss": 0.5336,
      "step": 7420
    },
    {
      "epoch": 0.6681221724549281,
      "grad_norm": 0.9617749794612995,
      "learning_rate": 5.2418084688100635e-06,
      "loss": 0.4858,
      "step": 7421
    },
    {
      "epoch": 0.6682122037407999,
      "grad_norm": 0.9504715736523847,
      "learning_rate": 5.239243771635622e-06,
      "loss": 0.5374,
      "step": 7422
    },
    {
      "epoch": 0.6683022350266717,
      "grad_norm": 0.9227379471295245,
      "learning_rate": 5.236679479318847e-06,
      "loss": 0.4958,
      "step": 7423
    },
    {
      "epoch": 0.6683922663125436,
      "grad_norm": 0.7724938096193026,
      "learning_rate": 5.234115592077809e-06,
      "loss": 0.4798,
      "step": 7424
    },
    {
      "epoch": 0.6684822975984155,
      "grad_norm": 0.9461668415110063,
      "learning_rate": 5.231552110130538e-06,
      "loss": 0.5011,
      "step": 7425
    },
    {
      "epoch": 0.6685723288842873,
      "grad_norm": 0.9371222779583298,
      "learning_rate": 5.228989033695035e-06,
      "loss": 0.4795,
      "step": 7426
    },
    {
      "epoch": 0.6686623601701591,
      "grad_norm": 0.937742560999716,
      "learning_rate": 5.226426362989269e-06,
      "loss": 0.5561,
      "step": 7427
    },
    {
      "epoch": 0.6687523914560309,
      "grad_norm": 0.773378635774425,
      "learning_rate": 5.223864098231168e-06,
      "loss": 0.4539,
      "step": 7428
    },
    {
      "epoch": 0.6688424227419029,
      "grad_norm": 1.106992643025065,
      "learning_rate": 5.22130223963863e-06,
      "loss": 0.5193,
      "step": 7429
    },
    {
      "epoch": 0.6689324540277747,
      "grad_norm": 0.8559029070422769,
      "learning_rate": 5.2187407874295215e-06,
      "loss": 0.597,
      "step": 7430
    },
    {
      "epoch": 0.6690224853136465,
      "grad_norm": 1.1302626694133489,
      "learning_rate": 5.216179741821656e-06,
      "loss": 0.5979,
      "step": 7431
    },
    {
      "epoch": 0.6691125165995183,
      "grad_norm": 0.9267679359230573,
      "learning_rate": 5.213619103032845e-06,
      "loss": 0.531,
      "step": 7432
    },
    {
      "epoch": 0.6692025478853901,
      "grad_norm": 1.036523473871524,
      "learning_rate": 5.211058871280832e-06,
      "loss": 0.4728,
      "step": 7433
    },
    {
      "epoch": 0.6692925791712621,
      "grad_norm": 0.9169250237453647,
      "learning_rate": 5.208499046783348e-06,
      "loss": 0.5984,
      "step": 7434
    },
    {
      "epoch": 0.6693826104571339,
      "grad_norm": 0.8082895956030814,
      "learning_rate": 5.2059396297580785e-06,
      "loss": 0.5215,
      "step": 7435
    },
    {
      "epoch": 0.6694726417430057,
      "grad_norm": 0.9165950141691015,
      "learning_rate": 5.203380620422683e-06,
      "loss": 0.5821,
      "step": 7436
    },
    {
      "epoch": 0.6695626730288775,
      "grad_norm": 1.1396094632103762,
      "learning_rate": 5.200822018994774e-06,
      "loss": 0.539,
      "step": 7437
    },
    {
      "epoch": 0.6696527043147493,
      "grad_norm": 0.8517925635281918,
      "learning_rate": 5.198263825691947e-06,
      "loss": 0.5315,
      "step": 7438
    },
    {
      "epoch": 0.6697427356006213,
      "grad_norm": 0.797620777345107,
      "learning_rate": 5.195706040731741e-06,
      "loss": 0.5582,
      "step": 7439
    },
    {
      "epoch": 0.6698327668864931,
      "grad_norm": 0.8259372023855319,
      "learning_rate": 5.193148664331677e-06,
      "loss": 0.5786,
      "step": 7440
    },
    {
      "epoch": 0.6699227981723649,
      "grad_norm": 1.0467848001777582,
      "learning_rate": 5.190591696709235e-06,
      "loss": 0.488,
      "step": 7441
    },
    {
      "epoch": 0.6700128294582367,
      "grad_norm": 0.9518679753984804,
      "learning_rate": 5.18803513808186e-06,
      "loss": 0.6103,
      "step": 7442
    },
    {
      "epoch": 0.6701028607441086,
      "grad_norm": 0.9439541228655569,
      "learning_rate": 5.1854789886669675e-06,
      "loss": 0.4194,
      "step": 7443
    },
    {
      "epoch": 0.6701928920299804,
      "grad_norm": 0.8544361687332499,
      "learning_rate": 5.182923248681934e-06,
      "loss": 0.5663,
      "step": 7444
    },
    {
      "epoch": 0.6702829233158523,
      "grad_norm": 0.9062547639754214,
      "learning_rate": 5.1803679183440905e-06,
      "loss": 0.4505,
      "step": 7445
    },
    {
      "epoch": 0.6703729546017241,
      "grad_norm": 0.8256864685059165,
      "learning_rate": 5.1778129978707594e-06,
      "loss": 0.4955,
      "step": 7446
    },
    {
      "epoch": 0.6704629858875959,
      "grad_norm": 0.800503188523712,
      "learning_rate": 5.175258487479202e-06,
      "loss": 0.4739,
      "step": 7447
    },
    {
      "epoch": 0.6705530171734678,
      "grad_norm": 0.9170716184367357,
      "learning_rate": 5.172704387386659e-06,
      "loss": 0.493,
      "step": 7448
    },
    {
      "epoch": 0.6706430484593396,
      "grad_norm": 0.9499817151568977,
      "learning_rate": 5.170150697810332e-06,
      "loss": 0.5401,
      "step": 7449
    },
    {
      "epoch": 0.6707330797452115,
      "grad_norm": 1.0606914569295667,
      "learning_rate": 5.16759741896739e-06,
      "loss": 0.5864,
      "step": 7450
    },
    {
      "epoch": 0.6708231110310833,
      "grad_norm": 1.0417046817843887,
      "learning_rate": 5.165044551074962e-06,
      "loss": 0.5418,
      "step": 7451
    },
    {
      "epoch": 0.6709131423169551,
      "grad_norm": 0.8540875605774334,
      "learning_rate": 5.162492094350151e-06,
      "loss": 0.4418,
      "step": 7452
    },
    {
      "epoch": 0.671003173602827,
      "grad_norm": 1.1433595750670353,
      "learning_rate": 5.159940049010015e-06,
      "loss": 0.5627,
      "step": 7453
    },
    {
      "epoch": 0.6710932048886988,
      "grad_norm": 0.9763473185157565,
      "learning_rate": 5.15738841527158e-06,
      "loss": 0.5597,
      "step": 7454
    },
    {
      "epoch": 0.6711832361745707,
      "grad_norm": 1.019594664772132,
      "learning_rate": 5.154837193351843e-06,
      "loss": 0.4801,
      "step": 7455
    },
    {
      "epoch": 0.6712732674604425,
      "grad_norm": 1.030277974561672,
      "learning_rate": 5.15228638346776e-06,
      "loss": 0.453,
      "step": 7456
    },
    {
      "epoch": 0.6713632987463144,
      "grad_norm": 1.12216229662921,
      "learning_rate": 5.149735985836251e-06,
      "loss": 0.5579,
      "step": 7457
    },
    {
      "epoch": 0.6714533300321862,
      "grad_norm": 1.0395129315275295,
      "learning_rate": 5.14718600067421e-06,
      "loss": 0.5772,
      "step": 7458
    },
    {
      "epoch": 0.671543361318058,
      "grad_norm": 1.0080285224454069,
      "learning_rate": 5.144636428198477e-06,
      "loss": 0.4639,
      "step": 7459
    },
    {
      "epoch": 0.6716333926039298,
      "grad_norm": 0.9034572973806214,
      "learning_rate": 5.142087268625886e-06,
      "loss": 0.5675,
      "step": 7460
    },
    {
      "epoch": 0.6717234238898017,
      "grad_norm": 0.9975649759705901,
      "learning_rate": 5.139538522173207e-06,
      "loss": 0.5266,
      "step": 7461
    },
    {
      "epoch": 0.6718134551756736,
      "grad_norm": 0.9672341696467028,
      "learning_rate": 5.136990189057187e-06,
      "loss": 0.4582,
      "step": 7462
    },
    {
      "epoch": 0.6719034864615454,
      "grad_norm": 1.0177864150372853,
      "learning_rate": 5.134442269494544e-06,
      "loss": 0.5424,
      "step": 7463
    },
    {
      "epoch": 0.6719935177474172,
      "grad_norm": 0.8402121993863898,
      "learning_rate": 5.13189476370195e-06,
      "loss": 0.5357,
      "step": 7464
    },
    {
      "epoch": 0.672083549033289,
      "grad_norm": 1.0662265137220612,
      "learning_rate": 5.129347671896048e-06,
      "loss": 0.5699,
      "step": 7465
    },
    {
      "epoch": 0.6721735803191609,
      "grad_norm": 1.047851892603025,
      "learning_rate": 5.126800994293448e-06,
      "loss": 0.5733,
      "step": 7466
    },
    {
      "epoch": 0.6722636116050328,
      "grad_norm": 1.0065985765443952,
      "learning_rate": 5.124254731110715e-06,
      "loss": 0.5023,
      "step": 7467
    },
    {
      "epoch": 0.6723536428909046,
      "grad_norm": 1.0022351367133921,
      "learning_rate": 5.121708882564387e-06,
      "loss": 0.6064,
      "step": 7468
    },
    {
      "epoch": 0.6724436741767764,
      "grad_norm": 0.8876558939798037,
      "learning_rate": 5.119163448870963e-06,
      "loss": 0.5193,
      "step": 7469
    },
    {
      "epoch": 0.6725337054626482,
      "grad_norm": 0.7878025795520696,
      "learning_rate": 5.116618430246911e-06,
      "loss": 0.5609,
      "step": 7470
    },
    {
      "epoch": 0.6726237367485202,
      "grad_norm": 1.142931687485608,
      "learning_rate": 5.114073826908661e-06,
      "loss": 0.4582,
      "step": 7471
    },
    {
      "epoch": 0.672713768034392,
      "grad_norm": 1.0939588530553506,
      "learning_rate": 5.11152963907261e-06,
      "loss": 0.612,
      "step": 7472
    },
    {
      "epoch": 0.6728037993202638,
      "grad_norm": 0.918533470059629,
      "learning_rate": 5.108985866955107e-06,
      "loss": 0.5317,
      "step": 7473
    },
    {
      "epoch": 0.6728938306061356,
      "grad_norm": 0.9937636749621904,
      "learning_rate": 5.106442510772489e-06,
      "loss": 0.4967,
      "step": 7474
    },
    {
      "epoch": 0.6729838618920074,
      "grad_norm": 1.1634453317624427,
      "learning_rate": 5.1038995707410355e-06,
      "loss": 0.5359,
      "step": 7475
    },
    {
      "epoch": 0.6730738931778794,
      "grad_norm": 0.9043658818813151,
      "learning_rate": 5.101357047077003e-06,
      "loss": 0.4912,
      "step": 7476
    },
    {
      "epoch": 0.6731639244637512,
      "grad_norm": 0.9593361124265835,
      "learning_rate": 5.0988149399966066e-06,
      "loss": 0.5536,
      "step": 7477
    },
    {
      "epoch": 0.673253955749623,
      "grad_norm": 0.9197776995706747,
      "learning_rate": 5.096273249716033e-06,
      "loss": 0.4854,
      "step": 7478
    },
    {
      "epoch": 0.6733439870354948,
      "grad_norm": 1.2204057903865413,
      "learning_rate": 5.093731976451428e-06,
      "loss": 0.5364,
      "step": 7479
    },
    {
      "epoch": 0.6734340183213666,
      "grad_norm": 0.9420418236057299,
      "learning_rate": 5.091191120418905e-06,
      "loss": 0.5245,
      "step": 7480
    },
    {
      "epoch": 0.6735240496072386,
      "grad_norm": 1.0829503567071075,
      "learning_rate": 5.088650681834533e-06,
      "loss": 0.5058,
      "step": 7481
    },
    {
      "epoch": 0.6736140808931104,
      "grad_norm": 0.8609729690009685,
      "learning_rate": 5.086110660914357e-06,
      "loss": 0.5201,
      "step": 7482
    },
    {
      "epoch": 0.6737041121789822,
      "grad_norm": 1.4533573455858986,
      "learning_rate": 5.083571057874381e-06,
      "loss": 0.4743,
      "step": 7483
    },
    {
      "epoch": 0.673794143464854,
      "grad_norm": 0.7969063179092587,
      "learning_rate": 5.081031872930577e-06,
      "loss": 0.5963,
      "step": 7484
    },
    {
      "epoch": 0.6738841747507259,
      "grad_norm": 0.9524961011828088,
      "learning_rate": 5.078493106298876e-06,
      "loss": 0.5215,
      "step": 7485
    },
    {
      "epoch": 0.6739742060365977,
      "grad_norm": 0.8166412276736141,
      "learning_rate": 5.075954758195181e-06,
      "loss": 0.5905,
      "step": 7486
    },
    {
      "epoch": 0.6740642373224696,
      "grad_norm": 1.135212171297586,
      "learning_rate": 5.073416828835342e-06,
      "loss": 0.627,
      "step": 7487
    },
    {
      "epoch": 0.6741542686083414,
      "grad_norm": 0.9044833127479813,
      "learning_rate": 5.0708793184352044e-06,
      "loss": 0.5158,
      "step": 7488
    },
    {
      "epoch": 0.6742442998942132,
      "grad_norm": 1.0578748126648474,
      "learning_rate": 5.0683422272105455e-06,
      "loss": 0.6094,
      "step": 7489
    },
    {
      "epoch": 0.6743343311800851,
      "grad_norm": 0.9893234536147699,
      "learning_rate": 5.065805555377126e-06,
      "loss": 0.5945,
      "step": 7490
    },
    {
      "epoch": 0.6744243624659569,
      "grad_norm": 0.9300272153449772,
      "learning_rate": 5.063269303150666e-06,
      "loss": 0.528,
      "step": 7491
    },
    {
      "epoch": 0.6745143937518288,
      "grad_norm": 0.8556382063001441,
      "learning_rate": 5.06073347074685e-06,
      "loss": 0.5712,
      "step": 7492
    },
    {
      "epoch": 0.6746044250377006,
      "grad_norm": 0.9064403694602311,
      "learning_rate": 5.058198058381327e-06,
      "loss": 0.5729,
      "step": 7493
    },
    {
      "epoch": 0.6746944563235724,
      "grad_norm": 0.8417365121990881,
      "learning_rate": 5.055663066269713e-06,
      "loss": 0.5509,
      "step": 7494
    },
    {
      "epoch": 0.6747844876094443,
      "grad_norm": 0.8276104404295594,
      "learning_rate": 5.053128494627578e-06,
      "loss": 0.602,
      "step": 7495
    },
    {
      "epoch": 0.6748745188953161,
      "grad_norm": 1.0124558568510436,
      "learning_rate": 5.050594343670467e-06,
      "loss": 0.5214,
      "step": 7496
    },
    {
      "epoch": 0.674964550181188,
      "grad_norm": 0.8484210818261271,
      "learning_rate": 5.048060613613887e-06,
      "loss": 0.5883,
      "step": 7497
    },
    {
      "epoch": 0.6750545814670598,
      "grad_norm": 1.0876846581587714,
      "learning_rate": 5.0455273046733075e-06,
      "loss": 0.592,
      "step": 7498
    },
    {
      "epoch": 0.6751446127529317,
      "grad_norm": 0.9708151704235121,
      "learning_rate": 5.042994417064163e-06,
      "loss": 0.5989,
      "step": 7499
    },
    {
      "epoch": 0.6752346440388035,
      "grad_norm": 1.0703260470004177,
      "learning_rate": 5.040461951001855e-06,
      "loss": 0.5393,
      "step": 7500
    },
    {
      "epoch": 0.6753246753246753,
      "grad_norm": 1.5361181910662305,
      "learning_rate": 5.037929906701734e-06,
      "loss": 0.5758,
      "step": 7501
    },
    {
      "epoch": 0.6754147066105471,
      "grad_norm": 0.8600172655984205,
      "learning_rate": 5.035398284379142e-06,
      "loss": 0.4541,
      "step": 7502
    },
    {
      "epoch": 0.675504737896419,
      "grad_norm": 0.9304341854183538,
      "learning_rate": 5.03286708424936e-06,
      "loss": 0.5936,
      "step": 7503
    },
    {
      "epoch": 0.6755947691822909,
      "grad_norm": 0.9343867997775298,
      "learning_rate": 5.030336306527645e-06,
      "loss": 0.5492,
      "step": 7504
    },
    {
      "epoch": 0.6756848004681627,
      "grad_norm": 0.9993127805487403,
      "learning_rate": 5.027805951429217e-06,
      "loss": 0.5056,
      "step": 7505
    },
    {
      "epoch": 0.6757748317540345,
      "grad_norm": 0.850212230353229,
      "learning_rate": 5.025276019169256e-06,
      "loss": 0.5416,
      "step": 7506
    },
    {
      "epoch": 0.6758648630399063,
      "grad_norm": 0.9154202133441909,
      "learning_rate": 5.022746509962913e-06,
      "loss": 0.6089,
      "step": 7507
    },
    {
      "epoch": 0.6759548943257782,
      "grad_norm": 0.8877775412495367,
      "learning_rate": 5.020217424025302e-06,
      "loss": 0.4598,
      "step": 7508
    },
    {
      "epoch": 0.6760449256116501,
      "grad_norm": 0.6852503979111605,
      "learning_rate": 5.0176887615714885e-06,
      "loss": 0.5985,
      "step": 7509
    },
    {
      "epoch": 0.6761349568975219,
      "grad_norm": 0.7595904235784939,
      "learning_rate": 5.015160522816516e-06,
      "loss": 0.4529,
      "step": 7510
    },
    {
      "epoch": 0.6762249881833937,
      "grad_norm": 0.9996500730731264,
      "learning_rate": 5.012632707975389e-06,
      "loss": 0.5177,
      "step": 7511
    },
    {
      "epoch": 0.6763150194692655,
      "grad_norm": 1.0594389879267752,
      "learning_rate": 5.010105317263072e-06,
      "loss": 0.5437,
      "step": 7512
    },
    {
      "epoch": 0.6764050507551375,
      "grad_norm": 0.8106222263465338,
      "learning_rate": 5.0075783508944996e-06,
      "loss": 0.5384,
      "step": 7513
    },
    {
      "epoch": 0.6764950820410093,
      "grad_norm": 0.884852843899745,
      "learning_rate": 5.005051809084567e-06,
      "loss": 0.5584,
      "step": 7514
    },
    {
      "epoch": 0.6765851133268811,
      "grad_norm": 0.8141427169874474,
      "learning_rate": 5.002525692048121e-06,
      "loss": 0.4518,
      "step": 7515
    },
    {
      "epoch": 0.6766751446127529,
      "grad_norm": 1.0173125206261355,
      "learning_rate": 5.000000000000003e-06,
      "loss": 0.5521,
      "step": 7516
    },
    {
      "epoch": 0.6767651758986247,
      "grad_norm": 0.9087143039137283,
      "learning_rate": 4.997474733154985e-06,
      "loss": 0.5575,
      "step": 7517
    },
    {
      "epoch": 0.6768552071844967,
      "grad_norm": 0.8495393372893304,
      "learning_rate": 4.994949891727822e-06,
      "loss": 0.4861,
      "step": 7518
    },
    {
      "epoch": 0.6769452384703685,
      "grad_norm": 0.8958913707632468,
      "learning_rate": 4.992425475933227e-06,
      "loss": 0.4848,
      "step": 7519
    },
    {
      "epoch": 0.6770352697562403,
      "grad_norm": 0.9430330712938636,
      "learning_rate": 4.989901485985884e-06,
      "loss": 0.5384,
      "step": 7520
    },
    {
      "epoch": 0.6771253010421121,
      "grad_norm": 0.983465330222108,
      "learning_rate": 4.987377922100422e-06,
      "loss": 0.5367,
      "step": 7521
    },
    {
      "epoch": 0.6772153323279839,
      "grad_norm": 0.7684210979614818,
      "learning_rate": 4.984854784491461e-06,
      "loss": 0.492,
      "step": 7522
    },
    {
      "epoch": 0.6773053636138558,
      "grad_norm": 1.0143394299064694,
      "learning_rate": 4.9823320733735605e-06,
      "loss": 0.5317,
      "step": 7523
    },
    {
      "epoch": 0.6773953948997277,
      "grad_norm": 0.7112624944663846,
      "learning_rate": 4.979809788961255e-06,
      "loss": 0.4669,
      "step": 7524
    },
    {
      "epoch": 0.6774854261855995,
      "grad_norm": 0.7916743992552311,
      "learning_rate": 4.977287931469043e-06,
      "loss": 0.4699,
      "step": 7525
    },
    {
      "epoch": 0.6775754574714713,
      "grad_norm": 1.0302981730600254,
      "learning_rate": 4.9747665011113845e-06,
      "loss": 0.5213,
      "step": 7526
    },
    {
      "epoch": 0.6776654887573432,
      "grad_norm": 0.8603901012660911,
      "learning_rate": 4.972245498102702e-06,
      "loss": 0.4821,
      "step": 7527
    },
    {
      "epoch": 0.677755520043215,
      "grad_norm": 0.8258000349683271,
      "learning_rate": 4.969724922657389e-06,
      "loss": 0.5453,
      "step": 7528
    },
    {
      "epoch": 0.6778455513290869,
      "grad_norm": 0.9184363337079096,
      "learning_rate": 4.967204774989784e-06,
      "loss": 0.5299,
      "step": 7529
    },
    {
      "epoch": 0.6779355826149587,
      "grad_norm": 0.8718782964564202,
      "learning_rate": 4.964685055314218e-06,
      "loss": 0.5287,
      "step": 7530
    },
    {
      "epoch": 0.6780256139008305,
      "grad_norm": 0.873837829456901,
      "learning_rate": 4.962165763844957e-06,
      "loss": 0.5169,
      "step": 7531
    },
    {
      "epoch": 0.6781156451867024,
      "grad_norm": 1.0452070545316439,
      "learning_rate": 4.959646900796248e-06,
      "loss": 0.5393,
      "step": 7532
    },
    {
      "epoch": 0.6782056764725742,
      "grad_norm": 0.928254995376576,
      "learning_rate": 4.957128466382296e-06,
      "loss": 0.5279,
      "step": 7533
    },
    {
      "epoch": 0.678295707758446,
      "grad_norm": 0.9797951962213511,
      "learning_rate": 4.954610460817275e-06,
      "loss": 0.4324,
      "step": 7534
    },
    {
      "epoch": 0.6783857390443179,
      "grad_norm": 0.8604579240252165,
      "learning_rate": 4.952092884315306e-06,
      "loss": 0.4247,
      "step": 7535
    },
    {
      "epoch": 0.6784757703301897,
      "grad_norm": 0.7784548987152178,
      "learning_rate": 4.9495757370905e-06,
      "loss": 0.5419,
      "step": 7536
    },
    {
      "epoch": 0.6785658016160616,
      "grad_norm": 0.8390960540573086,
      "learning_rate": 4.947059019356904e-06,
      "loss": 0.4613,
      "step": 7537
    },
    {
      "epoch": 0.6786558329019334,
      "grad_norm": 0.95051175896935,
      "learning_rate": 4.944542731328549e-06,
      "loss": 0.5563,
      "step": 7538
    },
    {
      "epoch": 0.6787458641878052,
      "grad_norm": 0.9216443036058812,
      "learning_rate": 4.942026873219418e-06,
      "loss": 0.5535,
      "step": 7539
    },
    {
      "epoch": 0.6788358954736771,
      "grad_norm": 0.9691679185056541,
      "learning_rate": 4.9395114452434625e-06,
      "loss": 0.5566,
      "step": 7540
    },
    {
      "epoch": 0.678925926759549,
      "grad_norm": 1.110437721629126,
      "learning_rate": 4.936996447614595e-06,
      "loss": 0.5879,
      "step": 7541
    },
    {
      "epoch": 0.6790159580454208,
      "grad_norm": 1.0249049410496314,
      "learning_rate": 4.934481880546699e-06,
      "loss": 0.517,
      "step": 7542
    },
    {
      "epoch": 0.6791059893312926,
      "grad_norm": 0.8438929932061664,
      "learning_rate": 4.931967744253601e-06,
      "loss": 0.5031,
      "step": 7543
    },
    {
      "epoch": 0.6791960206171644,
      "grad_norm": 0.8808124780544523,
      "learning_rate": 4.929454038949119e-06,
      "loss": 0.5705,
      "step": 7544
    },
    {
      "epoch": 0.6792860519030363,
      "grad_norm": 0.8106683386391276,
      "learning_rate": 4.926940764847012e-06,
      "loss": 0.504,
      "step": 7545
    },
    {
      "epoch": 0.6793760831889082,
      "grad_norm": 1.2268674492236809,
      "learning_rate": 4.924427922161013e-06,
      "loss": 0.5342,
      "step": 7546
    },
    {
      "epoch": 0.67946611447478,
      "grad_norm": 0.9261332670003449,
      "learning_rate": 4.921915511104813e-06,
      "loss": 0.4592,
      "step": 7547
    },
    {
      "epoch": 0.6795561457606518,
      "grad_norm": 0.9349979877751083,
      "learning_rate": 4.9194035318920765e-06,
      "loss": 0.4848,
      "step": 7548
    },
    {
      "epoch": 0.6796461770465236,
      "grad_norm": 1.0063503866364552,
      "learning_rate": 4.9168919847364105e-06,
      "loss": 0.5196,
      "step": 7549
    },
    {
      "epoch": 0.6797362083323955,
      "grad_norm": 0.9699917262490133,
      "learning_rate": 4.914380869851414e-06,
      "loss": 0.5034,
      "step": 7550
    },
    {
      "epoch": 0.6798262396182674,
      "grad_norm": 0.7962560445615136,
      "learning_rate": 4.911870187450622e-06,
      "loss": 0.5953,
      "step": 7551
    },
    {
      "epoch": 0.6799162709041392,
      "grad_norm": 0.9460337236418459,
      "learning_rate": 4.9093599377475496e-06,
      "loss": 0.555,
      "step": 7552
    },
    {
      "epoch": 0.680006302190011,
      "grad_norm": 1.1078243470365547,
      "learning_rate": 4.906850120955668e-06,
      "loss": 0.5535,
      "step": 7553
    },
    {
      "epoch": 0.6800963334758828,
      "grad_norm": 0.9202841155749187,
      "learning_rate": 4.904340737288415e-06,
      "loss": 0.5164,
      "step": 7554
    },
    {
      "epoch": 0.6801863647617548,
      "grad_norm": 0.9199228185815386,
      "learning_rate": 4.901831786959191e-06,
      "loss": 0.4995,
      "step": 7555
    },
    {
      "epoch": 0.6802763960476266,
      "grad_norm": 1.0420463932605413,
      "learning_rate": 4.89932327018136e-06,
      "loss": 0.5425,
      "step": 7556
    },
    {
      "epoch": 0.6803664273334984,
      "grad_norm": 0.9814925461168745,
      "learning_rate": 4.896815187168238e-06,
      "loss": 0.5366,
      "step": 7557
    },
    {
      "epoch": 0.6804564586193702,
      "grad_norm": 0.8046699077572006,
      "learning_rate": 4.89430753813313e-06,
      "loss": 0.4966,
      "step": 7558
    },
    {
      "epoch": 0.680546489905242,
      "grad_norm": 0.906367686273909,
      "learning_rate": 4.891800323289274e-06,
      "loss": 0.5897,
      "step": 7559
    },
    {
      "epoch": 0.680636521191114,
      "grad_norm": 0.9637593401701297,
      "learning_rate": 4.889293542849891e-06,
      "loss": 0.4995,
      "step": 7560
    },
    {
      "epoch": 0.6807265524769858,
      "grad_norm": 0.9516948599094097,
      "learning_rate": 4.8867871970281575e-06,
      "loss": 0.5368,
      "step": 7561
    },
    {
      "epoch": 0.6808165837628576,
      "grad_norm": 1.3674683784615473,
      "learning_rate": 4.884281286037221e-06,
      "loss": 0.5433,
      "step": 7562
    },
    {
      "epoch": 0.6809066150487294,
      "grad_norm": 0.9188980271436027,
      "learning_rate": 4.881775810090172e-06,
      "loss": 0.4824,
      "step": 7563
    },
    {
      "epoch": 0.6809966463346012,
      "grad_norm": 0.9639144845425677,
      "learning_rate": 4.879270769400094e-06,
      "loss": 0.5016,
      "step": 7564
    },
    {
      "epoch": 0.6810866776204731,
      "grad_norm": 0.8214108089950786,
      "learning_rate": 4.8767661641800065e-06,
      "loss": 0.5048,
      "step": 7565
    },
    {
      "epoch": 0.681176708906345,
      "grad_norm": 0.812394751583106,
      "learning_rate": 4.874261994642905e-06,
      "loss": 0.558,
      "step": 7566
    },
    {
      "epoch": 0.6812667401922168,
      "grad_norm": 0.945522241086101,
      "learning_rate": 4.8717582610017455e-06,
      "loss": 0.5282,
      "step": 7567
    },
    {
      "epoch": 0.6813567714780886,
      "grad_norm": 0.8509432693255429,
      "learning_rate": 4.86925496346945e-06,
      "loss": 0.4991,
      "step": 7568
    },
    {
      "epoch": 0.6814468027639605,
      "grad_norm": 0.8719452386773241,
      "learning_rate": 4.866752102258896e-06,
      "loss": 0.5269,
      "step": 7569
    },
    {
      "epoch": 0.6815368340498323,
      "grad_norm": 0.8317616169682703,
      "learning_rate": 4.864249677582935e-06,
      "loss": 0.4903,
      "step": 7570
    },
    {
      "epoch": 0.6816268653357042,
      "grad_norm": 0.7999598302754183,
      "learning_rate": 4.8617476896543635e-06,
      "loss": 0.5585,
      "step": 7571
    },
    {
      "epoch": 0.681716896621576,
      "grad_norm": 0.92272407995318,
      "learning_rate": 4.8592461386859665e-06,
      "loss": 0.5182,
      "step": 7572
    },
    {
      "epoch": 0.6818069279074478,
      "grad_norm": 1.0106281080516384,
      "learning_rate": 4.856745024890466e-06,
      "loss": 0.4296,
      "step": 7573
    },
    {
      "epoch": 0.6818969591933197,
      "grad_norm": 0.8127636131342828,
      "learning_rate": 4.854244348480562e-06,
      "loss": 0.5254,
      "step": 7574
    },
    {
      "epoch": 0.6819869904791915,
      "grad_norm": 0.9096929421774685,
      "learning_rate": 4.851744109668914e-06,
      "loss": 0.4682,
      "step": 7575
    },
    {
      "epoch": 0.6820770217650634,
      "grad_norm": 1.013394717947097,
      "learning_rate": 4.8492443086681475e-06,
      "loss": 0.5413,
      "step": 7576
    },
    {
      "epoch": 0.6821670530509352,
      "grad_norm": 1.1347691230357868,
      "learning_rate": 4.846744945690835e-06,
      "loss": 0.5139,
      "step": 7577
    },
    {
      "epoch": 0.6822570843368071,
      "grad_norm": 0.8554639784511657,
      "learning_rate": 4.8442460209495415e-06,
      "loss": 0.6061,
      "step": 7578
    },
    {
      "epoch": 0.6823471156226789,
      "grad_norm": 1.0342347045007454,
      "learning_rate": 4.8417475346567635e-06,
      "loss": 0.463,
      "step": 7579
    },
    {
      "epoch": 0.6824371469085507,
      "grad_norm": 0.998584625320435,
      "learning_rate": 4.839249487024977e-06,
      "loss": 0.5365,
      "step": 7580
    },
    {
      "epoch": 0.6825271781944225,
      "grad_norm": 0.8608794696911826,
      "learning_rate": 4.836751878266619e-06,
      "loss": 0.5398,
      "step": 7581
    },
    {
      "epoch": 0.6826172094802944,
      "grad_norm": 1.092931563960082,
      "learning_rate": 4.834254708594089e-06,
      "loss": 0.4695,
      "step": 7582
    },
    {
      "epoch": 0.6827072407661663,
      "grad_norm": 0.8938896860501763,
      "learning_rate": 4.831757978219743e-06,
      "loss": 0.4795,
      "step": 7583
    },
    {
      "epoch": 0.6827972720520381,
      "grad_norm": 0.8903978270638395,
      "learning_rate": 4.829261687355914e-06,
      "loss": 0.5501,
      "step": 7584
    },
    {
      "epoch": 0.6828873033379099,
      "grad_norm": 0.8664840845530765,
      "learning_rate": 4.826765836214874e-06,
      "loss": 0.4615,
      "step": 7585
    },
    {
      "epoch": 0.6829773346237817,
      "grad_norm": 1.131289635261408,
      "learning_rate": 4.8242704250088865e-06,
      "loss": 0.5933,
      "step": 7586
    },
    {
      "epoch": 0.6830673659096536,
      "grad_norm": 1.379937594482391,
      "learning_rate": 4.821775453950152e-06,
      "loss": 0.5599,
      "step": 7587
    },
    {
      "epoch": 0.6831573971955255,
      "grad_norm": 0.7224046317432566,
      "learning_rate": 4.819280923250849e-06,
      "loss": 0.4572,
      "step": 7588
    },
    {
      "epoch": 0.6832474284813973,
      "grad_norm": 1.2693481217291844,
      "learning_rate": 4.816786833123113e-06,
      "loss": 0.6016,
      "step": 7589
    },
    {
      "epoch": 0.6833374597672691,
      "grad_norm": 1.0889297028923803,
      "learning_rate": 4.814293183779046e-06,
      "loss": 0.5855,
      "step": 7590
    },
    {
      "epoch": 0.6834274910531409,
      "grad_norm": 0.7883561304251171,
      "learning_rate": 4.811799975430701e-06,
      "loss": 0.4382,
      "step": 7591
    },
    {
      "epoch": 0.6835175223390129,
      "grad_norm": 0.8489783245617121,
      "learning_rate": 4.809307208290114e-06,
      "loss": 0.5459,
      "step": 7592
    },
    {
      "epoch": 0.6836075536248847,
      "grad_norm": 0.9737256884278216,
      "learning_rate": 4.806814882569261e-06,
      "loss": 0.6153,
      "step": 7593
    },
    {
      "epoch": 0.6836975849107565,
      "grad_norm": 0.909215271952967,
      "learning_rate": 4.804322998480095e-06,
      "loss": 0.5057,
      "step": 7594
    },
    {
      "epoch": 0.6837876161966283,
      "grad_norm": 0.9550155340496407,
      "learning_rate": 4.801831556234528e-06,
      "loss": 0.5539,
      "step": 7595
    },
    {
      "epoch": 0.6838776474825001,
      "grad_norm": 0.9015142356383008,
      "learning_rate": 4.799340556044434e-06,
      "loss": 0.4913,
      "step": 7596
    },
    {
      "epoch": 0.6839676787683721,
      "grad_norm": 1.0352916870720605,
      "learning_rate": 4.796849998121647e-06,
      "loss": 0.5409,
      "step": 7597
    },
    {
      "epoch": 0.6840577100542439,
      "grad_norm": 0.9739304466825752,
      "learning_rate": 4.794359882677971e-06,
      "loss": 0.4502,
      "step": 7598
    },
    {
      "epoch": 0.6841477413401157,
      "grad_norm": 0.9728678769146468,
      "learning_rate": 4.791870209925157e-06,
      "loss": 0.5062,
      "step": 7599
    },
    {
      "epoch": 0.6842377726259875,
      "grad_norm": 0.7655877242750097,
      "learning_rate": 4.78938098007494e-06,
      "loss": 0.4658,
      "step": 7600
    },
    {
      "epoch": 0.6843278039118593,
      "grad_norm": 1.37698289105821,
      "learning_rate": 4.786892193338997e-06,
      "loss": 0.5077,
      "step": 7601
    },
    {
      "epoch": 0.6844178351977313,
      "grad_norm": 0.9607468568182542,
      "learning_rate": 4.784403849928979e-06,
      "loss": 0.4845,
      "step": 7602
    },
    {
      "epoch": 0.6845078664836031,
      "grad_norm": 1.0373519900171135,
      "learning_rate": 4.781915950056496e-06,
      "loss": 0.423,
      "step": 7603
    },
    {
      "epoch": 0.6845978977694749,
      "grad_norm": 0.9440102823626626,
      "learning_rate": 4.779428493933124e-06,
      "loss": 0.6119,
      "step": 7604
    },
    {
      "epoch": 0.6846879290553467,
      "grad_norm": 0.9397396728076404,
      "learning_rate": 4.7769414817703874e-06,
      "loss": 0.5129,
      "step": 7605
    },
    {
      "epoch": 0.6847779603412186,
      "grad_norm": 0.947108594792726,
      "learning_rate": 4.774454913779798e-06,
      "loss": 0.4306,
      "step": 7606
    },
    {
      "epoch": 0.6848679916270904,
      "grad_norm": 1.0248879281275567,
      "learning_rate": 4.771968790172804e-06,
      "loss": 0.4346,
      "step": 7607
    },
    {
      "epoch": 0.6849580229129623,
      "grad_norm": 1.0826730868466423,
      "learning_rate": 4.76948311116083e-06,
      "loss": 0.5019,
      "step": 7608
    },
    {
      "epoch": 0.6850480541988341,
      "grad_norm": 0.8746442637445221,
      "learning_rate": 4.766997876955261e-06,
      "loss": 0.5926,
      "step": 7609
    },
    {
      "epoch": 0.6851380854847059,
      "grad_norm": 0.8917265629818143,
      "learning_rate": 4.76451308776744e-06,
      "loss": 0.4951,
      "step": 7610
    },
    {
      "epoch": 0.6852281167705778,
      "grad_norm": 0.8130397546750183,
      "learning_rate": 4.7620287438086785e-06,
      "loss": 0.473,
      "step": 7611
    },
    {
      "epoch": 0.6853181480564496,
      "grad_norm": 1.0589768821452725,
      "learning_rate": 4.759544845290248e-06,
      "loss": 0.5738,
      "step": 7612
    },
    {
      "epoch": 0.6854081793423215,
      "grad_norm": 0.7906895759441489,
      "learning_rate": 4.757061392423371e-06,
      "loss": 0.5615,
      "step": 7613
    },
    {
      "epoch": 0.6854982106281933,
      "grad_norm": 0.8180903789583878,
      "learning_rate": 4.754578385419256e-06,
      "loss": 0.4615,
      "step": 7614
    },
    {
      "epoch": 0.6855882419140651,
      "grad_norm": 0.8624264310794327,
      "learning_rate": 4.752095824489049e-06,
      "loss": 0.5499,
      "step": 7615
    },
    {
      "epoch": 0.685678273199937,
      "grad_norm": 0.7483257190947725,
      "learning_rate": 4.749613709843871e-06,
      "loss": 0.4165,
      "step": 7616
    },
    {
      "epoch": 0.6857683044858088,
      "grad_norm": 0.9369637188536775,
      "learning_rate": 4.747132041694803e-06,
      "loss": 0.5498,
      "step": 7617
    },
    {
      "epoch": 0.6858583357716806,
      "grad_norm": 0.9975674706743044,
      "learning_rate": 4.744650820252893e-06,
      "loss": 0.4777,
      "step": 7618
    },
    {
      "epoch": 0.6859483670575525,
      "grad_norm": 1.147869835231439,
      "learning_rate": 4.742170045729132e-06,
      "loss": 0.546,
      "step": 7619
    },
    {
      "epoch": 0.6860383983434244,
      "grad_norm": 0.9213215599460566,
      "learning_rate": 4.7396897183345025e-06,
      "loss": 0.546,
      "step": 7620
    },
    {
      "epoch": 0.6861284296292962,
      "grad_norm": 0.8594798724184641,
      "learning_rate": 4.737209838279923e-06,
      "loss": 0.526,
      "step": 7621
    },
    {
      "epoch": 0.686218460915168,
      "grad_norm": 0.927089536137192,
      "learning_rate": 4.734730405776285e-06,
      "loss": 0.578,
      "step": 7622
    },
    {
      "epoch": 0.6863084922010398,
      "grad_norm": 1.0413680910198992,
      "learning_rate": 4.732251421034444e-06,
      "loss": 0.4832,
      "step": 7623
    },
    {
      "epoch": 0.6863985234869117,
      "grad_norm": 1.002684853083661,
      "learning_rate": 4.729772884265212e-06,
      "loss": 0.57,
      "step": 7624
    },
    {
      "epoch": 0.6864885547727836,
      "grad_norm": 1.0374985780394486,
      "learning_rate": 4.727294795679368e-06,
      "loss": 0.5639,
      "step": 7625
    },
    {
      "epoch": 0.6865785860586554,
      "grad_norm": 0.900358018365848,
      "learning_rate": 4.724817155487652e-06,
      "loss": 0.5356,
      "step": 7626
    },
    {
      "epoch": 0.6866686173445272,
      "grad_norm": 0.9622660459009598,
      "learning_rate": 4.7223399639007525e-06,
      "loss": 0.5689,
      "step": 7627
    },
    {
      "epoch": 0.686758648630399,
      "grad_norm": 1.228073669745015,
      "learning_rate": 4.719863221129347e-06,
      "loss": 0.5073,
      "step": 7628
    },
    {
      "epoch": 0.6868486799162709,
      "grad_norm": 0.9110215806003251,
      "learning_rate": 4.717386927384049e-06,
      "loss": 0.4947,
      "step": 7629
    },
    {
      "epoch": 0.6869387112021428,
      "grad_norm": 0.9071743400139003,
      "learning_rate": 4.714911082875446e-06,
      "loss": 0.5184,
      "step": 7630
    },
    {
      "epoch": 0.6870287424880146,
      "grad_norm": 1.4357447903994769,
      "learning_rate": 4.712435687814086e-06,
      "loss": 0.435,
      "step": 7631
    },
    {
      "epoch": 0.6871187737738864,
      "grad_norm": 0.8344434345350955,
      "learning_rate": 4.709960742410483e-06,
      "loss": 0.5465,
      "step": 7632
    },
    {
      "epoch": 0.6872088050597582,
      "grad_norm": 1.022295759601743,
      "learning_rate": 4.707486246875096e-06,
      "loss": 0.5743,
      "step": 7633
    },
    {
      "epoch": 0.6872988363456302,
      "grad_norm": 1.0200698603387395,
      "learning_rate": 4.705012201418372e-06,
      "loss": 0.4413,
      "step": 7634
    },
    {
      "epoch": 0.687388867631502,
      "grad_norm": 1.0060712726548253,
      "learning_rate": 4.7025386062506954e-06,
      "loss": 0.4998,
      "step": 7635
    },
    {
      "epoch": 0.6874788989173738,
      "grad_norm": 1.003231724482764,
      "learning_rate": 4.700065461582423e-06,
      "loss": 0.5793,
      "step": 7636
    },
    {
      "epoch": 0.6875689302032456,
      "grad_norm": 1.1901397707584673,
      "learning_rate": 4.6975927676238765e-06,
      "loss": 0.5102,
      "step": 7637
    },
    {
      "epoch": 0.6876589614891174,
      "grad_norm": 1.2195382621864839,
      "learning_rate": 4.6951205245853324e-06,
      "loss": 0.5106,
      "step": 7638
    },
    {
      "epoch": 0.6877489927749894,
      "grad_norm": 0.8110042868081422,
      "learning_rate": 4.692648732677033e-06,
      "loss": 0.4325,
      "step": 7639
    },
    {
      "epoch": 0.6878390240608612,
      "grad_norm": 1.1858165308385922,
      "learning_rate": 4.690177392109186e-06,
      "loss": 0.5182,
      "step": 7640
    },
    {
      "epoch": 0.687929055346733,
      "grad_norm": 0.9632792958781087,
      "learning_rate": 4.6877065030919425e-06,
      "loss": 0.5258,
      "step": 7641
    },
    {
      "epoch": 0.6880190866326048,
      "grad_norm": 0.9247882995234773,
      "learning_rate": 4.685236065835443e-06,
      "loss": 0.524,
      "step": 7642
    },
    {
      "epoch": 0.6881091179184766,
      "grad_norm": 0.8366524412954217,
      "learning_rate": 4.682766080549767e-06,
      "loss": 0.4476,
      "step": 7643
    },
    {
      "epoch": 0.6881991492043485,
      "grad_norm": 0.9549282141201131,
      "learning_rate": 4.6802965474449635e-06,
      "loss": 0.52,
      "step": 7644
    },
    {
      "epoch": 0.6882891804902204,
      "grad_norm": 0.7963612707506252,
      "learning_rate": 4.677827466731045e-06,
      "loss": 0.4382,
      "step": 7645
    },
    {
      "epoch": 0.6883792117760922,
      "grad_norm": 0.8645642075730332,
      "learning_rate": 4.675358838617987e-06,
      "loss": 0.5114,
      "step": 7646
    },
    {
      "epoch": 0.688469243061964,
      "grad_norm": 0.9281941838408901,
      "learning_rate": 4.672890663315713e-06,
      "loss": 0.4349,
      "step": 7647
    },
    {
      "epoch": 0.6885592743478359,
      "grad_norm": 1.2325722474476375,
      "learning_rate": 4.6704229410341315e-06,
      "loss": 0.6072,
      "step": 7648
    },
    {
      "epoch": 0.6886493056337077,
      "grad_norm": 0.905925267210905,
      "learning_rate": 4.66795567198309e-06,
      "loss": 0.5952,
      "step": 7649
    },
    {
      "epoch": 0.6887393369195796,
      "grad_norm": 0.9698048877633271,
      "learning_rate": 4.665488856372408e-06,
      "loss": 0.5458,
      "step": 7650
    },
    {
      "epoch": 0.6888293682054514,
      "grad_norm": 0.9051514378288484,
      "learning_rate": 4.663022494411866e-06,
      "loss": 0.6102,
      "step": 7651
    },
    {
      "epoch": 0.6889193994913232,
      "grad_norm": 1.0854937785479937,
      "learning_rate": 4.660556586311206e-06,
      "loss": 0.6003,
      "step": 7652
    },
    {
      "epoch": 0.6890094307771951,
      "grad_norm": 1.0016924902699014,
      "learning_rate": 4.658091132280128e-06,
      "loss": 0.4893,
      "step": 7653
    },
    {
      "epoch": 0.6890994620630669,
      "grad_norm": 1.1643463469650073,
      "learning_rate": 4.655626132528302e-06,
      "loss": 0.5038,
      "step": 7654
    },
    {
      "epoch": 0.6891894933489388,
      "grad_norm": 0.8947316118289587,
      "learning_rate": 4.65316158726534e-06,
      "loss": 0.5392,
      "step": 7655
    },
    {
      "epoch": 0.6892795246348106,
      "grad_norm": 0.7825736968923034,
      "learning_rate": 4.650697496700845e-06,
      "loss": 0.5005,
      "step": 7656
    },
    {
      "epoch": 0.6893695559206824,
      "grad_norm": 1.0612453849376198,
      "learning_rate": 4.648233861044352e-06,
      "loss": 0.525,
      "step": 7657
    },
    {
      "epoch": 0.6894595872065543,
      "grad_norm": 0.8566224679308777,
      "learning_rate": 4.645770680505375e-06,
      "loss": 0.4587,
      "step": 7658
    },
    {
      "epoch": 0.6895496184924261,
      "grad_norm": 1.038994305048786,
      "learning_rate": 4.643307955293384e-06,
      "loss": 0.4809,
      "step": 7659
    },
    {
      "epoch": 0.689639649778298,
      "grad_norm": 0.9355281690960319,
      "learning_rate": 4.640845685617814e-06,
      "loss": 0.5471,
      "step": 7660
    },
    {
      "epoch": 0.6897296810641698,
      "grad_norm": 1.1098323461786859,
      "learning_rate": 4.638383871688048e-06,
      "loss": 0.4601,
      "step": 7661
    },
    {
      "epoch": 0.6898197123500417,
      "grad_norm": 1.0077729751438642,
      "learning_rate": 4.635922513713453e-06,
      "loss": 0.4496,
      "step": 7662
    },
    {
      "epoch": 0.6899097436359135,
      "grad_norm": 0.9045792048580598,
      "learning_rate": 4.633461611903336e-06,
      "loss": 0.5078,
      "step": 7663
    },
    {
      "epoch": 0.6899997749217853,
      "grad_norm": 1.0062555950222398,
      "learning_rate": 4.631001166466975e-06,
      "loss": 0.5308,
      "step": 7664
    },
    {
      "epoch": 0.6900898062076571,
      "grad_norm": 1.1666459664556785,
      "learning_rate": 4.628541177613608e-06,
      "loss": 0.6148,
      "step": 7665
    },
    {
      "epoch": 0.690179837493529,
      "grad_norm": 0.8375162563787092,
      "learning_rate": 4.626081645552436e-06,
      "loss": 0.5479,
      "step": 7666
    },
    {
      "epoch": 0.6902698687794009,
      "grad_norm": 1.0217486200454018,
      "learning_rate": 4.623622570492616e-06,
      "loss": 0.5585,
      "step": 7667
    },
    {
      "epoch": 0.6903599000652727,
      "grad_norm": 1.0553049190432402,
      "learning_rate": 4.621163952643276e-06,
      "loss": 0.558,
      "step": 7668
    },
    {
      "epoch": 0.6904499313511445,
      "grad_norm": 0.7974127191695911,
      "learning_rate": 4.6187057922134855e-06,
      "loss": 0.4261,
      "step": 7669
    },
    {
      "epoch": 0.6905399626370163,
      "grad_norm": 1.0805159706418324,
      "learning_rate": 4.616248089412304e-06,
      "loss": 0.5089,
      "step": 7670
    },
    {
      "epoch": 0.6906299939228882,
      "grad_norm": 0.6762954969555472,
      "learning_rate": 4.613790844448724e-06,
      "loss": 0.4675,
      "step": 7671
    },
    {
      "epoch": 0.6907200252087601,
      "grad_norm": 1.0312038238588852,
      "learning_rate": 4.611334057531714e-06,
      "loss": 0.4932,
      "step": 7672
    },
    {
      "epoch": 0.6908100564946319,
      "grad_norm": 1.0415414245464303,
      "learning_rate": 4.608877728870203e-06,
      "loss": 0.5035,
      "step": 7673
    },
    {
      "epoch": 0.6909000877805037,
      "grad_norm": 0.8268094421867513,
      "learning_rate": 4.606421858673081e-06,
      "loss": 0.5111,
      "step": 7674
    },
    {
      "epoch": 0.6909901190663755,
      "grad_norm": 0.8915642940475226,
      "learning_rate": 4.6039664471491865e-06,
      "loss": 0.5258,
      "step": 7675
    },
    {
      "epoch": 0.6910801503522475,
      "grad_norm": 0.7974950126295302,
      "learning_rate": 4.6015114945073425e-06,
      "loss": 0.5493,
      "step": 7676
    },
    {
      "epoch": 0.6911701816381193,
      "grad_norm": 1.2256929040532336,
      "learning_rate": 4.59905700095631e-06,
      "loss": 0.6075,
      "step": 7677
    },
    {
      "epoch": 0.6912602129239911,
      "grad_norm": 1.0146652467167592,
      "learning_rate": 4.596602966704823e-06,
      "loss": 0.6113,
      "step": 7678
    },
    {
      "epoch": 0.6913502442098629,
      "grad_norm": 1.0532192057654506,
      "learning_rate": 4.594149391961575e-06,
      "loss": 0.5208,
      "step": 7679
    },
    {
      "epoch": 0.6914402754957347,
      "grad_norm": 0.8552079707203353,
      "learning_rate": 4.59169627693522e-06,
      "loss": 0.4754,
      "step": 7680
    },
    {
      "epoch": 0.6915303067816067,
      "grad_norm": 0.9496878414250132,
      "learning_rate": 4.589243621834371e-06,
      "loss": 0.4713,
      "step": 7681
    },
    {
      "epoch": 0.6916203380674785,
      "grad_norm": 0.9316553447162091,
      "learning_rate": 4.58679142686761e-06,
      "loss": 0.5127,
      "step": 7682
    },
    {
      "epoch": 0.6917103693533503,
      "grad_norm": 1.0553273520778188,
      "learning_rate": 4.584339692243458e-06,
      "loss": 0.5011,
      "step": 7683
    },
    {
      "epoch": 0.6918004006392221,
      "grad_norm": 0.9474263481783698,
      "learning_rate": 4.581888418170429e-06,
      "loss": 0.4953,
      "step": 7684
    },
    {
      "epoch": 0.6918904319250939,
      "grad_norm": 1.019548203954919,
      "learning_rate": 4.57943760485697e-06,
      "loss": 0.5083,
      "step": 7685
    },
    {
      "epoch": 0.6919804632109658,
      "grad_norm": 0.7777069003999665,
      "learning_rate": 4.576987252511502e-06,
      "loss": 0.5058,
      "step": 7686
    },
    {
      "epoch": 0.6920704944968377,
      "grad_norm": 0.9177149009257667,
      "learning_rate": 4.5745373613424075e-06,
      "loss": 0.6127,
      "step": 7687
    },
    {
      "epoch": 0.6921605257827095,
      "grad_norm": 0.6939010116809154,
      "learning_rate": 4.5720879315580255e-06,
      "loss": 0.4554,
      "step": 7688
    },
    {
      "epoch": 0.6922505570685813,
      "grad_norm": 0.9251241001365177,
      "learning_rate": 4.56963896336665e-06,
      "loss": 0.6301,
      "step": 7689
    },
    {
      "epoch": 0.6923405883544532,
      "grad_norm": 1.0181065604926818,
      "learning_rate": 4.567190456976558e-06,
      "loss": 0.5284,
      "step": 7690
    },
    {
      "epoch": 0.692430619640325,
      "grad_norm": 0.8723028639149131,
      "learning_rate": 4.564742412595957e-06,
      "loss": 0.5388,
      "step": 7691
    },
    {
      "epoch": 0.6925206509261969,
      "grad_norm": 0.9155457284670906,
      "learning_rate": 4.562294830433038e-06,
      "loss": 0.5632,
      "step": 7692
    },
    {
      "epoch": 0.6926106822120687,
      "grad_norm": 0.9383906231576168,
      "learning_rate": 4.559847710695943e-06,
      "loss": 0.4746,
      "step": 7693
    },
    {
      "epoch": 0.6927007134979405,
      "grad_norm": 0.9327895902173043,
      "learning_rate": 4.557401053592775e-06,
      "loss": 0.4752,
      "step": 7694
    },
    {
      "epoch": 0.6927907447838124,
      "grad_norm": 0.9251812400018418,
      "learning_rate": 4.554954859331602e-06,
      "loss": 0.4924,
      "step": 7695
    },
    {
      "epoch": 0.6928807760696842,
      "grad_norm": 0.8862738331715173,
      "learning_rate": 4.5525091281204524e-06,
      "loss": 0.5288,
      "step": 7696
    },
    {
      "epoch": 0.692970807355556,
      "grad_norm": 0.8486833826621468,
      "learning_rate": 4.550063860167302e-06,
      "loss": 0.5261,
      "step": 7697
    },
    {
      "epoch": 0.6930608386414279,
      "grad_norm": 0.9311661017881284,
      "learning_rate": 4.547619055680113e-06,
      "loss": 0.5639,
      "step": 7698
    },
    {
      "epoch": 0.6931508699272997,
      "grad_norm": 0.8086117413322952,
      "learning_rate": 4.54517471486678e-06,
      "loss": 0.5454,
      "step": 7699
    },
    {
      "epoch": 0.6932409012131716,
      "grad_norm": 0.7692255738479282,
      "learning_rate": 4.542730837935177e-06,
      "loss": 0.4502,
      "step": 7700
    },
    {
      "epoch": 0.6933309324990434,
      "grad_norm": 0.9338392955015249,
      "learning_rate": 4.540287425093133e-06,
      "loss": 0.473,
      "step": 7701
    },
    {
      "epoch": 0.6934209637849152,
      "grad_norm": 0.9644714286127462,
      "learning_rate": 4.5378444765484386e-06,
      "loss": 0.4861,
      "step": 7702
    },
    {
      "epoch": 0.6935109950707871,
      "grad_norm": 0.8188849047604303,
      "learning_rate": 4.535401992508836e-06,
      "loss": 0.4948,
      "step": 7703
    },
    {
      "epoch": 0.693601026356659,
      "grad_norm": 1.0471744267885683,
      "learning_rate": 4.532959973182047e-06,
      "loss": 0.505,
      "step": 7704
    },
    {
      "epoch": 0.6936910576425308,
      "grad_norm": 0.8607118257593139,
      "learning_rate": 4.530518418775734e-06,
      "loss": 0.5029,
      "step": 7705
    },
    {
      "epoch": 0.6937810889284026,
      "grad_norm": 1.0582498649158891,
      "learning_rate": 4.52807732949753e-06,
      "loss": 0.3888,
      "step": 7706
    },
    {
      "epoch": 0.6938711202142744,
      "grad_norm": 0.9839765562858832,
      "learning_rate": 4.525636705555028e-06,
      "loss": 0.6008,
      "step": 7707
    },
    {
      "epoch": 0.6939611515001463,
      "grad_norm": 1.0358448795306066,
      "learning_rate": 4.52319654715578e-06,
      "loss": 0.6217,
      "step": 7708
    },
    {
      "epoch": 0.6940511827860182,
      "grad_norm": 1.1118190751056276,
      "learning_rate": 4.520756854507298e-06,
      "loss": 0.4988,
      "step": 7709
    },
    {
      "epoch": 0.69414121407189,
      "grad_norm": 0.7497051792738678,
      "learning_rate": 4.518317627817059e-06,
      "loss": 0.5331,
      "step": 7710
    },
    {
      "epoch": 0.6942312453577618,
      "grad_norm": 0.8881015600850678,
      "learning_rate": 4.5158788672924855e-06,
      "loss": 0.5094,
      "step": 7711
    },
    {
      "epoch": 0.6943212766436336,
      "grad_norm": 0.8356821730249779,
      "learning_rate": 4.513440573140987e-06,
      "loss": 0.5861,
      "step": 7712
    },
    {
      "epoch": 0.6944113079295055,
      "grad_norm": 1.0635774462115635,
      "learning_rate": 4.511002745569905e-06,
      "loss": 0.5749,
      "step": 7713
    },
    {
      "epoch": 0.6945013392153774,
      "grad_norm": 0.9067806349779656,
      "learning_rate": 4.508565384786557e-06,
      "loss": 0.4765,
      "step": 7714
    },
    {
      "epoch": 0.6945913705012492,
      "grad_norm": 1.0742504503980466,
      "learning_rate": 4.506128490998219e-06,
      "loss": 0.4862,
      "step": 7715
    },
    {
      "epoch": 0.694681401787121,
      "grad_norm": 0.7667058434807995,
      "learning_rate": 4.503692064412129e-06,
      "loss": 0.4327,
      "step": 7716
    },
    {
      "epoch": 0.6947714330729928,
      "grad_norm": 1.0099273616731534,
      "learning_rate": 4.501256105235471e-06,
      "loss": 0.4459,
      "step": 7717
    },
    {
      "epoch": 0.6948614643588648,
      "grad_norm": 0.9087440459557734,
      "learning_rate": 4.498820613675416e-06,
      "loss": 0.5244,
      "step": 7718
    },
    {
      "epoch": 0.6949514956447366,
      "grad_norm": 1.0492877527494475,
      "learning_rate": 4.496385589939067e-06,
      "loss": 0.5972,
      "step": 7719
    },
    {
      "epoch": 0.6950415269306084,
      "grad_norm": 0.9532478587818634,
      "learning_rate": 4.493951034233507e-06,
      "loss": 0.5001,
      "step": 7720
    },
    {
      "epoch": 0.6951315582164802,
      "grad_norm": 1.0488008231644492,
      "learning_rate": 4.491516946765768e-06,
      "loss": 0.5792,
      "step": 7721
    },
    {
      "epoch": 0.695221589502352,
      "grad_norm": 0.9810893801765016,
      "learning_rate": 4.489083327742849e-06,
      "loss": 0.4347,
      "step": 7722
    },
    {
      "epoch": 0.695311620788224,
      "grad_norm": 1.0180702899068632,
      "learning_rate": 4.486650177371707e-06,
      "loss": 0.4588,
      "step": 7723
    },
    {
      "epoch": 0.6954016520740958,
      "grad_norm": 0.8531033625047308,
      "learning_rate": 4.48421749585926e-06,
      "loss": 0.5722,
      "step": 7724
    },
    {
      "epoch": 0.6954916833599676,
      "grad_norm": 0.6920496070288218,
      "learning_rate": 4.481785283412375e-06,
      "loss": 0.5002,
      "step": 7725
    },
    {
      "epoch": 0.6955817146458394,
      "grad_norm": 0.8996896206294971,
      "learning_rate": 4.479353540237903e-06,
      "loss": 0.5227,
      "step": 7726
    },
    {
      "epoch": 0.6956717459317112,
      "grad_norm": 0.9634674005670424,
      "learning_rate": 4.47692226654263e-06,
      "loss": 0.5809,
      "step": 7727
    },
    {
      "epoch": 0.6957617772175831,
      "grad_norm": 0.9798891532832557,
      "learning_rate": 4.474491462533318e-06,
      "loss": 0.4125,
      "step": 7728
    },
    {
      "epoch": 0.695851808503455,
      "grad_norm": 0.9327074075529007,
      "learning_rate": 4.472061128416683e-06,
      "loss": 0.4446,
      "step": 7729
    },
    {
      "epoch": 0.6959418397893268,
      "grad_norm": 1.0079934760269553,
      "learning_rate": 4.469631264399404e-06,
      "loss": 0.5041,
      "step": 7730
    },
    {
      "epoch": 0.6960318710751986,
      "grad_norm": 0.9489561658441676,
      "learning_rate": 4.4672018706881105e-06,
      "loss": 0.6062,
      "step": 7731
    },
    {
      "epoch": 0.6961219023610705,
      "grad_norm": 0.8860870451590405,
      "learning_rate": 4.464772947489413e-06,
      "loss": 0.5826,
      "step": 7732
    },
    {
      "epoch": 0.6962119336469423,
      "grad_norm": 1.1264802118515131,
      "learning_rate": 4.4623444950098575e-06,
      "loss": 0.5052,
      "step": 7733
    },
    {
      "epoch": 0.6963019649328142,
      "grad_norm": 0.895726549781786,
      "learning_rate": 4.459916513455964e-06,
      "loss": 0.5815,
      "step": 7734
    },
    {
      "epoch": 0.696391996218686,
      "grad_norm": 0.9400836738527047,
      "learning_rate": 4.45748900303421e-06,
      "loss": 0.4541,
      "step": 7735
    },
    {
      "epoch": 0.6964820275045578,
      "grad_norm": 0.9432482847294453,
      "learning_rate": 4.455061963951034e-06,
      "loss": 0.65,
      "step": 7736
    },
    {
      "epoch": 0.6965720587904297,
      "grad_norm": 1.0190657206056588,
      "learning_rate": 4.45263539641283e-06,
      "loss": 0.5629,
      "step": 7737
    },
    {
      "epoch": 0.6966620900763015,
      "grad_norm": 0.8907973528239506,
      "learning_rate": 4.450209300625962e-06,
      "loss": 0.5061,
      "step": 7738
    },
    {
      "epoch": 0.6967521213621733,
      "grad_norm": 0.7981422882926332,
      "learning_rate": 4.4477836767967316e-06,
      "loss": 0.5229,
      "step": 7739
    },
    {
      "epoch": 0.6968421526480452,
      "grad_norm": 1.1105815351193187,
      "learning_rate": 4.445358525131435e-06,
      "loss": 0.521,
      "step": 7740
    },
    {
      "epoch": 0.696932183933917,
      "grad_norm": 0.952723168905304,
      "learning_rate": 4.442933845836293e-06,
      "loss": 0.5417,
      "step": 7741
    },
    {
      "epoch": 0.6970222152197889,
      "grad_norm": 0.883128940653589,
      "learning_rate": 4.440509639117507e-06,
      "loss": 0.5284,
      "step": 7742
    },
    {
      "epoch": 0.6971122465056607,
      "grad_norm": 0.8830226096391037,
      "learning_rate": 4.438085905181235e-06,
      "loss": 0.4357,
      "step": 7743
    },
    {
      "epoch": 0.6972022777915325,
      "grad_norm": 0.8235094257230386,
      "learning_rate": 4.435662644233594e-06,
      "loss": 0.5428,
      "step": 7744
    },
    {
      "epoch": 0.6972923090774044,
      "grad_norm": 1.3374884195968735,
      "learning_rate": 4.43323985648065e-06,
      "loss": 0.5392,
      "step": 7745
    },
    {
      "epoch": 0.6973823403632763,
      "grad_norm": 0.7440688190974545,
      "learning_rate": 4.430817542128452e-06,
      "loss": 0.5184,
      "step": 7746
    },
    {
      "epoch": 0.6974723716491481,
      "grad_norm": 0.9675906762214115,
      "learning_rate": 4.4283957013829845e-06,
      "loss": 0.5117,
      "step": 7747
    },
    {
      "epoch": 0.6975624029350199,
      "grad_norm": 1.051822044200796,
      "learning_rate": 4.425974334450207e-06,
      "loss": 0.4438,
      "step": 7748
    },
    {
      "epoch": 0.6976524342208917,
      "grad_norm": 0.9423852136762323,
      "learning_rate": 4.423553441536032e-06,
      "loss": 0.5793,
      "step": 7749
    },
    {
      "epoch": 0.6977424655067636,
      "grad_norm": 0.9258067689871281,
      "learning_rate": 4.421133022846336e-06,
      "loss": 0.4933,
      "step": 7750
    },
    {
      "epoch": 0.6978324967926355,
      "grad_norm": 1.0301410730558196,
      "learning_rate": 4.418713078586952e-06,
      "loss": 0.5923,
      "step": 7751
    },
    {
      "epoch": 0.6979225280785073,
      "grad_norm": 0.8043771006115482,
      "learning_rate": 4.416293608963678e-06,
      "loss": 0.4629,
      "step": 7752
    },
    {
      "epoch": 0.6980125593643791,
      "grad_norm": 0.9013816358762085,
      "learning_rate": 4.413874614182254e-06,
      "loss": 0.5171,
      "step": 7753
    },
    {
      "epoch": 0.6981025906502509,
      "grad_norm": 1.058944955274258,
      "learning_rate": 4.4114560944484084e-06,
      "loss": 0.5601,
      "step": 7754
    },
    {
      "epoch": 0.6981926219361227,
      "grad_norm": 1.0627801319372956,
      "learning_rate": 4.4090380499678045e-06,
      "loss": 0.5789,
      "step": 7755
    },
    {
      "epoch": 0.6982826532219947,
      "grad_norm": 0.8871501778161875,
      "learning_rate": 4.406620480946074e-06,
      "loss": 0.5702,
      "step": 7756
    },
    {
      "epoch": 0.6983726845078665,
      "grad_norm": 1.1034662985617003,
      "learning_rate": 4.404203387588814e-06,
      "loss": 0.5708,
      "step": 7757
    },
    {
      "epoch": 0.6984627157937383,
      "grad_norm": 1.9664605874385506,
      "learning_rate": 4.4017867701015745e-06,
      "loss": 0.6187,
      "step": 7758
    },
    {
      "epoch": 0.6985527470796101,
      "grad_norm": 0.9497009936556948,
      "learning_rate": 4.399370628689857e-06,
      "loss": 0.4529,
      "step": 7759
    },
    {
      "epoch": 0.698642778365482,
      "grad_norm": 0.9032015217017646,
      "learning_rate": 4.3969549635591466e-06,
      "loss": 0.5277,
      "step": 7760
    },
    {
      "epoch": 0.6987328096513539,
      "grad_norm": 0.8318991760263267,
      "learning_rate": 4.394539774914861e-06,
      "loss": 0.5516,
      "step": 7761
    },
    {
      "epoch": 0.6988228409372257,
      "grad_norm": 0.8740518534858733,
      "learning_rate": 4.392125062962395e-06,
      "loss": 0.5018,
      "step": 7762
    },
    {
      "epoch": 0.6989128722230975,
      "grad_norm": 0.9745231217542566,
      "learning_rate": 4.3897108279070944e-06,
      "loss": 0.5262,
      "step": 7763
    },
    {
      "epoch": 0.6990029035089693,
      "grad_norm": 1.7469649158482017,
      "learning_rate": 4.38729706995427e-06,
      "loss": 0.5148,
      "step": 7764
    },
    {
      "epoch": 0.6990929347948412,
      "grad_norm": 0.8590396251079564,
      "learning_rate": 4.384883789309187e-06,
      "loss": 0.5225,
      "step": 7765
    },
    {
      "epoch": 0.6991829660807131,
      "grad_norm": 0.8750367346075677,
      "learning_rate": 4.382470986177077e-06,
      "loss": 0.5148,
      "step": 7766
    },
    {
      "epoch": 0.6992729973665849,
      "grad_norm": 0.8689515068139304,
      "learning_rate": 4.380058660763114e-06,
      "loss": 0.531,
      "step": 7767
    },
    {
      "epoch": 0.6993630286524567,
      "grad_norm": 0.9541325996116273,
      "learning_rate": 4.3776468132724605e-06,
      "loss": 0.5067,
      "step": 7768
    },
    {
      "epoch": 0.6994530599383285,
      "grad_norm": 0.9672346653600958,
      "learning_rate": 4.37523544391021e-06,
      "loss": 0.5232,
      "step": 7769
    },
    {
      "epoch": 0.6995430912242004,
      "grad_norm": 1.242053060833223,
      "learning_rate": 4.372824552881429e-06,
      "loss": 0.4647,
      "step": 7770
    },
    {
      "epoch": 0.6996331225100723,
      "grad_norm": 0.8576683869805525,
      "learning_rate": 4.3704141403911435e-06,
      "loss": 0.5474,
      "step": 7771
    },
    {
      "epoch": 0.6997231537959441,
      "grad_norm": 0.9203820001661746,
      "learning_rate": 4.368004206644338e-06,
      "loss": 0.5541,
      "step": 7772
    },
    {
      "epoch": 0.6998131850818159,
      "grad_norm": 0.9908338203798669,
      "learning_rate": 4.365594751845944e-06,
      "loss": 0.5365,
      "step": 7773
    },
    {
      "epoch": 0.6999032163676878,
      "grad_norm": 1.0342880447420266,
      "learning_rate": 4.363185776200879e-06,
      "loss": 0.5189,
      "step": 7774
    },
    {
      "epoch": 0.6999932476535596,
      "grad_norm": 0.9279122543306976,
      "learning_rate": 4.360777279913993e-06,
      "loss": 0.482,
      "step": 7775
    },
    {
      "epoch": 0.7000832789394315,
      "grad_norm": 0.7943626248943579,
      "learning_rate": 4.358369263190108e-06,
      "loss": 0.5006,
      "step": 7776
    },
    {
      "epoch": 0.7001733102253033,
      "grad_norm": 0.9899842798163316,
      "learning_rate": 4.355961726234003e-06,
      "loss": 0.5947,
      "step": 7777
    },
    {
      "epoch": 0.7002633415111751,
      "grad_norm": 0.8645570716174074,
      "learning_rate": 4.353554669250418e-06,
      "loss": 0.4839,
      "step": 7778
    },
    {
      "epoch": 0.700353372797047,
      "grad_norm": 1.1253870067082907,
      "learning_rate": 4.3511480924440495e-06,
      "loss": 0.4743,
      "step": 7779
    },
    {
      "epoch": 0.7004434040829188,
      "grad_norm": 0.8662814716602993,
      "learning_rate": 4.3487419960195595e-06,
      "loss": 0.5482,
      "step": 7780
    },
    {
      "epoch": 0.7005334353687906,
      "grad_norm": 0.888521463074428,
      "learning_rate": 4.346336380181552e-06,
      "loss": 0.5546,
      "step": 7781
    },
    {
      "epoch": 0.7006234666546625,
      "grad_norm": 0.9147408485084043,
      "learning_rate": 4.343931245134616e-06,
      "loss": 0.6136,
      "step": 7782
    },
    {
      "epoch": 0.7007134979405344,
      "grad_norm": 0.8941370265075014,
      "learning_rate": 4.341526591083276e-06,
      "loss": 0.5488,
      "step": 7783
    },
    {
      "epoch": 0.7008035292264062,
      "grad_norm": 0.8793222228870818,
      "learning_rate": 4.339122418232028e-06,
      "loss": 0.5319,
      "step": 7784
    },
    {
      "epoch": 0.700893560512278,
      "grad_norm": 1.0493916684354836,
      "learning_rate": 4.336718726785324e-06,
      "loss": 0.5733,
      "step": 7785
    },
    {
      "epoch": 0.7009835917981498,
      "grad_norm": 0.8435312316297083,
      "learning_rate": 4.33431551694758e-06,
      "loss": 0.5429,
      "step": 7786
    },
    {
      "epoch": 0.7010736230840217,
      "grad_norm": 0.8703425461811105,
      "learning_rate": 4.331912788923156e-06,
      "loss": 0.5183,
      "step": 7787
    },
    {
      "epoch": 0.7011636543698936,
      "grad_norm": 0.9053038939564881,
      "learning_rate": 4.329510542916394e-06,
      "loss": 0.5095,
      "step": 7788
    },
    {
      "epoch": 0.7012536856557654,
      "grad_norm": 0.8776104563426506,
      "learning_rate": 4.327108779131573e-06,
      "loss": 0.5216,
      "step": 7789
    },
    {
      "epoch": 0.7013437169416372,
      "grad_norm": 0.7994487991666255,
      "learning_rate": 4.324707497772945e-06,
      "loss": 0.5624,
      "step": 7790
    },
    {
      "epoch": 0.701433748227509,
      "grad_norm": 0.8347312582765464,
      "learning_rate": 4.322306699044715e-06,
      "loss": 0.4978,
      "step": 7791
    },
    {
      "epoch": 0.7015237795133809,
      "grad_norm": 0.8921157646895933,
      "learning_rate": 4.319906383151049e-06,
      "loss": 0.5794,
      "step": 7792
    },
    {
      "epoch": 0.7016138107992528,
      "grad_norm": 0.9189889940099917,
      "learning_rate": 4.317506550296071e-06,
      "loss": 0.523,
      "step": 7793
    },
    {
      "epoch": 0.7017038420851246,
      "grad_norm": 0.8447979997719041,
      "learning_rate": 4.3151072006838676e-06,
      "loss": 0.5386,
      "step": 7794
    },
    {
      "epoch": 0.7017938733709964,
      "grad_norm": 0.8326899328418229,
      "learning_rate": 4.312708334518471e-06,
      "loss": 0.5659,
      "step": 7795
    },
    {
      "epoch": 0.7018839046568682,
      "grad_norm": 0.780405775453824,
      "learning_rate": 4.310309952003896e-06,
      "loss": 0.5105,
      "step": 7796
    },
    {
      "epoch": 0.7019739359427402,
      "grad_norm": 1.1305605204405031,
      "learning_rate": 4.307912053344093e-06,
      "loss": 0.52,
      "step": 7797
    },
    {
      "epoch": 0.702063967228612,
      "grad_norm": 0.874364964435185,
      "learning_rate": 4.305514638742982e-06,
      "loss": 0.458,
      "step": 7798
    },
    {
      "epoch": 0.7021539985144838,
      "grad_norm": 0.7986727886452197,
      "learning_rate": 4.303117708404444e-06,
      "loss": 0.5736,
      "step": 7799
    },
    {
      "epoch": 0.7022440298003556,
      "grad_norm": 0.9793481670314671,
      "learning_rate": 4.300721262532316e-06,
      "loss": 0.6136,
      "step": 7800
    },
    {
      "epoch": 0.7023340610862274,
      "grad_norm": 0.9657283623233917,
      "learning_rate": 4.298325301330383e-06,
      "loss": 0.5251,
      "step": 7801
    },
    {
      "epoch": 0.7024240923720994,
      "grad_norm": 0.9054568143014273,
      "learning_rate": 4.295929825002414e-06,
      "loss": 0.4957,
      "step": 7802
    },
    {
      "epoch": 0.7025141236579712,
      "grad_norm": 1.1224414302211503,
      "learning_rate": 4.293534833752112e-06,
      "loss": 0.5841,
      "step": 7803
    },
    {
      "epoch": 0.702604154943843,
      "grad_norm": 1.201774664940301,
      "learning_rate": 4.291140327783151e-06,
      "loss": 0.522,
      "step": 7804
    },
    {
      "epoch": 0.7026941862297148,
      "grad_norm": 1.0802609749692458,
      "learning_rate": 4.288746307299161e-06,
      "loss": 0.4911,
      "step": 7805
    },
    {
      "epoch": 0.7027842175155866,
      "grad_norm": 1.228020948107562,
      "learning_rate": 4.286352772503731e-06,
      "loss": 0.6452,
      "step": 7806
    },
    {
      "epoch": 0.7028742488014585,
      "grad_norm": 0.8855380695856464,
      "learning_rate": 4.28395972360041e-06,
      "loss": 0.4803,
      "step": 7807
    },
    {
      "epoch": 0.7029642800873304,
      "grad_norm": 0.9382487981819047,
      "learning_rate": 4.281567160792705e-06,
      "loss": 0.6305,
      "step": 7808
    },
    {
      "epoch": 0.7030543113732022,
      "grad_norm": 0.9572273142585743,
      "learning_rate": 4.279175084284075e-06,
      "loss": 0.5162,
      "step": 7809
    },
    {
      "epoch": 0.703144342659074,
      "grad_norm": 0.9736367894429105,
      "learning_rate": 4.276783494277954e-06,
      "loss": 0.4607,
      "step": 7810
    },
    {
      "epoch": 0.7032343739449459,
      "grad_norm": 0.9074951935845808,
      "learning_rate": 4.2743923909777155e-06,
      "loss": 0.4065,
      "step": 7811
    },
    {
      "epoch": 0.7033244052308177,
      "grad_norm": 1.2262466293941545,
      "learning_rate": 4.272001774586701e-06,
      "loss": 0.4547,
      "step": 7812
    },
    {
      "epoch": 0.7034144365166896,
      "grad_norm": 1.2081660239099317,
      "learning_rate": 4.2696116453082145e-06,
      "loss": 0.4926,
      "step": 7813
    },
    {
      "epoch": 0.7035044678025614,
      "grad_norm": 0.8346623599159496,
      "learning_rate": 4.267222003345516e-06,
      "loss": 0.5417,
      "step": 7814
    },
    {
      "epoch": 0.7035944990884332,
      "grad_norm": 0.8345163588947899,
      "learning_rate": 4.26483284890181e-06,
      "loss": 0.5187,
      "step": 7815
    },
    {
      "epoch": 0.7036845303743051,
      "grad_norm": 1.115669790059918,
      "learning_rate": 4.262444182180286e-06,
      "loss": 0.5881,
      "step": 7816
    },
    {
      "epoch": 0.7037745616601769,
      "grad_norm": 0.9590876100446318,
      "learning_rate": 4.26005600338407e-06,
      "loss": 0.4931,
      "step": 7817
    },
    {
      "epoch": 0.7038645929460488,
      "grad_norm": 0.8214198719814895,
      "learning_rate": 4.257668312716255e-06,
      "loss": 0.4846,
      "step": 7818
    },
    {
      "epoch": 0.7039546242319206,
      "grad_norm": 1.0136236920411663,
      "learning_rate": 4.255281110379891e-06,
      "loss": 0.4765,
      "step": 7819
    },
    {
      "epoch": 0.7040446555177924,
      "grad_norm": 0.9459678155640354,
      "learning_rate": 4.252894396577989e-06,
      "loss": 0.4291,
      "step": 7820
    },
    {
      "epoch": 0.7041346868036643,
      "grad_norm": 0.8087277666685057,
      "learning_rate": 4.250508171513517e-06,
      "loss": 0.4996,
      "step": 7821
    },
    {
      "epoch": 0.7042247180895361,
      "grad_norm": 1.193915982725324,
      "learning_rate": 4.2481224353894024e-06,
      "loss": 0.4963,
      "step": 7822
    },
    {
      "epoch": 0.704314749375408,
      "grad_norm": 1.1226257158419382,
      "learning_rate": 4.245737188408521e-06,
      "loss": 0.5819,
      "step": 7823
    },
    {
      "epoch": 0.7044047806612798,
      "grad_norm": 0.7361818623746444,
      "learning_rate": 4.243352430773729e-06,
      "loss": 0.4446,
      "step": 7824
    },
    {
      "epoch": 0.7044948119471517,
      "grad_norm": 0.9639683726229182,
      "learning_rate": 4.240968162687819e-06,
      "loss": 0.4721,
      "step": 7825
    },
    {
      "epoch": 0.7045848432330235,
      "grad_norm": 0.8687792174114183,
      "learning_rate": 4.23858438435355e-06,
      "loss": 0.4944,
      "step": 7826
    },
    {
      "epoch": 0.7046748745188953,
      "grad_norm": 0.9153117407051531,
      "learning_rate": 4.2362010959736445e-06,
      "loss": 0.4194,
      "step": 7827
    },
    {
      "epoch": 0.7047649058047671,
      "grad_norm": 0.9532041371481779,
      "learning_rate": 4.233818297750779e-06,
      "loss": 0.5671,
      "step": 7828
    },
    {
      "epoch": 0.704854937090639,
      "grad_norm": 1.2755140733204884,
      "learning_rate": 4.2314359898875786e-06,
      "loss": 0.5343,
      "step": 7829
    },
    {
      "epoch": 0.7049449683765109,
      "grad_norm": 0.845054743242864,
      "learning_rate": 4.229054172586652e-06,
      "loss": 0.4619,
      "step": 7830
    },
    {
      "epoch": 0.7050349996623827,
      "grad_norm": 0.9541811477773712,
      "learning_rate": 4.226672846050538e-06,
      "loss": 0.647,
      "step": 7831
    },
    {
      "epoch": 0.7051250309482545,
      "grad_norm": 0.7994264877569045,
      "learning_rate": 4.22429201048175e-06,
      "loss": 0.544,
      "step": 7832
    },
    {
      "epoch": 0.7052150622341263,
      "grad_norm": 1.196668823856281,
      "learning_rate": 4.221911666082755e-06,
      "loss": 0.4854,
      "step": 7833
    },
    {
      "epoch": 0.7053050935199981,
      "grad_norm": 0.9745073631202471,
      "learning_rate": 4.2195318130559805e-06,
      "loss": 0.4934,
      "step": 7834
    },
    {
      "epoch": 0.7053951248058701,
      "grad_norm": 0.6752759070496804,
      "learning_rate": 4.21715245160381e-06,
      "loss": 0.4677,
      "step": 7835
    },
    {
      "epoch": 0.7054851560917419,
      "grad_norm": 1.0295192828382422,
      "learning_rate": 4.214773581928588e-06,
      "loss": 0.5159,
      "step": 7836
    },
    {
      "epoch": 0.7055751873776137,
      "grad_norm": 1.0926704214230158,
      "learning_rate": 4.212395204232608e-06,
      "loss": 0.5335,
      "step": 7837
    },
    {
      "epoch": 0.7056652186634855,
      "grad_norm": 0.918616559826878,
      "learning_rate": 4.210017318718139e-06,
      "loss": 0.5488,
      "step": 7838
    },
    {
      "epoch": 0.7057552499493575,
      "grad_norm": 0.8030924534635958,
      "learning_rate": 4.20763992558739e-06,
      "loss": 0.5451,
      "step": 7839
    },
    {
      "epoch": 0.7058452812352293,
      "grad_norm": 1.071544061871054,
      "learning_rate": 4.205263025042538e-06,
      "loss": 0.5539,
      "step": 7840
    },
    {
      "epoch": 0.7059353125211011,
      "grad_norm": 1.010095155567139,
      "learning_rate": 4.202886617285718e-06,
      "loss": 0.4795,
      "step": 7841
    },
    {
      "epoch": 0.7060253438069729,
      "grad_norm": 0.7954299389008513,
      "learning_rate": 4.20051070251902e-06,
      "loss": 0.4892,
      "step": 7842
    },
    {
      "epoch": 0.7061153750928447,
      "grad_norm": 0.809180148296069,
      "learning_rate": 4.198135280944489e-06,
      "loss": 0.5232,
      "step": 7843
    },
    {
      "epoch": 0.7062054063787166,
      "grad_norm": 1.0693064221789763,
      "learning_rate": 4.195760352764142e-06,
      "loss": 0.4928,
      "step": 7844
    },
    {
      "epoch": 0.7062954376645885,
      "grad_norm": 1.1180637614608695,
      "learning_rate": 4.1933859181799355e-06,
      "loss": 0.5009,
      "step": 7845
    },
    {
      "epoch": 0.7063854689504603,
      "grad_norm": 0.7230270035358044,
      "learning_rate": 4.191011977393795e-06,
      "loss": 0.5258,
      "step": 7846
    },
    {
      "epoch": 0.7064755002363321,
      "grad_norm": 0.8451891163185603,
      "learning_rate": 4.188638530607605e-06,
      "loss": 0.5148,
      "step": 7847
    },
    {
      "epoch": 0.7065655315222039,
      "grad_norm": 0.9374459003088487,
      "learning_rate": 4.186265578023202e-06,
      "loss": 0.4651,
      "step": 7848
    },
    {
      "epoch": 0.7066555628080758,
      "grad_norm": 0.8419167117816806,
      "learning_rate": 4.183893119842384e-06,
      "loss": 0.5064,
      "step": 7849
    },
    {
      "epoch": 0.7067455940939477,
      "grad_norm": 1.2170484657762626,
      "learning_rate": 4.18152115626691e-06,
      "loss": 0.5354,
      "step": 7850
    },
    {
      "epoch": 0.7068356253798195,
      "grad_norm": 0.9888958652020883,
      "learning_rate": 4.179149687498483e-06,
      "loss": 0.4315,
      "step": 7851
    },
    {
      "epoch": 0.7069256566656913,
      "grad_norm": 0.8937363172407053,
      "learning_rate": 4.176778713738787e-06,
      "loss": 0.4467,
      "step": 7852
    },
    {
      "epoch": 0.7070156879515632,
      "grad_norm": 0.7143107729076468,
      "learning_rate": 4.174408235189443e-06,
      "loss": 0.4737,
      "step": 7853
    },
    {
      "epoch": 0.707105719237435,
      "grad_norm": 0.9237854378890986,
      "learning_rate": 4.1720382520520395e-06,
      "loss": 0.6059,
      "step": 7854
    },
    {
      "epoch": 0.7071957505233069,
      "grad_norm": 1.117378652779845,
      "learning_rate": 4.169668764528122e-06,
      "loss": 0.5709,
      "step": 7855
    },
    {
      "epoch": 0.7072857818091787,
      "grad_norm": 0.8898186967606552,
      "learning_rate": 4.167299772819196e-06,
      "loss": 0.4923,
      "step": 7856
    },
    {
      "epoch": 0.7073758130950505,
      "grad_norm": 0.9363085659272169,
      "learning_rate": 4.164931277126712e-06,
      "loss": 0.5187,
      "step": 7857
    },
    {
      "epoch": 0.7074658443809224,
      "grad_norm": 0.8800467081894833,
      "learning_rate": 4.162563277652104e-06,
      "loss": 0.5339,
      "step": 7858
    },
    {
      "epoch": 0.7075558756667942,
      "grad_norm": 0.976178582946047,
      "learning_rate": 4.160195774596734e-06,
      "loss": 0.4583,
      "step": 7859
    },
    {
      "epoch": 0.707645906952666,
      "grad_norm": 1.1061270064246254,
      "learning_rate": 4.157828768161943e-06,
      "loss": 0.5283,
      "step": 7860
    },
    {
      "epoch": 0.7077359382385379,
      "grad_norm": 0.9582681728162689,
      "learning_rate": 4.155462258549021e-06,
      "loss": 0.5092,
      "step": 7861
    },
    {
      "epoch": 0.7078259695244097,
      "grad_norm": 0.9684489916514464,
      "learning_rate": 4.153096245959217e-06,
      "loss": 0.4855,
      "step": 7862
    },
    {
      "epoch": 0.7079160008102816,
      "grad_norm": 1.24458567859279,
      "learning_rate": 4.15073073059374e-06,
      "loss": 0.5407,
      "step": 7863
    },
    {
      "epoch": 0.7080060320961534,
      "grad_norm": 0.9018050443441372,
      "learning_rate": 4.148365712653757e-06,
      "loss": 0.4748,
      "step": 7864
    },
    {
      "epoch": 0.7080960633820252,
      "grad_norm": 1.1301539844674147,
      "learning_rate": 4.146001192340378e-06,
      "loss": 0.5204,
      "step": 7865
    },
    {
      "epoch": 0.7081860946678971,
      "grad_norm": 0.7213107253726823,
      "learning_rate": 4.143637169854703e-06,
      "loss": 0.4505,
      "step": 7866
    },
    {
      "epoch": 0.708276125953769,
      "grad_norm": 1.2111945562494872,
      "learning_rate": 4.1412736453977545e-06,
      "loss": 0.4649,
      "step": 7867
    },
    {
      "epoch": 0.7083661572396408,
      "grad_norm": 0.9677016457639849,
      "learning_rate": 4.138910619170533e-06,
      "loss": 0.5315,
      "step": 7868
    },
    {
      "epoch": 0.7084561885255126,
      "grad_norm": 0.8348782060931521,
      "learning_rate": 4.136548091373992e-06,
      "loss": 0.4776,
      "step": 7869
    },
    {
      "epoch": 0.7085462198113844,
      "grad_norm": 0.7696867614786008,
      "learning_rate": 4.134186062209045e-06,
      "loss": 0.4777,
      "step": 7870
    },
    {
      "epoch": 0.7086362510972563,
      "grad_norm": 0.9751045326527457,
      "learning_rate": 4.131824531876551e-06,
      "loss": 0.4929,
      "step": 7871
    },
    {
      "epoch": 0.7087262823831282,
      "grad_norm": 0.7569347239250451,
      "learning_rate": 4.12946350057735e-06,
      "loss": 0.4147,
      "step": 7872
    },
    {
      "epoch": 0.708816313669,
      "grad_norm": 0.885642503094646,
      "learning_rate": 4.127102968512214e-06,
      "loss": 0.5603,
      "step": 7873
    },
    {
      "epoch": 0.7089063449548718,
      "grad_norm": 0.9826960718551808,
      "learning_rate": 4.124742935881889e-06,
      "loss": 0.483,
      "step": 7874
    },
    {
      "epoch": 0.7089963762407436,
      "grad_norm": 0.9182499862471288,
      "learning_rate": 4.1223834028870725e-06,
      "loss": 0.504,
      "step": 7875
    },
    {
      "epoch": 0.7090864075266154,
      "grad_norm": 0.8787396998852036,
      "learning_rate": 4.120024369728421e-06,
      "loss": 0.5319,
      "step": 7876
    },
    {
      "epoch": 0.7091764388124874,
      "grad_norm": 1.0617665522777209,
      "learning_rate": 4.117665836606549e-06,
      "loss": 0.5114,
      "step": 7877
    },
    {
      "epoch": 0.7092664700983592,
      "grad_norm": 1.2269081923932723,
      "learning_rate": 4.115307803722029e-06,
      "loss": 0.5801,
      "step": 7878
    },
    {
      "epoch": 0.709356501384231,
      "grad_norm": 1.0865049184951239,
      "learning_rate": 4.112950271275382e-06,
      "loss": 0.4687,
      "step": 7879
    },
    {
      "epoch": 0.7094465326701028,
      "grad_norm": 0.8589736098874757,
      "learning_rate": 4.1105932394671054e-06,
      "loss": 0.4984,
      "step": 7880
    },
    {
      "epoch": 0.7095365639559748,
      "grad_norm": 1.0978522272395128,
      "learning_rate": 4.108236708497634e-06,
      "loss": 0.5312,
      "step": 7881
    },
    {
      "epoch": 0.7096265952418466,
      "grad_norm": 0.8831193093976761,
      "learning_rate": 4.105880678567371e-06,
      "loss": 0.588,
      "step": 7882
    },
    {
      "epoch": 0.7097166265277184,
      "grad_norm": 1.0928472535484004,
      "learning_rate": 4.103525149876675e-06,
      "loss": 0.4257,
      "step": 7883
    },
    {
      "epoch": 0.7098066578135902,
      "grad_norm": 0.8537279335434572,
      "learning_rate": 4.101170122625864e-06,
      "loss": 0.576,
      "step": 7884
    },
    {
      "epoch": 0.709896689099462,
      "grad_norm": 0.8933741166238548,
      "learning_rate": 4.098815597015204e-06,
      "loss": 0.5664,
      "step": 7885
    },
    {
      "epoch": 0.709986720385334,
      "grad_norm": 0.8768865960597522,
      "learning_rate": 4.096461573244934e-06,
      "loss": 0.5808,
      "step": 7886
    },
    {
      "epoch": 0.7100767516712058,
      "grad_norm": 0.9791663911553201,
      "learning_rate": 4.094108051515236e-06,
      "loss": 0.4773,
      "step": 7887
    },
    {
      "epoch": 0.7101667829570776,
      "grad_norm": 1.011784802303749,
      "learning_rate": 4.091755032026255e-06,
      "loss": 0.5297,
      "step": 7888
    },
    {
      "epoch": 0.7102568142429494,
      "grad_norm": 1.1322191017994592,
      "learning_rate": 4.089402514978097e-06,
      "loss": 0.5154,
      "step": 7889
    },
    {
      "epoch": 0.7103468455288212,
      "grad_norm": 0.870780308372655,
      "learning_rate": 4.087050500570817e-06,
      "loss": 0.5191,
      "step": 7890
    },
    {
      "epoch": 0.7104368768146931,
      "grad_norm": 1.0169591998910057,
      "learning_rate": 4.0846989890044356e-06,
      "loss": 0.496,
      "step": 7891
    },
    {
      "epoch": 0.710526908100565,
      "grad_norm": 1.0666139459708615,
      "learning_rate": 4.082347980478929e-06,
      "loss": 0.5717,
      "step": 7892
    },
    {
      "epoch": 0.7106169393864368,
      "grad_norm": 1.069826612236988,
      "learning_rate": 4.079997475194216e-06,
      "loss": 0.5351,
      "step": 7893
    },
    {
      "epoch": 0.7107069706723086,
      "grad_norm": 1.1013236757502969,
      "learning_rate": 4.077647473350201e-06,
      "loss": 0.5926,
      "step": 7894
    },
    {
      "epoch": 0.7107970019581805,
      "grad_norm": 0.9151180684253002,
      "learning_rate": 4.07529797514672e-06,
      "loss": 0.5462,
      "step": 7895
    },
    {
      "epoch": 0.7108870332440523,
      "grad_norm": 1.1529282620504544,
      "learning_rate": 4.072948980783577e-06,
      "loss": 0.5851,
      "step": 7896
    },
    {
      "epoch": 0.7109770645299242,
      "grad_norm": 0.9297936486855111,
      "learning_rate": 4.070600490460535e-06,
      "loss": 0.5188,
      "step": 7897
    },
    {
      "epoch": 0.711067095815796,
      "grad_norm": 1.0460816106869135,
      "learning_rate": 4.06825250437731e-06,
      "loss": 0.5439,
      "step": 7898
    },
    {
      "epoch": 0.7111571271016678,
      "grad_norm": 1.1002110158908136,
      "learning_rate": 4.06590502273357e-06,
      "loss": 0.4745,
      "step": 7899
    },
    {
      "epoch": 0.7112471583875397,
      "grad_norm": 1.367669800615181,
      "learning_rate": 4.063558045728958e-06,
      "loss": 0.5877,
      "step": 7900
    },
    {
      "epoch": 0.7113371896734115,
      "grad_norm": 0.7927478086244852,
      "learning_rate": 4.061211573563052e-06,
      "loss": 0.4732,
      "step": 7901
    },
    {
      "epoch": 0.7114272209592833,
      "grad_norm": 1.1882912071826282,
      "learning_rate": 4.058865606435401e-06,
      "loss": 0.5175,
      "step": 7902
    },
    {
      "epoch": 0.7115172522451552,
      "grad_norm": 1.2806330691268484,
      "learning_rate": 4.056520144545509e-06,
      "loss": 0.4867,
      "step": 7903
    },
    {
      "epoch": 0.711607283531027,
      "grad_norm": 1.0101463566626696,
      "learning_rate": 4.054175188092835e-06,
      "loss": 0.5549,
      "step": 7904
    },
    {
      "epoch": 0.7116973148168989,
      "grad_norm": 0.9175849954169557,
      "learning_rate": 4.051830737276794e-06,
      "loss": 0.5457,
      "step": 7905
    },
    {
      "epoch": 0.7117873461027707,
      "grad_norm": 1.1337650500191505,
      "learning_rate": 4.049486792296765e-06,
      "loss": 0.4906,
      "step": 7906
    },
    {
      "epoch": 0.7118773773886425,
      "grad_norm": 1.0771519886853835,
      "learning_rate": 4.0471433533520665e-06,
      "loss": 0.605,
      "step": 7907
    },
    {
      "epoch": 0.7119674086745144,
      "grad_norm": 0.923589012177751,
      "learning_rate": 4.044800420642e-06,
      "loss": 0.4774,
      "step": 7908
    },
    {
      "epoch": 0.7120574399603863,
      "grad_norm": 0.8687823162859136,
      "learning_rate": 4.042457994365802e-06,
      "loss": 0.5378,
      "step": 7909
    },
    {
      "epoch": 0.7121474712462581,
      "grad_norm": 0.9751089223880388,
      "learning_rate": 4.040116074722673e-06,
      "loss": 0.5024,
      "step": 7910
    },
    {
      "epoch": 0.7122375025321299,
      "grad_norm": 1.7640853391377636,
      "learning_rate": 4.037774661911775e-06,
      "loss": 0.5005,
      "step": 7911
    },
    {
      "epoch": 0.7123275338180017,
      "grad_norm": 0.8942750140007297,
      "learning_rate": 4.035433756132226e-06,
      "loss": 0.5519,
      "step": 7912
    },
    {
      "epoch": 0.7124175651038736,
      "grad_norm": 1.0064234876797185,
      "learning_rate": 4.0330933575830855e-06,
      "loss": 0.5389,
      "step": 7913
    },
    {
      "epoch": 0.7125075963897455,
      "grad_norm": 1.237518600075694,
      "learning_rate": 4.030753466463398e-06,
      "loss": 0.5915,
      "step": 7914
    },
    {
      "epoch": 0.7125976276756173,
      "grad_norm": 0.7409459061246453,
      "learning_rate": 4.028414082972141e-06,
      "loss": 0.4662,
      "step": 7915
    },
    {
      "epoch": 0.7126876589614891,
      "grad_norm": 1.1504129552864817,
      "learning_rate": 4.026075207308256e-06,
      "loss": 0.4693,
      "step": 7916
    },
    {
      "epoch": 0.7127776902473609,
      "grad_norm": 1.052573928301547,
      "learning_rate": 4.023736839670646e-06,
      "loss": 0.5718,
      "step": 7917
    },
    {
      "epoch": 0.7128677215332327,
      "grad_norm": 1.1467295465805831,
      "learning_rate": 4.021398980258166e-06,
      "loss": 0.5403,
      "step": 7918
    },
    {
      "epoch": 0.7129577528191047,
      "grad_norm": 0.7488134486830863,
      "learning_rate": 4.019061629269629e-06,
      "loss": 0.5553,
      "step": 7919
    },
    {
      "epoch": 0.7130477841049765,
      "grad_norm": 0.8069095284410523,
      "learning_rate": 4.016724786903809e-06,
      "loss": 0.5522,
      "step": 7920
    },
    {
      "epoch": 0.7131378153908483,
      "grad_norm": 1.0250607947092976,
      "learning_rate": 4.01438845335942e-06,
      "loss": 0.5155,
      "step": 7921
    },
    {
      "epoch": 0.7132278466767201,
      "grad_norm": 1.0647668970449289,
      "learning_rate": 4.0120526288351616e-06,
      "loss": 0.4644,
      "step": 7922
    },
    {
      "epoch": 0.713317877962592,
      "grad_norm": 1.024176144083716,
      "learning_rate": 4.009717313529662e-06,
      "loss": 0.5333,
      "step": 7923
    },
    {
      "epoch": 0.7134079092484639,
      "grad_norm": 0.9235507961393783,
      "learning_rate": 4.007382507641522e-06,
      "loss": 0.5366,
      "step": 7924
    },
    {
      "epoch": 0.7134979405343357,
      "grad_norm": 1.3997137239115758,
      "learning_rate": 4.005048211369293e-06,
      "loss": 0.5805,
      "step": 7925
    },
    {
      "epoch": 0.7135879718202075,
      "grad_norm": 0.9229771726089732,
      "learning_rate": 4.002714424911493e-06,
      "loss": 0.4236,
      "step": 7926
    },
    {
      "epoch": 0.7136780031060793,
      "grad_norm": 0.9401585306467751,
      "learning_rate": 4.000381148466573e-06,
      "loss": 0.4767,
      "step": 7927
    },
    {
      "epoch": 0.7137680343919512,
      "grad_norm": 0.8155188791527558,
      "learning_rate": 3.998048382232973e-06,
      "loss": 0.4337,
      "step": 7928
    },
    {
      "epoch": 0.7138580656778231,
      "grad_norm": 0.9603768499849329,
      "learning_rate": 3.995716126409063e-06,
      "loss": 0.5118,
      "step": 7929
    },
    {
      "epoch": 0.7139480969636949,
      "grad_norm": 0.7703670634841846,
      "learning_rate": 3.993384381193181e-06,
      "loss": 0.4794,
      "step": 7930
    },
    {
      "epoch": 0.7140381282495667,
      "grad_norm": 0.768440722581082,
      "learning_rate": 3.991053146783621e-06,
      "loss": 0.5661,
      "step": 7931
    },
    {
      "epoch": 0.7141281595354385,
      "grad_norm": 1.0886544348379859,
      "learning_rate": 3.988722423378632e-06,
      "loss": 0.5325,
      "step": 7932
    },
    {
      "epoch": 0.7142181908213104,
      "grad_norm": 0.8304137474760416,
      "learning_rate": 3.986392211176422e-06,
      "loss": 0.4091,
      "step": 7933
    },
    {
      "epoch": 0.7143082221071823,
      "grad_norm": 1.06328314053634,
      "learning_rate": 3.984062510375155e-06,
      "loss": 0.5133,
      "step": 7934
    },
    {
      "epoch": 0.7143982533930541,
      "grad_norm": 0.9288158330402358,
      "learning_rate": 3.981733321172942e-06,
      "loss": 0.6127,
      "step": 7935
    },
    {
      "epoch": 0.7144882846789259,
      "grad_norm": 1.0648179260062212,
      "learning_rate": 3.9794046437678705e-06,
      "loss": 0.5842,
      "step": 7936
    },
    {
      "epoch": 0.7145783159647978,
      "grad_norm": 1.0102208145142686,
      "learning_rate": 3.977076478357962e-06,
      "loss": 0.611,
      "step": 7937
    },
    {
      "epoch": 0.7146683472506696,
      "grad_norm": 1.098491091938739,
      "learning_rate": 3.974748825141211e-06,
      "loss": 0.4842,
      "step": 7938
    },
    {
      "epoch": 0.7147583785365414,
      "grad_norm": 1.0814451987742888,
      "learning_rate": 3.97242168431556e-06,
      "loss": 0.5522,
      "step": 7939
    },
    {
      "epoch": 0.7148484098224133,
      "grad_norm": 1.0276493798979531,
      "learning_rate": 3.970095056078917e-06,
      "loss": 0.4399,
      "step": 7940
    },
    {
      "epoch": 0.7149384411082851,
      "grad_norm": 1.090155496134852,
      "learning_rate": 3.967768940629126e-06,
      "loss": 0.5378,
      "step": 7941
    },
    {
      "epoch": 0.715028472394157,
      "grad_norm": 0.9349821102526569,
      "learning_rate": 3.965443338164019e-06,
      "loss": 0.4924,
      "step": 7942
    },
    {
      "epoch": 0.7151185036800288,
      "grad_norm": 1.268899383679044,
      "learning_rate": 3.9631182488813525e-06,
      "loss": 0.5361,
      "step": 7943
    },
    {
      "epoch": 0.7152085349659006,
      "grad_norm": 0.9496206823812571,
      "learning_rate": 3.9607936729788586e-06,
      "loss": 0.5807,
      "step": 7944
    },
    {
      "epoch": 0.7152985662517725,
      "grad_norm": 0.7341890535312332,
      "learning_rate": 3.958469610654222e-06,
      "loss": 0.49,
      "step": 7945
    },
    {
      "epoch": 0.7153885975376443,
      "grad_norm": 1.1778148487270277,
      "learning_rate": 3.956146062105081e-06,
      "loss": 0.4919,
      "step": 7946
    },
    {
      "epoch": 0.7154786288235162,
      "grad_norm": 1.0646866478130315,
      "learning_rate": 3.9538230275290305e-06,
      "loss": 0.5897,
      "step": 7947
    },
    {
      "epoch": 0.715568660109388,
      "grad_norm": 1.0161325810707398,
      "learning_rate": 3.9515005071236274e-06,
      "loss": 0.4598,
      "step": 7948
    },
    {
      "epoch": 0.7156586913952598,
      "grad_norm": 0.8093859092391897,
      "learning_rate": 3.949178501086371e-06,
      "loss": 0.46,
      "step": 7949
    },
    {
      "epoch": 0.7157487226811317,
      "grad_norm": 1.0443561208751289,
      "learning_rate": 3.94685700961474e-06,
      "loss": 0.4808,
      "step": 7950
    },
    {
      "epoch": 0.7158387539670036,
      "grad_norm": 0.9655987366940523,
      "learning_rate": 3.944536032906142e-06,
      "loss": 0.505,
      "step": 7951
    },
    {
      "epoch": 0.7159287852528754,
      "grad_norm": 1.2162768119754712,
      "learning_rate": 3.94221557115796e-06,
      "loss": 0.5099,
      "step": 7952
    },
    {
      "epoch": 0.7160188165387472,
      "grad_norm": 1.0371433020178331,
      "learning_rate": 3.939895624567529e-06,
      "loss": 0.5104,
      "step": 7953
    },
    {
      "epoch": 0.716108847824619,
      "grad_norm": 1.0229313260615553,
      "learning_rate": 3.93757619333214e-06,
      "loss": 0.5887,
      "step": 7954
    },
    {
      "epoch": 0.7161988791104908,
      "grad_norm": 0.9152359792882626,
      "learning_rate": 3.935257277649028e-06,
      "loss": 0.5628,
      "step": 7955
    },
    {
      "epoch": 0.7162889103963628,
      "grad_norm": 1.2450154614850488,
      "learning_rate": 3.932938877715411e-06,
      "loss": 0.5734,
      "step": 7956
    },
    {
      "epoch": 0.7163789416822346,
      "grad_norm": 1.0619131085774962,
      "learning_rate": 3.930620993728434e-06,
      "loss": 0.4514,
      "step": 7957
    },
    {
      "epoch": 0.7164689729681064,
      "grad_norm": 0.9610379694766998,
      "learning_rate": 3.928303625885218e-06,
      "loss": 0.4769,
      "step": 7958
    },
    {
      "epoch": 0.7165590042539782,
      "grad_norm": 1.0879121220668506,
      "learning_rate": 3.925986774382832e-06,
      "loss": 0.5507,
      "step": 7959
    },
    {
      "epoch": 0.71664903553985,
      "grad_norm": 0.9804669447315645,
      "learning_rate": 3.9236704394183015e-06,
      "loss": 0.5449,
      "step": 7960
    },
    {
      "epoch": 0.716739066825722,
      "grad_norm": 0.8395966031740884,
      "learning_rate": 3.92135462118861e-06,
      "loss": 0.5119,
      "step": 7961
    },
    {
      "epoch": 0.7168290981115938,
      "grad_norm": 0.8232858054725168,
      "learning_rate": 3.919039319890699e-06,
      "loss": 0.4401,
      "step": 7962
    },
    {
      "epoch": 0.7169191293974656,
      "grad_norm": 1.4887999360625106,
      "learning_rate": 3.916724535721454e-06,
      "loss": 0.5204,
      "step": 7963
    },
    {
      "epoch": 0.7170091606833374,
      "grad_norm": 0.9059350856356392,
      "learning_rate": 3.9144102688777385e-06,
      "loss": 0.4869,
      "step": 7964
    },
    {
      "epoch": 0.7170991919692093,
      "grad_norm": 0.966755886560646,
      "learning_rate": 3.91209651955635e-06,
      "loss": 0.5581,
      "step": 7965
    },
    {
      "epoch": 0.7171892232550812,
      "grad_norm": 1.0346175868872312,
      "learning_rate": 3.9097832879540525e-06,
      "loss": 0.5239,
      "step": 7966
    },
    {
      "epoch": 0.717279254540953,
      "grad_norm": 0.7182984244067142,
      "learning_rate": 3.907470574267566e-06,
      "loss": 0.512,
      "step": 7967
    },
    {
      "epoch": 0.7173692858268248,
      "grad_norm": 1.051277362268979,
      "learning_rate": 3.905158378693569e-06,
      "loss": 0.4424,
      "step": 7968
    },
    {
      "epoch": 0.7174593171126966,
      "grad_norm": 1.0555956780982063,
      "learning_rate": 3.902846701428681e-06,
      "loss": 0.5846,
      "step": 7969
    },
    {
      "epoch": 0.7175493483985685,
      "grad_norm": 0.9826087776834365,
      "learning_rate": 3.900535542669501e-06,
      "loss": 0.504,
      "step": 7970
    },
    {
      "epoch": 0.7176393796844404,
      "grad_norm": 0.9877756178711392,
      "learning_rate": 3.898224902612564e-06,
      "loss": 0.5005,
      "step": 7971
    },
    {
      "epoch": 0.7177294109703122,
      "grad_norm": 0.8407501042362353,
      "learning_rate": 3.89591478145437e-06,
      "loss": 0.4798,
      "step": 7972
    },
    {
      "epoch": 0.717819442256184,
      "grad_norm": 1.0387964010579445,
      "learning_rate": 3.893605179391373e-06,
      "loss": 0.4964,
      "step": 7973
    },
    {
      "epoch": 0.7179094735420558,
      "grad_norm": 1.1212121267068398,
      "learning_rate": 3.891296096619983e-06,
      "loss": 0.5873,
      "step": 7974
    },
    {
      "epoch": 0.7179995048279277,
      "grad_norm": 0.8715434461889645,
      "learning_rate": 3.888987533336567e-06,
      "loss": 0.4276,
      "step": 7975
    },
    {
      "epoch": 0.7180895361137996,
      "grad_norm": 1.0967220164899962,
      "learning_rate": 3.886679489737448e-06,
      "loss": 0.5391,
      "step": 7976
    },
    {
      "epoch": 0.7181795673996714,
      "grad_norm": 0.8735447839213178,
      "learning_rate": 3.884371966018895e-06,
      "loss": 0.5743,
      "step": 7977
    },
    {
      "epoch": 0.7182695986855432,
      "grad_norm": 0.9337924217548877,
      "learning_rate": 3.882064962377154e-06,
      "loss": 0.4952,
      "step": 7978
    },
    {
      "epoch": 0.7183596299714151,
      "grad_norm": 1.2778318602709788,
      "learning_rate": 3.879758479008404e-06,
      "loss": 0.5596,
      "step": 7979
    },
    {
      "epoch": 0.7184496612572869,
      "grad_norm": 0.9384434834169593,
      "learning_rate": 3.877452516108794e-06,
      "loss": 0.5841,
      "step": 7980
    },
    {
      "epoch": 0.7185396925431587,
      "grad_norm": 1.2047343124209595,
      "learning_rate": 3.875147073874424e-06,
      "loss": 0.5108,
      "step": 7981
    },
    {
      "epoch": 0.7186297238290306,
      "grad_norm": 1.136410130519086,
      "learning_rate": 3.872842152501351e-06,
      "loss": 0.5454,
      "step": 7982
    },
    {
      "epoch": 0.7187197551149024,
      "grad_norm": 1.1094314328824493,
      "learning_rate": 3.870537752185582e-06,
      "loss": 0.5233,
      "step": 7983
    },
    {
      "epoch": 0.7188097864007743,
      "grad_norm": 0.8101065134071591,
      "learning_rate": 3.868233873123094e-06,
      "loss": 0.5107,
      "step": 7984
    },
    {
      "epoch": 0.7188998176866461,
      "grad_norm": 1.270264476256782,
      "learning_rate": 3.8659305155098016e-06,
      "loss": 0.5364,
      "step": 7985
    },
    {
      "epoch": 0.7189898489725179,
      "grad_norm": 0.9945891263913902,
      "learning_rate": 3.863627679541588e-06,
      "loss": 0.5166,
      "step": 7986
    },
    {
      "epoch": 0.7190798802583898,
      "grad_norm": 1.1778720637879305,
      "learning_rate": 3.861325365414286e-06,
      "loss": 0.613,
      "step": 7987
    },
    {
      "epoch": 0.7191699115442617,
      "grad_norm": 0.9544173153426376,
      "learning_rate": 3.859023573323686e-06,
      "loss": 0.5296,
      "step": 7988
    },
    {
      "epoch": 0.7192599428301335,
      "grad_norm": 1.4068268915529292,
      "learning_rate": 3.856722303465535e-06,
      "loss": 0.5113,
      "step": 7989
    },
    {
      "epoch": 0.7193499741160053,
      "grad_norm": 1.3684440843104384,
      "learning_rate": 3.854421556035538e-06,
      "loss": 0.565,
      "step": 7990
    },
    {
      "epoch": 0.7194400054018771,
      "grad_norm": 1.583252123317621,
      "learning_rate": 3.8521213312293404e-06,
      "loss": 0.5464,
      "step": 7991
    },
    {
      "epoch": 0.719530036687749,
      "grad_norm": 1.0875884270719585,
      "learning_rate": 3.8498216292425695e-06,
      "loss": 0.5405,
      "step": 7992
    },
    {
      "epoch": 0.7196200679736209,
      "grad_norm": 1.1407518527572091,
      "learning_rate": 3.847522450270783e-06,
      "loss": 0.565,
      "step": 7993
    },
    {
      "epoch": 0.7197100992594927,
      "grad_norm": 1.2302389652455883,
      "learning_rate": 3.845223794509507e-06,
      "loss": 0.5148,
      "step": 7994
    },
    {
      "epoch": 0.7198001305453645,
      "grad_norm": 1.2405385078388256,
      "learning_rate": 3.842925662154221e-06,
      "loss": 0.4983,
      "step": 7995
    },
    {
      "epoch": 0.7198901618312363,
      "grad_norm": 0.950597692167457,
      "learning_rate": 3.840628053400365e-06,
      "loss": 0.5747,
      "step": 7996
    },
    {
      "epoch": 0.7199801931171081,
      "grad_norm": 1.1580434017201329,
      "learning_rate": 3.838330968443316e-06,
      "loss": 0.5521,
      "step": 7997
    },
    {
      "epoch": 0.7200702244029801,
      "grad_norm": 1.030552683519027,
      "learning_rate": 3.836034407478434e-06,
      "loss": 0.527,
      "step": 7998
    },
    {
      "epoch": 0.7201602556888519,
      "grad_norm": 1.0500031307675501,
      "learning_rate": 3.83373837070101e-06,
      "loss": 0.4583,
      "step": 7999
    },
    {
      "epoch": 0.7202502869747237,
      "grad_norm": 0.9319792144625716,
      "learning_rate": 3.831442858306305e-06,
      "loss": 0.5488,
      "step": 8000
    },
    {
      "epoch": 0.7203403182605955,
      "grad_norm": 0.8890873438220819,
      "learning_rate": 3.829147870489528e-06,
      "loss": 0.549,
      "step": 8001
    },
    {
      "epoch": 0.7204303495464675,
      "grad_norm": 1.173847738756091,
      "learning_rate": 3.826853407445848e-06,
      "loss": 0.4735,
      "step": 8002
    },
    {
      "epoch": 0.7205203808323393,
      "grad_norm": 0.8593106240136388,
      "learning_rate": 3.824559469370389e-06,
      "loss": 0.5218,
      "step": 8003
    },
    {
      "epoch": 0.7206104121182111,
      "grad_norm": 1.0462255808725671,
      "learning_rate": 3.822266056458229e-06,
      "loss": 0.5288,
      "step": 8004
    },
    {
      "epoch": 0.7207004434040829,
      "grad_norm": 1.0400533750717826,
      "learning_rate": 3.819973168904394e-06,
      "loss": 0.5133,
      "step": 8005
    },
    {
      "epoch": 0.7207904746899547,
      "grad_norm": 0.9048980802064513,
      "learning_rate": 3.817680806903884e-06,
      "loss": 0.5755,
      "step": 8006
    },
    {
      "epoch": 0.7208805059758266,
      "grad_norm": 1.1282970356907118,
      "learning_rate": 3.815388970651635e-06,
      "loss": 0.4752,
      "step": 8007
    },
    {
      "epoch": 0.7209705372616985,
      "grad_norm": 1.4596776596754015,
      "learning_rate": 3.8130976603425473e-06,
      "loss": 0.4819,
      "step": 8008
    },
    {
      "epoch": 0.7210605685475703,
      "grad_norm": 0.8913619872264662,
      "learning_rate": 3.8108068761714754e-06,
      "loss": 0.5313,
      "step": 8009
    },
    {
      "epoch": 0.7211505998334421,
      "grad_norm": 0.9908961122176324,
      "learning_rate": 3.8085166183332346e-06,
      "loss": 0.5244,
      "step": 8010
    },
    {
      "epoch": 0.7212406311193139,
      "grad_norm": 0.9022096706627791,
      "learning_rate": 3.8062268870225784e-06,
      "loss": 0.5071,
      "step": 8011
    },
    {
      "epoch": 0.7213306624051858,
      "grad_norm": 1.337349567898808,
      "learning_rate": 3.80393768243424e-06,
      "loss": 0.5238,
      "step": 8012
    },
    {
      "epoch": 0.7214206936910577,
      "grad_norm": 1.077531531916771,
      "learning_rate": 3.801649004762885e-06,
      "loss": 0.5491,
      "step": 8013
    },
    {
      "epoch": 0.7215107249769295,
      "grad_norm": 1.1001024638713046,
      "learning_rate": 3.799360854203148e-06,
      "loss": 0.497,
      "step": 8014
    },
    {
      "epoch": 0.7216007562628013,
      "grad_norm": 0.935386921376745,
      "learning_rate": 3.7970732309496126e-06,
      "loss": 0.5373,
      "step": 8015
    },
    {
      "epoch": 0.7216907875486732,
      "grad_norm": 1.0748900107336596,
      "learning_rate": 3.7947861351968228e-06,
      "loss": 0.5283,
      "step": 8016
    },
    {
      "epoch": 0.721780818834545,
      "grad_norm": 1.3069668137144652,
      "learning_rate": 3.792499567139273e-06,
      "loss": 0.5076,
      "step": 8017
    },
    {
      "epoch": 0.7218708501204169,
      "grad_norm": 0.8845518221253691,
      "learning_rate": 3.790213526971418e-06,
      "loss": 0.5524,
      "step": 8018
    },
    {
      "epoch": 0.7219608814062887,
      "grad_norm": 0.9818106113815197,
      "learning_rate": 3.787928014887653e-06,
      "loss": 0.5666,
      "step": 8019
    },
    {
      "epoch": 0.7220509126921605,
      "grad_norm": 0.9503918815026863,
      "learning_rate": 3.7856430310823546e-06,
      "loss": 0.6032,
      "step": 8020
    },
    {
      "epoch": 0.7221409439780324,
      "grad_norm": 0.9824514903744943,
      "learning_rate": 3.783358575749828e-06,
      "loss": 0.6301,
      "step": 8021
    },
    {
      "epoch": 0.7222309752639042,
      "grad_norm": 1.2791682053971012,
      "learning_rate": 3.7810746490843475e-06,
      "loss": 0.5348,
      "step": 8022
    },
    {
      "epoch": 0.722321006549776,
      "grad_norm": 1.119169908285247,
      "learning_rate": 3.7787912512801416e-06,
      "loss": 0.3873,
      "step": 8023
    },
    {
      "epoch": 0.7224110378356479,
      "grad_norm": 1.04481316997039,
      "learning_rate": 3.776508382531393e-06,
      "loss": 0.5882,
      "step": 8024
    },
    {
      "epoch": 0.7225010691215197,
      "grad_norm": 1.8138664198447052,
      "learning_rate": 3.7742260430322295e-06,
      "loss": 0.531,
      "step": 8025
    },
    {
      "epoch": 0.7225911004073916,
      "grad_norm": 1.45685411187566,
      "learning_rate": 3.7719442329767562e-06,
      "loss": 0.5236,
      "step": 8026
    },
    {
      "epoch": 0.7226811316932634,
      "grad_norm": 1.2178131811521153,
      "learning_rate": 3.76966295255901e-06,
      "loss": 0.561,
      "step": 8027
    },
    {
      "epoch": 0.7227711629791352,
      "grad_norm": 0.8931244012505478,
      "learning_rate": 3.7673822019729946e-06,
      "loss": 0.6095,
      "step": 8028
    },
    {
      "epoch": 0.7228611942650071,
      "grad_norm": 1.0143270635008719,
      "learning_rate": 3.7651019814126656e-06,
      "loss": 0.5901,
      "step": 8029
    },
    {
      "epoch": 0.722951225550879,
      "grad_norm": 0.8882487020161288,
      "learning_rate": 3.762822291071937e-06,
      "loss": 0.5399,
      "step": 8030
    },
    {
      "epoch": 0.7230412568367508,
      "grad_norm": 1.0008104025933517,
      "learning_rate": 3.760543131144673e-06,
      "loss": 0.5165,
      "step": 8031
    },
    {
      "epoch": 0.7231312881226226,
      "grad_norm": 0.8352614645154354,
      "learning_rate": 3.7582645018246988e-06,
      "loss": 0.5493,
      "step": 8032
    },
    {
      "epoch": 0.7232213194084944,
      "grad_norm": 1.0053330900191855,
      "learning_rate": 3.75598640330578e-06,
      "loss": 0.4693,
      "step": 8033
    },
    {
      "epoch": 0.7233113506943663,
      "grad_norm": 0.8683058029520604,
      "learning_rate": 3.753708835781662e-06,
      "loss": 0.5197,
      "step": 8034
    },
    {
      "epoch": 0.7234013819802382,
      "grad_norm": 3.160042573627607,
      "learning_rate": 3.7514317994460205e-06,
      "loss": 0.5578,
      "step": 8035
    },
    {
      "epoch": 0.72349141326611,
      "grad_norm": 1.003502755109951,
      "learning_rate": 3.749155294492497e-06,
      "loss": 0.4888,
      "step": 8036
    },
    {
      "epoch": 0.7235814445519818,
      "grad_norm": 1.0433961324086398,
      "learning_rate": 3.7468793211146903e-06,
      "loss": 0.4788,
      "step": 8037
    },
    {
      "epoch": 0.7236714758378536,
      "grad_norm": 1.1199058170344287,
      "learning_rate": 3.744603879506151e-06,
      "loss": 0.5433,
      "step": 8038
    },
    {
      "epoch": 0.7237615071237254,
      "grad_norm": 0.9923247688452637,
      "learning_rate": 3.742328969860376e-06,
      "loss": 0.5321,
      "step": 8039
    },
    {
      "epoch": 0.7238515384095974,
      "grad_norm": 1.1374469928256319,
      "learning_rate": 3.7400545923708387e-06,
      "loss": 0.5331,
      "step": 8040
    },
    {
      "epoch": 0.7239415696954692,
      "grad_norm": 1.1234112311000448,
      "learning_rate": 3.737780747230941e-06,
      "loss": 0.5219,
      "step": 8041
    },
    {
      "epoch": 0.724031600981341,
      "grad_norm": 1.1064195643353625,
      "learning_rate": 3.735507434634058e-06,
      "loss": 0.4766,
      "step": 8042
    },
    {
      "epoch": 0.7241216322672128,
      "grad_norm": 1.034808558482496,
      "learning_rate": 3.733234654773512e-06,
      "loss": 0.6019,
      "step": 8043
    },
    {
      "epoch": 0.7242116635530848,
      "grad_norm": 1.9850643069063592,
      "learning_rate": 3.7309624078425823e-06,
      "loss": 0.6203,
      "step": 8044
    },
    {
      "epoch": 0.7243016948389566,
      "grad_norm": 1.0977088691403654,
      "learning_rate": 3.7286906940345025e-06,
      "loss": 0.5128,
      "step": 8045
    },
    {
      "epoch": 0.7243917261248284,
      "grad_norm": 0.964111755500321,
      "learning_rate": 3.726419513542464e-06,
      "loss": 0.4573,
      "step": 8046
    },
    {
      "epoch": 0.7244817574107002,
      "grad_norm": 0.8243677281885414,
      "learning_rate": 3.7241488665595983e-06,
      "loss": 0.4891,
      "step": 8047
    },
    {
      "epoch": 0.724571788696572,
      "grad_norm": 1.0550261551083098,
      "learning_rate": 3.7218787532790167e-06,
      "loss": 0.5457,
      "step": 8048
    },
    {
      "epoch": 0.7246618199824439,
      "grad_norm": 0.9898399351677766,
      "learning_rate": 3.71960917389376e-06,
      "loss": 0.6008,
      "step": 8049
    },
    {
      "epoch": 0.7247518512683158,
      "grad_norm": 1.365915909678785,
      "learning_rate": 3.7173401285968404e-06,
      "loss": 0.5351,
      "step": 8050
    },
    {
      "epoch": 0.7248418825541876,
      "grad_norm": 0.9732475648952641,
      "learning_rate": 3.7150716175812163e-06,
      "loss": 0.5629,
      "step": 8051
    },
    {
      "epoch": 0.7249319138400594,
      "grad_norm": 1.0363729224952258,
      "learning_rate": 3.7128036410398082e-06,
      "loss": 0.5517,
      "step": 8052
    },
    {
      "epoch": 0.7250219451259312,
      "grad_norm": 0.9384081969246233,
      "learning_rate": 3.7105361991654764e-06,
      "loss": 0.4255,
      "step": 8053
    },
    {
      "epoch": 0.7251119764118031,
      "grad_norm": 0.8677872669740501,
      "learning_rate": 3.708269292151058e-06,
      "loss": 0.5222,
      "step": 8054
    },
    {
      "epoch": 0.725202007697675,
      "grad_norm": 1.069083430542946,
      "learning_rate": 3.706002920189322e-06,
      "loss": 0.5346,
      "step": 8055
    },
    {
      "epoch": 0.7252920389835468,
      "grad_norm": 0.8908686412460713,
      "learning_rate": 3.703737083473006e-06,
      "loss": 0.4847,
      "step": 8056
    },
    {
      "epoch": 0.7253820702694186,
      "grad_norm": 0.9760181429887681,
      "learning_rate": 3.7014717821947976e-06,
      "loss": 0.4376,
      "step": 8057
    },
    {
      "epoch": 0.7254721015552905,
      "grad_norm": 0.9579229010243254,
      "learning_rate": 3.6992070165473405e-06,
      "loss": 0.5291,
      "step": 8058
    },
    {
      "epoch": 0.7255621328411623,
      "grad_norm": 0.9218130137736616,
      "learning_rate": 3.696942786723231e-06,
      "loss": 0.4832,
      "step": 8059
    },
    {
      "epoch": 0.7256521641270341,
      "grad_norm": 1.0451549262062436,
      "learning_rate": 3.6946790929150246e-06,
      "loss": 0.6172,
      "step": 8060
    },
    {
      "epoch": 0.725742195412906,
      "grad_norm": 0.8504760739377986,
      "learning_rate": 3.6924159353152156e-06,
      "loss": 0.5216,
      "step": 8061
    },
    {
      "epoch": 0.7258322266987778,
      "grad_norm": 0.8540112057685922,
      "learning_rate": 3.6901533141162804e-06,
      "loss": 0.5201,
      "step": 8062
    },
    {
      "epoch": 0.7259222579846497,
      "grad_norm": 0.935468494735209,
      "learning_rate": 3.687891229510621e-06,
      "loss": 0.4791,
      "step": 8063
    },
    {
      "epoch": 0.7260122892705215,
      "grad_norm": 1.0979744616561697,
      "learning_rate": 3.68562968169061e-06,
      "loss": 0.4441,
      "step": 8064
    },
    {
      "epoch": 0.7261023205563933,
      "grad_norm": 0.9661458948374391,
      "learning_rate": 3.6833686708485727e-06,
      "loss": 0.5159,
      "step": 8065
    },
    {
      "epoch": 0.7261923518422652,
      "grad_norm": 0.9970099013915968,
      "learning_rate": 3.6811081971767904e-06,
      "loss": 0.527,
      "step": 8066
    },
    {
      "epoch": 0.726282383128137,
      "grad_norm": 1.2814588345554032,
      "learning_rate": 3.6788482608674824e-06,
      "loss": 0.4939,
      "step": 8067
    },
    {
      "epoch": 0.7263724144140089,
      "grad_norm": 0.9970349864021183,
      "learning_rate": 3.67658886211285e-06,
      "loss": 0.527,
      "step": 8068
    },
    {
      "epoch": 0.7264624456998807,
      "grad_norm": 0.937565232794552,
      "learning_rate": 3.674330001105024e-06,
      "loss": 0.5201,
      "step": 8069
    },
    {
      "epoch": 0.7265524769857525,
      "grad_norm": 0.8039890647787278,
      "learning_rate": 3.672071678036103e-06,
      "loss": 0.5023,
      "step": 8070
    },
    {
      "epoch": 0.7266425082716244,
      "grad_norm": 1.121564369245275,
      "learning_rate": 3.6698138930981363e-06,
      "loss": 0.4566,
      "step": 8071
    },
    {
      "epoch": 0.7267325395574963,
      "grad_norm": 0.9722113619911905,
      "learning_rate": 3.667556646483126e-06,
      "loss": 0.5806,
      "step": 8072
    },
    {
      "epoch": 0.7268225708433681,
      "grad_norm": 0.9854396039458329,
      "learning_rate": 3.665299938383031e-06,
      "loss": 0.5646,
      "step": 8073
    },
    {
      "epoch": 0.7269126021292399,
      "grad_norm": 1.0014158261142387,
      "learning_rate": 3.663043768989766e-06,
      "loss": 0.4806,
      "step": 8074
    },
    {
      "epoch": 0.7270026334151117,
      "grad_norm": 1.0808043489705004,
      "learning_rate": 3.660788138495187e-06,
      "loss": 0.5225,
      "step": 8075
    },
    {
      "epoch": 0.7270926647009835,
      "grad_norm": 1.0669539946509654,
      "learning_rate": 3.6585330470911275e-06,
      "loss": 0.5765,
      "step": 8076
    },
    {
      "epoch": 0.7271826959868555,
      "grad_norm": 0.9865524491752149,
      "learning_rate": 3.656278494969352e-06,
      "loss": 0.5572,
      "step": 8077
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 1.3424209609915647,
      "learning_rate": 3.654024482321592e-06,
      "loss": 0.578,
      "step": 8078
    },
    {
      "epoch": 0.7273627585585991,
      "grad_norm": 0.904037154233676,
      "learning_rate": 3.651771009339532e-06,
      "loss": 0.5726,
      "step": 8079
    },
    {
      "epoch": 0.7274527898444709,
      "grad_norm": 0.8682706947286921,
      "learning_rate": 3.6495180762148087e-06,
      "loss": 0.5446,
      "step": 8080
    },
    {
      "epoch": 0.7275428211303427,
      "grad_norm": 1.0698044732846026,
      "learning_rate": 3.6472656831390065e-06,
      "loss": 0.6206,
      "step": 8081
    },
    {
      "epoch": 0.7276328524162147,
      "grad_norm": 0.9060304176174158,
      "learning_rate": 3.645013830303681e-06,
      "loss": 0.5606,
      "step": 8082
    },
    {
      "epoch": 0.7277228837020865,
      "grad_norm": 1.042535859996566,
      "learning_rate": 3.6427625179003223e-06,
      "loss": 0.4863,
      "step": 8083
    },
    {
      "epoch": 0.7278129149879583,
      "grad_norm": 1.0778700326060437,
      "learning_rate": 3.6405117461203864e-06,
      "loss": 0.4965,
      "step": 8084
    },
    {
      "epoch": 0.7279029462738301,
      "grad_norm": 0.9540163506750814,
      "learning_rate": 3.6382615151552804e-06,
      "loss": 0.5633,
      "step": 8085
    },
    {
      "epoch": 0.727992977559702,
      "grad_norm": 0.9047956718330019,
      "learning_rate": 3.636011825196365e-06,
      "loss": 0.5092,
      "step": 8086
    },
    {
      "epoch": 0.7280830088455739,
      "grad_norm": 0.8088996683991047,
      "learning_rate": 3.6337626764349565e-06,
      "loss": 0.5297,
      "step": 8087
    },
    {
      "epoch": 0.7281730401314457,
      "grad_norm": 0.9862843599677692,
      "learning_rate": 3.6315140690623264e-06,
      "loss": 0.5299,
      "step": 8088
    },
    {
      "epoch": 0.7282630714173175,
      "grad_norm": 0.8546507045647107,
      "learning_rate": 3.629266003269687e-06,
      "loss": 0.4937,
      "step": 8089
    },
    {
      "epoch": 0.7283531027031893,
      "grad_norm": 0.896327650227799,
      "learning_rate": 3.6270184792482298e-06,
      "loss": 0.5941,
      "step": 8090
    },
    {
      "epoch": 0.7284431339890612,
      "grad_norm": 1.3766966471729793,
      "learning_rate": 3.6247714971890745e-06,
      "loss": 0.5325,
      "step": 8091
    },
    {
      "epoch": 0.7285331652749331,
      "grad_norm": 1.74630188575693,
      "learning_rate": 3.6225250572833103e-06,
      "loss": 0.4765,
      "step": 8092
    },
    {
      "epoch": 0.7286231965608049,
      "grad_norm": 1.3586445985120983,
      "learning_rate": 3.6202791597219755e-06,
      "loss": 0.5335,
      "step": 8093
    },
    {
      "epoch": 0.7287132278466767,
      "grad_norm": 0.8774630200973761,
      "learning_rate": 3.618033804696065e-06,
      "loss": 0.4856,
      "step": 8094
    },
    {
      "epoch": 0.7288032591325485,
      "grad_norm": 1.0977893648988624,
      "learning_rate": 3.615788992396516e-06,
      "loss": 0.5348,
      "step": 8095
    },
    {
      "epoch": 0.7288932904184204,
      "grad_norm": 1.1629123032905517,
      "learning_rate": 3.6135447230142428e-06,
      "loss": 0.5282,
      "step": 8096
    },
    {
      "epoch": 0.7289833217042923,
      "grad_norm": 0.8264709735941603,
      "learning_rate": 3.611300996740088e-06,
      "loss": 0.5599,
      "step": 8097
    },
    {
      "epoch": 0.7290733529901641,
      "grad_norm": 1.407175588297777,
      "learning_rate": 3.6090578137648636e-06,
      "loss": 0.5197,
      "step": 8098
    },
    {
      "epoch": 0.7291633842760359,
      "grad_norm": 0.7900135093950301,
      "learning_rate": 3.6068151742793313e-06,
      "loss": 0.4736,
      "step": 8099
    },
    {
      "epoch": 0.7292534155619078,
      "grad_norm": 1.3695514002330245,
      "learning_rate": 3.604573078474205e-06,
      "loss": 0.5407,
      "step": 8100
    },
    {
      "epoch": 0.7293434468477796,
      "grad_norm": 0.8743291735356051,
      "learning_rate": 3.602331526540157e-06,
      "loss": 0.5278,
      "step": 8101
    },
    {
      "epoch": 0.7294334781336514,
      "grad_norm": 0.9957954833211589,
      "learning_rate": 3.6000905186678115e-06,
      "loss": 0.5467,
      "step": 8102
    },
    {
      "epoch": 0.7295235094195233,
      "grad_norm": 0.9305782795039363,
      "learning_rate": 3.5978500550477346e-06,
      "loss": 0.5576,
      "step": 8103
    },
    {
      "epoch": 0.7296135407053951,
      "grad_norm": 1.0479112880474086,
      "learning_rate": 3.595610135870472e-06,
      "loss": 0.4776,
      "step": 8104
    },
    {
      "epoch": 0.729703571991267,
      "grad_norm": 1.1706189952028507,
      "learning_rate": 3.593370761326497e-06,
      "loss": 0.5367,
      "step": 8105
    },
    {
      "epoch": 0.7297936032771388,
      "grad_norm": 1.476922805292168,
      "learning_rate": 3.59113193160625e-06,
      "loss": 0.494,
      "step": 8106
    },
    {
      "epoch": 0.7298836345630106,
      "grad_norm": 0.9630752258834939,
      "learning_rate": 3.5888936469001234e-06,
      "loss": 0.6114,
      "step": 8107
    },
    {
      "epoch": 0.7299736658488825,
      "grad_norm": 0.9834188040097717,
      "learning_rate": 3.586655907398465e-06,
      "loss": 0.5329,
      "step": 8108
    },
    {
      "epoch": 0.7300636971347543,
      "grad_norm": 0.9405404752856669,
      "learning_rate": 3.5844187132915644e-06,
      "loss": 0.5683,
      "step": 8109
    },
    {
      "epoch": 0.7301537284206262,
      "grad_norm": 1.2126759518717372,
      "learning_rate": 3.582182064769687e-06,
      "loss": 0.5518,
      "step": 8110
    },
    {
      "epoch": 0.730243759706498,
      "grad_norm": 1.0285056992207213,
      "learning_rate": 3.579945962023028e-06,
      "loss": 0.4934,
      "step": 8111
    },
    {
      "epoch": 0.7303337909923698,
      "grad_norm": 1.0498385852231362,
      "learning_rate": 3.5777104052417513e-06,
      "loss": 0.5882,
      "step": 8112
    },
    {
      "epoch": 0.7304238222782417,
      "grad_norm": 0.8957110611553063,
      "learning_rate": 3.575475394615968e-06,
      "loss": 0.4658,
      "step": 8113
    },
    {
      "epoch": 0.7305138535641136,
      "grad_norm": 1.1522441598724191,
      "learning_rate": 3.5732409303357486e-06,
      "loss": 0.5359,
      "step": 8114
    },
    {
      "epoch": 0.7306038848499854,
      "grad_norm": 1.4229833079065854,
      "learning_rate": 3.5710070125911102e-06,
      "loss": 0.5424,
      "step": 8115
    },
    {
      "epoch": 0.7306939161358572,
      "grad_norm": 0.9482000384062858,
      "learning_rate": 3.568773641572031e-06,
      "loss": 0.5577,
      "step": 8116
    },
    {
      "epoch": 0.730783947421729,
      "grad_norm": 1.0673435322208922,
      "learning_rate": 3.566540817468428e-06,
      "loss": 0.5663,
      "step": 8117
    },
    {
      "epoch": 0.7308739787076008,
      "grad_norm": 0.9851286320666031,
      "learning_rate": 3.5643085404701947e-06,
      "loss": 0.6228,
      "step": 8118
    },
    {
      "epoch": 0.7309640099934728,
      "grad_norm": 1.0408296311154466,
      "learning_rate": 3.5620768107671566e-06,
      "loss": 0.5213,
      "step": 8119
    },
    {
      "epoch": 0.7310540412793446,
      "grad_norm": 0.9519333203784959,
      "learning_rate": 3.5598456285491045e-06,
      "loss": 0.5827,
      "step": 8120
    },
    {
      "epoch": 0.7311440725652164,
      "grad_norm": 1.2887452187309476,
      "learning_rate": 3.557614994005778e-06,
      "loss": 0.472,
      "step": 8121
    },
    {
      "epoch": 0.7312341038510882,
      "grad_norm": 1.1843587662344601,
      "learning_rate": 3.5553849073268767e-06,
      "loss": 0.492,
      "step": 8122
    },
    {
      "epoch": 0.73132413513696,
      "grad_norm": 1.1768225306060618,
      "learning_rate": 3.5531553687020383e-06,
      "loss": 0.4718,
      "step": 8123
    },
    {
      "epoch": 0.731414166422832,
      "grad_norm": 1.0013940472692953,
      "learning_rate": 3.5509263783208768e-06,
      "loss": 0.5066,
      "step": 8124
    },
    {
      "epoch": 0.7315041977087038,
      "grad_norm": 1.0044664251932351,
      "learning_rate": 3.548697936372937e-06,
      "loss": 0.4612,
      "step": 8125
    },
    {
      "epoch": 0.7315942289945756,
      "grad_norm": 0.8248317024450493,
      "learning_rate": 3.5464700430477304e-06,
      "loss": 0.5111,
      "step": 8126
    },
    {
      "epoch": 0.7316842602804474,
      "grad_norm": 1.0427320529930628,
      "learning_rate": 3.54424269853472e-06,
      "loss": 0.5361,
      "step": 8127
    },
    {
      "epoch": 0.7317742915663193,
      "grad_norm": 1.0061208104705814,
      "learning_rate": 3.5420159030233183e-06,
      "loss": 0.5509,
      "step": 8128
    },
    {
      "epoch": 0.7318643228521912,
      "grad_norm": 1.1592305373316445,
      "learning_rate": 3.5397896567028935e-06,
      "loss": 0.5699,
      "step": 8129
    },
    {
      "epoch": 0.731954354138063,
      "grad_norm": 1.2790895310274348,
      "learning_rate": 3.537563959762772e-06,
      "loss": 0.5341,
      "step": 8130
    },
    {
      "epoch": 0.7320443854239348,
      "grad_norm": 1.0511441365283463,
      "learning_rate": 3.5353388123922174e-06,
      "loss": 0.5237,
      "step": 8131
    },
    {
      "epoch": 0.7321344167098066,
      "grad_norm": 0.9961274084518082,
      "learning_rate": 3.53311421478047e-06,
      "loss": 0.5323,
      "step": 8132
    },
    {
      "epoch": 0.7322244479956785,
      "grad_norm": 0.9400838804637465,
      "learning_rate": 3.530890167116703e-06,
      "loss": 0.5469,
      "step": 8133
    },
    {
      "epoch": 0.7323144792815504,
      "grad_norm": 1.1147305171042645,
      "learning_rate": 3.5286666695900528e-06,
      "loss": 0.5321,
      "step": 8134
    },
    {
      "epoch": 0.7324045105674222,
      "grad_norm": 1.0399921066498234,
      "learning_rate": 3.5264437223896074e-06,
      "loss": 0.5148,
      "step": 8135
    },
    {
      "epoch": 0.732494541853294,
      "grad_norm": 0.897911589166843,
      "learning_rate": 3.524221325704411e-06,
      "loss": 0.5635,
      "step": 8136
    },
    {
      "epoch": 0.7325845731391658,
      "grad_norm": 1.1396210201700252,
      "learning_rate": 3.521999479723448e-06,
      "loss": 0.5072,
      "step": 8137
    },
    {
      "epoch": 0.7326746044250377,
      "grad_norm": 1.3972005218689136,
      "learning_rate": 3.5197781846356773e-06,
      "loss": 0.5087,
      "step": 8138
    },
    {
      "epoch": 0.7327646357109096,
      "grad_norm": 1.0073020643584607,
      "learning_rate": 3.5175574406299907e-06,
      "loss": 0.5921,
      "step": 8139
    },
    {
      "epoch": 0.7328546669967814,
      "grad_norm": 1.7141874821359335,
      "learning_rate": 3.5153372478952454e-06,
      "loss": 0.5355,
      "step": 8140
    },
    {
      "epoch": 0.7329446982826532,
      "grad_norm": 1.0949025914634263,
      "learning_rate": 3.5131176066202467e-06,
      "loss": 0.562,
      "step": 8141
    },
    {
      "epoch": 0.7330347295685251,
      "grad_norm": 1.0443428498730365,
      "learning_rate": 3.5108985169937537e-06,
      "loss": 0.5245,
      "step": 8142
    },
    {
      "epoch": 0.7331247608543969,
      "grad_norm": 1.0434015211490153,
      "learning_rate": 3.5086799792044812e-06,
      "loss": 0.5264,
      "step": 8143
    },
    {
      "epoch": 0.7332147921402687,
      "grad_norm": 1.0183928579914951,
      "learning_rate": 3.506461993441097e-06,
      "loss": 0.493,
      "step": 8144
    },
    {
      "epoch": 0.7333048234261406,
      "grad_norm": 1.1571500658980916,
      "learning_rate": 3.5042445598922105e-06,
      "loss": 0.4878,
      "step": 8145
    },
    {
      "epoch": 0.7333948547120124,
      "grad_norm": 1.512681040083162,
      "learning_rate": 3.5020276787464058e-06,
      "loss": 0.5554,
      "step": 8146
    },
    {
      "epoch": 0.7334848859978843,
      "grad_norm": 1.2930696391364391,
      "learning_rate": 3.4998113501922005e-06,
      "loss": 0.5741,
      "step": 8147
    },
    {
      "epoch": 0.7335749172837561,
      "grad_norm": 1.0170570987617276,
      "learning_rate": 3.4975955744180735e-06,
      "loss": 0.4915,
      "step": 8148
    },
    {
      "epoch": 0.7336649485696279,
      "grad_norm": 2.906773885921064,
      "learning_rate": 3.495380351612456e-06,
      "loss": 0.4919,
      "step": 8149
    },
    {
      "epoch": 0.7337549798554998,
      "grad_norm": 1.1543194273845936,
      "learning_rate": 3.493165681963736e-06,
      "loss": 0.4886,
      "step": 8150
    },
    {
      "epoch": 0.7338450111413716,
      "grad_norm": 1.3487839537813309,
      "learning_rate": 3.49095156566024e-06,
      "loss": 0.453,
      "step": 8151
    },
    {
      "epoch": 0.7339350424272435,
      "grad_norm": 1.3401890133052723,
      "learning_rate": 3.4887380028902716e-06,
      "loss": 0.4472,
      "step": 8152
    },
    {
      "epoch": 0.7340250737131153,
      "grad_norm": 1.156430338201757,
      "learning_rate": 3.4865249938420633e-06,
      "loss": 0.4652,
      "step": 8153
    },
    {
      "epoch": 0.7341151049989871,
      "grad_norm": 1.0788831855705447,
      "learning_rate": 3.4843125387038134e-06,
      "loss": 0.5438,
      "step": 8154
    },
    {
      "epoch": 0.734205136284859,
      "grad_norm": 0.9724593954198338,
      "learning_rate": 3.4821006376636712e-06,
      "loss": 0.4702,
      "step": 8155
    },
    {
      "epoch": 0.7342951675707309,
      "grad_norm": 0.9978186622656936,
      "learning_rate": 3.479889290909738e-06,
      "loss": 0.5425,
      "step": 8156
    },
    {
      "epoch": 0.7343851988566027,
      "grad_norm": 1.4984171808074191,
      "learning_rate": 3.477678498630069e-06,
      "loss": 0.569,
      "step": 8157
    },
    {
      "epoch": 0.7344752301424745,
      "grad_norm": 1.4189708680279585,
      "learning_rate": 3.4754682610126733e-06,
      "loss": 0.558,
      "step": 8158
    },
    {
      "epoch": 0.7345652614283463,
      "grad_norm": 1.0439765862513433,
      "learning_rate": 3.473258578245501e-06,
      "loss": 0.4654,
      "step": 8159
    },
    {
      "epoch": 0.7346552927142181,
      "grad_norm": 1.0456585665238678,
      "learning_rate": 3.4710494505164795e-06,
      "loss": 0.4779,
      "step": 8160
    },
    {
      "epoch": 0.7347453240000901,
      "grad_norm": 1.0841492320862423,
      "learning_rate": 3.468840878013462e-06,
      "loss": 0.6119,
      "step": 8161
    },
    {
      "epoch": 0.7348353552859619,
      "grad_norm": 1.1564581267038394,
      "learning_rate": 3.4666328609242728e-06,
      "loss": 0.6039,
      "step": 8162
    },
    {
      "epoch": 0.7349253865718337,
      "grad_norm": 1.1999630335166234,
      "learning_rate": 3.4644253994366816e-06,
      "loss": 0.4858,
      "step": 8163
    },
    {
      "epoch": 0.7350154178577055,
      "grad_norm": 1.041376095640155,
      "learning_rate": 3.4622184937384164e-06,
      "loss": 0.5271,
      "step": 8164
    },
    {
      "epoch": 0.7351054491435773,
      "grad_norm": 1.1750801324439255,
      "learning_rate": 3.460012144017143e-06,
      "loss": 0.5206,
      "step": 8165
    },
    {
      "epoch": 0.7351954804294493,
      "grad_norm": 0.9013244333513452,
      "learning_rate": 3.457806350460504e-06,
      "loss": 0.43,
      "step": 8166
    },
    {
      "epoch": 0.7352855117153211,
      "grad_norm": 1.0697821977519488,
      "learning_rate": 3.455601113256073e-06,
      "loss": 0.4784,
      "step": 8167
    },
    {
      "epoch": 0.7353755430011929,
      "grad_norm": 0.9692040130030736,
      "learning_rate": 3.453396432591386e-06,
      "loss": 0.5278,
      "step": 8168
    },
    {
      "epoch": 0.7354655742870647,
      "grad_norm": 1.056072386107653,
      "learning_rate": 3.4511923086539324e-06,
      "loss": 0.5693,
      "step": 8169
    },
    {
      "epoch": 0.7355556055729366,
      "grad_norm": 1.3195221753036706,
      "learning_rate": 3.4489887416311506e-06,
      "loss": 0.4712,
      "step": 8170
    },
    {
      "epoch": 0.7356456368588085,
      "grad_norm": 0.9036512322737932,
      "learning_rate": 3.4467857317104347e-06,
      "loss": 0.4935,
      "step": 8171
    },
    {
      "epoch": 0.7357356681446803,
      "grad_norm": 1.4002510389079332,
      "learning_rate": 3.4445832790791324e-06,
      "loss": 0.4992,
      "step": 8172
    },
    {
      "epoch": 0.7358256994305521,
      "grad_norm": 1.171164035886328,
      "learning_rate": 3.442381383924531e-06,
      "loss": 0.4735,
      "step": 8173
    },
    {
      "epoch": 0.7359157307164239,
      "grad_norm": 1.1004812019329515,
      "learning_rate": 3.4401800464338964e-06,
      "loss": 0.5683,
      "step": 8174
    },
    {
      "epoch": 0.7360057620022958,
      "grad_norm": 1.053754376168683,
      "learning_rate": 3.4379792667944203e-06,
      "loss": 0.5287,
      "step": 8175
    },
    {
      "epoch": 0.7360957932881677,
      "grad_norm": 1.4568550989375848,
      "learning_rate": 3.4357790451932615e-06,
      "loss": 0.4919,
      "step": 8176
    },
    {
      "epoch": 0.7361858245740395,
      "grad_norm": 1.1360790015164468,
      "learning_rate": 3.4335793818175288e-06,
      "loss": 0.5105,
      "step": 8177
    },
    {
      "epoch": 0.7362758558599113,
      "grad_norm": 1.0848642601591367,
      "learning_rate": 3.431380276854287e-06,
      "loss": 0.5459,
      "step": 8178
    },
    {
      "epoch": 0.7363658871457831,
      "grad_norm": 1.1974448833547662,
      "learning_rate": 3.4291817304905385e-06,
      "loss": 0.575,
      "step": 8179
    },
    {
      "epoch": 0.736455918431655,
      "grad_norm": 1.2315921761337454,
      "learning_rate": 3.4269837429132613e-06,
      "loss": 0.451,
      "step": 8180
    },
    {
      "epoch": 0.7365459497175268,
      "grad_norm": 0.9988437449877433,
      "learning_rate": 3.424786314309365e-06,
      "loss": 0.5096,
      "step": 8181
    },
    {
      "epoch": 0.7366359810033987,
      "grad_norm": 1.1117325155289497,
      "learning_rate": 3.422589444865724e-06,
      "loss": 0.4659,
      "step": 8182
    },
    {
      "epoch": 0.7367260122892705,
      "grad_norm": 0.9813651303464372,
      "learning_rate": 3.42039313476916e-06,
      "loss": 0.5427,
      "step": 8183
    },
    {
      "epoch": 0.7368160435751424,
      "grad_norm": 1.1189954507081603,
      "learning_rate": 3.41819738420645e-06,
      "loss": 0.5034,
      "step": 8184
    },
    {
      "epoch": 0.7369060748610142,
      "grad_norm": 1.0119328663781444,
      "learning_rate": 3.416002193364322e-06,
      "loss": 0.4864,
      "step": 8185
    },
    {
      "epoch": 0.736996106146886,
      "grad_norm": 0.8562350121639722,
      "learning_rate": 3.4138075624294575e-06,
      "loss": 0.504,
      "step": 8186
    },
    {
      "epoch": 0.7370861374327579,
      "grad_norm": 1.5301607683541172,
      "learning_rate": 3.4116134915884825e-06,
      "loss": 0.4654,
      "step": 8187
    },
    {
      "epoch": 0.7371761687186297,
      "grad_norm": 1.3056204048451179,
      "learning_rate": 3.4094199810279926e-06,
      "loss": 0.417,
      "step": 8188
    },
    {
      "epoch": 0.7372662000045016,
      "grad_norm": 1.277085141903494,
      "learning_rate": 3.407227030934517e-06,
      "loss": 0.562,
      "step": 8189
    },
    {
      "epoch": 0.7373562312903734,
      "grad_norm": 0.9353547724498025,
      "learning_rate": 3.405034641494548e-06,
      "loss": 0.4373,
      "step": 8190
    },
    {
      "epoch": 0.7374462625762452,
      "grad_norm": 1.045070687731512,
      "learning_rate": 3.402842812894529e-06,
      "loss": 0.433,
      "step": 8191
    },
    {
      "epoch": 0.737536293862117,
      "grad_norm": 1.0795976074629507,
      "learning_rate": 3.4006515453208567e-06,
      "loss": 0.5228,
      "step": 8192
    },
    {
      "epoch": 0.737626325147989,
      "grad_norm": 0.731817430775815,
      "learning_rate": 3.3984608389598672e-06,
      "loss": 0.5327,
      "step": 8193
    },
    {
      "epoch": 0.7377163564338608,
      "grad_norm": 1.3874417069020148,
      "learning_rate": 3.396270693997875e-06,
      "loss": 0.5244,
      "step": 8194
    },
    {
      "epoch": 0.7378063877197326,
      "grad_norm": 0.9480545693353175,
      "learning_rate": 3.39408111062112e-06,
      "loss": 0.46,
      "step": 8195
    },
    {
      "epoch": 0.7378964190056044,
      "grad_norm": 0.8678891368797012,
      "learning_rate": 3.391892089015809e-06,
      "loss": 0.4658,
      "step": 8196
    },
    {
      "epoch": 0.7379864502914762,
      "grad_norm": 0.916002207962922,
      "learning_rate": 3.3897036293680975e-06,
      "loss": 0.5591,
      "step": 8197
    },
    {
      "epoch": 0.7380764815773482,
      "grad_norm": 0.8922496544866276,
      "learning_rate": 3.387515731864095e-06,
      "loss": 0.5209,
      "step": 8198
    },
    {
      "epoch": 0.73816651286322,
      "grad_norm": 1.3490801453847823,
      "learning_rate": 3.3853283966898597e-06,
      "loss": 0.6237,
      "step": 8199
    },
    {
      "epoch": 0.7382565441490918,
      "grad_norm": 0.9629599571535502,
      "learning_rate": 3.3831416240314085e-06,
      "loss": 0.5105,
      "step": 8200
    },
    {
      "epoch": 0.7383465754349636,
      "grad_norm": 0.9330390791185079,
      "learning_rate": 3.3809554140746958e-06,
      "loss": 0.5342,
      "step": 8201
    },
    {
      "epoch": 0.7384366067208354,
      "grad_norm": 0.9683226371164549,
      "learning_rate": 3.3787697670056517e-06,
      "loss": 0.5396,
      "step": 8202
    },
    {
      "epoch": 0.7385266380067074,
      "grad_norm": 1.2319444844248923,
      "learning_rate": 3.3765846830101335e-06,
      "loss": 0.5244,
      "step": 8203
    },
    {
      "epoch": 0.7386166692925792,
      "grad_norm": 2.7978333800076216,
      "learning_rate": 3.3744001622739675e-06,
      "loss": 0.5202,
      "step": 8204
    },
    {
      "epoch": 0.738706700578451,
      "grad_norm": 1.1501955105227262,
      "learning_rate": 3.3722162049829267e-06,
      "loss": 0.5017,
      "step": 8205
    },
    {
      "epoch": 0.7387967318643228,
      "grad_norm": 0.9868724708920324,
      "learning_rate": 3.3700328113227365e-06,
      "loss": 0.5132,
      "step": 8206
    },
    {
      "epoch": 0.7388867631501947,
      "grad_norm": 1.460088475443318,
      "learning_rate": 3.3678499814790676e-06,
      "loss": 0.5316,
      "step": 8207
    },
    {
      "epoch": 0.7389767944360666,
      "grad_norm": 0.9603494645299789,
      "learning_rate": 3.3656677156375605e-06,
      "loss": 0.4884,
      "step": 8208
    },
    {
      "epoch": 0.7390668257219384,
      "grad_norm": 0.8569399824101295,
      "learning_rate": 3.3634860139837877e-06,
      "loss": 0.4706,
      "step": 8209
    },
    {
      "epoch": 0.7391568570078102,
      "grad_norm": 1.3232137863297802,
      "learning_rate": 3.3613048767032853e-06,
      "loss": 0.5111,
      "step": 8210
    },
    {
      "epoch": 0.739246888293682,
      "grad_norm": 0.983732823524417,
      "learning_rate": 3.359124303981538e-06,
      "loss": 0.5257,
      "step": 8211
    },
    {
      "epoch": 0.7393369195795539,
      "grad_norm": 0.7648013751745081,
      "learning_rate": 3.3569442960039833e-06,
      "loss": 0.564,
      "step": 8212
    },
    {
      "epoch": 0.7394269508654258,
      "grad_norm": 0.9805993490894137,
      "learning_rate": 3.3547648529560105e-06,
      "loss": 0.4988,
      "step": 8213
    },
    {
      "epoch": 0.7395169821512976,
      "grad_norm": 1.0351196470120823,
      "learning_rate": 3.352585975022965e-06,
      "loss": 0.477,
      "step": 8214
    },
    {
      "epoch": 0.7396070134371694,
      "grad_norm": 1.0095143907631041,
      "learning_rate": 3.3504076623901283e-06,
      "loss": 0.5536,
      "step": 8215
    },
    {
      "epoch": 0.7396970447230412,
      "grad_norm": 1.1513665606905092,
      "learning_rate": 3.34822991524276e-06,
      "loss": 0.5187,
      "step": 8216
    },
    {
      "epoch": 0.7397870760089131,
      "grad_norm": 1.6362904108746685,
      "learning_rate": 3.3460527337660477e-06,
      "loss": 0.5152,
      "step": 8217
    },
    {
      "epoch": 0.739877107294785,
      "grad_norm": 1.0112703524503834,
      "learning_rate": 3.343876118145142e-06,
      "loss": 0.4302,
      "step": 8218
    },
    {
      "epoch": 0.7399671385806568,
      "grad_norm": 1.8275854305107047,
      "learning_rate": 3.3417000685651437e-06,
      "loss": 0.5,
      "step": 8219
    },
    {
      "epoch": 0.7400571698665286,
      "grad_norm": 1.4216974720595532,
      "learning_rate": 3.339524585211109e-06,
      "loss": 0.5455,
      "step": 8220
    },
    {
      "epoch": 0.7401472011524005,
      "grad_norm": 1.329503911010981,
      "learning_rate": 3.337349668268034e-06,
      "loss": 0.6417,
      "step": 8221
    },
    {
      "epoch": 0.7402372324382723,
      "grad_norm": 1.5416498961187508,
      "learning_rate": 3.3351753179208857e-06,
      "loss": 0.551,
      "step": 8222
    },
    {
      "epoch": 0.7403272637241441,
      "grad_norm": 1.0040827511248538,
      "learning_rate": 3.333001534354564e-06,
      "loss": 0.5209,
      "step": 8223
    },
    {
      "epoch": 0.740417295010016,
      "grad_norm": 1.3994812609017633,
      "learning_rate": 3.3308283177539327e-06,
      "loss": 0.6481,
      "step": 8224
    },
    {
      "epoch": 0.7405073262958878,
      "grad_norm": 1.12779772170212,
      "learning_rate": 3.3286556683038008e-06,
      "loss": 0.5643,
      "step": 8225
    },
    {
      "epoch": 0.7405973575817597,
      "grad_norm": 0.9310574536653062,
      "learning_rate": 3.3264835861889343e-06,
      "loss": 0.4947,
      "step": 8226
    },
    {
      "epoch": 0.7406873888676315,
      "grad_norm": 1.1406018374024112,
      "learning_rate": 3.324312071594047e-06,
      "loss": 0.496,
      "step": 8227
    },
    {
      "epoch": 0.7407774201535033,
      "grad_norm": 1.1050157679769055,
      "learning_rate": 3.3221411247038103e-06,
      "loss": 0.4995,
      "step": 8228
    },
    {
      "epoch": 0.7408674514393752,
      "grad_norm": 0.9661277349998282,
      "learning_rate": 3.319970745702832e-06,
      "loss": 0.4764,
      "step": 8229
    },
    {
      "epoch": 0.740957482725247,
      "grad_norm": 0.9816638717707518,
      "learning_rate": 3.317800934775696e-06,
      "loss": 0.5025,
      "step": 8230
    },
    {
      "epoch": 0.7410475140111189,
      "grad_norm": 1.0667561614832812,
      "learning_rate": 3.3156316921069145e-06,
      "loss": 0.4654,
      "step": 8231
    },
    {
      "epoch": 0.7411375452969907,
      "grad_norm": 1.2164977834169566,
      "learning_rate": 3.3134630178809648e-06,
      "loss": 0.5109,
      "step": 8232
    },
    {
      "epoch": 0.7412275765828625,
      "grad_norm": 1.2671682172422074,
      "learning_rate": 3.3112949122822713e-06,
      "loss": 0.5404,
      "step": 8233
    },
    {
      "epoch": 0.7413176078687344,
      "grad_norm": 1.2597209408809384,
      "learning_rate": 3.3091273754952145e-06,
      "loss": 0.4888,
      "step": 8234
    },
    {
      "epoch": 0.7414076391546063,
      "grad_norm": 1.0189705855059557,
      "learning_rate": 3.306960407704115e-06,
      "loss": 0.5689,
      "step": 8235
    },
    {
      "epoch": 0.7414976704404781,
      "grad_norm": 1.0173297767532694,
      "learning_rate": 3.304794009093265e-06,
      "loss": 0.5346,
      "step": 8236
    },
    {
      "epoch": 0.7415877017263499,
      "grad_norm": 1.3517801487005074,
      "learning_rate": 3.302628179846886e-06,
      "loss": 0.5624,
      "step": 8237
    },
    {
      "epoch": 0.7416777330122217,
      "grad_norm": 1.4873615074775017,
      "learning_rate": 3.3004629201491656e-06,
      "loss": 0.5148,
      "step": 8238
    },
    {
      "epoch": 0.7417677642980935,
      "grad_norm": 1.0610790354702508,
      "learning_rate": 3.298298230184238e-06,
      "loss": 0.4698,
      "step": 8239
    },
    {
      "epoch": 0.7418577955839655,
      "grad_norm": 1.4921280729370434,
      "learning_rate": 3.296134110136191e-06,
      "loss": 0.5746,
      "step": 8240
    },
    {
      "epoch": 0.7419478268698373,
      "grad_norm": 1.1952726619277787,
      "learning_rate": 3.2939705601890616e-06,
      "loss": 0.4778,
      "step": 8241
    },
    {
      "epoch": 0.7420378581557091,
      "grad_norm": 1.0503763800183903,
      "learning_rate": 3.291807580526842e-06,
      "loss": 0.5258,
      "step": 8242
    },
    {
      "epoch": 0.7421278894415809,
      "grad_norm": 2.0933381506694007,
      "learning_rate": 3.2896451713334666e-06,
      "loss": 0.423,
      "step": 8243
    },
    {
      "epoch": 0.7422179207274527,
      "grad_norm": 1.0148103631565395,
      "learning_rate": 3.287483332792838e-06,
      "loss": 0.5783,
      "step": 8244
    },
    {
      "epoch": 0.7423079520133247,
      "grad_norm": 1.1209264461905326,
      "learning_rate": 3.285322065088792e-06,
      "loss": 0.501,
      "step": 8245
    },
    {
      "epoch": 0.7423979832991965,
      "grad_norm": 1.119650096553456,
      "learning_rate": 3.2831613684051268e-06,
      "loss": 0.5024,
      "step": 8246
    },
    {
      "epoch": 0.7424880145850683,
      "grad_norm": 1.145630146165288,
      "learning_rate": 3.281001242925589e-06,
      "loss": 0.4795,
      "step": 8247
    },
    {
      "epoch": 0.7425780458709401,
      "grad_norm": 1.3503289897147903,
      "learning_rate": 3.2788416888338814e-06,
      "loss": 0.5114,
      "step": 8248
    },
    {
      "epoch": 0.742668077156812,
      "grad_norm": 1.223141054212269,
      "learning_rate": 3.2766827063136432e-06,
      "loss": 0.5538,
      "step": 8249
    },
    {
      "epoch": 0.7427581084426839,
      "grad_norm": 1.4180520786523316,
      "learning_rate": 3.27452429554849e-06,
      "loss": 0.4238,
      "step": 8250
    },
    {
      "epoch": 0.7428481397285557,
      "grad_norm": 1.1647760925767958,
      "learning_rate": 3.2723664567219627e-06,
      "loss": 0.5315,
      "step": 8251
    },
    {
      "epoch": 0.7429381710144275,
      "grad_norm": 1.116208367196659,
      "learning_rate": 3.2702091900175693e-06,
      "loss": 0.5338,
      "step": 8252
    },
    {
      "epoch": 0.7430282023002993,
      "grad_norm": 1.4282585583152092,
      "learning_rate": 3.2680524956187654e-06,
      "loss": 0.5093,
      "step": 8253
    },
    {
      "epoch": 0.7431182335861712,
      "grad_norm": 1.4632893792641404,
      "learning_rate": 3.265896373708958e-06,
      "loss": 0.5519,
      "step": 8254
    },
    {
      "epoch": 0.7432082648720431,
      "grad_norm": 1.232785506580539,
      "learning_rate": 3.263740824471504e-06,
      "loss": 0.4391,
      "step": 8255
    },
    {
      "epoch": 0.7432982961579149,
      "grad_norm": 1.0525915055093003,
      "learning_rate": 3.2615858480897158e-06,
      "loss": 0.4125,
      "step": 8256
    },
    {
      "epoch": 0.7433883274437867,
      "grad_norm": 1.129363563313411,
      "learning_rate": 3.2594314447468457e-06,
      "loss": 0.5414,
      "step": 8257
    },
    {
      "epoch": 0.7434783587296585,
      "grad_norm": 1.1122541420314296,
      "learning_rate": 3.2572776146261166e-06,
      "loss": 0.5441,
      "step": 8258
    },
    {
      "epoch": 0.7435683900155304,
      "grad_norm": 0.8666235691454031,
      "learning_rate": 3.255124357910684e-06,
      "loss": 0.4296,
      "step": 8259
    },
    {
      "epoch": 0.7436584213014023,
      "grad_norm": 1.2047207448789143,
      "learning_rate": 3.2529716747836626e-06,
      "loss": 0.441,
      "step": 8260
    },
    {
      "epoch": 0.7437484525872741,
      "grad_norm": 1.0326597544349951,
      "learning_rate": 3.2508195654281205e-06,
      "loss": 0.5015,
      "step": 8261
    },
    {
      "epoch": 0.7438384838731459,
      "grad_norm": 1.4438945780800976,
      "learning_rate": 3.248668030027077e-06,
      "loss": 0.4641,
      "step": 8262
    },
    {
      "epoch": 0.7439285151590178,
      "grad_norm": 1.3663133526654192,
      "learning_rate": 3.2465170687634895e-06,
      "loss": 0.5199,
      "step": 8263
    },
    {
      "epoch": 0.7440185464448896,
      "grad_norm": 1.4080674154458555,
      "learning_rate": 3.24436668182029e-06,
      "loss": 0.5292,
      "step": 8264
    },
    {
      "epoch": 0.7441085777307614,
      "grad_norm": 0.9360593949174097,
      "learning_rate": 3.2422168693803404e-06,
      "loss": 0.4405,
      "step": 8265
    },
    {
      "epoch": 0.7441986090166333,
      "grad_norm": 1.40529048141523,
      "learning_rate": 3.2400676316264636e-06,
      "loss": 0.4414,
      "step": 8266
    },
    {
      "epoch": 0.7442886403025051,
      "grad_norm": 1.1912360843218144,
      "learning_rate": 3.2379189687414326e-06,
      "loss": 0.5715,
      "step": 8267
    },
    {
      "epoch": 0.744378671588377,
      "grad_norm": 1.2515407315398184,
      "learning_rate": 3.2357708809079726e-06,
      "loss": 0.5661,
      "step": 8268
    },
    {
      "epoch": 0.7444687028742488,
      "grad_norm": 1.5090328164903237,
      "learning_rate": 3.2336233683087557e-06,
      "loss": 0.5048,
      "step": 8269
    },
    {
      "epoch": 0.7445587341601206,
      "grad_norm": 0.9976809502493512,
      "learning_rate": 3.2314764311264134e-06,
      "loss": 0.5055,
      "step": 8270
    },
    {
      "epoch": 0.7446487654459925,
      "grad_norm": 1.293445936761954,
      "learning_rate": 3.2293300695435117e-06,
      "loss": 0.5514,
      "step": 8271
    },
    {
      "epoch": 0.7447387967318643,
      "grad_norm": 0.9699675883459896,
      "learning_rate": 3.2271842837425917e-06,
      "loss": 0.4865,
      "step": 8272
    },
    {
      "epoch": 0.7448288280177362,
      "grad_norm": 2.4781267022311217,
      "learning_rate": 3.225039073906121e-06,
      "loss": 0.5777,
      "step": 8273
    },
    {
      "epoch": 0.744918859303608,
      "grad_norm": 0.9535356907456417,
      "learning_rate": 3.222894440216536e-06,
      "loss": 0.6266,
      "step": 8274
    },
    {
      "epoch": 0.7450088905894798,
      "grad_norm": 1.0749451784982236,
      "learning_rate": 3.2207503828562147e-06,
      "loss": 0.4725,
      "step": 8275
    },
    {
      "epoch": 0.7450989218753516,
      "grad_norm": 0.9709017760839876,
      "learning_rate": 3.2186069020074952e-06,
      "loss": 0.5883,
      "step": 8276
    },
    {
      "epoch": 0.7451889531612236,
      "grad_norm": 1.0124505862487754,
      "learning_rate": 3.216463997852648e-06,
      "loss": 0.5099,
      "step": 8277
    },
    {
      "epoch": 0.7452789844470954,
      "grad_norm": 1.2211527899564496,
      "learning_rate": 3.214321670573921e-06,
      "loss": 0.532,
      "step": 8278
    },
    {
      "epoch": 0.7453690157329672,
      "grad_norm": 1.1030739823563176,
      "learning_rate": 3.2121799203534896e-06,
      "loss": 0.5903,
      "step": 8279
    },
    {
      "epoch": 0.745459047018839,
      "grad_norm": 0.972349655589007,
      "learning_rate": 3.210038747373493e-06,
      "loss": 0.4755,
      "step": 8280
    },
    {
      "epoch": 0.7455490783047108,
      "grad_norm": 1.876298957389056,
      "learning_rate": 3.2078981518160167e-06,
      "loss": 0.4897,
      "step": 8281
    },
    {
      "epoch": 0.7456391095905828,
      "grad_norm": 1.0756886677626663,
      "learning_rate": 3.2057581338631007e-06,
      "loss": 0.5388,
      "step": 8282
    },
    {
      "epoch": 0.7457291408764546,
      "grad_norm": 1.0678496790949243,
      "learning_rate": 3.203618693696731e-06,
      "loss": 0.6379,
      "step": 8283
    },
    {
      "epoch": 0.7458191721623264,
      "grad_norm": 0.8969997238292958,
      "learning_rate": 3.2014798314988503e-06,
      "loss": 0.562,
      "step": 8284
    },
    {
      "epoch": 0.7459092034481982,
      "grad_norm": 1.3621595562388604,
      "learning_rate": 3.19934154745134e-06,
      "loss": 0.5281,
      "step": 8285
    },
    {
      "epoch": 0.74599923473407,
      "grad_norm": 1.3055503474891528,
      "learning_rate": 3.197203841736054e-06,
      "loss": 0.5662,
      "step": 8286
    },
    {
      "epoch": 0.746089266019942,
      "grad_norm": 1.2711987877521227,
      "learning_rate": 3.1950667145347737e-06,
      "loss": 0.4041,
      "step": 8287
    },
    {
      "epoch": 0.7461792973058138,
      "grad_norm": 1.1400898383565474,
      "learning_rate": 3.1929301660292467e-06,
      "loss": 0.5387,
      "step": 8288
    },
    {
      "epoch": 0.7462693285916856,
      "grad_norm": 1.2872089700559082,
      "learning_rate": 3.1907941964011634e-06,
      "loss": 0.5648,
      "step": 8289
    },
    {
      "epoch": 0.7463593598775574,
      "grad_norm": 1.0104936915215306,
      "learning_rate": 3.1886588058321743e-06,
      "loss": 0.5081,
      "step": 8290
    },
    {
      "epoch": 0.7464493911634293,
      "grad_norm": 1.1279974835139894,
      "learning_rate": 3.186523994503863e-06,
      "loss": 0.5532,
      "step": 8291
    },
    {
      "epoch": 0.7465394224493012,
      "grad_norm": 0.950562048634876,
      "learning_rate": 3.1843897625977883e-06,
      "loss": 0.5591,
      "step": 8292
    },
    {
      "epoch": 0.746629453735173,
      "grad_norm": 0.9935754939888166,
      "learning_rate": 3.1822561102954373e-06,
      "loss": 0.482,
      "step": 8293
    },
    {
      "epoch": 0.7467194850210448,
      "grad_norm": 1.5205738714727957,
      "learning_rate": 3.18012303777826e-06,
      "loss": 0.4961,
      "step": 8294
    },
    {
      "epoch": 0.7468095163069166,
      "grad_norm": 1.220410961855372,
      "learning_rate": 3.1779905452276547e-06,
      "loss": 0.5021,
      "step": 8295
    },
    {
      "epoch": 0.7468995475927885,
      "grad_norm": 1.0492387833620216,
      "learning_rate": 3.1758586328249684e-06,
      "loss": 0.5716,
      "step": 8296
    },
    {
      "epoch": 0.7469895788786604,
      "grad_norm": 1.133285840390853,
      "learning_rate": 3.173727300751502e-06,
      "loss": 0.5441,
      "step": 8297
    },
    {
      "epoch": 0.7470796101645322,
      "grad_norm": 1.037327243419387,
      "learning_rate": 3.1715965491885092e-06,
      "loss": 0.4705,
      "step": 8298
    },
    {
      "epoch": 0.747169641450404,
      "grad_norm": 0.7823313134398736,
      "learning_rate": 3.1694663783171775e-06,
      "loss": 0.5574,
      "step": 8299
    },
    {
      "epoch": 0.7472596727362758,
      "grad_norm": 0.9672888795322699,
      "learning_rate": 3.1673367883186735e-06,
      "loss": 0.5847,
      "step": 8300
    },
    {
      "epoch": 0.7473497040221477,
      "grad_norm": 0.8282199236722743,
      "learning_rate": 3.165207779374089e-06,
      "loss": 0.459,
      "step": 8301
    },
    {
      "epoch": 0.7474397353080195,
      "grad_norm": 0.8326017092305739,
      "learning_rate": 3.1630793516644787e-06,
      "loss": 0.5379,
      "step": 8302
    },
    {
      "epoch": 0.7475297665938914,
      "grad_norm": 1.028243027928499,
      "learning_rate": 3.160951505370845e-06,
      "loss": 0.5423,
      "step": 8303
    },
    {
      "epoch": 0.7476197978797632,
      "grad_norm": 1.8277632263846657,
      "learning_rate": 3.1588242406741455e-06,
      "loss": 0.6455,
      "step": 8304
    },
    {
      "epoch": 0.7477098291656351,
      "grad_norm": 0.9264453978616658,
      "learning_rate": 3.156697557755275e-06,
      "loss": 0.4229,
      "step": 8305
    },
    {
      "epoch": 0.7477998604515069,
      "grad_norm": 0.9625069473770554,
      "learning_rate": 3.1545714567950993e-06,
      "loss": 0.5615,
      "step": 8306
    },
    {
      "epoch": 0.7478898917373787,
      "grad_norm": 1.2552043410923277,
      "learning_rate": 3.1524459379744133e-06,
      "loss": 0.5753,
      "step": 8307
    },
    {
      "epoch": 0.7479799230232506,
      "grad_norm": 1.1323742731951172,
      "learning_rate": 3.150321001473977e-06,
      "loss": 0.5296,
      "step": 8308
    },
    {
      "epoch": 0.7480699543091224,
      "grad_norm": 1.3759853526499901,
      "learning_rate": 3.1481966474744973e-06,
      "loss": 0.5179,
      "step": 8309
    },
    {
      "epoch": 0.7481599855949943,
      "grad_norm": 0.9325096200029184,
      "learning_rate": 3.1460728761566284e-06,
      "loss": 0.5025,
      "step": 8310
    },
    {
      "epoch": 0.7482500168808661,
      "grad_norm": 0.8808421130911492,
      "learning_rate": 3.143949687700978e-06,
      "loss": 0.5048,
      "step": 8311
    },
    {
      "epoch": 0.7483400481667379,
      "grad_norm": 0.9529458487775647,
      "learning_rate": 3.141827082288107e-06,
      "loss": 0.4946,
      "step": 8312
    },
    {
      "epoch": 0.7484300794526098,
      "grad_norm": 1.494595783537093,
      "learning_rate": 3.1397050600985124e-06,
      "loss": 0.5346,
      "step": 8313
    },
    {
      "epoch": 0.7485201107384816,
      "grad_norm": 1.1089706888435367,
      "learning_rate": 3.1375836213126653e-06,
      "loss": 0.6102,
      "step": 8314
    },
    {
      "epoch": 0.7486101420243535,
      "grad_norm": 0.8932997649503375,
      "learning_rate": 3.135462766110966e-06,
      "loss": 0.4964,
      "step": 8315
    },
    {
      "epoch": 0.7487001733102253,
      "grad_norm": 1.080360417056061,
      "learning_rate": 3.1333424946737757e-06,
      "loss": 0.5231,
      "step": 8316
    },
    {
      "epoch": 0.7487902045960971,
      "grad_norm": 1.0330831517293255,
      "learning_rate": 3.131222807181403e-06,
      "loss": 0.4367,
      "step": 8317
    },
    {
      "epoch": 0.748880235881969,
      "grad_norm": 1.6047936003292715,
      "learning_rate": 3.1291037038141115e-06,
      "loss": 0.5146,
      "step": 8318
    },
    {
      "epoch": 0.7489702671678409,
      "grad_norm": 1.0566598934029257,
      "learning_rate": 3.1269851847521004e-06,
      "loss": 0.5388,
      "step": 8319
    },
    {
      "epoch": 0.7490602984537127,
      "grad_norm": 1.0178618798681354,
      "learning_rate": 3.124867250175544e-06,
      "loss": 0.5208,
      "step": 8320
    },
    {
      "epoch": 0.7491503297395845,
      "grad_norm": 0.8410390343617581,
      "learning_rate": 3.122749900264541e-06,
      "loss": 0.5055,
      "step": 8321
    },
    {
      "epoch": 0.7492403610254563,
      "grad_norm": 1.0360328451687593,
      "learning_rate": 3.120633135199158e-06,
      "loss": 0.496,
      "step": 8322
    },
    {
      "epoch": 0.7493303923113281,
      "grad_norm": 1.6590900115373786,
      "learning_rate": 3.1185169551594053e-06,
      "loss": 0.5677,
      "step": 8323
    },
    {
      "epoch": 0.7494204235972001,
      "grad_norm": 1.068039292251591,
      "learning_rate": 3.116401360325243e-06,
      "loss": 0.3899,
      "step": 8324
    },
    {
      "epoch": 0.7495104548830719,
      "grad_norm": 0.902032018103438,
      "learning_rate": 3.114286350876584e-06,
      "loss": 0.5947,
      "step": 8325
    },
    {
      "epoch": 0.7496004861689437,
      "grad_norm": 1.077366667040878,
      "learning_rate": 3.1121719269932914e-06,
      "loss": 0.6749,
      "step": 8326
    },
    {
      "epoch": 0.7496905174548155,
      "grad_norm": 1.0326757371207596,
      "learning_rate": 3.11005808885517e-06,
      "loss": 0.5071,
      "step": 8327
    },
    {
      "epoch": 0.7497805487406873,
      "grad_norm": 1.5252522042372398,
      "learning_rate": 3.107944836641993e-06,
      "loss": 0.5333,
      "step": 8328
    },
    {
      "epoch": 0.7498705800265593,
      "grad_norm": 1.0703604731045806,
      "learning_rate": 3.1058321705334626e-06,
      "loss": 0.5145,
      "step": 8329
    },
    {
      "epoch": 0.7499606113124311,
      "grad_norm": 1.019912749489202,
      "learning_rate": 3.103720090709247e-06,
      "loss": 0.5207,
      "step": 8330
    },
    {
      "epoch": 0.7500506425983029,
      "grad_norm": 1.2345752464987023,
      "learning_rate": 3.101608597348955e-06,
      "loss": 0.4111,
      "step": 8331
    },
    {
      "epoch": 0.7501406738841747,
      "grad_norm": 1.0399406544375203,
      "learning_rate": 3.099497690632156e-06,
      "loss": 0.5484,
      "step": 8332
    },
    {
      "epoch": 0.7502307051700466,
      "grad_norm": 0.992022843011272,
      "learning_rate": 3.0973873707383518e-06,
      "loss": 0.5368,
      "step": 8333
    },
    {
      "epoch": 0.7503207364559185,
      "grad_norm": 1.0176528409121688,
      "learning_rate": 3.095277637847017e-06,
      "loss": 0.4785,
      "step": 8334
    },
    {
      "epoch": 0.7504107677417903,
      "grad_norm": 1.9040225443028318,
      "learning_rate": 3.0931684921375572e-06,
      "loss": 0.5428,
      "step": 8335
    },
    {
      "epoch": 0.7505007990276621,
      "grad_norm": 1.1435912489823596,
      "learning_rate": 3.0910599337893375e-06,
      "loss": 0.5161,
      "step": 8336
    },
    {
      "epoch": 0.7505908303135339,
      "grad_norm": 0.9944129913090601,
      "learning_rate": 3.08895196298167e-06,
      "loss": 0.4998,
      "step": 8337
    },
    {
      "epoch": 0.7506808615994058,
      "grad_norm": 1.08599929641245,
      "learning_rate": 3.0868445798938195e-06,
      "loss": 0.5844,
      "step": 8338
    },
    {
      "epoch": 0.7507708928852777,
      "grad_norm": 0.9978692008051573,
      "learning_rate": 3.0847377847049996e-06,
      "loss": 0.4994,
      "step": 8339
    },
    {
      "epoch": 0.7508609241711495,
      "grad_norm": 1.3444749148292423,
      "learning_rate": 3.082631577594375e-06,
      "loss": 0.4546,
      "step": 8340
    },
    {
      "epoch": 0.7509509554570213,
      "grad_norm": 1.6910534966732649,
      "learning_rate": 3.0805259587410496e-06,
      "loss": 0.6051,
      "step": 8341
    },
    {
      "epoch": 0.7510409867428931,
      "grad_norm": 1.1871263696615197,
      "learning_rate": 3.0784209283240995e-06,
      "loss": 0.5411,
      "step": 8342
    },
    {
      "epoch": 0.751131018028765,
      "grad_norm": 1.2650500439561387,
      "learning_rate": 3.076316486522529e-06,
      "loss": 0.6217,
      "step": 8343
    },
    {
      "epoch": 0.7512210493146368,
      "grad_norm": 1.1166380012644461,
      "learning_rate": 3.074212633515303e-06,
      "loss": 0.6321,
      "step": 8344
    },
    {
      "epoch": 0.7513110806005087,
      "grad_norm": 1.0277791829550242,
      "learning_rate": 3.0721093694813364e-06,
      "loss": 0.4822,
      "step": 8345
    },
    {
      "epoch": 0.7514011118863805,
      "grad_norm": 0.8429664672066839,
      "learning_rate": 3.070006694599492e-06,
      "loss": 0.4352,
      "step": 8346
    },
    {
      "epoch": 0.7514911431722524,
      "grad_norm": 0.9713778820962108,
      "learning_rate": 3.0679046090485765e-06,
      "loss": 0.4368,
      "step": 8347
    },
    {
      "epoch": 0.7515811744581242,
      "grad_norm": 1.0878767934375564,
      "learning_rate": 3.065803113007364e-06,
      "loss": 0.4678,
      "step": 8348
    },
    {
      "epoch": 0.751671205743996,
      "grad_norm": 1.0760937581278727,
      "learning_rate": 3.0637022066545563e-06,
      "loss": 0.5614,
      "step": 8349
    },
    {
      "epoch": 0.7517612370298679,
      "grad_norm": 0.9296976898027123,
      "learning_rate": 3.0616018901688216e-06,
      "loss": 0.5511,
      "step": 8350
    },
    {
      "epoch": 0.7518512683157397,
      "grad_norm": 1.142858195723065,
      "learning_rate": 3.0595021637287694e-06,
      "loss": 0.4682,
      "step": 8351
    },
    {
      "epoch": 0.7519412996016116,
      "grad_norm": 0.9858534217067584,
      "learning_rate": 3.057403027512963e-06,
      "loss": 0.4691,
      "step": 8352
    },
    {
      "epoch": 0.7520313308874834,
      "grad_norm": 1.5444223693612547,
      "learning_rate": 3.0553044816999133e-06,
      "loss": 0.5667,
      "step": 8353
    },
    {
      "epoch": 0.7521213621733552,
      "grad_norm": 1.247032646463229,
      "learning_rate": 3.0532065264680865e-06,
      "loss": 0.4694,
      "step": 8354
    },
    {
      "epoch": 0.752211393459227,
      "grad_norm": 0.8053017715092992,
      "learning_rate": 3.0511091619958875e-06,
      "loss": 0.4657,
      "step": 8355
    },
    {
      "epoch": 0.7523014247450989,
      "grad_norm": 1.3687622520881486,
      "learning_rate": 3.0490123884616795e-06,
      "loss": 0.5662,
      "step": 8356
    },
    {
      "epoch": 0.7523914560309708,
      "grad_norm": 1.259190387560964,
      "learning_rate": 3.0469162060437752e-06,
      "loss": 0.6104,
      "step": 8357
    },
    {
      "epoch": 0.7524814873168426,
      "grad_norm": 1.2131687937741304,
      "learning_rate": 3.044820614920433e-06,
      "loss": 0.4745,
      "step": 8358
    },
    {
      "epoch": 0.7525715186027144,
      "grad_norm": 0.996012558152521,
      "learning_rate": 3.0427256152698647e-06,
      "loss": 0.5686,
      "step": 8359
    },
    {
      "epoch": 0.7526615498885862,
      "grad_norm": 1.0842041984422899,
      "learning_rate": 3.0406312072702335e-06,
      "loss": 0.4885,
      "step": 8360
    },
    {
      "epoch": 0.7527515811744582,
      "grad_norm": 0.9825751563161964,
      "learning_rate": 3.0385373910996407e-06,
      "loss": 0.4887,
      "step": 8361
    },
    {
      "epoch": 0.75284161246033,
      "grad_norm": 0.8477371372658953,
      "learning_rate": 3.0364441669361554e-06,
      "loss": 0.5633,
      "step": 8362
    },
    {
      "epoch": 0.7529316437462018,
      "grad_norm": 0.9958096332760776,
      "learning_rate": 3.0343515349577802e-06,
      "loss": 0.4365,
      "step": 8363
    },
    {
      "epoch": 0.7530216750320736,
      "grad_norm": 1.0118996701808567,
      "learning_rate": 3.0322594953424744e-06,
      "loss": 0.4967,
      "step": 8364
    },
    {
      "epoch": 0.7531117063179454,
      "grad_norm": 0.9486460464722519,
      "learning_rate": 3.0301680482681485e-06,
      "loss": 0.5338,
      "step": 8365
    },
    {
      "epoch": 0.7532017376038174,
      "grad_norm": 1.0243458732839754,
      "learning_rate": 3.02807719391266e-06,
      "loss": 0.4708,
      "step": 8366
    },
    {
      "epoch": 0.7532917688896892,
      "grad_norm": 0.9901195203285161,
      "learning_rate": 3.0259869324538148e-06,
      "loss": 0.5341,
      "step": 8367
    },
    {
      "epoch": 0.753381800175561,
      "grad_norm": 1.0560631508367482,
      "learning_rate": 3.0238972640693764e-06,
      "loss": 0.512,
      "step": 8368
    },
    {
      "epoch": 0.7534718314614328,
      "grad_norm": 1.0143691095126304,
      "learning_rate": 3.021808188937042e-06,
      "loss": 0.4857,
      "step": 8369
    },
    {
      "epoch": 0.7535618627473046,
      "grad_norm": 1.1125735315235359,
      "learning_rate": 3.019719707234472e-06,
      "loss": 0.6287,
      "step": 8370
    },
    {
      "epoch": 0.7536518940331766,
      "grad_norm": 1.025623147697374,
      "learning_rate": 3.017631819139273e-06,
      "loss": 0.5525,
      "step": 8371
    },
    {
      "epoch": 0.7537419253190484,
      "grad_norm": 1.1961764975060243,
      "learning_rate": 3.0155445248289995e-06,
      "loss": 0.4011,
      "step": 8372
    },
    {
      "epoch": 0.7538319566049202,
      "grad_norm": 0.9663564573457178,
      "learning_rate": 3.013457824481156e-06,
      "loss": 0.4974,
      "step": 8373
    },
    {
      "epoch": 0.753921987890792,
      "grad_norm": 1.1424605392993084,
      "learning_rate": 3.0113717182732018e-06,
      "loss": 0.5141,
      "step": 8374
    },
    {
      "epoch": 0.7540120191766639,
      "grad_norm": 1.4129337786896288,
      "learning_rate": 3.009286206382529e-06,
      "loss": 0.5642,
      "step": 8375
    },
    {
      "epoch": 0.7541020504625358,
      "grad_norm": 1.0631030442159952,
      "learning_rate": 3.0072012889865056e-06,
      "loss": 0.4833,
      "step": 8376
    },
    {
      "epoch": 0.7541920817484076,
      "grad_norm": 1.6806159083917176,
      "learning_rate": 3.0051169662624224e-06,
      "loss": 0.5469,
      "step": 8377
    },
    {
      "epoch": 0.7542821130342794,
      "grad_norm": 1.1780121402808703,
      "learning_rate": 3.003033238387536e-06,
      "loss": 0.5839,
      "step": 8378
    },
    {
      "epoch": 0.7543721443201512,
      "grad_norm": 1.0451711909967516,
      "learning_rate": 3.0009501055390466e-06,
      "loss": 0.5488,
      "step": 8379
    },
    {
      "epoch": 0.7544621756060231,
      "grad_norm": 0.9442166742808333,
      "learning_rate": 2.998867567894108e-06,
      "loss": 0.4608,
      "step": 8380
    },
    {
      "epoch": 0.754552206891895,
      "grad_norm": 1.0707630832985955,
      "learning_rate": 2.996785625629818e-06,
      "loss": 0.558,
      "step": 8381
    },
    {
      "epoch": 0.7546422381777668,
      "grad_norm": 1.214832900193286,
      "learning_rate": 2.9947042789232307e-06,
      "loss": 0.5934,
      "step": 8382
    },
    {
      "epoch": 0.7547322694636386,
      "grad_norm": 1.1934064180751496,
      "learning_rate": 2.9926235279513373e-06,
      "loss": 0.603,
      "step": 8383
    },
    {
      "epoch": 0.7548223007495104,
      "grad_norm": 1.3970701233400264,
      "learning_rate": 2.9905433728910903e-06,
      "loss": 0.5288,
      "step": 8384
    },
    {
      "epoch": 0.7549123320353823,
      "grad_norm": 2.416132107068697,
      "learning_rate": 2.988463813919388e-06,
      "loss": 0.4941,
      "step": 8385
    },
    {
      "epoch": 0.7550023633212541,
      "grad_norm": 1.0055861036625608,
      "learning_rate": 2.9863848512130768e-06,
      "loss": 0.5601,
      "step": 8386
    },
    {
      "epoch": 0.755092394607126,
      "grad_norm": 0.9738228617823754,
      "learning_rate": 2.9843064849489524e-06,
      "loss": 0.5285,
      "step": 8387
    },
    {
      "epoch": 0.7551824258929978,
      "grad_norm": 1.1866911273778682,
      "learning_rate": 2.9822287153037643e-06,
      "loss": 0.4375,
      "step": 8388
    },
    {
      "epoch": 0.7552724571788697,
      "grad_norm": 1.0481440275214908,
      "learning_rate": 2.9801515424541973e-06,
      "loss": 0.5596,
      "step": 8389
    },
    {
      "epoch": 0.7553624884647415,
      "grad_norm": 1.3300385007014128,
      "learning_rate": 2.978074966576908e-06,
      "loss": 0.5237,
      "step": 8390
    },
    {
      "epoch": 0.7554525197506133,
      "grad_norm": 1.0613923090799957,
      "learning_rate": 2.9759989878484807e-06,
      "loss": 0.5194,
      "step": 8391
    },
    {
      "epoch": 0.7555425510364852,
      "grad_norm": 1.1768669364591222,
      "learning_rate": 2.9739236064454626e-06,
      "loss": 0.4858,
      "step": 8392
    },
    {
      "epoch": 0.755632582322357,
      "grad_norm": 1.0184437089063654,
      "learning_rate": 2.971848822544342e-06,
      "loss": 0.4885,
      "step": 8393
    },
    {
      "epoch": 0.7557226136082289,
      "grad_norm": 0.9695775165194797,
      "learning_rate": 2.969774636321562e-06,
      "loss": 0.4747,
      "step": 8394
    },
    {
      "epoch": 0.7558126448941007,
      "grad_norm": 1.1550646031864067,
      "learning_rate": 2.9677010479535127e-06,
      "loss": 0.5559,
      "step": 8395
    },
    {
      "epoch": 0.7559026761799725,
      "grad_norm": 0.9066181665487784,
      "learning_rate": 2.9656280576165354e-06,
      "loss": 0.4856,
      "step": 8396
    },
    {
      "epoch": 0.7559927074658443,
      "grad_norm": 0.8086545656236173,
      "learning_rate": 2.9635556654869134e-06,
      "loss": 0.4267,
      "step": 8397
    },
    {
      "epoch": 0.7560827387517163,
      "grad_norm": 0.9170683181094703,
      "learning_rate": 2.9614838717408866e-06,
      "loss": 0.463,
      "step": 8398
    },
    {
      "epoch": 0.7561727700375881,
      "grad_norm": 1.6747549030796414,
      "learning_rate": 2.9594126765546416e-06,
      "loss": 0.5809,
      "step": 8399
    },
    {
      "epoch": 0.7562628013234599,
      "grad_norm": 1.5443232144030925,
      "learning_rate": 2.9573420801043153e-06,
      "loss": 0.5056,
      "step": 8400
    },
    {
      "epoch": 0.7563528326093317,
      "grad_norm": 1.1867817570546193,
      "learning_rate": 2.955272082565991e-06,
      "loss": 0.4691,
      "step": 8401
    },
    {
      "epoch": 0.7564428638952035,
      "grad_norm": 0.8961423731513162,
      "learning_rate": 2.9532026841157068e-06,
      "loss": 0.5388,
      "step": 8402
    },
    {
      "epoch": 0.7565328951810755,
      "grad_norm": 1.5049756989467837,
      "learning_rate": 2.9511338849294345e-06,
      "loss": 0.5183,
      "step": 8403
    },
    {
      "epoch": 0.7566229264669473,
      "grad_norm": 1.1799501821116631,
      "learning_rate": 2.949065685183121e-06,
      "loss": 0.5104,
      "step": 8404
    },
    {
      "epoch": 0.7567129577528191,
      "grad_norm": 1.5164371308295668,
      "learning_rate": 2.946998085052636e-06,
      "loss": 0.4847,
      "step": 8405
    },
    {
      "epoch": 0.7568029890386909,
      "grad_norm": 1.2516810377737015,
      "learning_rate": 2.9449310847138134e-06,
      "loss": 0.5381,
      "step": 8406
    },
    {
      "epoch": 0.7568930203245627,
      "grad_norm": 0.9928460140106675,
      "learning_rate": 2.9428646843424326e-06,
      "loss": 0.4462,
      "step": 8407
    },
    {
      "epoch": 0.7569830516104347,
      "grad_norm": 1.0147960923732633,
      "learning_rate": 2.940798884114221e-06,
      "loss": 0.4693,
      "step": 8408
    },
    {
      "epoch": 0.7570730828963065,
      "grad_norm": 1.4816529546566162,
      "learning_rate": 2.9387336842048555e-06,
      "loss": 0.6072,
      "step": 8409
    },
    {
      "epoch": 0.7571631141821783,
      "grad_norm": 1.2445572609715982,
      "learning_rate": 2.936669084789966e-06,
      "loss": 0.5772,
      "step": 8410
    },
    {
      "epoch": 0.7572531454680501,
      "grad_norm": 1.548405783616658,
      "learning_rate": 2.934605086045119e-06,
      "loss": 0.615,
      "step": 8411
    },
    {
      "epoch": 0.757343176753922,
      "grad_norm": 1.0932623081143926,
      "learning_rate": 2.9325416881458433e-06,
      "loss": 0.637,
      "step": 8412
    },
    {
      "epoch": 0.7574332080397939,
      "grad_norm": 1.200439549702219,
      "learning_rate": 2.9304788912676107e-06,
      "loss": 0.4506,
      "step": 8413
    },
    {
      "epoch": 0.7575232393256657,
      "grad_norm": 1.0883372752728755,
      "learning_rate": 2.928416695585844e-06,
      "loss": 0.5584,
      "step": 8414
    },
    {
      "epoch": 0.7576132706115375,
      "grad_norm": 1.1990098262093671,
      "learning_rate": 2.926355101275912e-06,
      "loss": 0.474,
      "step": 8415
    },
    {
      "epoch": 0.7577033018974093,
      "grad_norm": 1.2106973993588224,
      "learning_rate": 2.924294108513137e-06,
      "loss": 0.4458,
      "step": 8416
    },
    {
      "epoch": 0.7577933331832812,
      "grad_norm": 1.1845375811497068,
      "learning_rate": 2.922233717472779e-06,
      "loss": 0.5995,
      "step": 8417
    },
    {
      "epoch": 0.757883364469153,
      "grad_norm": 1.2887402857987214,
      "learning_rate": 2.9201739283300666e-06,
      "loss": 0.6009,
      "step": 8418
    },
    {
      "epoch": 0.7579733957550249,
      "grad_norm": 1.013752667283169,
      "learning_rate": 2.918114741260156e-06,
      "loss": 0.537,
      "step": 8419
    },
    {
      "epoch": 0.7580634270408967,
      "grad_norm": 1.176458252177214,
      "learning_rate": 2.916056156438165e-06,
      "loss": 0.4872,
      "step": 8420
    },
    {
      "epoch": 0.7581534583267685,
      "grad_norm": 1.280602392123427,
      "learning_rate": 2.9139981740391587e-06,
      "loss": 0.5467,
      "step": 8421
    },
    {
      "epoch": 0.7582434896126404,
      "grad_norm": 1.0007604993112782,
      "learning_rate": 2.9119407942381463e-06,
      "loss": 0.4658,
      "step": 8422
    },
    {
      "epoch": 0.7583335208985122,
      "grad_norm": 1.2124279823880542,
      "learning_rate": 2.9098840172100907e-06,
      "loss": 0.573,
      "step": 8423
    },
    {
      "epoch": 0.7584235521843841,
      "grad_norm": 1.0109780845286422,
      "learning_rate": 2.9078278431299033e-06,
      "loss": 0.5402,
      "step": 8424
    },
    {
      "epoch": 0.7585135834702559,
      "grad_norm": 1.018820572261562,
      "learning_rate": 2.905772272172438e-06,
      "loss": 0.5396,
      "step": 8425
    },
    {
      "epoch": 0.7586036147561278,
      "grad_norm": 0.9637911105747189,
      "learning_rate": 2.9037173045125023e-06,
      "loss": 0.5694,
      "step": 8426
    },
    {
      "epoch": 0.7586936460419996,
      "grad_norm": 1.059667453056816,
      "learning_rate": 2.901662940324854e-06,
      "loss": 0.4068,
      "step": 8427
    },
    {
      "epoch": 0.7587836773278714,
      "grad_norm": 0.8875037309897582,
      "learning_rate": 2.8996091797841976e-06,
      "loss": 0.4101,
      "step": 8428
    },
    {
      "epoch": 0.7588737086137433,
      "grad_norm": 1.2973705444156856,
      "learning_rate": 2.8975560230651845e-06,
      "loss": 0.4966,
      "step": 8429
    },
    {
      "epoch": 0.7589637398996151,
      "grad_norm": 1.6246827538426094,
      "learning_rate": 2.8955034703424213e-06,
      "loss": 0.4069,
      "step": 8430
    },
    {
      "epoch": 0.759053771185487,
      "grad_norm": 1.0124911739221214,
      "learning_rate": 2.893451521790448e-06,
      "loss": 0.5193,
      "step": 8431
    },
    {
      "epoch": 0.7591438024713588,
      "grad_norm": 1.0447033567450128,
      "learning_rate": 2.891400177583775e-06,
      "loss": 0.4781,
      "step": 8432
    },
    {
      "epoch": 0.7592338337572306,
      "grad_norm": 1.2591395815372166,
      "learning_rate": 2.8893494378968432e-06,
      "loss": 0.5566,
      "step": 8433
    },
    {
      "epoch": 0.7593238650431025,
      "grad_norm": 0.9936495418444642,
      "learning_rate": 2.8872993029040506e-06,
      "loss": 0.5738,
      "step": 8434
    },
    {
      "epoch": 0.7594138963289743,
      "grad_norm": 0.9463660180094512,
      "learning_rate": 2.8852497727797413e-06,
      "loss": 0.5109,
      "step": 8435
    },
    {
      "epoch": 0.7595039276148462,
      "grad_norm": 1.4223331966690458,
      "learning_rate": 2.8832008476982097e-06,
      "loss": 0.4743,
      "step": 8436
    },
    {
      "epoch": 0.759593958900718,
      "grad_norm": 1.022360200004624,
      "learning_rate": 2.881152527833697e-06,
      "loss": 0.5454,
      "step": 8437
    },
    {
      "epoch": 0.7596839901865898,
      "grad_norm": 0.9456682381272244,
      "learning_rate": 2.879104813360396e-06,
      "loss": 0.4101,
      "step": 8438
    },
    {
      "epoch": 0.7597740214724616,
      "grad_norm": 0.9435483898966317,
      "learning_rate": 2.8770577044524408e-06,
      "loss": 0.531,
      "step": 8439
    },
    {
      "epoch": 0.7598640527583336,
      "grad_norm": 1.0606496272087826,
      "learning_rate": 2.8750112012839215e-06,
      "loss": 0.565,
      "step": 8440
    },
    {
      "epoch": 0.7599540840442054,
      "grad_norm": 1.1028655598266364,
      "learning_rate": 2.8729653040288743e-06,
      "loss": 0.5276,
      "step": 8441
    },
    {
      "epoch": 0.7600441153300772,
      "grad_norm": 0.9668684328905842,
      "learning_rate": 2.8709200128612822e-06,
      "loss": 0.5134,
      "step": 8442
    },
    {
      "epoch": 0.760134146615949,
      "grad_norm": 1.0776863775518641,
      "learning_rate": 2.8688753279550798e-06,
      "loss": 0.5638,
      "step": 8443
    },
    {
      "epoch": 0.7602241779018208,
      "grad_norm": 0.9929667379408269,
      "learning_rate": 2.86683124948415e-06,
      "loss": 0.4802,
      "step": 8444
    },
    {
      "epoch": 0.7603142091876928,
      "grad_norm": 0.8237593625487614,
      "learning_rate": 2.864787777622313e-06,
      "loss": 0.4697,
      "step": 8445
    },
    {
      "epoch": 0.7604042404735646,
      "grad_norm": 1.5190300459060697,
      "learning_rate": 2.8627449125433615e-06,
      "loss": 0.6047,
      "step": 8446
    },
    {
      "epoch": 0.7604942717594364,
      "grad_norm": 1.6701690034086123,
      "learning_rate": 2.8607026544210115e-06,
      "loss": 0.5091,
      "step": 8447
    },
    {
      "epoch": 0.7605843030453082,
      "grad_norm": 0.9299332167275819,
      "learning_rate": 2.8586610034289395e-06,
      "loss": 0.574,
      "step": 8448
    },
    {
      "epoch": 0.76067433433118,
      "grad_norm": 1.485968463077198,
      "learning_rate": 2.856619959740772e-06,
      "loss": 0.4435,
      "step": 8449
    },
    {
      "epoch": 0.760764365617052,
      "grad_norm": 1.2344370436693748,
      "learning_rate": 2.8545795235300776e-06,
      "loss": 0.5041,
      "step": 8450
    },
    {
      "epoch": 0.7608543969029238,
      "grad_norm": 1.0251327736890574,
      "learning_rate": 2.8525396949703775e-06,
      "loss": 0.4492,
      "step": 8451
    },
    {
      "epoch": 0.7609444281887956,
      "grad_norm": 1.1263027062720417,
      "learning_rate": 2.8505004742351426e-06,
      "loss": 0.5663,
      "step": 8452
    },
    {
      "epoch": 0.7610344594746674,
      "grad_norm": 1.0934847605378053,
      "learning_rate": 2.8484618614977843e-06,
      "loss": 0.5147,
      "step": 8453
    },
    {
      "epoch": 0.7611244907605393,
      "grad_norm": 1.0528424674778647,
      "learning_rate": 2.8464238569316694e-06,
      "loss": 0.4861,
      "step": 8454
    },
    {
      "epoch": 0.7612145220464112,
      "grad_norm": 1.038726791888751,
      "learning_rate": 2.8443864607101124e-06,
      "loss": 0.5029,
      "step": 8455
    },
    {
      "epoch": 0.761304553332283,
      "grad_norm": 1.4750416684223173,
      "learning_rate": 2.842349673006374e-06,
      "loss": 0.4753,
      "step": 8456
    },
    {
      "epoch": 0.7613945846181548,
      "grad_norm": 1.948201435774032,
      "learning_rate": 2.8403134939936627e-06,
      "loss": 0.5343,
      "step": 8457
    },
    {
      "epoch": 0.7614846159040266,
      "grad_norm": 1.1129584096218543,
      "learning_rate": 2.838277923845142e-06,
      "loss": 0.5432,
      "step": 8458
    },
    {
      "epoch": 0.7615746471898985,
      "grad_norm": 1.1810964762937501,
      "learning_rate": 2.8362429627339073e-06,
      "loss": 0.5171,
      "step": 8459
    },
    {
      "epoch": 0.7616646784757704,
      "grad_norm": 1.0895610257225123,
      "learning_rate": 2.8342086108330247e-06,
      "loss": 0.5343,
      "step": 8460
    },
    {
      "epoch": 0.7617547097616422,
      "grad_norm": 1.2902506345104938,
      "learning_rate": 2.8321748683154893e-06,
      "loss": 0.5678,
      "step": 8461
    },
    {
      "epoch": 0.761844741047514,
      "grad_norm": 1.2612636990670187,
      "learning_rate": 2.830141735354254e-06,
      "loss": 0.5437,
      "step": 8462
    },
    {
      "epoch": 0.7619347723333858,
      "grad_norm": 1.0769024560479556,
      "learning_rate": 2.8281092121222163e-06,
      "loss": 0.5509,
      "step": 8463
    },
    {
      "epoch": 0.7620248036192577,
      "grad_norm": 1.4706420911894618,
      "learning_rate": 2.8260772987922248e-06,
      "loss": 0.5451,
      "step": 8464
    },
    {
      "epoch": 0.7621148349051295,
      "grad_norm": 1.3118234632168495,
      "learning_rate": 2.8240459955370757e-06,
      "loss": 0.4898,
      "step": 8465
    },
    {
      "epoch": 0.7622048661910014,
      "grad_norm": 1.20296826149356,
      "learning_rate": 2.8220153025295126e-06,
      "loss": 0.5577,
      "step": 8466
    },
    {
      "epoch": 0.7622948974768732,
      "grad_norm": 1.1319564257204167,
      "learning_rate": 2.819985219942223e-06,
      "loss": 0.5126,
      "step": 8467
    },
    {
      "epoch": 0.7623849287627451,
      "grad_norm": 1.372892106738549,
      "learning_rate": 2.817955747947848e-06,
      "loss": 0.508,
      "step": 8468
    },
    {
      "epoch": 0.7624749600486169,
      "grad_norm": 1.4365664126620397,
      "learning_rate": 2.815926886718977e-06,
      "loss": 0.4226,
      "step": 8469
    },
    {
      "epoch": 0.7625649913344887,
      "grad_norm": 1.2467476980290741,
      "learning_rate": 2.813898636428144e-06,
      "loss": 0.5081,
      "step": 8470
    },
    {
      "epoch": 0.7626550226203606,
      "grad_norm": 0.971916767711335,
      "learning_rate": 2.8118709972478332e-06,
      "loss": 0.5494,
      "step": 8471
    },
    {
      "epoch": 0.7627450539062324,
      "grad_norm": 1.5809122727961138,
      "learning_rate": 2.809843969350479e-06,
      "loss": 0.4714,
      "step": 8472
    },
    {
      "epoch": 0.7628350851921043,
      "grad_norm": 1.1766161397082122,
      "learning_rate": 2.807817552908452e-06,
      "loss": 0.4844,
      "step": 8473
    },
    {
      "epoch": 0.7629251164779761,
      "grad_norm": 1.1048574234918438,
      "learning_rate": 2.8057917480940923e-06,
      "loss": 0.5136,
      "step": 8474
    },
    {
      "epoch": 0.7630151477638479,
      "grad_norm": 1.5912359825344724,
      "learning_rate": 2.8037665550796665e-06,
      "loss": 0.5167,
      "step": 8475
    },
    {
      "epoch": 0.7631051790497198,
      "grad_norm": 0.9417950825869965,
      "learning_rate": 2.801741974037401e-06,
      "loss": 0.5231,
      "step": 8476
    },
    {
      "epoch": 0.7631952103355916,
      "grad_norm": 1.32274876110941,
      "learning_rate": 2.799718005139469e-06,
      "loss": 0.4925,
      "step": 8477
    },
    {
      "epoch": 0.7632852416214635,
      "grad_norm": 1.3269849486326888,
      "learning_rate": 2.797694648557987e-06,
      "loss": 0.5567,
      "step": 8478
    },
    {
      "epoch": 0.7633752729073353,
      "grad_norm": 2.5203071893741402,
      "learning_rate": 2.795671904465026e-06,
      "loss": 0.4685,
      "step": 8479
    },
    {
      "epoch": 0.7634653041932071,
      "grad_norm": 1.2438416836635364,
      "learning_rate": 2.7936497730326017e-06,
      "loss": 0.5424,
      "step": 8480
    },
    {
      "epoch": 0.763555335479079,
      "grad_norm": 0.8486780465814505,
      "learning_rate": 2.791628254432673e-06,
      "loss": 0.5579,
      "step": 8481
    },
    {
      "epoch": 0.7636453667649509,
      "grad_norm": 1.0511503261512272,
      "learning_rate": 2.7896073488371535e-06,
      "loss": 0.5163,
      "step": 8482
    },
    {
      "epoch": 0.7637353980508227,
      "grad_norm": 1.1524027526427003,
      "learning_rate": 2.7875870564179018e-06,
      "loss": 0.5314,
      "step": 8483
    },
    {
      "epoch": 0.7638254293366945,
      "grad_norm": 1.0741982841303181,
      "learning_rate": 2.785567377346725e-06,
      "loss": 0.5541,
      "step": 8484
    },
    {
      "epoch": 0.7639154606225663,
      "grad_norm": 1.6372280331398583,
      "learning_rate": 2.783548311795379e-06,
      "loss": 0.5012,
      "step": 8485
    },
    {
      "epoch": 0.7640054919084381,
      "grad_norm": 1.0509007331810982,
      "learning_rate": 2.781529859935569e-06,
      "loss": 0.5914,
      "step": 8486
    },
    {
      "epoch": 0.7640955231943101,
      "grad_norm": 1.2243148840423679,
      "learning_rate": 2.779512021938935e-06,
      "loss": 0.5347,
      "step": 8487
    },
    {
      "epoch": 0.7641855544801819,
      "grad_norm": 1.468827156939027,
      "learning_rate": 2.7774947979770882e-06,
      "loss": 0.5447,
      "step": 8488
    },
    {
      "epoch": 0.7642755857660537,
      "grad_norm": 1.2241035227119343,
      "learning_rate": 2.775478188221565e-06,
      "loss": 0.4776,
      "step": 8489
    },
    {
      "epoch": 0.7643656170519255,
      "grad_norm": 1.069542064352583,
      "learning_rate": 2.7734621928438645e-06,
      "loss": 0.5118,
      "step": 8490
    },
    {
      "epoch": 0.7644556483377973,
      "grad_norm": 1.3899493484580725,
      "learning_rate": 2.7714468120154247e-06,
      "loss": 0.4784,
      "step": 8491
    },
    {
      "epoch": 0.7645456796236693,
      "grad_norm": 0.9477269073105318,
      "learning_rate": 2.7694320459076375e-06,
      "loss": 0.4881,
      "step": 8492
    },
    {
      "epoch": 0.7646357109095411,
      "grad_norm": 1.2524915471483142,
      "learning_rate": 2.7674178946918386e-06,
      "loss": 0.5268,
      "step": 8493
    },
    {
      "epoch": 0.7647257421954129,
      "grad_norm": 1.0862884590339792,
      "learning_rate": 2.7654043585393154e-06,
      "loss": 0.5436,
      "step": 8494
    },
    {
      "epoch": 0.7648157734812847,
      "grad_norm": 1.1931057056313261,
      "learning_rate": 2.7633914376212955e-06,
      "loss": 0.4452,
      "step": 8495
    },
    {
      "epoch": 0.7649058047671566,
      "grad_norm": 1.4244408925176786,
      "learning_rate": 2.761379132108961e-06,
      "loss": 0.5814,
      "step": 8496
    },
    {
      "epoch": 0.7649958360530285,
      "grad_norm": 1.0196471933676063,
      "learning_rate": 2.7593674421734406e-06,
      "loss": 0.5016,
      "step": 8497
    },
    {
      "epoch": 0.7650858673389003,
      "grad_norm": 1.2374452740623683,
      "learning_rate": 2.7573563679858073e-06,
      "loss": 0.4788,
      "step": 8498
    },
    {
      "epoch": 0.7651758986247721,
      "grad_norm": 1.4053290607667115,
      "learning_rate": 2.7553459097170876e-06,
      "loss": 0.6437,
      "step": 8499
    },
    {
      "epoch": 0.7652659299106439,
      "grad_norm": 0.9653746049863944,
      "learning_rate": 2.7533360675382514e-06,
      "loss": 0.4677,
      "step": 8500
    },
    {
      "epoch": 0.7653559611965158,
      "grad_norm": 1.123347555017841,
      "learning_rate": 2.751326841620211e-06,
      "loss": 0.5528,
      "step": 8501
    },
    {
      "epoch": 0.7654459924823876,
      "grad_norm": 1.059869162762782,
      "learning_rate": 2.749318232133843e-06,
      "loss": 0.5754,
      "step": 8502
    },
    {
      "epoch": 0.7655360237682595,
      "grad_norm": 1.0928663334227464,
      "learning_rate": 2.7473102392499517e-06,
      "loss": 0.5886,
      "step": 8503
    },
    {
      "epoch": 0.7656260550541313,
      "grad_norm": 0.9502374246374268,
      "learning_rate": 2.745302863139301e-06,
      "loss": 0.4786,
      "step": 8504
    },
    {
      "epoch": 0.7657160863400031,
      "grad_norm": 1.2092579386251894,
      "learning_rate": 2.7432961039725992e-06,
      "loss": 0.5512,
      "step": 8505
    },
    {
      "epoch": 0.765806117625875,
      "grad_norm": 0.9795781460460896,
      "learning_rate": 2.7412899619205023e-06,
      "loss": 0.5474,
      "step": 8506
    },
    {
      "epoch": 0.7658961489117468,
      "grad_norm": 1.4182997089099711,
      "learning_rate": 2.7392844371536153e-06,
      "loss": 0.4841,
      "step": 8507
    },
    {
      "epoch": 0.7659861801976187,
      "grad_norm": 1.0643036786515006,
      "learning_rate": 2.7372795298424914e-06,
      "loss": 0.5597,
      "step": 8508
    },
    {
      "epoch": 0.7660762114834905,
      "grad_norm": 1.0294488682045597,
      "learning_rate": 2.735275240157622e-06,
      "loss": 0.4936,
      "step": 8509
    },
    {
      "epoch": 0.7661662427693624,
      "grad_norm": 1.0470242911365317,
      "learning_rate": 2.7332715682694576e-06,
      "loss": 0.458,
      "step": 8510
    },
    {
      "epoch": 0.7662562740552342,
      "grad_norm": 1.2473592762841537,
      "learning_rate": 2.7312685143483918e-06,
      "loss": 0.4458,
      "step": 8511
    },
    {
      "epoch": 0.766346305341106,
      "grad_norm": 0.8516122456208088,
      "learning_rate": 2.729266078564764e-06,
      "loss": 0.4883,
      "step": 8512
    },
    {
      "epoch": 0.7664363366269779,
      "grad_norm": 1.0487933025518177,
      "learning_rate": 2.7272642610888643e-06,
      "loss": 0.5521,
      "step": 8513
    },
    {
      "epoch": 0.7665263679128497,
      "grad_norm": 0.9990512092587673,
      "learning_rate": 2.7252630620909303e-06,
      "loss": 0.5162,
      "step": 8514
    },
    {
      "epoch": 0.7666163991987216,
      "grad_norm": 1.5057134620126404,
      "learning_rate": 2.723262481741138e-06,
      "loss": 0.5006,
      "step": 8515
    },
    {
      "epoch": 0.7667064304845934,
      "grad_norm": 1.3272035997725256,
      "learning_rate": 2.7212625202096276e-06,
      "loss": 0.5894,
      "step": 8516
    },
    {
      "epoch": 0.7667964617704652,
      "grad_norm": 1.1971310094333776,
      "learning_rate": 2.719263177666469e-06,
      "loss": 0.5424,
      "step": 8517
    },
    {
      "epoch": 0.766886493056337,
      "grad_norm": 1.179103585661279,
      "learning_rate": 2.717264454281691e-06,
      "loss": 0.4823,
      "step": 8518
    },
    {
      "epoch": 0.7669765243422089,
      "grad_norm": 0.9141040763189855,
      "learning_rate": 2.7152663502252652e-06,
      "loss": 0.4919,
      "step": 8519
    },
    {
      "epoch": 0.7670665556280808,
      "grad_norm": 1.2469144011576703,
      "learning_rate": 2.7132688656671134e-06,
      "loss": 0.4569,
      "step": 8520
    },
    {
      "epoch": 0.7671565869139526,
      "grad_norm": 1.665204809418765,
      "learning_rate": 2.711272000777102e-06,
      "loss": 0.5121,
      "step": 8521
    },
    {
      "epoch": 0.7672466181998244,
      "grad_norm": 1.3443552373952325,
      "learning_rate": 2.7092757557250482e-06,
      "loss": 0.4743,
      "step": 8522
    },
    {
      "epoch": 0.7673366494856962,
      "grad_norm": 1.121139640249978,
      "learning_rate": 2.7072801306807075e-06,
      "loss": 0.5075,
      "step": 8523
    },
    {
      "epoch": 0.7674266807715682,
      "grad_norm": 1.1824985854872894,
      "learning_rate": 2.7052851258137936e-06,
      "loss": 0.6107,
      "step": 8524
    },
    {
      "epoch": 0.76751671205744,
      "grad_norm": 1.6189662541756964,
      "learning_rate": 2.7032907412939626e-06,
      "loss": 0.4743,
      "step": 8525
    },
    {
      "epoch": 0.7676067433433118,
      "grad_norm": 1.3946328828804166,
      "learning_rate": 2.701296977290817e-06,
      "loss": 0.5375,
      "step": 8526
    },
    {
      "epoch": 0.7676967746291836,
      "grad_norm": 1.145092571425989,
      "learning_rate": 2.6993038339739087e-06,
      "loss": 0.513,
      "step": 8527
    },
    {
      "epoch": 0.7677868059150554,
      "grad_norm": 1.0584400158325364,
      "learning_rate": 2.697311311512739e-06,
      "loss": 0.5873,
      "step": 8528
    },
    {
      "epoch": 0.7678768372009274,
      "grad_norm": 1.0820168934428693,
      "learning_rate": 2.695319410076743e-06,
      "loss": 0.512,
      "step": 8529
    },
    {
      "epoch": 0.7679668684867992,
      "grad_norm": 1.2541497752245863,
      "learning_rate": 2.6933281298353274e-06,
      "loss": 0.4944,
      "step": 8530
    },
    {
      "epoch": 0.768056899772671,
      "grad_norm": 1.1272815657098647,
      "learning_rate": 2.691337470957821e-06,
      "loss": 0.6246,
      "step": 8531
    },
    {
      "epoch": 0.7681469310585428,
      "grad_norm": 1.41730027278373,
      "learning_rate": 2.689347433613514e-06,
      "loss": 0.5168,
      "step": 8532
    },
    {
      "epoch": 0.7682369623444146,
      "grad_norm": 1.568500656269611,
      "learning_rate": 2.687358017971642e-06,
      "loss": 0.5264,
      "step": 8533
    },
    {
      "epoch": 0.7683269936302866,
      "grad_norm": 1.1229897391515251,
      "learning_rate": 2.685369224201383e-06,
      "loss": 0.465,
      "step": 8534
    },
    {
      "epoch": 0.7684170249161584,
      "grad_norm": 1.1110238669917192,
      "learning_rate": 2.6833810524718684e-06,
      "loss": 0.4795,
      "step": 8535
    },
    {
      "epoch": 0.7685070562020302,
      "grad_norm": 0.9638895119135715,
      "learning_rate": 2.6813935029521754e-06,
      "loss": 0.4985,
      "step": 8536
    },
    {
      "epoch": 0.768597087487902,
      "grad_norm": 1.1638671702167533,
      "learning_rate": 2.67940657581132e-06,
      "loss": 0.5656,
      "step": 8537
    },
    {
      "epoch": 0.7686871187737739,
      "grad_norm": 1.1599945525030735,
      "learning_rate": 2.6774202712182752e-06,
      "loss": 0.4612,
      "step": 8538
    },
    {
      "epoch": 0.7687771500596458,
      "grad_norm": 1.0774158781836634,
      "learning_rate": 2.6754345893419564e-06,
      "loss": 0.5562,
      "step": 8539
    },
    {
      "epoch": 0.7688671813455176,
      "grad_norm": 1.79844563427855,
      "learning_rate": 2.67344953035123e-06,
      "loss": 0.6281,
      "step": 8540
    },
    {
      "epoch": 0.7689572126313894,
      "grad_norm": 1.112087006063089,
      "learning_rate": 2.6714650944149034e-06,
      "loss": 0.4807,
      "step": 8541
    },
    {
      "epoch": 0.7690472439172612,
      "grad_norm": 1.1769407411856199,
      "learning_rate": 2.669481281701739e-06,
      "loss": 0.3908,
      "step": 8542
    },
    {
      "epoch": 0.7691372752031331,
      "grad_norm": 1.0873721135642194,
      "learning_rate": 2.6674980923804327e-06,
      "loss": 0.5595,
      "step": 8543
    },
    {
      "epoch": 0.769227306489005,
      "grad_norm": 1.1186945502099288,
      "learning_rate": 2.6655155266196486e-06,
      "loss": 0.4963,
      "step": 8544
    },
    {
      "epoch": 0.7693173377748768,
      "grad_norm": 1.3248366129017357,
      "learning_rate": 2.663533584587974e-06,
      "loss": 0.5519,
      "step": 8545
    },
    {
      "epoch": 0.7694073690607486,
      "grad_norm": 1.1169577577319534,
      "learning_rate": 2.6615522664539596e-06,
      "loss": 0.5299,
      "step": 8546
    },
    {
      "epoch": 0.7694974003466204,
      "grad_norm": 1.2701038822895734,
      "learning_rate": 2.659571572386096e-06,
      "loss": 0.5539,
      "step": 8547
    },
    {
      "epoch": 0.7695874316324923,
      "grad_norm": 1.539680643688758,
      "learning_rate": 2.6575915025528254e-06,
      "loss": 0.6322,
      "step": 8548
    },
    {
      "epoch": 0.7696774629183641,
      "grad_norm": 1.4389788251276234,
      "learning_rate": 2.655612057122532e-06,
      "loss": 0.4583,
      "step": 8549
    },
    {
      "epoch": 0.769767494204236,
      "grad_norm": 1.6225604967926373,
      "learning_rate": 2.653633236263553e-06,
      "loss": 0.5418,
      "step": 8550
    },
    {
      "epoch": 0.7698575254901078,
      "grad_norm": 1.364706544140962,
      "learning_rate": 2.651655040144161e-06,
      "loss": 0.4676,
      "step": 8551
    },
    {
      "epoch": 0.7699475567759797,
      "grad_norm": 1.0275230829250646,
      "learning_rate": 2.649677468932589e-06,
      "loss": 0.5308,
      "step": 8552
    },
    {
      "epoch": 0.7700375880618515,
      "grad_norm": 0.9795984806887792,
      "learning_rate": 2.6477005227970077e-06,
      "loss": 0.4233,
      "step": 8553
    },
    {
      "epoch": 0.7701276193477233,
      "grad_norm": 1.8538345649542844,
      "learning_rate": 2.645724201905541e-06,
      "loss": 0.4351,
      "step": 8554
    },
    {
      "epoch": 0.7702176506335952,
      "grad_norm": 1.078296679143489,
      "learning_rate": 2.6437485064262534e-06,
      "loss": 0.4989,
      "step": 8555
    },
    {
      "epoch": 0.770307681919467,
      "grad_norm": 1.0272347034429408,
      "learning_rate": 2.641773436527164e-06,
      "loss": 0.4847,
      "step": 8556
    },
    {
      "epoch": 0.7703977132053389,
      "grad_norm": 0.9487292727144068,
      "learning_rate": 2.6397989923762248e-06,
      "loss": 0.409,
      "step": 8557
    },
    {
      "epoch": 0.7704877444912107,
      "grad_norm": 0.9375494096291124,
      "learning_rate": 2.637825174141355e-06,
      "loss": 0.5656,
      "step": 8558
    },
    {
      "epoch": 0.7705777757770825,
      "grad_norm": 1.1767929079582016,
      "learning_rate": 2.6358519819904003e-06,
      "loss": 0.6064,
      "step": 8559
    },
    {
      "epoch": 0.7706678070629543,
      "grad_norm": 1.9903398580183123,
      "learning_rate": 2.633879416091167e-06,
      "loss": 0.4697,
      "step": 8560
    },
    {
      "epoch": 0.7707578383488262,
      "grad_norm": 1.6813737115422835,
      "learning_rate": 2.6319074766114017e-06,
      "loss": 0.5641,
      "step": 8561
    },
    {
      "epoch": 0.7708478696346981,
      "grad_norm": 1.3319328364085097,
      "learning_rate": 2.6299361637188002e-06,
      "loss": 0.532,
      "step": 8562
    },
    {
      "epoch": 0.7709379009205699,
      "grad_norm": 1.1990124317850996,
      "learning_rate": 2.6279654775810037e-06,
      "loss": 0.4152,
      "step": 8563
    },
    {
      "epoch": 0.7710279322064417,
      "grad_norm": 0.9386687844314497,
      "learning_rate": 2.6259954183656023e-06,
      "loss": 0.4531,
      "step": 8564
    },
    {
      "epoch": 0.7711179634923135,
      "grad_norm": 1.1118071616280156,
      "learning_rate": 2.6240259862401273e-06,
      "loss": 0.473,
      "step": 8565
    },
    {
      "epoch": 0.7712079947781855,
      "grad_norm": 0.8410987980993814,
      "learning_rate": 2.622057181372063e-06,
      "loss": 0.502,
      "step": 8566
    },
    {
      "epoch": 0.7712980260640573,
      "grad_norm": 1.0955068315398342,
      "learning_rate": 2.620089003928836e-06,
      "loss": 0.4552,
      "step": 8567
    },
    {
      "epoch": 0.7713880573499291,
      "grad_norm": 0.9014743141383358,
      "learning_rate": 2.6181214540778234e-06,
      "loss": 0.4953,
      "step": 8568
    },
    {
      "epoch": 0.7714780886358009,
      "grad_norm": 1.2216067964801405,
      "learning_rate": 2.6161545319863457e-06,
      "loss": 0.5504,
      "step": 8569
    },
    {
      "epoch": 0.7715681199216727,
      "grad_norm": 1.2586104631239228,
      "learning_rate": 2.614188237821675e-06,
      "loss": 0.4595,
      "step": 8570
    },
    {
      "epoch": 0.7716581512075447,
      "grad_norm": 0.8445246154469581,
      "learning_rate": 2.6122225717510162e-06,
      "loss": 0.4254,
      "step": 8571
    },
    {
      "epoch": 0.7717481824934165,
      "grad_norm": 1.2947684352151847,
      "learning_rate": 2.610257533941544e-06,
      "loss": 0.4747,
      "step": 8572
    },
    {
      "epoch": 0.7718382137792883,
      "grad_norm": 1.1387166435596796,
      "learning_rate": 2.608293124560356e-06,
      "loss": 0.4706,
      "step": 8573
    },
    {
      "epoch": 0.7719282450651601,
      "grad_norm": 1.387724452007253,
      "learning_rate": 2.6063293437745106e-06,
      "loss": 0.4332,
      "step": 8574
    },
    {
      "epoch": 0.7720182763510319,
      "grad_norm": 1.2558417874742887,
      "learning_rate": 2.6043661917510098e-06,
      "loss": 0.4988,
      "step": 8575
    },
    {
      "epoch": 0.7721083076369039,
      "grad_norm": 1.0697897403351535,
      "learning_rate": 2.6024036686568e-06,
      "loss": 0.5458,
      "step": 8576
    },
    {
      "epoch": 0.7721983389227757,
      "grad_norm": 1.0575622082986713,
      "learning_rate": 2.600441774658775e-06,
      "loss": 0.5865,
      "step": 8577
    },
    {
      "epoch": 0.7722883702086475,
      "grad_norm": 1.4035084818997303,
      "learning_rate": 2.5984805099237785e-06,
      "loss": 0.5755,
      "step": 8578
    },
    {
      "epoch": 0.7723784014945193,
      "grad_norm": 1.0965078080449617,
      "learning_rate": 2.5965198746185936e-06,
      "loss": 0.4645,
      "step": 8579
    },
    {
      "epoch": 0.7724684327803912,
      "grad_norm": 1.390786921345309,
      "learning_rate": 2.594559868909956e-06,
      "loss": 0.51,
      "step": 8580
    },
    {
      "epoch": 0.772558464066263,
      "grad_norm": 1.342550890990463,
      "learning_rate": 2.5926004929645444e-06,
      "loss": 0.4919,
      "step": 8581
    },
    {
      "epoch": 0.7726484953521349,
      "grad_norm": 1.0973857372979734,
      "learning_rate": 2.5906417469489876e-06,
      "loss": 0.5491,
      "step": 8582
    },
    {
      "epoch": 0.7727385266380067,
      "grad_norm": 1.0054244793068463,
      "learning_rate": 2.5886836310298557e-06,
      "loss": 0.4527,
      "step": 8583
    },
    {
      "epoch": 0.7728285579238785,
      "grad_norm": 1.349319985721374,
      "learning_rate": 2.5867261453736745e-06,
      "loss": 0.427,
      "step": 8584
    },
    {
      "epoch": 0.7729185892097504,
      "grad_norm": 1.8512924545105105,
      "learning_rate": 2.584769290146898e-06,
      "loss": 0.5613,
      "step": 8585
    },
    {
      "epoch": 0.7730086204956222,
      "grad_norm": 0.9703822654862954,
      "learning_rate": 2.5828130655159523e-06,
      "loss": 0.4442,
      "step": 8586
    },
    {
      "epoch": 0.7730986517814941,
      "grad_norm": 0.939104677736687,
      "learning_rate": 2.580857471647186e-06,
      "loss": 0.5629,
      "step": 8587
    },
    {
      "epoch": 0.7731886830673659,
      "grad_norm": 1.4784066797974236,
      "learning_rate": 2.5789025087069064e-06,
      "loss": 0.5861,
      "step": 8588
    },
    {
      "epoch": 0.7732787143532377,
      "grad_norm": 1.3222343295938945,
      "learning_rate": 2.576948176861367e-06,
      "loss": 0.5139,
      "step": 8589
    },
    {
      "epoch": 0.7733687456391096,
      "grad_norm": 1.1487277749351135,
      "learning_rate": 2.574994476276762e-06,
      "loss": 0.5272,
      "step": 8590
    },
    {
      "epoch": 0.7734587769249814,
      "grad_norm": 1.2458256073706981,
      "learning_rate": 2.573041407119239e-06,
      "loss": 0.4422,
      "step": 8591
    },
    {
      "epoch": 0.7735488082108533,
      "grad_norm": 1.287371735617506,
      "learning_rate": 2.571088969554889e-06,
      "loss": 0.4936,
      "step": 8592
    },
    {
      "epoch": 0.7736388394967251,
      "grad_norm": 0.9609073305355018,
      "learning_rate": 2.569137163749742e-06,
      "loss": 0.4701,
      "step": 8593
    },
    {
      "epoch": 0.773728870782597,
      "grad_norm": 1.2673473945798106,
      "learning_rate": 2.567185989869786e-06,
      "loss": 0.4878,
      "step": 8594
    },
    {
      "epoch": 0.7738189020684688,
      "grad_norm": 1.1809266489857444,
      "learning_rate": 2.5652354480809483e-06,
      "loss": 0.4729,
      "step": 8595
    },
    {
      "epoch": 0.7739089333543406,
      "grad_norm": 1.1959113377173285,
      "learning_rate": 2.563285538549104e-06,
      "loss": 0.5684,
      "step": 8596
    },
    {
      "epoch": 0.7739989646402125,
      "grad_norm": 0.8869793213060857,
      "learning_rate": 2.561336261440076e-06,
      "loss": 0.4684,
      "step": 8597
    },
    {
      "epoch": 0.7740889959260843,
      "grad_norm": 1.0853504629082475,
      "learning_rate": 2.5593876169196342e-06,
      "loss": 0.521,
      "step": 8598
    },
    {
      "epoch": 0.7741790272119562,
      "grad_norm": 1.2134879347011456,
      "learning_rate": 2.5574396051534835e-06,
      "loss": 0.516,
      "step": 8599
    },
    {
      "epoch": 0.774269058497828,
      "grad_norm": 0.9560572826577493,
      "learning_rate": 2.555492226307296e-06,
      "loss": 0.4873,
      "step": 8600
    },
    {
      "epoch": 0.7743590897836998,
      "grad_norm": 1.3674426349719957,
      "learning_rate": 2.5535454805466696e-06,
      "loss": 0.5348,
      "step": 8601
    },
    {
      "epoch": 0.7744491210695716,
      "grad_norm": 1.072570180415195,
      "learning_rate": 2.5515993680371586e-06,
      "loss": 0.4802,
      "step": 8602
    },
    {
      "epoch": 0.7745391523554436,
      "grad_norm": 0.9114182962553523,
      "learning_rate": 2.5496538889442634e-06,
      "loss": 0.5175,
      "step": 8603
    },
    {
      "epoch": 0.7746291836413154,
      "grad_norm": 0.9574186327930486,
      "learning_rate": 2.5477090434334272e-06,
      "loss": 0.5058,
      "step": 8604
    },
    {
      "epoch": 0.7747192149271872,
      "grad_norm": 1.0428262455875752,
      "learning_rate": 2.545764831670041e-06,
      "loss": 0.4552,
      "step": 8605
    },
    {
      "epoch": 0.774809246213059,
      "grad_norm": 1.2766372363221345,
      "learning_rate": 2.5438212538194464e-06,
      "loss": 0.4954,
      "step": 8606
    },
    {
      "epoch": 0.7748992774989308,
      "grad_norm": 0.8629631100204972,
      "learning_rate": 2.541878310046918e-06,
      "loss": 0.5235,
      "step": 8607
    },
    {
      "epoch": 0.7749893087848028,
      "grad_norm": 2.1957290716322695,
      "learning_rate": 2.539936000517689e-06,
      "loss": 0.5352,
      "step": 8608
    },
    {
      "epoch": 0.7750793400706746,
      "grad_norm": 1.3714434563624511,
      "learning_rate": 2.537994325396935e-06,
      "loss": 0.4875,
      "step": 8609
    },
    {
      "epoch": 0.7751693713565464,
      "grad_norm": 1.1467018798437552,
      "learning_rate": 2.5360532848497774e-06,
      "loss": 0.5968,
      "step": 8610
    },
    {
      "epoch": 0.7752594026424182,
      "grad_norm": 1.4142426005744728,
      "learning_rate": 2.534112879041284e-06,
      "loss": 0.5207,
      "step": 8611
    },
    {
      "epoch": 0.77534943392829,
      "grad_norm": 1.0368633133549563,
      "learning_rate": 2.532173108136469e-06,
      "loss": 0.5189,
      "step": 8612
    },
    {
      "epoch": 0.775439465214162,
      "grad_norm": 1.3308230528570162,
      "learning_rate": 2.5302339723002845e-06,
      "loss": 0.5798,
      "step": 8613
    },
    {
      "epoch": 0.7755294965000338,
      "grad_norm": 1.1309214038474962,
      "learning_rate": 2.528295471697647e-06,
      "loss": 0.474,
      "step": 8614
    },
    {
      "epoch": 0.7756195277859056,
      "grad_norm": 0.916754297658662,
      "learning_rate": 2.526357606493399e-06,
      "loss": 0.5305,
      "step": 8615
    },
    {
      "epoch": 0.7757095590717774,
      "grad_norm": 1.171704938725725,
      "learning_rate": 2.524420376852341e-06,
      "loss": 0.6112,
      "step": 8616
    },
    {
      "epoch": 0.7757995903576493,
      "grad_norm": 1.0517237091523568,
      "learning_rate": 2.5224837829392166e-06,
      "loss": 0.5787,
      "step": 8617
    },
    {
      "epoch": 0.7758896216435212,
      "grad_norm": 1.8880334011192603,
      "learning_rate": 2.5205478249187145e-06,
      "loss": 0.5385,
      "step": 8618
    },
    {
      "epoch": 0.775979652929393,
      "grad_norm": 0.9695372598244089,
      "learning_rate": 2.5186125029554686e-06,
      "loss": 0.4978,
      "step": 8619
    },
    {
      "epoch": 0.7760696842152648,
      "grad_norm": 1.0779624500909948,
      "learning_rate": 2.5166778172140637e-06,
      "loss": 0.4743,
      "step": 8620
    },
    {
      "epoch": 0.7761597155011366,
      "grad_norm": 1.3164182766634185,
      "learning_rate": 2.5147437678590226e-06,
      "loss": 0.5823,
      "step": 8621
    },
    {
      "epoch": 0.7762497467870085,
      "grad_norm": 1.1566455015031707,
      "learning_rate": 2.5128103550548177e-06,
      "loss": 0.5097,
      "step": 8622
    },
    {
      "epoch": 0.7763397780728803,
      "grad_norm": 1.121448992033187,
      "learning_rate": 2.51087757896587e-06,
      "loss": 0.4931,
      "step": 8623
    },
    {
      "epoch": 0.7764298093587522,
      "grad_norm": 1.1372152907500301,
      "learning_rate": 2.508945439756544e-06,
      "loss": 0.4891,
      "step": 8624
    },
    {
      "epoch": 0.776519840644624,
      "grad_norm": 1.0557201943022037,
      "learning_rate": 2.5070139375911485e-06,
      "loss": 0.5001,
      "step": 8625
    },
    {
      "epoch": 0.7766098719304958,
      "grad_norm": 1.3639270426757104,
      "learning_rate": 2.5050830726339436e-06,
      "loss": 0.5094,
      "step": 8626
    },
    {
      "epoch": 0.7766999032163677,
      "grad_norm": 1.1251655469253299,
      "learning_rate": 2.503152845049123e-06,
      "loss": 0.4689,
      "step": 8627
    },
    {
      "epoch": 0.7767899345022395,
      "grad_norm": 1.2868075978692408,
      "learning_rate": 2.501223255000844e-06,
      "loss": 0.5559,
      "step": 8628
    },
    {
      "epoch": 0.7768799657881114,
      "grad_norm": 1.075257645539248,
      "learning_rate": 2.4992943026531935e-06,
      "loss": 0.45,
      "step": 8629
    },
    {
      "epoch": 0.7769699970739832,
      "grad_norm": 1.3485523289268468,
      "learning_rate": 2.497365988170213e-06,
      "loss": 0.5073,
      "step": 8630
    },
    {
      "epoch": 0.7770600283598551,
      "grad_norm": 1.1854918791355842,
      "learning_rate": 2.4954383117158877e-06,
      "loss": 0.5575,
      "step": 8631
    },
    {
      "epoch": 0.7771500596457269,
      "grad_norm": 1.0304745668207493,
      "learning_rate": 2.493511273454151e-06,
      "loss": 0.5009,
      "step": 8632
    },
    {
      "epoch": 0.7772400909315987,
      "grad_norm": 1.1484406222831969,
      "learning_rate": 2.4915848735488702e-06,
      "loss": 0.5067,
      "step": 8633
    },
    {
      "epoch": 0.7773301222174706,
      "grad_norm": 1.447315169698494,
      "learning_rate": 2.4896591121638814e-06,
      "loss": 0.462,
      "step": 8634
    },
    {
      "epoch": 0.7774201535033424,
      "grad_norm": 1.2443181233451177,
      "learning_rate": 2.4877339894629402e-06,
      "loss": 0.5937,
      "step": 8635
    },
    {
      "epoch": 0.7775101847892143,
      "grad_norm": 1.99085100531297,
      "learning_rate": 2.4858095056097676e-06,
      "loss": 0.5103,
      "step": 8636
    },
    {
      "epoch": 0.7776002160750861,
      "grad_norm": 1.2745285732929998,
      "learning_rate": 2.4838856607680185e-06,
      "loss": 0.5486,
      "step": 8637
    },
    {
      "epoch": 0.7776902473609579,
      "grad_norm": 1.276543962612046,
      "learning_rate": 2.481962455101301e-06,
      "loss": 0.4551,
      "step": 8638
    },
    {
      "epoch": 0.7777802786468297,
      "grad_norm": 1.3696982611599038,
      "learning_rate": 2.4800398887731647e-06,
      "loss": 0.4914,
      "step": 8639
    },
    {
      "epoch": 0.7778703099327016,
      "grad_norm": 1.372022338958423,
      "learning_rate": 2.4781179619471072e-06,
      "loss": 0.5226,
      "step": 8640
    },
    {
      "epoch": 0.7779603412185735,
      "grad_norm": 1.1459128095369202,
      "learning_rate": 2.476196674786565e-06,
      "loss": 0.4996,
      "step": 8641
    },
    {
      "epoch": 0.7780503725044453,
      "grad_norm": 0.8830824455491154,
      "learning_rate": 2.474276027454934e-06,
      "loss": 0.3837,
      "step": 8642
    },
    {
      "epoch": 0.7781404037903171,
      "grad_norm": 1.139361366698886,
      "learning_rate": 2.4723560201155408e-06,
      "loss": 0.4993,
      "step": 8643
    },
    {
      "epoch": 0.7782304350761889,
      "grad_norm": 0.9449765547553856,
      "learning_rate": 2.4704366529316647e-06,
      "loss": 0.6211,
      "step": 8644
    },
    {
      "epoch": 0.7783204663620609,
      "grad_norm": 1.8710108646625183,
      "learning_rate": 2.4685179260665317e-06,
      "loss": 0.55,
      "step": 8645
    },
    {
      "epoch": 0.7784104976479327,
      "grad_norm": 1.5042409975206357,
      "learning_rate": 2.4665998396833135e-06,
      "loss": 0.5339,
      "step": 8646
    },
    {
      "epoch": 0.7785005289338045,
      "grad_norm": 1.4063700421906986,
      "learning_rate": 2.464682393945117e-06,
      "loss": 0.5338,
      "step": 8647
    },
    {
      "epoch": 0.7785905602196763,
      "grad_norm": 1.2062417540306407,
      "learning_rate": 2.462765589015015e-06,
      "loss": 0.511,
      "step": 8648
    },
    {
      "epoch": 0.7786805915055481,
      "grad_norm": 1.1229817243847713,
      "learning_rate": 2.4608494250560035e-06,
      "loss": 0.4668,
      "step": 8649
    },
    {
      "epoch": 0.7787706227914201,
      "grad_norm": 1.4035793443162912,
      "learning_rate": 2.4589339022310386e-06,
      "loss": 0.5652,
      "step": 8650
    },
    {
      "epoch": 0.7788606540772919,
      "grad_norm": 1.4620888243496681,
      "learning_rate": 2.4570190207030178e-06,
      "loss": 0.5169,
      "step": 8651
    },
    {
      "epoch": 0.7789506853631637,
      "grad_norm": 1.1755586104491496,
      "learning_rate": 2.4551047806347826e-06,
      "loss": 0.4639,
      "step": 8652
    },
    {
      "epoch": 0.7790407166490355,
      "grad_norm": 1.5725949379115896,
      "learning_rate": 2.453191182189123e-06,
      "loss": 0.4416,
      "step": 8653
    },
    {
      "epoch": 0.7791307479349073,
      "grad_norm": 1.1077190919676678,
      "learning_rate": 2.4512782255287724e-06,
      "loss": 0.5437,
      "step": 8654
    },
    {
      "epoch": 0.7792207792207793,
      "grad_norm": 0.908353255385694,
      "learning_rate": 2.4493659108164047e-06,
      "loss": 0.5966,
      "step": 8655
    },
    {
      "epoch": 0.7793108105066511,
      "grad_norm": 1.3422425074625728,
      "learning_rate": 2.447454238214654e-06,
      "loss": 0.5766,
      "step": 8656
    },
    {
      "epoch": 0.7794008417925229,
      "grad_norm": 2.012295806797778,
      "learning_rate": 2.4455432078860806e-06,
      "loss": 0.5466,
      "step": 8657
    },
    {
      "epoch": 0.7794908730783947,
      "grad_norm": 1.1073431055672316,
      "learning_rate": 2.4436328199932045e-06,
      "loss": 0.4612,
      "step": 8658
    },
    {
      "epoch": 0.7795809043642666,
      "grad_norm": 1.3026132431126096,
      "learning_rate": 2.4417230746984853e-06,
      "loss": 0.4406,
      "step": 8659
    },
    {
      "epoch": 0.7796709356501385,
      "grad_norm": 0.9433110475069869,
      "learning_rate": 2.4398139721643322e-06,
      "loss": 0.5793,
      "step": 8660
    },
    {
      "epoch": 0.7797609669360103,
      "grad_norm": 1.0928425469535643,
      "learning_rate": 2.437905512553087e-06,
      "loss": 0.5021,
      "step": 8661
    },
    {
      "epoch": 0.7798509982218821,
      "grad_norm": 1.3495966852430137,
      "learning_rate": 2.435997696027059e-06,
      "loss": 0.4793,
      "step": 8662
    },
    {
      "epoch": 0.7799410295077539,
      "grad_norm": 1.502876312708429,
      "learning_rate": 2.4340905227484813e-06,
      "loss": 0.544,
      "step": 8663
    },
    {
      "epoch": 0.7800310607936258,
      "grad_norm": 0.9987147718229994,
      "learning_rate": 2.4321839928795422e-06,
      "loss": 0.5715,
      "step": 8664
    },
    {
      "epoch": 0.7801210920794976,
      "grad_norm": 1.110874255362613,
      "learning_rate": 2.4302781065823765e-06,
      "loss": 0.5474,
      "step": 8665
    },
    {
      "epoch": 0.7802111233653695,
      "grad_norm": 1.0377324800691485,
      "learning_rate": 2.4283728640190605e-06,
      "loss": 0.4741,
      "step": 8666
    },
    {
      "epoch": 0.7803011546512413,
      "grad_norm": 1.6388803144778972,
      "learning_rate": 2.426468265351618e-06,
      "loss": 0.4731,
      "step": 8667
    },
    {
      "epoch": 0.7803911859371131,
      "grad_norm": 1.4961907887393724,
      "learning_rate": 2.424564310742019e-06,
      "loss": 0.4895,
      "step": 8668
    },
    {
      "epoch": 0.780481217222985,
      "grad_norm": 1.2991420756978536,
      "learning_rate": 2.42266100035217e-06,
      "loss": 0.5031,
      "step": 8669
    },
    {
      "epoch": 0.7805712485088568,
      "grad_norm": 1.2369942742160278,
      "learning_rate": 2.42075833434394e-06,
      "loss": 0.5511,
      "step": 8670
    },
    {
      "epoch": 0.7806612797947287,
      "grad_norm": 1.3757409476504292,
      "learning_rate": 2.4188563128791255e-06,
      "loss": 0.5732,
      "step": 8671
    },
    {
      "epoch": 0.7807513110806005,
      "grad_norm": 1.2019544564240687,
      "learning_rate": 2.4169549361194764e-06,
      "loss": 0.5155,
      "step": 8672
    },
    {
      "epoch": 0.7808413423664724,
      "grad_norm": 1.1743481454431244,
      "learning_rate": 2.4150542042266887e-06,
      "loss": 0.5291,
      "step": 8673
    },
    {
      "epoch": 0.7809313736523442,
      "grad_norm": 1.0711671406047534,
      "learning_rate": 2.413154117362404e-06,
      "loss": 0.5172,
      "step": 8674
    },
    {
      "epoch": 0.781021404938216,
      "grad_norm": 1.0892370360715677,
      "learning_rate": 2.4112546756881996e-06,
      "loss": 0.457,
      "step": 8675
    },
    {
      "epoch": 0.7811114362240879,
      "grad_norm": 1.0966473187411996,
      "learning_rate": 2.4093558793656136e-06,
      "loss": 0.5832,
      "step": 8676
    },
    {
      "epoch": 0.7812014675099597,
      "grad_norm": 1.2146284369301688,
      "learning_rate": 2.407457728556115e-06,
      "loss": 0.5722,
      "step": 8677
    },
    {
      "epoch": 0.7812914987958316,
      "grad_norm": 0.8889845091755374,
      "learning_rate": 2.4055602234211263e-06,
      "loss": 0.5091,
      "step": 8678
    },
    {
      "epoch": 0.7813815300817034,
      "grad_norm": 1.286109482342513,
      "learning_rate": 2.403663364122011e-06,
      "loss": 0.511,
      "step": 8679
    },
    {
      "epoch": 0.7814715613675752,
      "grad_norm": 1.1350382024024754,
      "learning_rate": 2.40176715082008e-06,
      "loss": 0.6235,
      "step": 8680
    },
    {
      "epoch": 0.781561592653447,
      "grad_norm": 0.9991665731098711,
      "learning_rate": 2.3998715836765884e-06,
      "loss": 0.5274,
      "step": 8681
    },
    {
      "epoch": 0.7816516239393189,
      "grad_norm": 1.0940611753937683,
      "learning_rate": 2.3979766628527403e-06,
      "loss": 0.4467,
      "step": 8682
    },
    {
      "epoch": 0.7817416552251908,
      "grad_norm": 1.107838778176658,
      "learning_rate": 2.396082388509672e-06,
      "loss": 0.4996,
      "step": 8683
    },
    {
      "epoch": 0.7818316865110626,
      "grad_norm": 1.5892135196078796,
      "learning_rate": 2.3941887608084834e-06,
      "loss": 0.517,
      "step": 8684
    },
    {
      "epoch": 0.7819217177969344,
      "grad_norm": 1.1145319277998447,
      "learning_rate": 2.392295779910203e-06,
      "loss": 0.4613,
      "step": 8685
    },
    {
      "epoch": 0.7820117490828062,
      "grad_norm": 1.3325007717736541,
      "learning_rate": 2.390403445975813e-06,
      "loss": 0.5105,
      "step": 8686
    },
    {
      "epoch": 0.7821017803686782,
      "grad_norm": 1.4055697394108349,
      "learning_rate": 2.3885117591662388e-06,
      "loss": 0.6123,
      "step": 8687
    },
    {
      "epoch": 0.78219181165455,
      "grad_norm": 1.1312432937876502,
      "learning_rate": 2.386620719642355e-06,
      "loss": 0.5241,
      "step": 8688
    },
    {
      "epoch": 0.7822818429404218,
      "grad_norm": 1.3285009987914043,
      "learning_rate": 2.3847303275649657e-06,
      "loss": 0.489,
      "step": 8689
    },
    {
      "epoch": 0.7823718742262936,
      "grad_norm": 1.1370097123923042,
      "learning_rate": 2.382840583094844e-06,
      "loss": 0.5562,
      "step": 8690
    },
    {
      "epoch": 0.7824619055121654,
      "grad_norm": 1.0740267133425432,
      "learning_rate": 2.3809514863926876e-06,
      "loss": 0.5552,
      "step": 8691
    },
    {
      "epoch": 0.7825519367980374,
      "grad_norm": 1.3199532678092074,
      "learning_rate": 2.379063037619146e-06,
      "loss": 0.4327,
      "step": 8692
    },
    {
      "epoch": 0.7826419680839092,
      "grad_norm": 1.6184524721588192,
      "learning_rate": 2.3771752369348167e-06,
      "loss": 0.4997,
      "step": 8693
    },
    {
      "epoch": 0.782731999369781,
      "grad_norm": 1.156098490080814,
      "learning_rate": 2.375288084500238e-06,
      "loss": 0.4948,
      "step": 8694
    },
    {
      "epoch": 0.7828220306556528,
      "grad_norm": 1.1048036983796865,
      "learning_rate": 2.3734015804758947e-06,
      "loss": 0.5129,
      "step": 8695
    },
    {
      "epoch": 0.7829120619415246,
      "grad_norm": 1.2749303245249828,
      "learning_rate": 2.371515725022221e-06,
      "loss": 0.6012,
      "step": 8696
    },
    {
      "epoch": 0.7830020932273966,
      "grad_norm": 1.3931599764560603,
      "learning_rate": 2.3696305182995804e-06,
      "loss": 0.4595,
      "step": 8697
    },
    {
      "epoch": 0.7830921245132684,
      "grad_norm": 1.2163235652870077,
      "learning_rate": 2.367745960468304e-06,
      "loss": 0.4671,
      "step": 8698
    },
    {
      "epoch": 0.7831821557991402,
      "grad_norm": 1.4161684440906075,
      "learning_rate": 2.3658620516886477e-06,
      "loss": 0.5377,
      "step": 8699
    },
    {
      "epoch": 0.783272187085012,
      "grad_norm": 1.2774317578514633,
      "learning_rate": 2.3639787921208223e-06,
      "loss": 0.5059,
      "step": 8700
    },
    {
      "epoch": 0.7833622183708839,
      "grad_norm": 0.9633844512937205,
      "learning_rate": 2.362096181924983e-06,
      "loss": 0.4945,
      "step": 8701
    },
    {
      "epoch": 0.7834522496567558,
      "grad_norm": 1.2019805157593024,
      "learning_rate": 2.360214221261229e-06,
      "loss": 0.4621,
      "step": 8702
    },
    {
      "epoch": 0.7835422809426276,
      "grad_norm": 1.2136240179471052,
      "learning_rate": 2.358332910289597e-06,
      "loss": 0.4922,
      "step": 8703
    },
    {
      "epoch": 0.7836323122284994,
      "grad_norm": 1.0203586682991297,
      "learning_rate": 2.3564522491700835e-06,
      "loss": 0.4843,
      "step": 8704
    },
    {
      "epoch": 0.7837223435143712,
      "grad_norm": 1.313576018734151,
      "learning_rate": 2.354572238062616e-06,
      "loss": 0.5816,
      "step": 8705
    },
    {
      "epoch": 0.7838123748002431,
      "grad_norm": 1.3985500650119496,
      "learning_rate": 2.352692877127071e-06,
      "loss": 0.5189,
      "step": 8706
    },
    {
      "epoch": 0.783902406086115,
      "grad_norm": 1.2598537479164797,
      "learning_rate": 2.350814166523273e-06,
      "loss": 0.6313,
      "step": 8707
    },
    {
      "epoch": 0.7839924373719868,
      "grad_norm": 1.3076622862829625,
      "learning_rate": 2.348936106410987e-06,
      "loss": 0.4832,
      "step": 8708
    },
    {
      "epoch": 0.7840824686578586,
      "grad_norm": 1.1007103905678974,
      "learning_rate": 2.3470586969499263e-06,
      "loss": 0.4572,
      "step": 8709
    },
    {
      "epoch": 0.7841724999437304,
      "grad_norm": 1.416462085813978,
      "learning_rate": 2.345181938299749e-06,
      "loss": 0.4771,
      "step": 8710
    },
    {
      "epoch": 0.7842625312296023,
      "grad_norm": 1.6053896417834335,
      "learning_rate": 2.3433058306200474e-06,
      "loss": 0.4958,
      "step": 8711
    },
    {
      "epoch": 0.7843525625154741,
      "grad_norm": 1.092971439148753,
      "learning_rate": 2.341430374070377e-06,
      "loss": 0.5709,
      "step": 8712
    },
    {
      "epoch": 0.784442593801346,
      "grad_norm": 1.3515754250482237,
      "learning_rate": 2.339555568810221e-06,
      "loss": 0.539,
      "step": 8713
    },
    {
      "epoch": 0.7845326250872178,
      "grad_norm": 1.1450503519966793,
      "learning_rate": 2.337681414999016e-06,
      "loss": 0.5196,
      "step": 8714
    },
    {
      "epoch": 0.7846226563730897,
      "grad_norm": 1.545057969929321,
      "learning_rate": 2.3358079127961407e-06,
      "loss": 0.5071,
      "step": 8715
    },
    {
      "epoch": 0.7847126876589615,
      "grad_norm": 1.702581823337587,
      "learning_rate": 2.3339350623609234e-06,
      "loss": 0.4789,
      "step": 8716
    },
    {
      "epoch": 0.7848027189448333,
      "grad_norm": 0.966007786078883,
      "learning_rate": 2.3320628638526222e-06,
      "loss": 0.5481,
      "step": 8717
    },
    {
      "epoch": 0.7848927502307051,
      "grad_norm": 1.3105522892180301,
      "learning_rate": 2.3301913174304625e-06,
      "loss": 0.5092,
      "step": 8718
    },
    {
      "epoch": 0.784982781516577,
      "grad_norm": 1.6477157844939456,
      "learning_rate": 2.3283204232535926e-06,
      "loss": 0.5861,
      "step": 8719
    },
    {
      "epoch": 0.7850728128024489,
      "grad_norm": 1.1051684812366895,
      "learning_rate": 2.326450181481118e-06,
      "loss": 0.5201,
      "step": 8720
    },
    {
      "epoch": 0.7851628440883207,
      "grad_norm": 1.1635913044848265,
      "learning_rate": 2.3245805922720844e-06,
      "loss": 0.5255,
      "step": 8721
    },
    {
      "epoch": 0.7852528753741925,
      "grad_norm": 1.406958123500261,
      "learning_rate": 2.3227116557854836e-06,
      "loss": 0.5557,
      "step": 8722
    },
    {
      "epoch": 0.7853429066600643,
      "grad_norm": 0.960943327381256,
      "learning_rate": 2.3208433721802504e-06,
      "loss": 0.5263,
      "step": 8723
    },
    {
      "epoch": 0.7854329379459362,
      "grad_norm": 1.2024390178997262,
      "learning_rate": 2.318975741615268e-06,
      "loss": 0.4768,
      "step": 8724
    },
    {
      "epoch": 0.7855229692318081,
      "grad_norm": 1.2881476176345465,
      "learning_rate": 2.3171087642493515e-06,
      "loss": 0.5301,
      "step": 8725
    },
    {
      "epoch": 0.7856130005176799,
      "grad_norm": 0.9956599375010703,
      "learning_rate": 2.3152424402412834e-06,
      "loss": 0.5743,
      "step": 8726
    },
    {
      "epoch": 0.7857030318035517,
      "grad_norm": 1.0531735096782457,
      "learning_rate": 2.3133767697497665e-06,
      "loss": 0.4659,
      "step": 8727
    },
    {
      "epoch": 0.7857930630894235,
      "grad_norm": 1.3972530642944487,
      "learning_rate": 2.3115117529334617e-06,
      "loss": 0.4605,
      "step": 8728
    },
    {
      "epoch": 0.7858830943752955,
      "grad_norm": 1.3539745017535985,
      "learning_rate": 2.309647389950972e-06,
      "loss": 0.4562,
      "step": 8729
    },
    {
      "epoch": 0.7859731256611673,
      "grad_norm": 1.3274138603974037,
      "learning_rate": 2.307783680960847e-06,
      "loss": 0.5747,
      "step": 8730
    },
    {
      "epoch": 0.7860631569470391,
      "grad_norm": 1.097565352401103,
      "learning_rate": 2.3059206261215673e-06,
      "loss": 0.4577,
      "step": 8731
    },
    {
      "epoch": 0.7861531882329109,
      "grad_norm": 1.0835995768556572,
      "learning_rate": 2.304058225591581e-06,
      "loss": 0.588,
      "step": 8732
    },
    {
      "epoch": 0.7862432195187827,
      "grad_norm": 1.1700384600803786,
      "learning_rate": 2.3021964795292594e-06,
      "loss": 0.5332,
      "step": 8733
    },
    {
      "epoch": 0.7863332508046547,
      "grad_norm": 0.9632205685741032,
      "learning_rate": 2.300335388092929e-06,
      "loss": 0.445,
      "step": 8734
    },
    {
      "epoch": 0.7864232820905265,
      "grad_norm": 1.1139084408414777,
      "learning_rate": 2.298474951440859e-06,
      "loss": 0.4599,
      "step": 8735
    },
    {
      "epoch": 0.7865133133763983,
      "grad_norm": 1.4857327425659212,
      "learning_rate": 2.29661516973126e-06,
      "loss": 0.5434,
      "step": 8736
    },
    {
      "epoch": 0.7866033446622701,
      "grad_norm": 1.2652658858769592,
      "learning_rate": 2.2947560431222903e-06,
      "loss": 0.4094,
      "step": 8737
    },
    {
      "epoch": 0.7866933759481419,
      "grad_norm": 0.9394793753366737,
      "learning_rate": 2.2928975717720547e-06,
      "loss": 0.433,
      "step": 8738
    },
    {
      "epoch": 0.7867834072340139,
      "grad_norm": 1.16183192212772,
      "learning_rate": 2.29103975583859e-06,
      "loss": 0.5909,
      "step": 8739
    },
    {
      "epoch": 0.7868734385198857,
      "grad_norm": 1.0613579329871161,
      "learning_rate": 2.2891825954798964e-06,
      "loss": 0.5057,
      "step": 8740
    },
    {
      "epoch": 0.7869634698057575,
      "grad_norm": 1.4644712335859875,
      "learning_rate": 2.2873260908539008e-06,
      "loss": 0.6117,
      "step": 8741
    },
    {
      "epoch": 0.7870535010916293,
      "grad_norm": 2.517878496873186,
      "learning_rate": 2.2854702421184828e-06,
      "loss": 0.4932,
      "step": 8742
    },
    {
      "epoch": 0.7871435323775012,
      "grad_norm": 1.2395311689652926,
      "learning_rate": 2.2836150494314658e-06,
      "loss": 0.4672,
      "step": 8743
    },
    {
      "epoch": 0.787233563663373,
      "grad_norm": 1.0339094506892101,
      "learning_rate": 2.2817605129506194e-06,
      "loss": 0.5009,
      "step": 8744
    },
    {
      "epoch": 0.7873235949492449,
      "grad_norm": 1.468090219748771,
      "learning_rate": 2.279906632833645e-06,
      "loss": 0.3769,
      "step": 8745
    },
    {
      "epoch": 0.7874136262351167,
      "grad_norm": 1.5336677258116802,
      "learning_rate": 2.278053409238211e-06,
      "loss": 0.5653,
      "step": 8746
    },
    {
      "epoch": 0.7875036575209885,
      "grad_norm": 1.1772618977252465,
      "learning_rate": 2.276200842321906e-06,
      "loss": 0.5307,
      "step": 8747
    },
    {
      "epoch": 0.7875936888068604,
      "grad_norm": 1.0570567760020713,
      "learning_rate": 2.2743489322422786e-06,
      "loss": 0.5461,
      "step": 8748
    },
    {
      "epoch": 0.7876837200927322,
      "grad_norm": 1.1926747152556152,
      "learning_rate": 2.2724976791568154e-06,
      "loss": 0.5146,
      "step": 8749
    },
    {
      "epoch": 0.7877737513786041,
      "grad_norm": 1.1558731109376899,
      "learning_rate": 2.2706470832229464e-06,
      "loss": 0.4374,
      "step": 8750
    },
    {
      "epoch": 0.7878637826644759,
      "grad_norm": 1.1487191330195894,
      "learning_rate": 2.2687971445980506e-06,
      "loss": 0.5574,
      "step": 8751
    },
    {
      "epoch": 0.7879538139503477,
      "grad_norm": 1.265881840431567,
      "learning_rate": 2.2669478634394482e-06,
      "loss": 0.4792,
      "step": 8752
    },
    {
      "epoch": 0.7880438452362196,
      "grad_norm": 1.2522151912711246,
      "learning_rate": 2.2650992399043957e-06,
      "loss": 0.5153,
      "step": 8753
    },
    {
      "epoch": 0.7881338765220914,
      "grad_norm": 1.119090522845863,
      "learning_rate": 2.263251274150112e-06,
      "loss": 0.5026,
      "step": 8754
    },
    {
      "epoch": 0.7882239078079633,
      "grad_norm": 1.114653939515251,
      "learning_rate": 2.261403966333742e-06,
      "loss": 0.4066,
      "step": 8755
    },
    {
      "epoch": 0.7883139390938351,
      "grad_norm": 1.1057016249870517,
      "learning_rate": 2.259557316612383e-06,
      "loss": 0.4327,
      "step": 8756
    },
    {
      "epoch": 0.788403970379707,
      "grad_norm": 0.9259942441992923,
      "learning_rate": 2.2577113251430773e-06,
      "loss": 0.4895,
      "step": 8757
    },
    {
      "epoch": 0.7884940016655788,
      "grad_norm": 1.5457771038787051,
      "learning_rate": 2.2558659920828095e-06,
      "loss": 0.4904,
      "step": 8758
    },
    {
      "epoch": 0.7885840329514506,
      "grad_norm": 0.9565214027309905,
      "learning_rate": 2.2540213175885016e-06,
      "loss": 0.4689,
      "step": 8759
    },
    {
      "epoch": 0.7886740642373224,
      "grad_norm": 0.9957453713820101,
      "learning_rate": 2.252177301817037e-06,
      "loss": 0.4955,
      "step": 8760
    },
    {
      "epoch": 0.7887640955231943,
      "grad_norm": 1.3394172566764422,
      "learning_rate": 2.2503339449252215e-06,
      "loss": 0.5653,
      "step": 8761
    },
    {
      "epoch": 0.7888541268090662,
      "grad_norm": 1.3182559596005563,
      "learning_rate": 2.2484912470698195e-06,
      "loss": 0.5893,
      "step": 8762
    },
    {
      "epoch": 0.788944158094938,
      "grad_norm": 1.1102211225628273,
      "learning_rate": 2.2466492084075355e-06,
      "loss": 0.5685,
      "step": 8763
    },
    {
      "epoch": 0.7890341893808098,
      "grad_norm": 1.5517717231639527,
      "learning_rate": 2.244807829095017e-06,
      "loss": 0.5589,
      "step": 8764
    },
    {
      "epoch": 0.7891242206666816,
      "grad_norm": 1.0313534280488643,
      "learning_rate": 2.242967109288856e-06,
      "loss": 0.4609,
      "step": 8765
    },
    {
      "epoch": 0.7892142519525535,
      "grad_norm": 1.099204445540415,
      "learning_rate": 2.241127049145593e-06,
      "loss": 0.446,
      "step": 8766
    },
    {
      "epoch": 0.7893042832384254,
      "grad_norm": 1.4609222499166534,
      "learning_rate": 2.2392876488216965e-06,
      "loss": 0.5265,
      "step": 8767
    },
    {
      "epoch": 0.7893943145242972,
      "grad_norm": 1.067729230096312,
      "learning_rate": 2.2374489084736027e-06,
      "loss": 0.4742,
      "step": 8768
    },
    {
      "epoch": 0.789484345810169,
      "grad_norm": 1.2480172689331694,
      "learning_rate": 2.235610828257672e-06,
      "loss": 0.5056,
      "step": 8769
    },
    {
      "epoch": 0.7895743770960408,
      "grad_norm": 1.5164499268631775,
      "learning_rate": 2.2337734083302164e-06,
      "loss": 0.4621,
      "step": 8770
    },
    {
      "epoch": 0.7896644083819128,
      "grad_norm": 1.2453514975847986,
      "learning_rate": 2.2319366488474934e-06,
      "loss": 0.483,
      "step": 8771
    },
    {
      "epoch": 0.7897544396677846,
      "grad_norm": 1.3036605572743947,
      "learning_rate": 2.2301005499657046e-06,
      "loss": 0.5235,
      "step": 8772
    },
    {
      "epoch": 0.7898444709536564,
      "grad_norm": 1.0753810020040953,
      "learning_rate": 2.2282651118409835e-06,
      "loss": 0.5002,
      "step": 8773
    },
    {
      "epoch": 0.7899345022395282,
      "grad_norm": 0.9603153155356579,
      "learning_rate": 2.226430334629429e-06,
      "loss": 0.5353,
      "step": 8774
    },
    {
      "epoch": 0.7900245335254,
      "grad_norm": 1.1825647471700091,
      "learning_rate": 2.224596218487063e-06,
      "loss": 0.5652,
      "step": 8775
    },
    {
      "epoch": 0.790114564811272,
      "grad_norm": 1.1363663170833949,
      "learning_rate": 2.2227627635698624e-06,
      "loss": 0.5043,
      "step": 8776
    },
    {
      "epoch": 0.7902045960971438,
      "grad_norm": 1.0938405384089562,
      "learning_rate": 2.220929970033745e-06,
      "loss": 0.4963,
      "step": 8777
    },
    {
      "epoch": 0.7902946273830156,
      "grad_norm": 1.5474931907218283,
      "learning_rate": 2.219097838034574e-06,
      "loss": 0.4156,
      "step": 8778
    },
    {
      "epoch": 0.7903846586688874,
      "grad_norm": 1.3408131669671803,
      "learning_rate": 2.2172663677281536e-06,
      "loss": 0.416,
      "step": 8779
    },
    {
      "epoch": 0.7904746899547592,
      "grad_norm": 1.2119467949020324,
      "learning_rate": 2.215435559270238e-06,
      "loss": 0.5079,
      "step": 8780
    },
    {
      "epoch": 0.7905647212406312,
      "grad_norm": 1.640000868078543,
      "learning_rate": 2.2136054128165097e-06,
      "loss": 0.4748,
      "step": 8781
    },
    {
      "epoch": 0.790654752526503,
      "grad_norm": 1.4558247625783445,
      "learning_rate": 2.211775928522618e-06,
      "loss": 0.4466,
      "step": 8782
    },
    {
      "epoch": 0.7907447838123748,
      "grad_norm": 1.6017914484636802,
      "learning_rate": 2.2099471065441346e-06,
      "loss": 0.4601,
      "step": 8783
    },
    {
      "epoch": 0.7908348150982466,
      "grad_norm": 1.2613863668434562,
      "learning_rate": 2.208118947036586e-06,
      "loss": 0.5338,
      "step": 8784
    },
    {
      "epoch": 0.7909248463841185,
      "grad_norm": 1.206162137122559,
      "learning_rate": 2.206291450155441e-06,
      "loss": 0.4785,
      "step": 8785
    },
    {
      "epoch": 0.7910148776699903,
      "grad_norm": 1.1782510847165881,
      "learning_rate": 2.2044646160561123e-06,
      "loss": 0.5891,
      "step": 8786
    },
    {
      "epoch": 0.7911049089558622,
      "grad_norm": 1.138165595152883,
      "learning_rate": 2.202638444893949e-06,
      "loss": 0.486,
      "step": 8787
    },
    {
      "epoch": 0.791194940241734,
      "grad_norm": 1.014646399753404,
      "learning_rate": 2.200812936824258e-06,
      "loss": 0.4998,
      "step": 8788
    },
    {
      "epoch": 0.7912849715276058,
      "grad_norm": 1.2500809529194112,
      "learning_rate": 2.1989880920022764e-06,
      "loss": 0.5298,
      "step": 8789
    },
    {
      "epoch": 0.7913750028134777,
      "grad_norm": 1.202283082154178,
      "learning_rate": 2.197163910583191e-06,
      "loss": 0.5167,
      "step": 8790
    },
    {
      "epoch": 0.7914650340993495,
      "grad_norm": 1.113090944442057,
      "learning_rate": 2.1953403927221315e-06,
      "loss": 0.5108,
      "step": 8791
    },
    {
      "epoch": 0.7915550653852214,
      "grad_norm": 1.182587983502861,
      "learning_rate": 2.1935175385741716e-06,
      "loss": 0.5885,
      "step": 8792
    },
    {
      "epoch": 0.7916450966710932,
      "grad_norm": 1.4862340161358414,
      "learning_rate": 2.191695348294327e-06,
      "loss": 0.4298,
      "step": 8793
    },
    {
      "epoch": 0.791735127956965,
      "grad_norm": 1.214195531418293,
      "learning_rate": 2.1898738220375614e-06,
      "loss": 0.5138,
      "step": 8794
    },
    {
      "epoch": 0.7918251592428369,
      "grad_norm": 1.2536057993064642,
      "learning_rate": 2.1880529599587706e-06,
      "loss": 0.5579,
      "step": 8795
    },
    {
      "epoch": 0.7919151905287087,
      "grad_norm": 0.983075319324659,
      "learning_rate": 2.186232762212812e-06,
      "loss": 0.4647,
      "step": 8796
    },
    {
      "epoch": 0.7920052218145806,
      "grad_norm": 1.2074211131377603,
      "learning_rate": 2.1844132289544684e-06,
      "loss": 0.5261,
      "step": 8797
    },
    {
      "epoch": 0.7920952531004524,
      "grad_norm": 1.2104836722449064,
      "learning_rate": 2.182594360338477e-06,
      "loss": 0.4701,
      "step": 8798
    },
    {
      "epoch": 0.7921852843863243,
      "grad_norm": 1.2473804747508925,
      "learning_rate": 2.180776156519516e-06,
      "loss": 0.5039,
      "step": 8799
    },
    {
      "epoch": 0.7922753156721961,
      "grad_norm": 1.1360401303099956,
      "learning_rate": 2.178958617652208e-06,
      "loss": 0.5148,
      "step": 8800
    },
    {
      "epoch": 0.7923653469580679,
      "grad_norm": 1.3928648922922655,
      "learning_rate": 2.17714174389111e-06,
      "loss": 0.6165,
      "step": 8801
    },
    {
      "epoch": 0.7924553782439397,
      "grad_norm": 1.5360470202163674,
      "learning_rate": 2.175325535390741e-06,
      "loss": 0.5735,
      "step": 8802
    },
    {
      "epoch": 0.7925454095298116,
      "grad_norm": 0.95319717811782,
      "learning_rate": 2.1735099923055445e-06,
      "loss": 0.5233,
      "step": 8803
    },
    {
      "epoch": 0.7926354408156835,
      "grad_norm": 1.4728836199184892,
      "learning_rate": 2.171695114789918e-06,
      "loss": 0.4375,
      "step": 8804
    },
    {
      "epoch": 0.7927254721015553,
      "grad_norm": 1.177111140518102,
      "learning_rate": 2.1698809029982005e-06,
      "loss": 0.5588,
      "step": 8805
    },
    {
      "epoch": 0.7928155033874271,
      "grad_norm": 1.1384931194956707,
      "learning_rate": 2.168067357084672e-06,
      "loss": 0.5143,
      "step": 8806
    },
    {
      "epoch": 0.7929055346732989,
      "grad_norm": 0.9452761088320418,
      "learning_rate": 2.16625447720356e-06,
      "loss": 0.4906,
      "step": 8807
    },
    {
      "epoch": 0.7929955659591709,
      "grad_norm": 1.57531022659008,
      "learning_rate": 2.1644422635090334e-06,
      "loss": 0.4797,
      "step": 8808
    },
    {
      "epoch": 0.7930855972450427,
      "grad_norm": 1.2239375114677777,
      "learning_rate": 2.1626307161551962e-06,
      "loss": 0.4721,
      "step": 8809
    },
    {
      "epoch": 0.7931756285309145,
      "grad_norm": 1.286707569175901,
      "learning_rate": 2.1608198352961153e-06,
      "loss": 0.4213,
      "step": 8810
    },
    {
      "epoch": 0.7932656598167863,
      "grad_norm": 1.00756924362331,
      "learning_rate": 2.159009621085781e-06,
      "loss": 0.498,
      "step": 8811
    },
    {
      "epoch": 0.7933556911026581,
      "grad_norm": 1.2634143341892605,
      "learning_rate": 2.157200073678137e-06,
      "loss": 0.5498,
      "step": 8812
    },
    {
      "epoch": 0.7934457223885301,
      "grad_norm": 1.3286689875557762,
      "learning_rate": 2.1553911932270686e-06,
      "loss": 0.6342,
      "step": 8813
    },
    {
      "epoch": 0.7935357536744019,
      "grad_norm": 1.0564367012477658,
      "learning_rate": 2.153582979886407e-06,
      "loss": 0.5596,
      "step": 8814
    },
    {
      "epoch": 0.7936257849602737,
      "grad_norm": 0.8758826493179969,
      "learning_rate": 2.151775433809916e-06,
      "loss": 0.414,
      "step": 8815
    },
    {
      "epoch": 0.7937158162461455,
      "grad_norm": 1.3208031398069933,
      "learning_rate": 2.1499685551513205e-06,
      "loss": 0.5505,
      "step": 8816
    },
    {
      "epoch": 0.7938058475320173,
      "grad_norm": 1.1684789043433437,
      "learning_rate": 2.1481623440642706e-06,
      "loss": 0.5071,
      "step": 8817
    },
    {
      "epoch": 0.7938958788178893,
      "grad_norm": 1.1428243342773876,
      "learning_rate": 2.1463568007023706e-06,
      "loss": 0.4603,
      "step": 8818
    },
    {
      "epoch": 0.7939859101037611,
      "grad_norm": 1.054395849524834,
      "learning_rate": 2.144551925219165e-06,
      "loss": 0.5093,
      "step": 8819
    },
    {
      "epoch": 0.7940759413896329,
      "grad_norm": 1.2520710461436102,
      "learning_rate": 2.142747717768141e-06,
      "loss": 0.5446,
      "step": 8820
    },
    {
      "epoch": 0.7941659726755047,
      "grad_norm": 1.0797629390151418,
      "learning_rate": 2.14094417850273e-06,
      "loss": 0.4334,
      "step": 8821
    },
    {
      "epoch": 0.7942560039613766,
      "grad_norm": 1.9768600539392336,
      "learning_rate": 2.139141307576309e-06,
      "loss": 0.5094,
      "step": 8822
    },
    {
      "epoch": 0.7943460352472484,
      "grad_norm": 1.2324273067918972,
      "learning_rate": 2.1373391051421865e-06,
      "loss": 0.5235,
      "step": 8823
    },
    {
      "epoch": 0.7944360665331203,
      "grad_norm": 1.2427081869243157,
      "learning_rate": 2.135537571353635e-06,
      "loss": 0.5438,
      "step": 8824
    },
    {
      "epoch": 0.7945260978189921,
      "grad_norm": 1.2405906498626618,
      "learning_rate": 2.1337367063638493e-06,
      "loss": 0.4021,
      "step": 8825
    },
    {
      "epoch": 0.7946161291048639,
      "grad_norm": 1.3106202790845551,
      "learning_rate": 2.131936510325978e-06,
      "loss": 0.526,
      "step": 8826
    },
    {
      "epoch": 0.7947061603907358,
      "grad_norm": 1.7191586405916461,
      "learning_rate": 2.130136983393112e-06,
      "loss": 0.58,
      "step": 8827
    },
    {
      "epoch": 0.7947961916766076,
      "grad_norm": 1.1679618453969767,
      "learning_rate": 2.1283381257182858e-06,
      "loss": 0.516,
      "step": 8828
    },
    {
      "epoch": 0.7948862229624795,
      "grad_norm": 1.5240974157639635,
      "learning_rate": 2.126539937454467e-06,
      "loss": 0.4927,
      "step": 8829
    },
    {
      "epoch": 0.7949762542483513,
      "grad_norm": 1.6799184484394172,
      "learning_rate": 2.1247424187545874e-06,
      "loss": 0.5894,
      "step": 8830
    },
    {
      "epoch": 0.7950662855342231,
      "grad_norm": 1.5002438910420477,
      "learning_rate": 2.1229455697714997e-06,
      "loss": 0.4912,
      "step": 8831
    },
    {
      "epoch": 0.795156316820095,
      "grad_norm": 1.5017982596533053,
      "learning_rate": 2.1211493906580117e-06,
      "loss": 0.4861,
      "step": 8832
    },
    {
      "epoch": 0.7952463481059668,
      "grad_norm": 1.3043369775242721,
      "learning_rate": 2.1193538815668724e-06,
      "loss": 0.4818,
      "step": 8833
    },
    {
      "epoch": 0.7953363793918387,
      "grad_norm": 1.6418650948166356,
      "learning_rate": 2.1175590426507717e-06,
      "loss": 0.4499,
      "step": 8834
    },
    {
      "epoch": 0.7954264106777105,
      "grad_norm": 2.8654522750462412,
      "learning_rate": 2.115764874062345e-06,
      "loss": 0.505,
      "step": 8835
    },
    {
      "epoch": 0.7955164419635824,
      "grad_norm": 1.585341246022993,
      "learning_rate": 2.1139713759541725e-06,
      "loss": 0.607,
      "step": 8836
    },
    {
      "epoch": 0.7956064732494542,
      "grad_norm": 0.9373892604860522,
      "learning_rate": 2.1121785484787638e-06,
      "loss": 0.522,
      "step": 8837
    },
    {
      "epoch": 0.795696504535326,
      "grad_norm": 0.9473219274258013,
      "learning_rate": 2.1103863917885957e-06,
      "loss": 0.3798,
      "step": 8838
    },
    {
      "epoch": 0.7957865358211978,
      "grad_norm": 1.718241873744032,
      "learning_rate": 2.1085949060360654e-06,
      "loss": 0.5023,
      "step": 8839
    },
    {
      "epoch": 0.7958765671070697,
      "grad_norm": 1.6540868470591057,
      "learning_rate": 2.106804091373524e-06,
      "loss": 0.487,
      "step": 8840
    },
    {
      "epoch": 0.7959665983929416,
      "grad_norm": 2.160105051098448,
      "learning_rate": 2.105013947953264e-06,
      "loss": 0.5625,
      "step": 8841
    },
    {
      "epoch": 0.7960566296788134,
      "grad_norm": 1.7239241829081493,
      "learning_rate": 2.1032244759275222e-06,
      "loss": 0.4708,
      "step": 8842
    },
    {
      "epoch": 0.7961466609646852,
      "grad_norm": 1.132452701532664,
      "learning_rate": 2.10143567544847e-06,
      "loss": 0.5205,
      "step": 8843
    },
    {
      "epoch": 0.796236692250557,
      "grad_norm": 1.2223283258603166,
      "learning_rate": 2.0996475466682375e-06,
      "loss": 0.5611,
      "step": 8844
    },
    {
      "epoch": 0.7963267235364289,
      "grad_norm": 1.541852745631853,
      "learning_rate": 2.09786008973888e-06,
      "loss": 0.5847,
      "step": 8845
    },
    {
      "epoch": 0.7964167548223008,
      "grad_norm": 1.39210724440298,
      "learning_rate": 2.0960733048124082e-06,
      "loss": 0.5308,
      "step": 8846
    },
    {
      "epoch": 0.7965067861081726,
      "grad_norm": 1.6254841138880693,
      "learning_rate": 2.0942871920407704e-06,
      "loss": 0.4707,
      "step": 8847
    },
    {
      "epoch": 0.7965968173940444,
      "grad_norm": 2.3210582056289355,
      "learning_rate": 2.0925017515758583e-06,
      "loss": 0.645,
      "step": 8848
    },
    {
      "epoch": 0.7966868486799162,
      "grad_norm": 1.5677847445494462,
      "learning_rate": 2.0907169835695073e-06,
      "loss": 0.4948,
      "step": 8849
    },
    {
      "epoch": 0.7967768799657882,
      "grad_norm": 2.5029515047862954,
      "learning_rate": 2.088932888173498e-06,
      "loss": 0.5758,
      "step": 8850
    },
    {
      "epoch": 0.79686691125166,
      "grad_norm": 1.954925677661661,
      "learning_rate": 2.087149465539543e-06,
      "loss": 0.5353,
      "step": 8851
    },
    {
      "epoch": 0.7969569425375318,
      "grad_norm": 1.6124077074255574,
      "learning_rate": 2.085366715819317e-06,
      "loss": 0.5367,
      "step": 8852
    },
    {
      "epoch": 0.7970469738234036,
      "grad_norm": 1.4012979329380426,
      "learning_rate": 2.0835846391644156e-06,
      "loss": 0.4734,
      "step": 8853
    },
    {
      "epoch": 0.7971370051092754,
      "grad_norm": 1.3379835480028968,
      "learning_rate": 2.0818032357263927e-06,
      "loss": 0.5114,
      "step": 8854
    },
    {
      "epoch": 0.7972270363951474,
      "grad_norm": 1.5078453807715397,
      "learning_rate": 2.0800225056567404e-06,
      "loss": 0.4326,
      "step": 8855
    },
    {
      "epoch": 0.7973170676810192,
      "grad_norm": 1.4284610833952025,
      "learning_rate": 2.0782424491068943e-06,
      "loss": 0.5452,
      "step": 8856
    },
    {
      "epoch": 0.797407098966891,
      "grad_norm": 2.0315281779801957,
      "learning_rate": 2.0764630662282237e-06,
      "loss": 0.5342,
      "step": 8857
    },
    {
      "epoch": 0.7974971302527628,
      "grad_norm": 1.3001691411765515,
      "learning_rate": 2.0746843571720588e-06,
      "loss": 0.5273,
      "step": 8858
    },
    {
      "epoch": 0.7975871615386346,
      "grad_norm": 1.2097356308819176,
      "learning_rate": 2.072906322089655e-06,
      "loss": 0.52,
      "step": 8859
    },
    {
      "epoch": 0.7976771928245066,
      "grad_norm": 1.0026316572695113,
      "learning_rate": 2.0711289611322204e-06,
      "loss": 0.5068,
      "step": 8860
    },
    {
      "epoch": 0.7977672241103784,
      "grad_norm": 1.7806432882765888,
      "learning_rate": 2.0693522744509022e-06,
      "loss": 0.5383,
      "step": 8861
    },
    {
      "epoch": 0.7978572553962502,
      "grad_norm": 1.4341448203346967,
      "learning_rate": 2.06757626219679e-06,
      "loss": 0.4486,
      "step": 8862
    },
    {
      "epoch": 0.797947286682122,
      "grad_norm": 1.3243134075290586,
      "learning_rate": 2.0658009245209186e-06,
      "loss": 0.5336,
      "step": 8863
    },
    {
      "epoch": 0.7980373179679939,
      "grad_norm": 1.1740718157517616,
      "learning_rate": 2.0640262615742667e-06,
      "loss": 0.5152,
      "step": 8864
    },
    {
      "epoch": 0.7981273492538657,
      "grad_norm": 1.2242656323290764,
      "learning_rate": 2.0622522735077434e-06,
      "loss": 0.4933,
      "step": 8865
    },
    {
      "epoch": 0.7982173805397376,
      "grad_norm": 1.0710160665611959,
      "learning_rate": 2.0604789604722208e-06,
      "loss": 0.4315,
      "step": 8866
    },
    {
      "epoch": 0.7983074118256094,
      "grad_norm": 1.8364538537533786,
      "learning_rate": 2.0587063226184956e-06,
      "loss": 0.4375,
      "step": 8867
    },
    {
      "epoch": 0.7983974431114812,
      "grad_norm": 1.470629142495256,
      "learning_rate": 2.0569343600973146e-06,
      "loss": 0.5609,
      "step": 8868
    },
    {
      "epoch": 0.7984874743973531,
      "grad_norm": 1.1107487626253834,
      "learning_rate": 2.0551630730593686e-06,
      "loss": 0.5407,
      "step": 8869
    },
    {
      "epoch": 0.7985775056832249,
      "grad_norm": 1.3308050758768624,
      "learning_rate": 2.0533924616552903e-06,
      "loss": 0.5766,
      "step": 8870
    },
    {
      "epoch": 0.7986675369690968,
      "grad_norm": 1.2217351969355614,
      "learning_rate": 2.051622526035646e-06,
      "loss": 0.517,
      "step": 8871
    },
    {
      "epoch": 0.7987575682549686,
      "grad_norm": 1.7200141984955872,
      "learning_rate": 2.0498532663509627e-06,
      "loss": 0.4878,
      "step": 8872
    },
    {
      "epoch": 0.7988475995408404,
      "grad_norm": 1.0789290022845872,
      "learning_rate": 2.048084682751693e-06,
      "loss": 0.4972,
      "step": 8873
    },
    {
      "epoch": 0.7989376308267123,
      "grad_norm": 1.4178946006425914,
      "learning_rate": 2.0463167753882386e-06,
      "loss": 0.4503,
      "step": 8874
    },
    {
      "epoch": 0.7990276621125841,
      "grad_norm": 1.1844155123782536,
      "learning_rate": 2.044549544410944e-06,
      "loss": 0.533,
      "step": 8875
    },
    {
      "epoch": 0.799117693398456,
      "grad_norm": 1.4176844174353986,
      "learning_rate": 2.042782989970097e-06,
      "loss": 0.4941,
      "step": 8876
    },
    {
      "epoch": 0.7992077246843278,
      "grad_norm": 1.3225062993523375,
      "learning_rate": 2.041017112215925e-06,
      "loss": 0.5443,
      "step": 8877
    },
    {
      "epoch": 0.7992977559701997,
      "grad_norm": 1.121308261508235,
      "learning_rate": 2.039251911298602e-06,
      "loss": 0.5355,
      "step": 8878
    },
    {
      "epoch": 0.7993877872560715,
      "grad_norm": 1.1713520880637056,
      "learning_rate": 2.0374873873682345e-06,
      "loss": 0.5065,
      "step": 8879
    },
    {
      "epoch": 0.7994778185419433,
      "grad_norm": 1.442025080302077,
      "learning_rate": 2.03572354057489e-06,
      "loss": 0.5698,
      "step": 8880
    },
    {
      "epoch": 0.7995678498278151,
      "grad_norm": 1.15127217835147,
      "learning_rate": 2.0339603710685574e-06,
      "loss": 0.5382,
      "step": 8881
    },
    {
      "epoch": 0.799657881113687,
      "grad_norm": 1.3984324687678789,
      "learning_rate": 2.0321978789991816e-06,
      "loss": 0.4917,
      "step": 8882
    },
    {
      "epoch": 0.7997479123995589,
      "grad_norm": 1.0871924782135929,
      "learning_rate": 2.0304360645166452e-06,
      "loss": 0.596,
      "step": 8883
    },
    {
      "epoch": 0.7998379436854307,
      "grad_norm": 1.346527359301504,
      "learning_rate": 2.0286749277707783e-06,
      "loss": 0.4474,
      "step": 8884
    },
    {
      "epoch": 0.7999279749713025,
      "grad_norm": 0.9696733084293399,
      "learning_rate": 2.0269144689113397e-06,
      "loss": 0.3591,
      "step": 8885
    },
    {
      "epoch": 0.8000180062571743,
      "grad_norm": 1.464949453513728,
      "learning_rate": 2.02515468808805e-06,
      "loss": 0.5957,
      "step": 8886
    },
    {
      "epoch": 0.8001080375430462,
      "grad_norm": 1.0355282363676583,
      "learning_rate": 2.023395585450555e-06,
      "loss": 0.5395,
      "step": 8887
    },
    {
      "epoch": 0.8001980688289181,
      "grad_norm": 1.0348603247620265,
      "learning_rate": 2.021637161148453e-06,
      "loss": 0.6145,
      "step": 8888
    },
    {
      "epoch": 0.8002881001147899,
      "grad_norm": 1.2895629271907898,
      "learning_rate": 2.019879415331281e-06,
      "loss": 0.4838,
      "step": 8889
    },
    {
      "epoch": 0.8003781314006617,
      "grad_norm": 1.4450175572992705,
      "learning_rate": 2.018122348148518e-06,
      "loss": 0.4736,
      "step": 8890
    },
    {
      "epoch": 0.8004681626865335,
      "grad_norm": 1.4024555168110384,
      "learning_rate": 2.0163659597495866e-06,
      "loss": 0.5263,
      "step": 8891
    },
    {
      "epoch": 0.8005581939724055,
      "grad_norm": 2.4337731602440407,
      "learning_rate": 2.014610250283855e-06,
      "loss": 0.5461,
      "step": 8892
    },
    {
      "epoch": 0.8006482252582773,
      "grad_norm": 1.1491871866669081,
      "learning_rate": 2.01285521990062e-06,
      "loss": 0.5333,
      "step": 8893
    },
    {
      "epoch": 0.8007382565441491,
      "grad_norm": 1.2626676103727639,
      "learning_rate": 2.011100868749143e-06,
      "loss": 0.5542,
      "step": 8894
    },
    {
      "epoch": 0.8008282878300209,
      "grad_norm": 1.1895813285871124,
      "learning_rate": 2.0093471969786048e-06,
      "loss": 0.4623,
      "step": 8895
    },
    {
      "epoch": 0.8009183191158927,
      "grad_norm": 1.1895547603160403,
      "learning_rate": 2.0075942047381437e-06,
      "loss": 0.5104,
      "step": 8896
    },
    {
      "epoch": 0.8010083504017647,
      "grad_norm": 1.3467225643414558,
      "learning_rate": 2.005841892176834e-06,
      "loss": 0.5214,
      "step": 8897
    },
    {
      "epoch": 0.8010983816876365,
      "grad_norm": 1.932481436798177,
      "learning_rate": 2.004090259443696e-06,
      "loss": 0.5869,
      "step": 8898
    },
    {
      "epoch": 0.8011884129735083,
      "grad_norm": 1.268623969228139,
      "learning_rate": 2.0023393066876827e-06,
      "loss": 0.525,
      "step": 8899
    },
    {
      "epoch": 0.8012784442593801,
      "grad_norm": 1.9214482367945038,
      "learning_rate": 2.000589034057706e-06,
      "loss": 0.54,
      "step": 8900
    },
    {
      "epoch": 0.8013684755452519,
      "grad_norm": 1.1487997438238455,
      "learning_rate": 1.9988394417026025e-06,
      "loss": 0.5679,
      "step": 8901
    },
    {
      "epoch": 0.8014585068311239,
      "grad_norm": 1.1420721910826124,
      "learning_rate": 1.9970905297711606e-06,
      "loss": 0.5367,
      "step": 8902
    },
    {
      "epoch": 0.8015485381169957,
      "grad_norm": 1.2499622740444911,
      "learning_rate": 1.9953422984121096e-06,
      "loss": 0.5054,
      "step": 8903
    },
    {
      "epoch": 0.8016385694028675,
      "grad_norm": 1.3928832990678874,
      "learning_rate": 1.993594747774119e-06,
      "loss": 0.5188,
      "step": 8904
    },
    {
      "epoch": 0.8017286006887393,
      "grad_norm": 1.2010611097616697,
      "learning_rate": 1.991847878005804e-06,
      "loss": 0.5892,
      "step": 8905
    },
    {
      "epoch": 0.8018186319746112,
      "grad_norm": 1.230698425943512,
      "learning_rate": 1.99010168925572e-06,
      "loss": 0.6294,
      "step": 8906
    },
    {
      "epoch": 0.801908663260483,
      "grad_norm": 1.0857064007178918,
      "learning_rate": 1.9883561816723575e-06,
      "loss": 0.5243,
      "step": 8907
    },
    {
      "epoch": 0.8019986945463549,
      "grad_norm": 1.0861429242062082,
      "learning_rate": 1.9866113554041643e-06,
      "loss": 0.4222,
      "step": 8908
    },
    {
      "epoch": 0.8020887258322267,
      "grad_norm": 1.231289154583358,
      "learning_rate": 1.984867210599516e-06,
      "loss": 0.4197,
      "step": 8909
    },
    {
      "epoch": 0.8021787571180985,
      "grad_norm": 1.3266606788270936,
      "learning_rate": 1.9831237474067355e-06,
      "loss": 0.6084,
      "step": 8910
    },
    {
      "epoch": 0.8022687884039704,
      "grad_norm": 2.4996801918709,
      "learning_rate": 1.98138096597409e-06,
      "loss": 0.6052,
      "step": 8911
    },
    {
      "epoch": 0.8023588196898422,
      "grad_norm": 1.3873988747096067,
      "learning_rate": 1.979638866449789e-06,
      "loss": 0.5313,
      "step": 8912
    },
    {
      "epoch": 0.8024488509757141,
      "grad_norm": 1.0915801377297216,
      "learning_rate": 1.977897448981975e-06,
      "loss": 0.5892,
      "step": 8913
    },
    {
      "epoch": 0.8025388822615859,
      "grad_norm": 1.2829235431678565,
      "learning_rate": 1.9761567137187467e-06,
      "loss": 0.48,
      "step": 8914
    },
    {
      "epoch": 0.8026289135474577,
      "grad_norm": 1.1577826503059125,
      "learning_rate": 1.9744166608081317e-06,
      "loss": 0.5396,
      "step": 8915
    },
    {
      "epoch": 0.8027189448333296,
      "grad_norm": 1.3626915627300737,
      "learning_rate": 1.9726772903981074e-06,
      "loss": 0.4168,
      "step": 8916
    },
    {
      "epoch": 0.8028089761192014,
      "grad_norm": 1.083492526678778,
      "learning_rate": 1.9709386026365907e-06,
      "loss": 0.5545,
      "step": 8917
    },
    {
      "epoch": 0.8028990074050733,
      "grad_norm": 0.9810960088449887,
      "learning_rate": 1.9692005976714414e-06,
      "loss": 0.4559,
      "step": 8918
    },
    {
      "epoch": 0.8029890386909451,
      "grad_norm": 1.0900807973021114,
      "learning_rate": 1.9674632756504587e-06,
      "loss": 0.583,
      "step": 8919
    },
    {
      "epoch": 0.803079069976817,
      "grad_norm": 1.1138744963808815,
      "learning_rate": 1.96572663672139e-06,
      "loss": 0.5925,
      "step": 8920
    },
    {
      "epoch": 0.8031691012626888,
      "grad_norm": 1.1958142427335716,
      "learning_rate": 1.963990681031912e-06,
      "loss": 0.5445,
      "step": 8921
    },
    {
      "epoch": 0.8032591325485606,
      "grad_norm": 1.3515761323188618,
      "learning_rate": 1.962255408729662e-06,
      "loss": 0.5947,
      "step": 8922
    },
    {
      "epoch": 0.8033491638344324,
      "grad_norm": 1.8592898022320399,
      "learning_rate": 1.9605208199621993e-06,
      "loss": 0.461,
      "step": 8923
    },
    {
      "epoch": 0.8034391951203043,
      "grad_norm": 1.5756175285863447,
      "learning_rate": 1.958786914877039e-06,
      "loss": 0.5554,
      "step": 8924
    },
    {
      "epoch": 0.8035292264061762,
      "grad_norm": 1.7308915046312938,
      "learning_rate": 1.9570536936216323e-06,
      "loss": 0.4655,
      "step": 8925
    },
    {
      "epoch": 0.803619257692048,
      "grad_norm": 1.6386612174204167,
      "learning_rate": 1.955321156343376e-06,
      "loss": 0.5628,
      "step": 8926
    },
    {
      "epoch": 0.8037092889779198,
      "grad_norm": 1.0437419926875522,
      "learning_rate": 1.9535893031895982e-06,
      "loss": 0.4937,
      "step": 8927
    },
    {
      "epoch": 0.8037993202637916,
      "grad_norm": 0.9465713637337638,
      "learning_rate": 1.951858134307588e-06,
      "loss": 0.4989,
      "step": 8928
    },
    {
      "epoch": 0.8038893515496635,
      "grad_norm": 1.0695641567773206,
      "learning_rate": 1.9501276498445575e-06,
      "loss": 0.5091,
      "step": 8929
    },
    {
      "epoch": 0.8039793828355354,
      "grad_norm": 1.1088678479583642,
      "learning_rate": 1.9483978499476698e-06,
      "loss": 0.5184,
      "step": 8930
    },
    {
      "epoch": 0.8040694141214072,
      "grad_norm": 1.256964608391651,
      "learning_rate": 1.946668734764029e-06,
      "loss": 0.5974,
      "step": 8931
    },
    {
      "epoch": 0.804159445407279,
      "grad_norm": 0.8895387317730724,
      "learning_rate": 1.944940304440679e-06,
      "loss": 0.462,
      "step": 8932
    },
    {
      "epoch": 0.8042494766931508,
      "grad_norm": 0.8751242419508708,
      "learning_rate": 1.9432125591246077e-06,
      "loss": 0.5007,
      "step": 8933
    },
    {
      "epoch": 0.8043395079790228,
      "grad_norm": 1.317624007832789,
      "learning_rate": 1.9414854989627454e-06,
      "loss": 0.5629,
      "step": 8934
    },
    {
      "epoch": 0.8044295392648946,
      "grad_norm": 1.1612504191409574,
      "learning_rate": 1.9397591241019552e-06,
      "loss": 0.4421,
      "step": 8935
    },
    {
      "epoch": 0.8045195705507664,
      "grad_norm": 1.1421211288413895,
      "learning_rate": 1.938033434689058e-06,
      "loss": 0.437,
      "step": 8936
    },
    {
      "epoch": 0.8046096018366382,
      "grad_norm": 1.3127108055642327,
      "learning_rate": 1.9363084308708023e-06,
      "loss": 0.5424,
      "step": 8937
    },
    {
      "epoch": 0.80469963312251,
      "grad_norm": 1.1107699539919107,
      "learning_rate": 1.9345841127938846e-06,
      "loss": 0.5476,
      "step": 8938
    },
    {
      "epoch": 0.804789664408382,
      "grad_norm": 1.1758195293584877,
      "learning_rate": 1.9328604806049424e-06,
      "loss": 0.5046,
      "step": 8939
    },
    {
      "epoch": 0.8048796956942538,
      "grad_norm": 1.613825339295187,
      "learning_rate": 1.9311375344505555e-06,
      "loss": 0.5355,
      "step": 8940
    },
    {
      "epoch": 0.8049697269801256,
      "grad_norm": 1.2400512641449168,
      "learning_rate": 1.929415274477239e-06,
      "loss": 0.4397,
      "step": 8941
    },
    {
      "epoch": 0.8050597582659974,
      "grad_norm": 1.100816665173984,
      "learning_rate": 1.9276937008314634e-06,
      "loss": 0.5291,
      "step": 8942
    },
    {
      "epoch": 0.8051497895518692,
      "grad_norm": 1.0448226482469314,
      "learning_rate": 1.9259728136596254e-06,
      "loss": 0.5433,
      "step": 8943
    },
    {
      "epoch": 0.8052398208377411,
      "grad_norm": 1.0806419944698935,
      "learning_rate": 1.924252613108073e-06,
      "loss": 0.5148,
      "step": 8944
    },
    {
      "epoch": 0.805329852123613,
      "grad_norm": 0.9725477265236474,
      "learning_rate": 1.922533099323094e-06,
      "loss": 0.517,
      "step": 8945
    },
    {
      "epoch": 0.8054198834094848,
      "grad_norm": 0.9920537880333803,
      "learning_rate": 1.9208142724509137e-06,
      "loss": 0.5413,
      "step": 8946
    },
    {
      "epoch": 0.8055099146953566,
      "grad_norm": 1.7078246094603544,
      "learning_rate": 1.9190961326377056e-06,
      "loss": 0.5373,
      "step": 8947
    },
    {
      "epoch": 0.8055999459812285,
      "grad_norm": 1.066304777393526,
      "learning_rate": 1.9173786800295837e-06,
      "loss": 0.5335,
      "step": 8948
    },
    {
      "epoch": 0.8056899772671003,
      "grad_norm": 1.411491125622132,
      "learning_rate": 1.915661914772591e-06,
      "loss": 0.5701,
      "step": 8949
    },
    {
      "epoch": 0.8057800085529722,
      "grad_norm": 1.3230287910973166,
      "learning_rate": 1.913945837012734e-06,
      "loss": 0.5834,
      "step": 8950
    },
    {
      "epoch": 0.805870039838844,
      "grad_norm": 0.891646145839159,
      "learning_rate": 1.912230446895943e-06,
      "loss": 0.504,
      "step": 8951
    },
    {
      "epoch": 0.8059600711247158,
      "grad_norm": 1.4039614255577615,
      "learning_rate": 1.9105157445680954e-06,
      "loss": 0.5707,
      "step": 8952
    },
    {
      "epoch": 0.8060501024105877,
      "grad_norm": 2.0099007476121606,
      "learning_rate": 1.9088017301750126e-06,
      "loss": 0.4602,
      "step": 8953
    },
    {
      "epoch": 0.8061401336964595,
      "grad_norm": 1.3598052880969589,
      "learning_rate": 1.907088403862457e-06,
      "loss": 0.5181,
      "step": 8954
    },
    {
      "epoch": 0.8062301649823314,
      "grad_norm": 2.028441168393953,
      "learning_rate": 1.905375765776124e-06,
      "loss": 0.4599,
      "step": 8955
    },
    {
      "epoch": 0.8063201962682032,
      "grad_norm": 1.052883539286587,
      "learning_rate": 1.903663816061667e-06,
      "loss": 0.5379,
      "step": 8956
    },
    {
      "epoch": 0.806410227554075,
      "grad_norm": 1.298338992382888,
      "learning_rate": 1.9019525548646644e-06,
      "loss": 0.452,
      "step": 8957
    },
    {
      "epoch": 0.8065002588399469,
      "grad_norm": 1.0880034025255074,
      "learning_rate": 1.900241982330644e-06,
      "loss": 0.473,
      "step": 8958
    },
    {
      "epoch": 0.8065902901258187,
      "grad_norm": 1.4921120118506055,
      "learning_rate": 1.898532098605076e-06,
      "loss": 0.5151,
      "step": 8959
    },
    {
      "epoch": 0.8066803214116905,
      "grad_norm": 1.1928513611556217,
      "learning_rate": 1.8968229038333675e-06,
      "loss": 0.5488,
      "step": 8960
    },
    {
      "epoch": 0.8067703526975624,
      "grad_norm": 1.122637222913637,
      "learning_rate": 1.895114398160872e-06,
      "loss": 0.592,
      "step": 8961
    },
    {
      "epoch": 0.8068603839834343,
      "grad_norm": 1.4236622323593255,
      "learning_rate": 1.8934065817328828e-06,
      "loss": 0.5114,
      "step": 8962
    },
    {
      "epoch": 0.8069504152693061,
      "grad_norm": 1.300975530501675,
      "learning_rate": 1.8916994546946265e-06,
      "loss": 0.5115,
      "step": 8963
    },
    {
      "epoch": 0.8070404465551779,
      "grad_norm": 1.5099518862180585,
      "learning_rate": 1.8899930171912896e-06,
      "loss": 0.4865,
      "step": 8964
    },
    {
      "epoch": 0.8071304778410497,
      "grad_norm": 1.7313158740576946,
      "learning_rate": 1.8882872693679787e-06,
      "loss": 0.5977,
      "step": 8965
    },
    {
      "epoch": 0.8072205091269216,
      "grad_norm": 1.4002986490407576,
      "learning_rate": 1.886582211369755e-06,
      "loss": 0.5027,
      "step": 8966
    },
    {
      "epoch": 0.8073105404127935,
      "grad_norm": 1.911190270430074,
      "learning_rate": 1.8848778433416193e-06,
      "loss": 0.5892,
      "step": 8967
    },
    {
      "epoch": 0.8074005716986653,
      "grad_norm": 1.5538554313217057,
      "learning_rate": 1.8831741654285119e-06,
      "loss": 0.4716,
      "step": 8968
    },
    {
      "epoch": 0.8074906029845371,
      "grad_norm": 1.4447396058173172,
      "learning_rate": 1.8814711777753092e-06,
      "loss": 0.5899,
      "step": 8969
    },
    {
      "epoch": 0.8075806342704089,
      "grad_norm": 1.3306064555282864,
      "learning_rate": 1.8797688805268443e-06,
      "loss": 0.4877,
      "step": 8970
    },
    {
      "epoch": 0.8076706655562808,
      "grad_norm": 1.3964941235036146,
      "learning_rate": 1.8780672738278727e-06,
      "loss": 0.5447,
      "step": 8971
    },
    {
      "epoch": 0.8077606968421527,
      "grad_norm": 1.692471198033248,
      "learning_rate": 1.8763663578231028e-06,
      "loss": 0.6007,
      "step": 8972
    },
    {
      "epoch": 0.8078507281280245,
      "grad_norm": 2.4228989388643787,
      "learning_rate": 1.8746661326571836e-06,
      "loss": 0.5039,
      "step": 8973
    },
    {
      "epoch": 0.8079407594138963,
      "grad_norm": 1.8899633483452745,
      "learning_rate": 1.8729665984747002e-06,
      "loss": 0.5439,
      "step": 8974
    },
    {
      "epoch": 0.8080307906997681,
      "grad_norm": 1.4102967796258412,
      "learning_rate": 1.8712677554201853e-06,
      "loss": 0.499,
      "step": 8975
    },
    {
      "epoch": 0.8081208219856401,
      "grad_norm": 1.3835320483267277,
      "learning_rate": 1.8695696036381095e-06,
      "loss": 0.5732,
      "step": 8976
    },
    {
      "epoch": 0.8082108532715119,
      "grad_norm": 1.15630909833245,
      "learning_rate": 1.8678721432728787e-06,
      "loss": 0.5014,
      "step": 8977
    },
    {
      "epoch": 0.8083008845573837,
      "grad_norm": 1.3650187808101482,
      "learning_rate": 1.8661753744688548e-06,
      "loss": 0.5208,
      "step": 8978
    },
    {
      "epoch": 0.8083909158432555,
      "grad_norm": 1.2319883511406715,
      "learning_rate": 1.8644792973703252e-06,
      "loss": 0.6264,
      "step": 8979
    },
    {
      "epoch": 0.8084809471291273,
      "grad_norm": 1.2448711289692918,
      "learning_rate": 1.8627839121215285e-06,
      "loss": 0.5262,
      "step": 8980
    },
    {
      "epoch": 0.8085709784149993,
      "grad_norm": 1.3356273433715502,
      "learning_rate": 1.8610892188666408e-06,
      "loss": 0.5077,
      "step": 8981
    },
    {
      "epoch": 0.8086610097008711,
      "grad_norm": 1.2672570459922945,
      "learning_rate": 1.859395217749781e-06,
      "loss": 0.4689,
      "step": 8982
    },
    {
      "epoch": 0.8087510409867429,
      "grad_norm": 1.0275249050089013,
      "learning_rate": 1.8577019089150028e-06,
      "loss": 0.497,
      "step": 8983
    },
    {
      "epoch": 0.8088410722726147,
      "grad_norm": 1.4121716081222853,
      "learning_rate": 1.8560092925063145e-06,
      "loss": 0.5728,
      "step": 8984
    },
    {
      "epoch": 0.8089311035584865,
      "grad_norm": 1.2908550493886992,
      "learning_rate": 1.8543173686676507e-06,
      "loss": 0.5293,
      "step": 8985
    },
    {
      "epoch": 0.8090211348443584,
      "grad_norm": 1.244099788949084,
      "learning_rate": 1.8526261375428955e-06,
      "loss": 0.4791,
      "step": 8986
    },
    {
      "epoch": 0.8091111661302303,
      "grad_norm": 1.8072369670091413,
      "learning_rate": 1.8509355992758738e-06,
      "loss": 0.486,
      "step": 8987
    },
    {
      "epoch": 0.8092011974161021,
      "grad_norm": 1.1650997893944381,
      "learning_rate": 1.849245754010347e-06,
      "loss": 0.4644,
      "step": 8988
    },
    {
      "epoch": 0.8092912287019739,
      "grad_norm": 1.6719278909481878,
      "learning_rate": 1.8475566018900238e-06,
      "loss": 0.4806,
      "step": 8989
    },
    {
      "epoch": 0.8093812599878458,
      "grad_norm": 0.9493120615899922,
      "learning_rate": 1.8458681430585513e-06,
      "loss": 0.4262,
      "step": 8990
    },
    {
      "epoch": 0.8094712912737176,
      "grad_norm": 1.181366053465422,
      "learning_rate": 1.8441803776595102e-06,
      "loss": 0.4777,
      "step": 8991
    },
    {
      "epoch": 0.8095613225595895,
      "grad_norm": 1.112517954949644,
      "learning_rate": 1.8424933058364392e-06,
      "loss": 0.4201,
      "step": 8992
    },
    {
      "epoch": 0.8096513538454613,
      "grad_norm": 1.5151743698044393,
      "learning_rate": 1.840806927732801e-06,
      "loss": 0.4861,
      "step": 8993
    },
    {
      "epoch": 0.8097413851313331,
      "grad_norm": 0.9609035533134259,
      "learning_rate": 1.839121243492007e-06,
      "loss": 0.3962,
      "step": 8994
    },
    {
      "epoch": 0.809831416417205,
      "grad_norm": 2.0503189686308607,
      "learning_rate": 1.8374362532574109e-06,
      "loss": 0.4947,
      "step": 8995
    },
    {
      "epoch": 0.8099214477030768,
      "grad_norm": 1.4814901675576533,
      "learning_rate": 1.835751957172306e-06,
      "loss": 0.4758,
      "step": 8996
    },
    {
      "epoch": 0.8100114789889487,
      "grad_norm": 1.2964800423681264,
      "learning_rate": 1.8340683553799189e-06,
      "loss": 0.592,
      "step": 8997
    },
    {
      "epoch": 0.8101015102748205,
      "grad_norm": 0.9726938515680071,
      "learning_rate": 1.8323854480234348e-06,
      "loss": 0.6145,
      "step": 8998
    },
    {
      "epoch": 0.8101915415606923,
      "grad_norm": 1.3346472638743638,
      "learning_rate": 1.830703235245962e-06,
      "loss": 0.5628,
      "step": 8999
    },
    {
      "epoch": 0.8102815728465642,
      "grad_norm": 1.5175973071800113,
      "learning_rate": 1.8290217171905577e-06,
      "loss": 0.5285,
      "step": 9000
    },
    {
      "epoch": 0.810371604132436,
      "grad_norm": 1.163718237345283,
      "learning_rate": 1.8273408940002202e-06,
      "loss": 0.4834,
      "step": 9001
    },
    {
      "epoch": 0.8104616354183078,
      "grad_norm": 1.2000354139288036,
      "learning_rate": 1.8256607658178893e-06,
      "loss": 0.5207,
      "step": 9002
    },
    {
      "epoch": 0.8105516667041797,
      "grad_norm": 1.1920817970918913,
      "learning_rate": 1.823981332786441e-06,
      "loss": 0.4308,
      "step": 9003
    },
    {
      "epoch": 0.8106416979900516,
      "grad_norm": 1.1550191036294457,
      "learning_rate": 1.8223025950487005e-06,
      "loss": 0.5068,
      "step": 9004
    },
    {
      "epoch": 0.8107317292759234,
      "grad_norm": 1.311195659846621,
      "learning_rate": 1.8206245527474199e-06,
      "loss": 0.5434,
      "step": 9005
    },
    {
      "epoch": 0.8108217605617952,
      "grad_norm": 1.6120113121359232,
      "learning_rate": 1.818947206025311e-06,
      "loss": 0.5812,
      "step": 9006
    },
    {
      "epoch": 0.810911791847667,
      "grad_norm": 0.9916750198787484,
      "learning_rate": 1.8172705550250093e-06,
      "loss": 0.5669,
      "step": 9007
    },
    {
      "epoch": 0.8110018231335389,
      "grad_norm": 1.1225855668092213,
      "learning_rate": 1.815594599889101e-06,
      "loss": 0.5498,
      "step": 9008
    },
    {
      "epoch": 0.8110918544194108,
      "grad_norm": 1.398065926986523,
      "learning_rate": 1.8139193407601098e-06,
      "loss": 0.4519,
      "step": 9009
    },
    {
      "epoch": 0.8111818857052826,
      "grad_norm": 1.1625666994984483,
      "learning_rate": 1.812244777780503e-06,
      "loss": 0.5688,
      "step": 9010
    },
    {
      "epoch": 0.8112719169911544,
      "grad_norm": 1.2754602995802284,
      "learning_rate": 1.8105709110926795e-06,
      "loss": 0.5873,
      "step": 9011
    },
    {
      "epoch": 0.8113619482770262,
      "grad_norm": 0.9185867323431742,
      "learning_rate": 1.8088977408389963e-06,
      "loss": 0.5377,
      "step": 9012
    },
    {
      "epoch": 0.8114519795628982,
      "grad_norm": 1.539113946155582,
      "learning_rate": 1.807225267161733e-06,
      "loss": 0.5205,
      "step": 9013
    },
    {
      "epoch": 0.81154201084877,
      "grad_norm": 1.5074510149326024,
      "learning_rate": 1.8055534902031191e-06,
      "loss": 0.5009,
      "step": 9014
    },
    {
      "epoch": 0.8116320421346418,
      "grad_norm": 1.2222448861832074,
      "learning_rate": 1.8038824101053253e-06,
      "loss": 0.5392,
      "step": 9015
    },
    {
      "epoch": 0.8117220734205136,
      "grad_norm": 1.2479522545430046,
      "learning_rate": 1.8022120270104615e-06,
      "loss": 0.4973,
      "step": 9016
    },
    {
      "epoch": 0.8118121047063854,
      "grad_norm": 1.000914216484134,
      "learning_rate": 1.8005423410605772e-06,
      "loss": 0.4984,
      "step": 9017
    },
    {
      "epoch": 0.8119021359922574,
      "grad_norm": 1.3918427099879238,
      "learning_rate": 1.7988733523976654e-06,
      "loss": 0.5251,
      "step": 9018
    },
    {
      "epoch": 0.8119921672781292,
      "grad_norm": 1.7029213994936254,
      "learning_rate": 1.7972050611636505e-06,
      "loss": 0.6131,
      "step": 9019
    },
    {
      "epoch": 0.812082198564001,
      "grad_norm": 1.045059884396168,
      "learning_rate": 1.7955374675004168e-06,
      "loss": 0.5525,
      "step": 9020
    },
    {
      "epoch": 0.8121722298498728,
      "grad_norm": 1.5150376236998997,
      "learning_rate": 1.7938705715497684e-06,
      "loss": 0.5611,
      "step": 9021
    },
    {
      "epoch": 0.8122622611357446,
      "grad_norm": 1.2320758006182224,
      "learning_rate": 1.7922043734534621e-06,
      "loss": 0.477,
      "step": 9022
    },
    {
      "epoch": 0.8123522924216166,
      "grad_norm": 1.1464524668543634,
      "learning_rate": 1.7905388733531925e-06,
      "loss": 0.5761,
      "step": 9023
    },
    {
      "epoch": 0.8124423237074884,
      "grad_norm": 1.045608063481651,
      "learning_rate": 1.7888740713905972e-06,
      "loss": 0.5412,
      "step": 9024
    },
    {
      "epoch": 0.8125323549933602,
      "grad_norm": 1.0785356653984526,
      "learning_rate": 1.7872099677072453e-06,
      "loss": 0.5269,
      "step": 9025
    },
    {
      "epoch": 0.812622386279232,
      "grad_norm": 1.0256566841653518,
      "learning_rate": 1.785546562444661e-06,
      "loss": 0.4705,
      "step": 9026
    },
    {
      "epoch": 0.8127124175651039,
      "grad_norm": 1.2907716229526913,
      "learning_rate": 1.783883855744296e-06,
      "loss": 0.4744,
      "step": 9027
    },
    {
      "epoch": 0.8128024488509757,
      "grad_norm": 1.406615063101477,
      "learning_rate": 1.7822218477475496e-06,
      "loss": 0.4867,
      "step": 9028
    },
    {
      "epoch": 0.8128924801368476,
      "grad_norm": 1.5107203588845963,
      "learning_rate": 1.7805605385957604e-06,
      "loss": 0.45,
      "step": 9029
    },
    {
      "epoch": 0.8129825114227194,
      "grad_norm": 1.189767992109048,
      "learning_rate": 1.7788999284302055e-06,
      "loss": 0.487,
      "step": 9030
    },
    {
      "epoch": 0.8130725427085912,
      "grad_norm": 0.9330794439418173,
      "learning_rate": 1.7772400173921067e-06,
      "loss": 0.3984,
      "step": 9031
    },
    {
      "epoch": 0.8131625739944631,
      "grad_norm": 1.193385400167364,
      "learning_rate": 1.7755808056226253e-06,
      "loss": 0.5093,
      "step": 9032
    },
    {
      "epoch": 0.8132526052803349,
      "grad_norm": 1.324134323483211,
      "learning_rate": 1.7739222932628531e-06,
      "loss": 0.4694,
      "step": 9033
    },
    {
      "epoch": 0.8133426365662068,
      "grad_norm": 1.5496442361100016,
      "learning_rate": 1.7722644804538424e-06,
      "loss": 0.5937,
      "step": 9034
    },
    {
      "epoch": 0.8134326678520786,
      "grad_norm": 1.0840331717276535,
      "learning_rate": 1.7706073673365677e-06,
      "loss": 0.5235,
      "step": 9035
    },
    {
      "epoch": 0.8135226991379504,
      "grad_norm": 1.7357724236018433,
      "learning_rate": 1.7689509540519512e-06,
      "loss": 0.5284,
      "step": 9036
    },
    {
      "epoch": 0.8136127304238223,
      "grad_norm": 1.359378359725979,
      "learning_rate": 1.7672952407408573e-06,
      "loss": 0.5203,
      "step": 9037
    },
    {
      "epoch": 0.8137027617096941,
      "grad_norm": 1.235197974658771,
      "learning_rate": 1.7656402275440897e-06,
      "loss": 0.437,
      "step": 9038
    },
    {
      "epoch": 0.813792792995566,
      "grad_norm": 1.0449745832278243,
      "learning_rate": 1.7639859146023862e-06,
      "loss": 0.5034,
      "step": 9039
    },
    {
      "epoch": 0.8138828242814378,
      "grad_norm": 2.891968146518121,
      "learning_rate": 1.7623323020564388e-06,
      "loss": 0.5288,
      "step": 9040
    },
    {
      "epoch": 0.8139728555673097,
      "grad_norm": 0.965354072672391,
      "learning_rate": 1.7606793900468654e-06,
      "loss": 0.4949,
      "step": 9041
    },
    {
      "epoch": 0.8140628868531815,
      "grad_norm": 1.167968091346063,
      "learning_rate": 1.7590271787142331e-06,
      "loss": 0.5603,
      "step": 9042
    },
    {
      "epoch": 0.8141529181390533,
      "grad_norm": 1.0705322414948575,
      "learning_rate": 1.7573756681990461e-06,
      "loss": 0.4735,
      "step": 9043
    },
    {
      "epoch": 0.8142429494249251,
      "grad_norm": 1.4145027711468174,
      "learning_rate": 1.7557248586417509e-06,
      "loss": 0.5684,
      "step": 9044
    },
    {
      "epoch": 0.814332980710797,
      "grad_norm": 1.1643192820403672,
      "learning_rate": 1.7540747501827338e-06,
      "loss": 0.5437,
      "step": 9045
    },
    {
      "epoch": 0.8144230119966689,
      "grad_norm": 1.3535626788883024,
      "learning_rate": 1.7524253429623217e-06,
      "loss": 0.5273,
      "step": 9046
    },
    {
      "epoch": 0.8145130432825407,
      "grad_norm": 1.0682880651563667,
      "learning_rate": 1.7507766371207747e-06,
      "loss": 0.5978,
      "step": 9047
    },
    {
      "epoch": 0.8146030745684125,
      "grad_norm": 1.3305560683583766,
      "learning_rate": 1.7491286327983093e-06,
      "loss": 0.5813,
      "step": 9048
    },
    {
      "epoch": 0.8146931058542843,
      "grad_norm": 1.0284148226845753,
      "learning_rate": 1.7474813301350668e-06,
      "loss": 0.5386,
      "step": 9049
    },
    {
      "epoch": 0.8147831371401562,
      "grad_norm": 1.1659947182462944,
      "learning_rate": 1.7458347292711352e-06,
      "loss": 0.4727,
      "step": 9050
    },
    {
      "epoch": 0.8148731684260281,
      "grad_norm": 1.3584879308297397,
      "learning_rate": 1.7441888303465448e-06,
      "loss": 0.5583,
      "step": 9051
    },
    {
      "epoch": 0.8149631997118999,
      "grad_norm": 0.9886964225967944,
      "learning_rate": 1.7425436335012646e-06,
      "loss": 0.5004,
      "step": 9052
    },
    {
      "epoch": 0.8150532309977717,
      "grad_norm": 1.2505467429631003,
      "learning_rate": 1.7408991388751961e-06,
      "loss": 0.4957,
      "step": 9053
    },
    {
      "epoch": 0.8151432622836435,
      "grad_norm": 1.0697991407743357,
      "learning_rate": 1.7392553466081985e-06,
      "loss": 0.5188,
      "step": 9054
    },
    {
      "epoch": 0.8152332935695155,
      "grad_norm": 1.0816800841496057,
      "learning_rate": 1.7376122568400533e-06,
      "loss": 0.5352,
      "step": 9055
    },
    {
      "epoch": 0.8153233248553873,
      "grad_norm": 1.3786663122401848,
      "learning_rate": 1.735969869710491e-06,
      "loss": 0.5488,
      "step": 9056
    },
    {
      "epoch": 0.8154133561412591,
      "grad_norm": 1.0799840873037092,
      "learning_rate": 1.7343281853591842e-06,
      "loss": 0.6012,
      "step": 9057
    },
    {
      "epoch": 0.8155033874271309,
      "grad_norm": 1.0891356180896898,
      "learning_rate": 1.7326872039257403e-06,
      "loss": 0.4624,
      "step": 9058
    },
    {
      "epoch": 0.8155934187130027,
      "grad_norm": 1.1277590202845749,
      "learning_rate": 1.7310469255497099e-06,
      "loss": 0.4783,
      "step": 9059
    },
    {
      "epoch": 0.8156834499988747,
      "grad_norm": 1.7912812682228985,
      "learning_rate": 1.7294073503705855e-06,
      "loss": 0.5645,
      "step": 9060
    },
    {
      "epoch": 0.8157734812847465,
      "grad_norm": 1.0157643966998735,
      "learning_rate": 1.7277684785277915e-06,
      "loss": 0.4965,
      "step": 9061
    },
    {
      "epoch": 0.8158635125706183,
      "grad_norm": 1.1604591236659467,
      "learning_rate": 1.7261303101607075e-06,
      "loss": 0.4759,
      "step": 9062
    },
    {
      "epoch": 0.8159535438564901,
      "grad_norm": 1.485722183283366,
      "learning_rate": 1.7244928454086362e-06,
      "loss": 0.4462,
      "step": 9063
    },
    {
      "epoch": 0.8160435751423619,
      "grad_norm": 1.306735102207615,
      "learning_rate": 1.7228560844108332e-06,
      "loss": 0.4566,
      "step": 9064
    },
    {
      "epoch": 0.8161336064282338,
      "grad_norm": 0.8616233428762832,
      "learning_rate": 1.7212200273064883e-06,
      "loss": 0.5369,
      "step": 9065
    },
    {
      "epoch": 0.8162236377141057,
      "grad_norm": 1.3277931681074484,
      "learning_rate": 1.7195846742347343e-06,
      "loss": 0.5089,
      "step": 9066
    },
    {
      "epoch": 0.8163136689999775,
      "grad_norm": 1.7130810221525266,
      "learning_rate": 1.717950025334637e-06,
      "loss": 0.4585,
      "step": 9067
    },
    {
      "epoch": 0.8164037002858493,
      "grad_norm": 1.1009133964754059,
      "learning_rate": 1.7163160807452173e-06,
      "loss": 0.5878,
      "step": 9068
    },
    {
      "epoch": 0.8164937315717212,
      "grad_norm": 1.5874224304432054,
      "learning_rate": 1.7146828406054194e-06,
      "loss": 0.5107,
      "step": 9069
    },
    {
      "epoch": 0.816583762857593,
      "grad_norm": 1.5271236092944123,
      "learning_rate": 1.7130503050541368e-06,
      "loss": 0.5548,
      "step": 9070
    },
    {
      "epoch": 0.8166737941434649,
      "grad_norm": 1.2241393176809925,
      "learning_rate": 1.711418474230202e-06,
      "loss": 0.508,
      "step": 9071
    },
    {
      "epoch": 0.8167638254293367,
      "grad_norm": 1.2639322672904933,
      "learning_rate": 1.709787348272387e-06,
      "loss": 0.4649,
      "step": 9072
    },
    {
      "epoch": 0.8168538567152085,
      "grad_norm": 1.4847349121593207,
      "learning_rate": 1.7081569273194042e-06,
      "loss": 0.5308,
      "step": 9073
    },
    {
      "epoch": 0.8169438880010804,
      "grad_norm": 1.0770982404516898,
      "learning_rate": 1.7065272115099075e-06,
      "loss": 0.5421,
      "step": 9074
    },
    {
      "epoch": 0.8170339192869522,
      "grad_norm": 1.1549729844204253,
      "learning_rate": 1.7048982009824811e-06,
      "loss": 0.5713,
      "step": 9075
    },
    {
      "epoch": 0.817123950572824,
      "grad_norm": 1.3332227065590054,
      "learning_rate": 1.7032698958756678e-06,
      "loss": 0.6422,
      "step": 9076
    },
    {
      "epoch": 0.8172139818586959,
      "grad_norm": 1.064836649350538,
      "learning_rate": 1.7016422963279312e-06,
      "loss": 0.4234,
      "step": 9077
    },
    {
      "epoch": 0.8173040131445677,
      "grad_norm": 1.3154653909941856,
      "learning_rate": 1.7000154024776872e-06,
      "loss": 0.5606,
      "step": 9078
    },
    {
      "epoch": 0.8173940444304396,
      "grad_norm": 1.7030508617857059,
      "learning_rate": 1.6983892144632863e-06,
      "loss": 0.554,
      "step": 9079
    },
    {
      "epoch": 0.8174840757163114,
      "grad_norm": 1.608678344060327,
      "learning_rate": 1.6967637324230247e-06,
      "loss": 0.4981,
      "step": 9080
    },
    {
      "epoch": 0.8175741070021832,
      "grad_norm": 1.3915672393798995,
      "learning_rate": 1.6951389564951259e-06,
      "loss": 0.5664,
      "step": 9081
    },
    {
      "epoch": 0.8176641382880551,
      "grad_norm": 1.5527088038157804,
      "learning_rate": 1.693514886817772e-06,
      "loss": 0.5394,
      "step": 9082
    },
    {
      "epoch": 0.817754169573927,
      "grad_norm": 1.0499768279713322,
      "learning_rate": 1.691891523529068e-06,
      "loss": 0.4666,
      "step": 9083
    },
    {
      "epoch": 0.8178442008597988,
      "grad_norm": 0.9505380102127262,
      "learning_rate": 1.6902688667670674e-06,
      "loss": 0.56,
      "step": 9084
    },
    {
      "epoch": 0.8179342321456706,
      "grad_norm": 1.2500617247900359,
      "learning_rate": 1.6886469166697616e-06,
      "loss": 0.5432,
      "step": 9085
    },
    {
      "epoch": 0.8180242634315424,
      "grad_norm": 1.1296228771262808,
      "learning_rate": 1.6870256733750833e-06,
      "loss": 0.5688,
      "step": 9086
    },
    {
      "epoch": 0.8181142947174143,
      "grad_norm": 0.9641530450099177,
      "learning_rate": 1.6854051370209034e-06,
      "loss": 0.5518,
      "step": 9087
    },
    {
      "epoch": 0.8182043260032862,
      "grad_norm": 1.6921606245248784,
      "learning_rate": 1.6837853077450373e-06,
      "loss": 0.5619,
      "step": 9088
    },
    {
      "epoch": 0.818294357289158,
      "grad_norm": 1.5117693786581725,
      "learning_rate": 1.6821661856852268e-06,
      "loss": 0.492,
      "step": 9089
    },
    {
      "epoch": 0.8183843885750298,
      "grad_norm": 1.2004554551380604,
      "learning_rate": 1.680547770979173e-06,
      "loss": 0.5774,
      "step": 9090
    },
    {
      "epoch": 0.8184744198609016,
      "grad_norm": 1.7187145876013386,
      "learning_rate": 1.6789300637645e-06,
      "loss": 0.5628,
      "step": 9091
    },
    {
      "epoch": 0.8185644511467735,
      "grad_norm": 1.3817047029126956,
      "learning_rate": 1.6773130641787828e-06,
      "loss": 0.5671,
      "step": 9092
    },
    {
      "epoch": 0.8186544824326454,
      "grad_norm": 1.3616119501547568,
      "learning_rate": 1.6756967723595297e-06,
      "loss": 0.5902,
      "step": 9093
    },
    {
      "epoch": 0.8187445137185172,
      "grad_norm": 1.337803194148141,
      "learning_rate": 1.6740811884441944e-06,
      "loss": 0.4961,
      "step": 9094
    },
    {
      "epoch": 0.818834545004389,
      "grad_norm": 1.2090460106257315,
      "learning_rate": 1.6724663125701591e-06,
      "loss": 0.4647,
      "step": 9095
    },
    {
      "epoch": 0.8189245762902608,
      "grad_norm": 1.1824771240161411,
      "learning_rate": 1.670852144874765e-06,
      "loss": 0.5158,
      "step": 9096
    },
    {
      "epoch": 0.8190146075761328,
      "grad_norm": 1.416949085559203,
      "learning_rate": 1.6692386854952746e-06,
      "loss": 0.4991,
      "step": 9097
    },
    {
      "epoch": 0.8191046388620046,
      "grad_norm": 1.5217079959971882,
      "learning_rate": 1.6676259345688982e-06,
      "loss": 0.5169,
      "step": 9098
    },
    {
      "epoch": 0.8191946701478764,
      "grad_norm": 1.64661624990934,
      "learning_rate": 1.6660138922327873e-06,
      "loss": 0.5386,
      "step": 9099
    },
    {
      "epoch": 0.8192847014337482,
      "grad_norm": 0.9727975607169688,
      "learning_rate": 1.6644025586240298e-06,
      "loss": 0.5149,
      "step": 9100
    },
    {
      "epoch": 0.81937473271962,
      "grad_norm": 1.425777089166163,
      "learning_rate": 1.662791933879654e-06,
      "loss": 0.5601,
      "step": 9101
    },
    {
      "epoch": 0.819464764005492,
      "grad_norm": 1.33308503102823,
      "learning_rate": 1.6611820181366322e-06,
      "loss": 0.4528,
      "step": 9102
    },
    {
      "epoch": 0.8195547952913638,
      "grad_norm": 1.1029960700503416,
      "learning_rate": 1.6595728115318644e-06,
      "loss": 0.5128,
      "step": 9103
    },
    {
      "epoch": 0.8196448265772356,
      "grad_norm": 1.0924123410939504,
      "learning_rate": 1.6579643142022083e-06,
      "loss": 0.5981,
      "step": 9104
    },
    {
      "epoch": 0.8197348578631074,
      "grad_norm": 1.3115453090933509,
      "learning_rate": 1.6563565262844438e-06,
      "loss": 0.4883,
      "step": 9105
    },
    {
      "epoch": 0.8198248891489792,
      "grad_norm": 1.1389751190793467,
      "learning_rate": 1.6547494479153027e-06,
      "loss": 0.487,
      "step": 9106
    },
    {
      "epoch": 0.8199149204348511,
      "grad_norm": 1.3085240467166452,
      "learning_rate": 1.6531430792314496e-06,
      "loss": 0.5243,
      "step": 9107
    },
    {
      "epoch": 0.820004951720723,
      "grad_norm": 1.646814392394213,
      "learning_rate": 1.6515374203694944e-06,
      "loss": 0.4865,
      "step": 9108
    },
    {
      "epoch": 0.8200949830065948,
      "grad_norm": 1.168130236188479,
      "learning_rate": 1.649932471465976e-06,
      "loss": 0.4774,
      "step": 9109
    },
    {
      "epoch": 0.8201850142924666,
      "grad_norm": 1.89189607298675,
      "learning_rate": 1.6483282326573902e-06,
      "loss": 0.6269,
      "step": 9110
    },
    {
      "epoch": 0.8202750455783385,
      "grad_norm": 2.258151906892837,
      "learning_rate": 1.6467247040801538e-06,
      "loss": 0.4567,
      "step": 9111
    },
    {
      "epoch": 0.8203650768642103,
      "grad_norm": 1.3617493777775604,
      "learning_rate": 1.6451218858706374e-06,
      "loss": 0.4346,
      "step": 9112
    },
    {
      "epoch": 0.8204551081500822,
      "grad_norm": 1.3847576940706863,
      "learning_rate": 1.6435197781651424e-06,
      "loss": 0.5178,
      "step": 9113
    },
    {
      "epoch": 0.820545139435954,
      "grad_norm": 1.5119541004398298,
      "learning_rate": 1.6419183810999151e-06,
      "loss": 0.4904,
      "step": 9114
    },
    {
      "epoch": 0.8206351707218258,
      "grad_norm": 1.0943651726601098,
      "learning_rate": 1.6403176948111376e-06,
      "loss": 0.545,
      "step": 9115
    },
    {
      "epoch": 0.8207252020076977,
      "grad_norm": 1.1938268124822211,
      "learning_rate": 1.638717719434938e-06,
      "loss": 0.4855,
      "step": 9116
    },
    {
      "epoch": 0.8208152332935695,
      "grad_norm": 1.4842456471673706,
      "learning_rate": 1.6371184551073694e-06,
      "loss": 0.5075,
      "step": 9117
    },
    {
      "epoch": 0.8209052645794414,
      "grad_norm": 1.4705100236600543,
      "learning_rate": 1.6355199019644452e-06,
      "loss": 0.4745,
      "step": 9118
    },
    {
      "epoch": 0.8209952958653132,
      "grad_norm": 1.5372259300956643,
      "learning_rate": 1.6339220601421003e-06,
      "loss": 0.5146,
      "step": 9119
    },
    {
      "epoch": 0.821085327151185,
      "grad_norm": 1.2998744332174428,
      "learning_rate": 1.6323249297762177e-06,
      "loss": 0.4831,
      "step": 9120
    },
    {
      "epoch": 0.8211753584370569,
      "grad_norm": 1.5333626448886188,
      "learning_rate": 1.6307285110026206e-06,
      "loss": 0.4917,
      "step": 9121
    },
    {
      "epoch": 0.8212653897229287,
      "grad_norm": 1.083419196624285,
      "learning_rate": 1.6291328039570685e-06,
      "loss": 0.4507,
      "step": 9122
    },
    {
      "epoch": 0.8213554210088005,
      "grad_norm": 1.2034959783081434,
      "learning_rate": 1.6275378087752558e-06,
      "loss": 0.4253,
      "step": 9123
    },
    {
      "epoch": 0.8214454522946724,
      "grad_norm": 1.2847191130707605,
      "learning_rate": 1.6259435255928324e-06,
      "loss": 0.5679,
      "step": 9124
    },
    {
      "epoch": 0.8215354835805443,
      "grad_norm": 1.2695563436971706,
      "learning_rate": 1.624349954545369e-06,
      "loss": 0.5089,
      "step": 9125
    },
    {
      "epoch": 0.8216255148664161,
      "grad_norm": 1.277900180750386,
      "learning_rate": 1.6227570957683847e-06,
      "loss": 0.4853,
      "step": 9126
    },
    {
      "epoch": 0.8217155461522879,
      "grad_norm": 1.3336662052828785,
      "learning_rate": 1.6211649493973402e-06,
      "loss": 0.473,
      "step": 9127
    },
    {
      "epoch": 0.8218055774381597,
      "grad_norm": 1.3906899136984547,
      "learning_rate": 1.6195735155676307e-06,
      "loss": 0.5069,
      "step": 9128
    },
    {
      "epoch": 0.8218956087240316,
      "grad_norm": 1.5082164136387746,
      "learning_rate": 1.6179827944145933e-06,
      "loss": 0.4171,
      "step": 9129
    },
    {
      "epoch": 0.8219856400099035,
      "grad_norm": 1.4297411505835165,
      "learning_rate": 1.6163927860735063e-06,
      "loss": 0.5562,
      "step": 9130
    },
    {
      "epoch": 0.8220756712957753,
      "grad_norm": 1.826369123369363,
      "learning_rate": 1.6148034906795772e-06,
      "loss": 0.5271,
      "step": 9131
    },
    {
      "epoch": 0.8221657025816471,
      "grad_norm": 1.0794210085331328,
      "learning_rate": 1.6132149083679716e-06,
      "loss": 0.5234,
      "step": 9132
    },
    {
      "epoch": 0.8222557338675189,
      "grad_norm": 1.7269430497997205,
      "learning_rate": 1.6116270392737753e-06,
      "loss": 0.6044,
      "step": 9133
    },
    {
      "epoch": 0.8223457651533908,
      "grad_norm": 1.2383306760007367,
      "learning_rate": 1.6100398835320241e-06,
      "loss": 0.5155,
      "step": 9134
    },
    {
      "epoch": 0.8224357964392627,
      "grad_norm": 1.071593816767657,
      "learning_rate": 1.6084534412776908e-06,
      "loss": 0.6447,
      "step": 9135
    },
    {
      "epoch": 0.8225258277251345,
      "grad_norm": 1.4553938719446922,
      "learning_rate": 1.60686771264569e-06,
      "loss": 0.509,
      "step": 9136
    },
    {
      "epoch": 0.8226158590110063,
      "grad_norm": 1.1747313275361835,
      "learning_rate": 1.6052826977708658e-06,
      "loss": 0.5077,
      "step": 9137
    },
    {
      "epoch": 0.8227058902968781,
      "grad_norm": 1.1926710524014712,
      "learning_rate": 1.6036983967880181e-06,
      "loss": 0.4796,
      "step": 9138
    },
    {
      "epoch": 0.8227959215827501,
      "grad_norm": 1.1227520302714296,
      "learning_rate": 1.6021148098318695e-06,
      "loss": 0.5637,
      "step": 9139
    },
    {
      "epoch": 0.8228859528686219,
      "grad_norm": 0.9968503338132221,
      "learning_rate": 1.6005319370370931e-06,
      "loss": 0.486,
      "step": 9140
    },
    {
      "epoch": 0.8229759841544937,
      "grad_norm": 1.1172725872356923,
      "learning_rate": 1.5989497785382958e-06,
      "loss": 0.4775,
      "step": 9141
    },
    {
      "epoch": 0.8230660154403655,
      "grad_norm": 1.2937844468832849,
      "learning_rate": 1.5973683344700263e-06,
      "loss": 0.5143,
      "step": 9142
    },
    {
      "epoch": 0.8231560467262373,
      "grad_norm": 1.7721757098717987,
      "learning_rate": 1.595787604966771e-06,
      "loss": 0.6576,
      "step": 9143
    },
    {
      "epoch": 0.8232460780121093,
      "grad_norm": 1.1401084764176528,
      "learning_rate": 1.5942075901629583e-06,
      "loss": 0.4567,
      "step": 9144
    },
    {
      "epoch": 0.8233361092979811,
      "grad_norm": 1.813684340316715,
      "learning_rate": 1.5926282901929479e-06,
      "loss": 0.4809,
      "step": 9145
    },
    {
      "epoch": 0.8234261405838529,
      "grad_norm": 1.2157328151190259,
      "learning_rate": 1.5910497051910522e-06,
      "loss": 0.4803,
      "step": 9146
    },
    {
      "epoch": 0.8235161718697247,
      "grad_norm": 1.2394122485867312,
      "learning_rate": 1.5894718352915085e-06,
      "loss": 0.5508,
      "step": 9147
    },
    {
      "epoch": 0.8236062031555965,
      "grad_norm": 2.328569486959722,
      "learning_rate": 1.5878946806285023e-06,
      "loss": 0.5372,
      "step": 9148
    },
    {
      "epoch": 0.8236962344414684,
      "grad_norm": 1.8448068843051086,
      "learning_rate": 1.5863182413361566e-06,
      "loss": 0.4846,
      "step": 9149
    },
    {
      "epoch": 0.8237862657273403,
      "grad_norm": 1.3530574900281203,
      "learning_rate": 1.5847425175485343e-06,
      "loss": 0.4602,
      "step": 9150
    },
    {
      "epoch": 0.8238762970132121,
      "grad_norm": 0.9822100976823215,
      "learning_rate": 1.583167509399629e-06,
      "loss": 0.6196,
      "step": 9151
    },
    {
      "epoch": 0.8239663282990839,
      "grad_norm": 1.2737599508489668,
      "learning_rate": 1.5815932170233894e-06,
      "loss": 0.5299,
      "step": 9152
    },
    {
      "epoch": 0.8240563595849558,
      "grad_norm": 1.968760942211567,
      "learning_rate": 1.5800196405536872e-06,
      "loss": 0.615,
      "step": 9153
    },
    {
      "epoch": 0.8241463908708276,
      "grad_norm": 1.146362805414644,
      "learning_rate": 1.578446780124344e-06,
      "loss": 0.4976,
      "step": 9154
    },
    {
      "epoch": 0.8242364221566995,
      "grad_norm": 1.4642368316411376,
      "learning_rate": 1.5768746358691156e-06,
      "loss": 0.4477,
      "step": 9155
    },
    {
      "epoch": 0.8243264534425713,
      "grad_norm": 1.9588744520667833,
      "learning_rate": 1.575303207921698e-06,
      "loss": 0.5117,
      "step": 9156
    },
    {
      "epoch": 0.8244164847284431,
      "grad_norm": 1.1328181265360353,
      "learning_rate": 1.5737324964157274e-06,
      "loss": 0.522,
      "step": 9157
    },
    {
      "epoch": 0.824506516014315,
      "grad_norm": 1.3141677228520632,
      "learning_rate": 1.5721625014847796e-06,
      "loss": 0.531,
      "step": 9158
    },
    {
      "epoch": 0.8245965473001868,
      "grad_norm": 1.1822163540221797,
      "learning_rate": 1.5705932232623612e-06,
      "loss": 0.4753,
      "step": 9159
    },
    {
      "epoch": 0.8246865785860586,
      "grad_norm": 1.107329880897126,
      "learning_rate": 1.5690246618819338e-06,
      "loss": 0.6033,
      "step": 9160
    },
    {
      "epoch": 0.8247766098719305,
      "grad_norm": 1.7263092807359566,
      "learning_rate": 1.5674568174768823e-06,
      "loss": 0.5378,
      "step": 9161
    },
    {
      "epoch": 0.8248666411578023,
      "grad_norm": 1.4449179589091052,
      "learning_rate": 1.5658896901805387e-06,
      "loss": 0.4657,
      "step": 9162
    },
    {
      "epoch": 0.8249566724436742,
      "grad_norm": 1.3495235787996074,
      "learning_rate": 1.5643232801261731e-06,
      "loss": 0.5406,
      "step": 9163
    },
    {
      "epoch": 0.825046703729546,
      "grad_norm": 1.3394308197110814,
      "learning_rate": 1.562757587446997e-06,
      "loss": 0.5161,
      "step": 9164
    },
    {
      "epoch": 0.8251367350154178,
      "grad_norm": 1.9660223141213609,
      "learning_rate": 1.561192612276149e-06,
      "loss": 0.5955,
      "step": 9165
    },
    {
      "epoch": 0.8252267663012897,
      "grad_norm": 1.3973928016903736,
      "learning_rate": 1.559628354746726e-06,
      "loss": 0.5229,
      "step": 9166
    },
    {
      "epoch": 0.8253167975871616,
      "grad_norm": 1.7475542022860941,
      "learning_rate": 1.5580648149917477e-06,
      "loss": 0.5809,
      "step": 9167
    },
    {
      "epoch": 0.8254068288730334,
      "grad_norm": 1.5988069257084676,
      "learning_rate": 1.5565019931441783e-06,
      "loss": 0.5003,
      "step": 9168
    },
    {
      "epoch": 0.8254968601589052,
      "grad_norm": 1.4261825783598459,
      "learning_rate": 1.5549398893369216e-06,
      "loss": 0.5043,
      "step": 9169
    },
    {
      "epoch": 0.825586891444777,
      "grad_norm": 1.1819574413248366,
      "learning_rate": 1.5533785037028214e-06,
      "loss": 0.5769,
      "step": 9170
    },
    {
      "epoch": 0.8256769227306489,
      "grad_norm": 1.3965591149007595,
      "learning_rate": 1.5518178363746584e-06,
      "loss": 0.527,
      "step": 9171
    },
    {
      "epoch": 0.8257669540165208,
      "grad_norm": 1.1594001247508696,
      "learning_rate": 1.5502578874851549e-06,
      "loss": 0.538,
      "step": 9172
    },
    {
      "epoch": 0.8258569853023926,
      "grad_norm": 2.092159232926122,
      "learning_rate": 1.5486986571669627e-06,
      "loss": 0.517,
      "step": 9173
    },
    {
      "epoch": 0.8259470165882644,
      "grad_norm": 1.3024877926491658,
      "learning_rate": 1.547140145552688e-06,
      "loss": 0.5736,
      "step": 9174
    },
    {
      "epoch": 0.8260370478741362,
      "grad_norm": 1.17640489944007,
      "learning_rate": 1.5455823527748626e-06,
      "loss": 0.5061,
      "step": 9175
    },
    {
      "epoch": 0.826127079160008,
      "grad_norm": 1.0796102486047907,
      "learning_rate": 1.5440252789659637e-06,
      "loss": 0.4615,
      "step": 9176
    },
    {
      "epoch": 0.82621711044588,
      "grad_norm": 1.387952603703102,
      "learning_rate": 1.5424689242584056e-06,
      "loss": 0.5628,
      "step": 9177
    },
    {
      "epoch": 0.8263071417317518,
      "grad_norm": 1.1330890955506834,
      "learning_rate": 1.5409132887845446e-06,
      "loss": 0.5256,
      "step": 9178
    },
    {
      "epoch": 0.8263971730176236,
      "grad_norm": 1.1221391647109125,
      "learning_rate": 1.5393583726766648e-06,
      "loss": 0.5771,
      "step": 9179
    },
    {
      "epoch": 0.8264872043034954,
      "grad_norm": 1.7572530810628952,
      "learning_rate": 1.5378041760670071e-06,
      "loss": 0.4831,
      "step": 9180
    },
    {
      "epoch": 0.8265772355893674,
      "grad_norm": 1.708241938765687,
      "learning_rate": 1.5362506990877357e-06,
      "loss": 0.5672,
      "step": 9181
    },
    {
      "epoch": 0.8266672668752392,
      "grad_norm": 0.9923657906599251,
      "learning_rate": 1.5346979418709595e-06,
      "loss": 0.5588,
      "step": 9182
    },
    {
      "epoch": 0.826757298161111,
      "grad_norm": 1.1308327718293036,
      "learning_rate": 1.5331459045487262e-06,
      "loss": 0.5744,
      "step": 9183
    },
    {
      "epoch": 0.8268473294469828,
      "grad_norm": 1.147328090206175,
      "learning_rate": 1.5315945872530236e-06,
      "loss": 0.5614,
      "step": 9184
    },
    {
      "epoch": 0.8269373607328546,
      "grad_norm": 1.3992590895583752,
      "learning_rate": 1.5300439901157749e-06,
      "loss": 0.5062,
      "step": 9185
    },
    {
      "epoch": 0.8270273920187265,
      "grad_norm": 1.0532373596764704,
      "learning_rate": 1.528494113268847e-06,
      "loss": 0.5386,
      "step": 9186
    },
    {
      "epoch": 0.8271174233045984,
      "grad_norm": 1.197422331229233,
      "learning_rate": 1.5269449568440365e-06,
      "loss": 0.5257,
      "step": 9187
    },
    {
      "epoch": 0.8272074545904702,
      "grad_norm": 2.0263765777016998,
      "learning_rate": 1.5253965209730915e-06,
      "loss": 0.5304,
      "step": 9188
    },
    {
      "epoch": 0.827297485876342,
      "grad_norm": 1.4332784743445173,
      "learning_rate": 1.5238488057876865e-06,
      "loss": 0.5195,
      "step": 9189
    },
    {
      "epoch": 0.8273875171622138,
      "grad_norm": 1.3281262212591658,
      "learning_rate": 1.5223018114194422e-06,
      "loss": 0.4918,
      "step": 9190
    },
    {
      "epoch": 0.8274775484480857,
      "grad_norm": 1.1934429117050072,
      "learning_rate": 1.5207555379999161e-06,
      "loss": 0.4651,
      "step": 9191
    },
    {
      "epoch": 0.8275675797339576,
      "grad_norm": 1.0965824488131286,
      "learning_rate": 1.5192099856606068e-06,
      "loss": 0.4636,
      "step": 9192
    },
    {
      "epoch": 0.8276576110198294,
      "grad_norm": 1.2486894554833887,
      "learning_rate": 1.5176651545329424e-06,
      "loss": 0.4793,
      "step": 9193
    },
    {
      "epoch": 0.8277476423057012,
      "grad_norm": 1.2278460436692655,
      "learning_rate": 1.5161210447483044e-06,
      "loss": 0.5843,
      "step": 9194
    },
    {
      "epoch": 0.8278376735915731,
      "grad_norm": 2.2412455147317814,
      "learning_rate": 1.5145776564379977e-06,
      "loss": 0.41,
      "step": 9195
    },
    {
      "epoch": 0.8279277048774449,
      "grad_norm": 1.2642099621440313,
      "learning_rate": 1.5130349897332764e-06,
      "loss": 0.4462,
      "step": 9196
    },
    {
      "epoch": 0.8280177361633168,
      "grad_norm": 1.3139973865527712,
      "learning_rate": 1.5114930447653299e-06,
      "loss": 0.4982,
      "step": 9197
    },
    {
      "epoch": 0.8281077674491886,
      "grad_norm": 1.154861428522328,
      "learning_rate": 1.5099518216652853e-06,
      "loss": 0.466,
      "step": 9198
    },
    {
      "epoch": 0.8281977987350604,
      "grad_norm": 1.1376685055707747,
      "learning_rate": 1.5084113205642104e-06,
      "loss": 0.5302,
      "step": 9199
    },
    {
      "epoch": 0.8282878300209323,
      "grad_norm": 0.9879100273251014,
      "learning_rate": 1.5068715415931112e-06,
      "loss": 0.5537,
      "step": 9200
    },
    {
      "epoch": 0.8283778613068041,
      "grad_norm": 1.047178578151821,
      "learning_rate": 1.5053324848829254e-06,
      "loss": 0.5436,
      "step": 9201
    },
    {
      "epoch": 0.828467892592676,
      "grad_norm": 1.7692682345633564,
      "learning_rate": 1.5037941505645437e-06,
      "loss": 0.4218,
      "step": 9202
    },
    {
      "epoch": 0.8285579238785478,
      "grad_norm": 1.3998627330237459,
      "learning_rate": 1.5022565387687816e-06,
      "loss": 0.6198,
      "step": 9203
    },
    {
      "epoch": 0.8286479551644196,
      "grad_norm": 1.2146497044823703,
      "learning_rate": 1.5007196496263998e-06,
      "loss": 0.5779,
      "step": 9204
    },
    {
      "epoch": 0.8287379864502915,
      "grad_norm": 1.267520507903043,
      "learning_rate": 1.4991834832680962e-06,
      "loss": 0.5269,
      "step": 9205
    },
    {
      "epoch": 0.8288280177361633,
      "grad_norm": 1.2037846154706668,
      "learning_rate": 1.4976480398245107e-06,
      "loss": 0.5603,
      "step": 9206
    },
    {
      "epoch": 0.8289180490220351,
      "grad_norm": 1.5361825249628378,
      "learning_rate": 1.4961133194262101e-06,
      "loss": 0.5498,
      "step": 9207
    },
    {
      "epoch": 0.829008080307907,
      "grad_norm": 1.3215571107909394,
      "learning_rate": 1.4945793222037186e-06,
      "loss": 0.5496,
      "step": 9208
    },
    {
      "epoch": 0.8290981115937789,
      "grad_norm": 1.2422947211295052,
      "learning_rate": 1.4930460482874797e-06,
      "loss": 0.483,
      "step": 9209
    },
    {
      "epoch": 0.8291881428796507,
      "grad_norm": 1.0243665102284039,
      "learning_rate": 1.4915134978078881e-06,
      "loss": 0.5198,
      "step": 9210
    },
    {
      "epoch": 0.8292781741655225,
      "grad_norm": 1.0922643543906005,
      "learning_rate": 1.4899816708952718e-06,
      "loss": 0.4624,
      "step": 9211
    },
    {
      "epoch": 0.8293682054513943,
      "grad_norm": 1.155653744261343,
      "learning_rate": 1.4884505676798976e-06,
      "loss": 0.4349,
      "step": 9212
    },
    {
      "epoch": 0.8294582367372662,
      "grad_norm": 1.0337740880156245,
      "learning_rate": 1.4869201882919738e-06,
      "loss": 0.4653,
      "step": 9213
    },
    {
      "epoch": 0.8295482680231381,
      "grad_norm": 1.9277736864289619,
      "learning_rate": 1.4853905328616446e-06,
      "loss": 0.5359,
      "step": 9214
    },
    {
      "epoch": 0.8296382993090099,
      "grad_norm": 1.7242360616048182,
      "learning_rate": 1.4838616015189867e-06,
      "loss": 0.5354,
      "step": 9215
    },
    {
      "epoch": 0.8297283305948817,
      "grad_norm": 1.451405948932057,
      "learning_rate": 1.482333394394031e-06,
      "loss": 0.4576,
      "step": 9216
    },
    {
      "epoch": 0.8298183618807535,
      "grad_norm": 1.6577799979834595,
      "learning_rate": 1.4808059116167306e-06,
      "loss": 0.4756,
      "step": 9217
    },
    {
      "epoch": 0.8299083931666255,
      "grad_norm": 1.3848925152157179,
      "learning_rate": 1.4792791533169848e-06,
      "loss": 0.4447,
      "step": 9218
    },
    {
      "epoch": 0.8299984244524973,
      "grad_norm": 1.1612930186578176,
      "learning_rate": 1.4777531196246308e-06,
      "loss": 0.4384,
      "step": 9219
    },
    {
      "epoch": 0.8300884557383691,
      "grad_norm": 1.377751145088921,
      "learning_rate": 1.4762278106694462e-06,
      "loss": 0.5497,
      "step": 9220
    },
    {
      "epoch": 0.8301784870242409,
      "grad_norm": 1.3546613744728377,
      "learning_rate": 1.4747032265811356e-06,
      "loss": 0.4675,
      "step": 9221
    },
    {
      "epoch": 0.8302685183101127,
      "grad_norm": 1.215094704409605,
      "learning_rate": 1.4731793674893624e-06,
      "loss": 0.522,
      "step": 9222
    },
    {
      "epoch": 0.8303585495959847,
      "grad_norm": 1.0643287780186976,
      "learning_rate": 1.4716562335237073e-06,
      "loss": 0.4309,
      "step": 9223
    },
    {
      "epoch": 0.8304485808818565,
      "grad_norm": 1.0854783190072055,
      "learning_rate": 1.470133824813702e-06,
      "loss": 0.4521,
      "step": 9224
    },
    {
      "epoch": 0.8305386121677283,
      "grad_norm": 1.4346061007374409,
      "learning_rate": 1.4686121414888122e-06,
      "loss": 0.4693,
      "step": 9225
    },
    {
      "epoch": 0.8306286434536001,
      "grad_norm": 1.0249954396768723,
      "learning_rate": 1.467091183678444e-06,
      "loss": 0.553,
      "step": 9226
    },
    {
      "epoch": 0.8307186747394719,
      "grad_norm": 1.2978649267882043,
      "learning_rate": 1.46557095151194e-06,
      "loss": 0.5095,
      "step": 9227
    },
    {
      "epoch": 0.8308087060253438,
      "grad_norm": 1.339578562654256,
      "learning_rate": 1.4640514451185827e-06,
      "loss": 0.6062,
      "step": 9228
    },
    {
      "epoch": 0.8308987373112157,
      "grad_norm": 1.4809103631042486,
      "learning_rate": 1.4625326646275884e-06,
      "loss": 0.5613,
      "step": 9229
    },
    {
      "epoch": 0.8309887685970875,
      "grad_norm": 1.7829720355489507,
      "learning_rate": 1.4610146101681199e-06,
      "loss": 0.4374,
      "step": 9230
    },
    {
      "epoch": 0.8310787998829593,
      "grad_norm": 1.3286676411730685,
      "learning_rate": 1.4594972818692709e-06,
      "loss": 0.4647,
      "step": 9231
    },
    {
      "epoch": 0.8311688311688312,
      "grad_norm": 1.0240646200754517,
      "learning_rate": 1.4579806798600749e-06,
      "loss": 0.4132,
      "step": 9232
    },
    {
      "epoch": 0.831258862454703,
      "grad_norm": 1.2002722902061187,
      "learning_rate": 1.4564648042695062e-06,
      "loss": 0.5104,
      "step": 9233
    },
    {
      "epoch": 0.8313488937405749,
      "grad_norm": 1.03425222387923,
      "learning_rate": 1.4549496552264787e-06,
      "loss": 0.5241,
      "step": 9234
    },
    {
      "epoch": 0.8314389250264467,
      "grad_norm": 1.3300109015373291,
      "learning_rate": 1.4534352328598333e-06,
      "loss": 0.511,
      "step": 9235
    },
    {
      "epoch": 0.8315289563123185,
      "grad_norm": 1.2860648975502404,
      "learning_rate": 1.4519215372983664e-06,
      "loss": 0.5179,
      "step": 9236
    },
    {
      "epoch": 0.8316189875981904,
      "grad_norm": 1.2920581196602927,
      "learning_rate": 1.4504085686707991e-06,
      "loss": 0.5823,
      "step": 9237
    },
    {
      "epoch": 0.8317090188840622,
      "grad_norm": 1.2229515629918164,
      "learning_rate": 1.4488963271057943e-06,
      "loss": 0.5121,
      "step": 9238
    },
    {
      "epoch": 0.831799050169934,
      "grad_norm": 1.5455997329482742,
      "learning_rate": 1.4473848127319568e-06,
      "loss": 0.5884,
      "step": 9239
    },
    {
      "epoch": 0.8318890814558059,
      "grad_norm": 1.5000138992556415,
      "learning_rate": 1.4458740256778247e-06,
      "loss": 0.7018,
      "step": 9240
    },
    {
      "epoch": 0.8319791127416777,
      "grad_norm": 1.3132924484195516,
      "learning_rate": 1.4443639660718778e-06,
      "loss": 0.4924,
      "step": 9241
    },
    {
      "epoch": 0.8320691440275496,
      "grad_norm": 1.33645031490201,
      "learning_rate": 1.4428546340425341e-06,
      "loss": 0.5349,
      "step": 9242
    },
    {
      "epoch": 0.8321591753134214,
      "grad_norm": 1.3142865800680035,
      "learning_rate": 1.4413460297181402e-06,
      "loss": 0.4712,
      "step": 9243
    },
    {
      "epoch": 0.8322492065992932,
      "grad_norm": 1.5971774899030708,
      "learning_rate": 1.4398381532270001e-06,
      "loss": 0.4753,
      "step": 9244
    },
    {
      "epoch": 0.8323392378851651,
      "grad_norm": 1.369933160871628,
      "learning_rate": 1.4383310046973365e-06,
      "loss": 0.4807,
      "step": 9245
    },
    {
      "epoch": 0.832429269171037,
      "grad_norm": 1.1625667792195902,
      "learning_rate": 1.4368245842573203e-06,
      "loss": 0.555,
      "step": 9246
    },
    {
      "epoch": 0.8325193004569088,
      "grad_norm": 1.1972497618187905,
      "learning_rate": 1.435318892035058e-06,
      "loss": 0.5411,
      "step": 9247
    },
    {
      "epoch": 0.8326093317427806,
      "grad_norm": 1.5177563547736508,
      "learning_rate": 1.4338139281585983e-06,
      "loss": 0.5622,
      "step": 9248
    },
    {
      "epoch": 0.8326993630286524,
      "grad_norm": 1.091413736958763,
      "learning_rate": 1.4323096927559166e-06,
      "loss": 0.4655,
      "step": 9249
    },
    {
      "epoch": 0.8327893943145243,
      "grad_norm": 1.4573894409329977,
      "learning_rate": 1.4308061859549426e-06,
      "loss": 0.5072,
      "step": 9250
    },
    {
      "epoch": 0.8328794256003962,
      "grad_norm": 1.2056093703905593,
      "learning_rate": 1.42930340788353e-06,
      "loss": 0.5245,
      "step": 9251
    },
    {
      "epoch": 0.832969456886268,
      "grad_norm": 1.161920330893378,
      "learning_rate": 1.4278013586694772e-06,
      "loss": 0.4992,
      "step": 9252
    },
    {
      "epoch": 0.8330594881721398,
      "grad_norm": 1.1787140978629174,
      "learning_rate": 1.4263000384405202e-06,
      "loss": 0.4852,
      "step": 9253
    },
    {
      "epoch": 0.8331495194580116,
      "grad_norm": 1.6607567358407382,
      "learning_rate": 1.424799447324331e-06,
      "loss": 0.5288,
      "step": 9254
    },
    {
      "epoch": 0.8332395507438835,
      "grad_norm": 1.6029410888061417,
      "learning_rate": 1.423299585448522e-06,
      "loss": 0.5173,
      "step": 9255
    },
    {
      "epoch": 0.8333295820297554,
      "grad_norm": 1.342088936600222,
      "learning_rate": 1.4218004529406437e-06,
      "loss": 0.5316,
      "step": 9256
    },
    {
      "epoch": 0.8334196133156272,
      "grad_norm": 1.3614187001916622,
      "learning_rate": 1.4203020499281771e-06,
      "loss": 0.538,
      "step": 9257
    },
    {
      "epoch": 0.833509644601499,
      "grad_norm": 1.2259784100707234,
      "learning_rate": 1.4188043765385561e-06,
      "loss": 0.5446,
      "step": 9258
    },
    {
      "epoch": 0.8335996758873708,
      "grad_norm": 1.5202725718839447,
      "learning_rate": 1.4173074328991376e-06,
      "loss": 0.5601,
      "step": 9259
    },
    {
      "epoch": 0.8336897071732428,
      "grad_norm": 2.0554339850398065,
      "learning_rate": 1.4158112191372231e-06,
      "loss": 0.4616,
      "step": 9260
    },
    {
      "epoch": 0.8337797384591146,
      "grad_norm": 1.162658675385683,
      "learning_rate": 1.4143157353800541e-06,
      "loss": 0.6001,
      "step": 9261
    },
    {
      "epoch": 0.8338697697449864,
      "grad_norm": 1.6245080523525495,
      "learning_rate": 1.4128209817548078e-06,
      "loss": 0.5285,
      "step": 9262
    },
    {
      "epoch": 0.8339598010308582,
      "grad_norm": 1.0333777677785323,
      "learning_rate": 1.4113269583885936e-06,
      "loss": 0.4454,
      "step": 9263
    },
    {
      "epoch": 0.83404983231673,
      "grad_norm": 1.5981412467812068,
      "learning_rate": 1.409833665408471e-06,
      "loss": 0.5347,
      "step": 9264
    },
    {
      "epoch": 0.834139863602602,
      "grad_norm": 1.343200121819788,
      "learning_rate": 1.4083411029414262e-06,
      "loss": 0.5167,
      "step": 9265
    },
    {
      "epoch": 0.8342298948884738,
      "grad_norm": 1.308825736246455,
      "learning_rate": 1.4068492711143888e-06,
      "loss": 0.4661,
      "step": 9266
    },
    {
      "epoch": 0.8343199261743456,
      "grad_norm": 1.6477476572466387,
      "learning_rate": 1.4053581700542252e-06,
      "loss": 0.4885,
      "step": 9267
    },
    {
      "epoch": 0.8344099574602174,
      "grad_norm": 1.479877163963059,
      "learning_rate": 1.4038677998877403e-06,
      "loss": 0.5323,
      "step": 9268
    },
    {
      "epoch": 0.8344999887460892,
      "grad_norm": 2.3239807225906466,
      "learning_rate": 1.4023781607416742e-06,
      "loss": 0.527,
      "step": 9269
    },
    {
      "epoch": 0.8345900200319611,
      "grad_norm": 0.9752804077389392,
      "learning_rate": 1.4008892527427109e-06,
      "loss": 0.4778,
      "step": 9270
    },
    {
      "epoch": 0.834680051317833,
      "grad_norm": 1.5257444881361042,
      "learning_rate": 1.3994010760174603e-06,
      "loss": 0.5744,
      "step": 9271
    },
    {
      "epoch": 0.8347700826037048,
      "grad_norm": 1.377409874757339,
      "learning_rate": 1.3979136306924879e-06,
      "loss": 0.46,
      "step": 9272
    },
    {
      "epoch": 0.8348601138895766,
      "grad_norm": 1.611082407691824,
      "learning_rate": 1.396426916894279e-06,
      "loss": 0.6012,
      "step": 9273
    },
    {
      "epoch": 0.8349501451754485,
      "grad_norm": 1.748396081947621,
      "learning_rate": 1.3949409347492672e-06,
      "loss": 0.442,
      "step": 9274
    },
    {
      "epoch": 0.8350401764613203,
      "grad_norm": 1.1940563786385345,
      "learning_rate": 1.393455684383821e-06,
      "loss": 0.5083,
      "step": 9275
    },
    {
      "epoch": 0.8351302077471922,
      "grad_norm": 1.3003932719779885,
      "learning_rate": 1.3919711659242507e-06,
      "loss": 0.548,
      "step": 9276
    },
    {
      "epoch": 0.835220239033064,
      "grad_norm": 1.3128119573640116,
      "learning_rate": 1.3904873794967932e-06,
      "loss": 0.5417,
      "step": 9277
    },
    {
      "epoch": 0.8353102703189358,
      "grad_norm": 1.4723727913295286,
      "learning_rate": 1.3890043252276387e-06,
      "loss": 0.5625,
      "step": 9278
    },
    {
      "epoch": 0.8354003016048077,
      "grad_norm": 1.3068532231299705,
      "learning_rate": 1.3875220032429015e-06,
      "loss": 0.4317,
      "step": 9279
    },
    {
      "epoch": 0.8354903328906795,
      "grad_norm": 1.5038109917166063,
      "learning_rate": 1.3860404136686411e-06,
      "loss": 0.5134,
      "step": 9280
    },
    {
      "epoch": 0.8355803641765513,
      "grad_norm": 1.2490899113024785,
      "learning_rate": 1.3845595566308522e-06,
      "loss": 0.4811,
      "step": 9281
    },
    {
      "epoch": 0.8356703954624232,
      "grad_norm": 1.41865013633394,
      "learning_rate": 1.3830794322554686e-06,
      "loss": 0.4997,
      "step": 9282
    },
    {
      "epoch": 0.835760426748295,
      "grad_norm": 1.0974524857998535,
      "learning_rate": 1.3816000406683604e-06,
      "loss": 0.4702,
      "step": 9283
    },
    {
      "epoch": 0.8358504580341669,
      "grad_norm": 1.1460751724681668,
      "learning_rate": 1.3801213819953385e-06,
      "loss": 0.5443,
      "step": 9284
    },
    {
      "epoch": 0.8359404893200387,
      "grad_norm": 1.5489382359694404,
      "learning_rate": 1.3786434563621431e-06,
      "loss": 0.5313,
      "step": 9285
    },
    {
      "epoch": 0.8360305206059105,
      "grad_norm": 1.1573041936129955,
      "learning_rate": 1.377166263894465e-06,
      "loss": 0.3763,
      "step": 9286
    },
    {
      "epoch": 0.8361205518917824,
      "grad_norm": 1.4538820889604014,
      "learning_rate": 1.3756898047179202e-06,
      "loss": 0.5534,
      "step": 9287
    },
    {
      "epoch": 0.8362105831776543,
      "grad_norm": 1.2048504439493484,
      "learning_rate": 1.3742140789580704e-06,
      "loss": 0.5454,
      "step": 9288
    },
    {
      "epoch": 0.8363006144635261,
      "grad_norm": 1.072506831466001,
      "learning_rate": 1.37273908674041e-06,
      "loss": 0.4814,
      "step": 9289
    },
    {
      "epoch": 0.8363906457493979,
      "grad_norm": 1.225813416014685,
      "learning_rate": 1.3712648281903785e-06,
      "loss": 0.5256,
      "step": 9290
    },
    {
      "epoch": 0.8364806770352697,
      "grad_norm": 1.8186608329614864,
      "learning_rate": 1.3697913034333377e-06,
      "loss": 0.4622,
      "step": 9291
    },
    {
      "epoch": 0.8365707083211416,
      "grad_norm": 1.4394878878570574,
      "learning_rate": 1.3683185125946096e-06,
      "loss": 0.4569,
      "step": 9292
    },
    {
      "epoch": 0.8366607396070135,
      "grad_norm": 1.5708845204218762,
      "learning_rate": 1.366846455799431e-06,
      "loss": 0.4482,
      "step": 9293
    },
    {
      "epoch": 0.8367507708928853,
      "grad_norm": 1.0611162171162336,
      "learning_rate": 1.3653751331729914e-06,
      "loss": 0.576,
      "step": 9294
    },
    {
      "epoch": 0.8368408021787571,
      "grad_norm": 1.196445522002884,
      "learning_rate": 1.363904544840412e-06,
      "loss": 0.488,
      "step": 9295
    },
    {
      "epoch": 0.8369308334646289,
      "grad_norm": 1.4510708058587982,
      "learning_rate": 1.3624346909267526e-06,
      "loss": 0.477,
      "step": 9296
    },
    {
      "epoch": 0.8370208647505007,
      "grad_norm": 1.6727122428632137,
      "learning_rate": 1.3609655715570113e-06,
      "loss": 0.549,
      "step": 9297
    },
    {
      "epoch": 0.8371108960363727,
      "grad_norm": 2.0820308786451376,
      "learning_rate": 1.3594971868561235e-06,
      "loss": 0.5416,
      "step": 9298
    },
    {
      "epoch": 0.8372009273222445,
      "grad_norm": 1.501989057393053,
      "learning_rate": 1.358029536948956e-06,
      "loss": 0.5088,
      "step": 9299
    },
    {
      "epoch": 0.8372909586081163,
      "grad_norm": 1.5936353486062633,
      "learning_rate": 1.3565626219603267e-06,
      "loss": 0.4999,
      "step": 9300
    },
    {
      "epoch": 0.8373809898939881,
      "grad_norm": 1.062810709424686,
      "learning_rate": 1.355096442014977e-06,
      "loss": 0.6362,
      "step": 9301
    },
    {
      "epoch": 0.83747102117986,
      "grad_norm": 1.2189177348169788,
      "learning_rate": 1.353630997237595e-06,
      "loss": 0.4463,
      "step": 9302
    },
    {
      "epoch": 0.8375610524657319,
      "grad_norm": 1.2408139095405954,
      "learning_rate": 1.3521662877528007e-06,
      "loss": 0.5036,
      "step": 9303
    },
    {
      "epoch": 0.8376510837516037,
      "grad_norm": 1.7174372528653399,
      "learning_rate": 1.3507023136851572e-06,
      "loss": 0.4782,
      "step": 9304
    },
    {
      "epoch": 0.8377411150374755,
      "grad_norm": 1.2815314143681817,
      "learning_rate": 1.3492390751591555e-06,
      "loss": 0.5835,
      "step": 9305
    },
    {
      "epoch": 0.8378311463233473,
      "grad_norm": 0.9823004397689702,
      "learning_rate": 1.3477765722992398e-06,
      "loss": 0.5372,
      "step": 9306
    },
    {
      "epoch": 0.8379211776092192,
      "grad_norm": 1.4185797491557133,
      "learning_rate": 1.3463148052297737e-06,
      "loss": 0.5205,
      "step": 9307
    },
    {
      "epoch": 0.8380112088950911,
      "grad_norm": 1.566905919689221,
      "learning_rate": 1.34485377407507e-06,
      "loss": 0.576,
      "step": 9308
    },
    {
      "epoch": 0.8381012401809629,
      "grad_norm": 1.3573538747313996,
      "learning_rate": 1.3433934789593762e-06,
      "loss": 0.4877,
      "step": 9309
    },
    {
      "epoch": 0.8381912714668347,
      "grad_norm": 1.4359988268000037,
      "learning_rate": 1.341933920006876e-06,
      "loss": 0.5316,
      "step": 9310
    },
    {
      "epoch": 0.8382813027527065,
      "grad_norm": 1.0791660868332265,
      "learning_rate": 1.3404750973416913e-06,
      "loss": 0.4749,
      "step": 9311
    },
    {
      "epoch": 0.8383713340385784,
      "grad_norm": 1.371405236999062,
      "learning_rate": 1.3390170110878841e-06,
      "loss": 0.4619,
      "step": 9312
    },
    {
      "epoch": 0.8384613653244503,
      "grad_norm": 1.412281939121685,
      "learning_rate": 1.337559661369443e-06,
      "loss": 0.5864,
      "step": 9313
    },
    {
      "epoch": 0.8385513966103221,
      "grad_norm": 1.7613919663357174,
      "learning_rate": 1.336103048310312e-06,
      "loss": 0.5097,
      "step": 9314
    },
    {
      "epoch": 0.8386414278961939,
      "grad_norm": 0.8930131748479799,
      "learning_rate": 1.3346471720343558e-06,
      "loss": 0.4229,
      "step": 9315
    },
    {
      "epoch": 0.8387314591820658,
      "grad_norm": 1.217273703222204,
      "learning_rate": 1.3331920326653835e-06,
      "loss": 0.4749,
      "step": 9316
    },
    {
      "epoch": 0.8388214904679376,
      "grad_norm": 1.2633827015204118,
      "learning_rate": 1.3317376303271434e-06,
      "loss": 0.4371,
      "step": 9317
    },
    {
      "epoch": 0.8389115217538095,
      "grad_norm": 1.530904190368824,
      "learning_rate": 1.3302839651433196e-06,
      "loss": 0.5358,
      "step": 9318
    },
    {
      "epoch": 0.8390015530396813,
      "grad_norm": 1.5122794583969994,
      "learning_rate": 1.3288310372375268e-06,
      "loss": 0.4978,
      "step": 9319
    },
    {
      "epoch": 0.8390915843255531,
      "grad_norm": 1.4653468447069624,
      "learning_rate": 1.3273788467333303e-06,
      "loss": 0.5297,
      "step": 9320
    },
    {
      "epoch": 0.839181615611425,
      "grad_norm": 1.1158749874192417,
      "learning_rate": 1.3259273937542206e-06,
      "loss": 0.4402,
      "step": 9321
    },
    {
      "epoch": 0.8392716468972968,
      "grad_norm": 1.708513013753758,
      "learning_rate": 1.3244766784236307e-06,
      "loss": 0.5611,
      "step": 9322
    },
    {
      "epoch": 0.8393616781831686,
      "grad_norm": 1.166493516433021,
      "learning_rate": 1.3230267008649322e-06,
      "loss": 0.5213,
      "step": 9323
    },
    {
      "epoch": 0.8394517094690405,
      "grad_norm": 1.3435750733044602,
      "learning_rate": 1.3215774612014298e-06,
      "loss": 0.5488,
      "step": 9324
    },
    {
      "epoch": 0.8395417407549123,
      "grad_norm": 1.0062352781425974,
      "learning_rate": 1.3201289595563693e-06,
      "loss": 0.4937,
      "step": 9325
    },
    {
      "epoch": 0.8396317720407842,
      "grad_norm": 0.970071424674398,
      "learning_rate": 1.3186811960529344e-06,
      "loss": 0.5526,
      "step": 9326
    },
    {
      "epoch": 0.839721803326656,
      "grad_norm": 1.334620622171516,
      "learning_rate": 1.3172341708142366e-06,
      "loss": 0.4743,
      "step": 9327
    },
    {
      "epoch": 0.8398118346125278,
      "grad_norm": 1.4950265583824363,
      "learning_rate": 1.3157878839633398e-06,
      "loss": 0.5472,
      "step": 9328
    },
    {
      "epoch": 0.8399018658983997,
      "grad_norm": 1.5941905212222045,
      "learning_rate": 1.314342335623232e-06,
      "loss": 0.4833,
      "step": 9329
    },
    {
      "epoch": 0.8399918971842716,
      "grad_norm": 1.408786501021736,
      "learning_rate": 1.3128975259168464e-06,
      "loss": 0.4648,
      "step": 9330
    },
    {
      "epoch": 0.8400819284701434,
      "grad_norm": 1.2307431841798548,
      "learning_rate": 1.3114534549670488e-06,
      "loss": 0.5781,
      "step": 9331
    },
    {
      "epoch": 0.8401719597560152,
      "grad_norm": 1.2602811189983143,
      "learning_rate": 1.3100101228966456e-06,
      "loss": 0.5529,
      "step": 9332
    },
    {
      "epoch": 0.840261991041887,
      "grad_norm": 1.0993527311475617,
      "learning_rate": 1.308567529828374e-06,
      "loss": 0.4377,
      "step": 9333
    },
    {
      "epoch": 0.8403520223277589,
      "grad_norm": 1.9992119696754125,
      "learning_rate": 1.3071256758849216e-06,
      "loss": 0.5092,
      "step": 9334
    },
    {
      "epoch": 0.8404420536136308,
      "grad_norm": 1.3973263832000167,
      "learning_rate": 1.3056845611888969e-06,
      "loss": 0.5543,
      "step": 9335
    },
    {
      "epoch": 0.8405320848995026,
      "grad_norm": 1.4262292904051515,
      "learning_rate": 1.3042441858628551e-06,
      "loss": 0.5669,
      "step": 9336
    },
    {
      "epoch": 0.8406221161853744,
      "grad_norm": 1.1147299950144636,
      "learning_rate": 1.3028045500292875e-06,
      "loss": 0.5348,
      "step": 9337
    },
    {
      "epoch": 0.8407121474712462,
      "grad_norm": 2.427796785661495,
      "learning_rate": 1.301365653810621e-06,
      "loss": 0.5048,
      "step": 9338
    },
    {
      "epoch": 0.840802178757118,
      "grad_norm": 2.321463676728964,
      "learning_rate": 1.299927497329222e-06,
      "loss": 0.619,
      "step": 9339
    },
    {
      "epoch": 0.84089221004299,
      "grad_norm": 1.2786644930068387,
      "learning_rate": 1.2984900807073919e-06,
      "loss": 0.5423,
      "step": 9340
    },
    {
      "epoch": 0.8409822413288618,
      "grad_norm": 1.375130538728106,
      "learning_rate": 1.2970534040673643e-06,
      "loss": 0.4701,
      "step": 9341
    },
    {
      "epoch": 0.8410722726147336,
      "grad_norm": 1.2219068687531074,
      "learning_rate": 1.2956174675313237e-06,
      "loss": 0.6189,
      "step": 9342
    },
    {
      "epoch": 0.8411623039006054,
      "grad_norm": 1.5229965322546963,
      "learning_rate": 1.294182271221377e-06,
      "loss": 0.4458,
      "step": 9343
    },
    {
      "epoch": 0.8412523351864774,
      "grad_norm": 1.5060193659314,
      "learning_rate": 1.2927478152595752e-06,
      "loss": 0.5463,
      "step": 9344
    },
    {
      "epoch": 0.8413423664723492,
      "grad_norm": 1.2143741662619716,
      "learning_rate": 1.2913140997679063e-06,
      "loss": 0.5258,
      "step": 9345
    },
    {
      "epoch": 0.841432397758221,
      "grad_norm": 1.0508085789558845,
      "learning_rate": 1.2898811248682952e-06,
      "loss": 0.4976,
      "step": 9346
    },
    {
      "epoch": 0.8415224290440928,
      "grad_norm": 1.4260878087205833,
      "learning_rate": 1.2884488906825988e-06,
      "loss": 0.5298,
      "step": 9347
    },
    {
      "epoch": 0.8416124603299646,
      "grad_norm": 1.5653826659147174,
      "learning_rate": 1.2870173973326205e-06,
      "loss": 0.4632,
      "step": 9348
    },
    {
      "epoch": 0.8417024916158365,
      "grad_norm": 1.0324230145094346,
      "learning_rate": 1.2855866449400923e-06,
      "loss": 0.5737,
      "step": 9349
    },
    {
      "epoch": 0.8417925229017084,
      "grad_norm": 1.0996697911437934,
      "learning_rate": 1.2841566336266865e-06,
      "loss": 0.4935,
      "step": 9350
    },
    {
      "epoch": 0.8418825541875802,
      "grad_norm": 1.1756828784930549,
      "learning_rate": 1.2827273635140125e-06,
      "loss": 0.5113,
      "step": 9351
    },
    {
      "epoch": 0.841972585473452,
      "grad_norm": 1.2650584992914455,
      "learning_rate": 1.2812988347236166e-06,
      "loss": 0.499,
      "step": 9352
    },
    {
      "epoch": 0.8420626167593238,
      "grad_norm": 1.6509849470262952,
      "learning_rate": 1.2798710473769815e-06,
      "loss": 0.5228,
      "step": 9353
    },
    {
      "epoch": 0.8421526480451957,
      "grad_norm": 1.9721699072871866,
      "learning_rate": 1.2784440015955302e-06,
      "loss": 0.5184,
      "step": 9354
    },
    {
      "epoch": 0.8422426793310676,
      "grad_norm": 1.038251925590218,
      "learning_rate": 1.277017697500611e-06,
      "loss": 0.4993,
      "step": 9355
    },
    {
      "epoch": 0.8423327106169394,
      "grad_norm": 2.2250945010862906,
      "learning_rate": 1.2755921352135282e-06,
      "loss": 0.5596,
      "step": 9356
    },
    {
      "epoch": 0.8424227419028112,
      "grad_norm": 1.2813684972427242,
      "learning_rate": 1.2741673148555056e-06,
      "loss": 0.5036,
      "step": 9357
    },
    {
      "epoch": 0.8425127731886831,
      "grad_norm": 1.0467436425684,
      "learning_rate": 1.272743236547712e-06,
      "loss": 0.5034,
      "step": 9358
    },
    {
      "epoch": 0.8426028044745549,
      "grad_norm": 2.581330831623907,
      "learning_rate": 1.2713199004112542e-06,
      "loss": 0.5165,
      "step": 9359
    },
    {
      "epoch": 0.8426928357604268,
      "grad_norm": 1.301329408966898,
      "learning_rate": 1.2698973065671727e-06,
      "loss": 0.5129,
      "step": 9360
    },
    {
      "epoch": 0.8427828670462986,
      "grad_norm": 1.4532642168129373,
      "learning_rate": 1.268475455136442e-06,
      "loss": 0.548,
      "step": 9361
    },
    {
      "epoch": 0.8428728983321704,
      "grad_norm": 1.3580249934271733,
      "learning_rate": 1.2670543462399832e-06,
      "loss": 0.5107,
      "step": 9362
    },
    {
      "epoch": 0.8429629296180423,
      "grad_norm": 1.1992605737358562,
      "learning_rate": 1.265633979998645e-06,
      "loss": 0.5799,
      "step": 9363
    },
    {
      "epoch": 0.8430529609039141,
      "grad_norm": 1.4213184124995721,
      "learning_rate": 1.2642143565332154e-06,
      "loss": 0.5382,
      "step": 9364
    },
    {
      "epoch": 0.843142992189786,
      "grad_norm": 1.2599138756689257,
      "learning_rate": 1.2627954759644224e-06,
      "loss": 0.5535,
      "step": 9365
    },
    {
      "epoch": 0.8432330234756578,
      "grad_norm": 1.5828326659424832,
      "learning_rate": 1.2613773384129257e-06,
      "loss": 0.441,
      "step": 9366
    },
    {
      "epoch": 0.8433230547615296,
      "grad_norm": 1.188278034828628,
      "learning_rate": 1.2599599439993283e-06,
      "loss": 0.5301,
      "step": 9367
    },
    {
      "epoch": 0.8434130860474015,
      "grad_norm": 1.2405079197575373,
      "learning_rate": 1.2585432928441653e-06,
      "loss": 0.4543,
      "step": 9368
    },
    {
      "epoch": 0.8435031173332733,
      "grad_norm": 1.613830593186749,
      "learning_rate": 1.2571273850679044e-06,
      "loss": 0.5725,
      "step": 9369
    },
    {
      "epoch": 0.8435931486191451,
      "grad_norm": 1.8018583854076207,
      "learning_rate": 1.2557122207909644e-06,
      "loss": 0.5067,
      "step": 9370
    },
    {
      "epoch": 0.843683179905017,
      "grad_norm": 1.381773810434881,
      "learning_rate": 1.2542978001336837e-06,
      "loss": 0.5173,
      "step": 9371
    },
    {
      "epoch": 0.8437732111908889,
      "grad_norm": 2.0739764717333933,
      "learning_rate": 1.2528841232163492e-06,
      "loss": 0.5211,
      "step": 9372
    },
    {
      "epoch": 0.8438632424767607,
      "grad_norm": 1.3006348462202693,
      "learning_rate": 1.2514711901591803e-06,
      "loss": 0.485,
      "step": 9373
    },
    {
      "epoch": 0.8439532737626325,
      "grad_norm": 1.4179219940239636,
      "learning_rate": 1.2500590010823366e-06,
      "loss": 0.5741,
      "step": 9374
    },
    {
      "epoch": 0.8440433050485043,
      "grad_norm": 1.3526072937232734,
      "learning_rate": 1.248647556105903e-06,
      "loss": 0.4964,
      "step": 9375
    },
    {
      "epoch": 0.8441333363343761,
      "grad_norm": 1.116257262071719,
      "learning_rate": 1.2472368553499204e-06,
      "loss": 0.5017,
      "step": 9376
    },
    {
      "epoch": 0.8442233676202481,
      "grad_norm": 1.1328251646958662,
      "learning_rate": 1.245826898934348e-06,
      "loss": 0.5304,
      "step": 9377
    },
    {
      "epoch": 0.8443133989061199,
      "grad_norm": 1.2265328856252244,
      "learning_rate": 1.2444176869790925e-06,
      "loss": 0.4911,
      "step": 9378
    },
    {
      "epoch": 0.8444034301919917,
      "grad_norm": 1.6985898185122286,
      "learning_rate": 1.2430092196039934e-06,
      "loss": 0.4961,
      "step": 9379
    },
    {
      "epoch": 0.8444934614778635,
      "grad_norm": 1.9081979821707116,
      "learning_rate": 1.2416014969288282e-06,
      "loss": 0.5817,
      "step": 9380
    },
    {
      "epoch": 0.8445834927637353,
      "grad_norm": 1.3709010456580166,
      "learning_rate": 1.240194519073309e-06,
      "loss": 0.5334,
      "step": 9381
    },
    {
      "epoch": 0.8446735240496073,
      "grad_norm": 1.0236732659675696,
      "learning_rate": 1.2387882861570899e-06,
      "loss": 0.4796,
      "step": 9382
    },
    {
      "epoch": 0.8447635553354791,
      "grad_norm": 1.3016321941886542,
      "learning_rate": 1.237382798299751e-06,
      "loss": 0.4336,
      "step": 9383
    },
    {
      "epoch": 0.8448535866213509,
      "grad_norm": 1.280358979003401,
      "learning_rate": 1.2359780556208246e-06,
      "loss": 0.535,
      "step": 9384
    },
    {
      "epoch": 0.8449436179072227,
      "grad_norm": 1.8112767130250622,
      "learning_rate": 1.2345740582397647e-06,
      "loss": 0.4737,
      "step": 9385
    },
    {
      "epoch": 0.8450336491930946,
      "grad_norm": 1.4761473270740362,
      "learning_rate": 1.2331708062759685e-06,
      "loss": 0.4976,
      "step": 9386
    },
    {
      "epoch": 0.8451236804789665,
      "grad_norm": 1.9904959100895263,
      "learning_rate": 1.2317682998487712e-06,
      "loss": 0.5008,
      "step": 9387
    },
    {
      "epoch": 0.8452137117648383,
      "grad_norm": 1.1894490202123733,
      "learning_rate": 1.2303665390774444e-06,
      "loss": 0.5658,
      "step": 9388
    },
    {
      "epoch": 0.8453037430507101,
      "grad_norm": 1.2982448559138995,
      "learning_rate": 1.2289655240811882e-06,
      "loss": 0.4999,
      "step": 9389
    },
    {
      "epoch": 0.8453937743365819,
      "grad_norm": 1.4469052408133727,
      "learning_rate": 1.2275652549791539e-06,
      "loss": 0.4143,
      "step": 9390
    },
    {
      "epoch": 0.8454838056224538,
      "grad_norm": 1.4706393543652554,
      "learning_rate": 1.2261657318904151e-06,
      "loss": 0.5124,
      "step": 9391
    },
    {
      "epoch": 0.8455738369083257,
      "grad_norm": 1.5722074194316635,
      "learning_rate": 1.2247669549339914e-06,
      "loss": 0.4704,
      "step": 9392
    },
    {
      "epoch": 0.8456638681941975,
      "grad_norm": 1.0722770451728383,
      "learning_rate": 1.223368924228835e-06,
      "loss": 0.484,
      "step": 9393
    },
    {
      "epoch": 0.8457538994800693,
      "grad_norm": 1.3201261683532257,
      "learning_rate": 1.221971639893834e-06,
      "loss": 0.5053,
      "step": 9394
    },
    {
      "epoch": 0.8458439307659411,
      "grad_norm": 1.377706870131698,
      "learning_rate": 1.2205751020478162e-06,
      "loss": 0.4183,
      "step": 9395
    },
    {
      "epoch": 0.845933962051813,
      "grad_norm": 1.5047580036883588,
      "learning_rate": 1.2191793108095452e-06,
      "loss": 0.4511,
      "step": 9396
    },
    {
      "epoch": 0.8460239933376849,
      "grad_norm": 1.4868420497674795,
      "learning_rate": 1.2177842662977136e-06,
      "loss": 0.5083,
      "step": 9397
    },
    {
      "epoch": 0.8461140246235567,
      "grad_norm": 1.2336749844604453,
      "learning_rate": 1.2163899686309666e-06,
      "loss": 0.4702,
      "step": 9398
    },
    {
      "epoch": 0.8462040559094285,
      "grad_norm": 1.340381929353834,
      "learning_rate": 1.2149964179278672e-06,
      "loss": 0.5437,
      "step": 9399
    },
    {
      "epoch": 0.8462940871953004,
      "grad_norm": 1.071550550679459,
      "learning_rate": 1.2136036143069274e-06,
      "loss": 0.4678,
      "step": 9400
    },
    {
      "epoch": 0.8463841184811722,
      "grad_norm": 1.2490793785035141,
      "learning_rate": 1.2122115578865933e-06,
      "loss": 0.404,
      "step": 9401
    },
    {
      "epoch": 0.846474149767044,
      "grad_norm": 1.399330870546163,
      "learning_rate": 1.210820248785246e-06,
      "loss": 0.4899,
      "step": 9402
    },
    {
      "epoch": 0.8465641810529159,
      "grad_norm": 0.9801158304147072,
      "learning_rate": 1.2094296871211986e-06,
      "loss": 0.5452,
      "step": 9403
    },
    {
      "epoch": 0.8466542123387877,
      "grad_norm": 1.3721298418566623,
      "learning_rate": 1.208039873012713e-06,
      "loss": 0.5686,
      "step": 9404
    },
    {
      "epoch": 0.8467442436246596,
      "grad_norm": 1.9648906953179268,
      "learning_rate": 1.2066508065779737e-06,
      "loss": 0.4523,
      "step": 9405
    },
    {
      "epoch": 0.8468342749105314,
      "grad_norm": 1.3221053375035965,
      "learning_rate": 1.2052624879351105e-06,
      "loss": 0.5159,
      "step": 9406
    },
    {
      "epoch": 0.8469243061964032,
      "grad_norm": 1.7003449792523788,
      "learning_rate": 1.2038749172021857e-06,
      "loss": 0.5581,
      "step": 9407
    },
    {
      "epoch": 0.8470143374822751,
      "grad_norm": 1.3541797101824669,
      "learning_rate": 1.2024880944972006e-06,
      "loss": 0.506,
      "step": 9408
    },
    {
      "epoch": 0.8471043687681469,
      "grad_norm": 2.5976405802725586,
      "learning_rate": 1.2011020199380897e-06,
      "loss": 0.5323,
      "step": 9409
    },
    {
      "epoch": 0.8471944000540188,
      "grad_norm": 1.1211502404234799,
      "learning_rate": 1.1997166936427297e-06,
      "loss": 0.6286,
      "step": 9410
    },
    {
      "epoch": 0.8472844313398906,
      "grad_norm": 1.5770039341446669,
      "learning_rate": 1.1983321157289218e-06,
      "loss": 0.4565,
      "step": 9411
    },
    {
      "epoch": 0.8473744626257624,
      "grad_norm": 1.4029612480442255,
      "learning_rate": 1.1969482863144199e-06,
      "loss": 0.4863,
      "step": 9412
    },
    {
      "epoch": 0.8474644939116343,
      "grad_norm": 1.6277000374711865,
      "learning_rate": 1.195565205516901e-06,
      "loss": 0.5433,
      "step": 9413
    },
    {
      "epoch": 0.8475545251975062,
      "grad_norm": 1.0504067634877718,
      "learning_rate": 1.194182873453983e-06,
      "loss": 0.5393,
      "step": 9414
    },
    {
      "epoch": 0.847644556483378,
      "grad_norm": 1.7320368554918686,
      "learning_rate": 1.1928012902432218e-06,
      "loss": 0.4175,
      "step": 9415
    },
    {
      "epoch": 0.8477345877692498,
      "grad_norm": 1.5447044982772604,
      "learning_rate": 1.1914204560021091e-06,
      "loss": 0.5231,
      "step": 9416
    },
    {
      "epoch": 0.8478246190551216,
      "grad_norm": 0.8953572769782522,
      "learning_rate": 1.1900403708480667e-06,
      "loss": 0.5117,
      "step": 9417
    },
    {
      "epoch": 0.8479146503409934,
      "grad_norm": 1.2045610852165973,
      "learning_rate": 1.1886610348984661e-06,
      "loss": 0.5496,
      "step": 9418
    },
    {
      "epoch": 0.8480046816268654,
      "grad_norm": 1.284262182777891,
      "learning_rate": 1.1872824482706003e-06,
      "loss": 0.5475,
      "step": 9419
    },
    {
      "epoch": 0.8480947129127372,
      "grad_norm": 1.8355772933257983,
      "learning_rate": 1.1859046110817074e-06,
      "loss": 0.5451,
      "step": 9420
    },
    {
      "epoch": 0.848184744198609,
      "grad_norm": 1.3807992352594591,
      "learning_rate": 1.1845275234489595e-06,
      "loss": 0.4739,
      "step": 9421
    },
    {
      "epoch": 0.8482747754844808,
      "grad_norm": 1.058597087978563,
      "learning_rate": 1.183151185489464e-06,
      "loss": 0.5859,
      "step": 9422
    },
    {
      "epoch": 0.8483648067703528,
      "grad_norm": 1.429630557905617,
      "learning_rate": 1.1817755973202671e-06,
      "loss": 0.445,
      "step": 9423
    },
    {
      "epoch": 0.8484548380562246,
      "grad_norm": 1.1080801488556955,
      "learning_rate": 1.180400759058351e-06,
      "loss": 0.518,
      "step": 9424
    },
    {
      "epoch": 0.8485448693420964,
      "grad_norm": 1.7684807107795841,
      "learning_rate": 1.1790266708206265e-06,
      "loss": 0.5145,
      "step": 9425
    },
    {
      "epoch": 0.8486349006279682,
      "grad_norm": 1.1567759810012004,
      "learning_rate": 1.177653332723956e-06,
      "loss": 0.5723,
      "step": 9426
    },
    {
      "epoch": 0.84872493191384,
      "grad_norm": 1.5480838699645079,
      "learning_rate": 1.176280744885121e-06,
      "loss": 0.5483,
      "step": 9427
    },
    {
      "epoch": 0.848814963199712,
      "grad_norm": 1.726826244770434,
      "learning_rate": 1.174908907420852e-06,
      "loss": 0.4318,
      "step": 9428
    },
    {
      "epoch": 0.8489049944855838,
      "grad_norm": 1.1752438309982818,
      "learning_rate": 1.1735378204478075e-06,
      "loss": 0.5967,
      "step": 9429
    },
    {
      "epoch": 0.8489950257714556,
      "grad_norm": 1.9684988107564543,
      "learning_rate": 1.1721674840825914e-06,
      "loss": 0.4808,
      "step": 9430
    },
    {
      "epoch": 0.8490850570573274,
      "grad_norm": 1.9287977679541126,
      "learning_rate": 1.170797898441729e-06,
      "loss": 0.5435,
      "step": 9431
    },
    {
      "epoch": 0.8491750883431992,
      "grad_norm": 1.3609752676828162,
      "learning_rate": 1.1694290636416993e-06,
      "loss": 0.481,
      "step": 9432
    },
    {
      "epoch": 0.8492651196290711,
      "grad_norm": 1.6126712350676788,
      "learning_rate": 1.168060979798904e-06,
      "loss": 0.4995,
      "step": 9433
    },
    {
      "epoch": 0.849355150914943,
      "grad_norm": 1.2358025057390498,
      "learning_rate": 1.1666936470296874e-06,
      "loss": 0.4592,
      "step": 9434
    },
    {
      "epoch": 0.8494451822008148,
      "grad_norm": 1.8747420504982086,
      "learning_rate": 1.165327065450328e-06,
      "loss": 0.4965,
      "step": 9435
    },
    {
      "epoch": 0.8495352134866866,
      "grad_norm": 1.0644168130943372,
      "learning_rate": 1.1639612351770401e-06,
      "loss": 0.4277,
      "step": 9436
    },
    {
      "epoch": 0.8496252447725585,
      "grad_norm": 1.2055567815702748,
      "learning_rate": 1.1625961563259757e-06,
      "loss": 0.4744,
      "step": 9437
    },
    {
      "epoch": 0.8497152760584303,
      "grad_norm": 1.339513841965518,
      "learning_rate": 1.161231829013223e-06,
      "loss": 0.5267,
      "step": 9438
    },
    {
      "epoch": 0.8498053073443022,
      "grad_norm": 2.1081703095968285,
      "learning_rate": 1.1598682533548012e-06,
      "loss": 0.487,
      "step": 9439
    },
    {
      "epoch": 0.849895338630174,
      "grad_norm": 0.8686993319047038,
      "learning_rate": 1.1585054294666752e-06,
      "loss": 0.5006,
      "step": 9440
    },
    {
      "epoch": 0.8499853699160458,
      "grad_norm": 1.5550911601892203,
      "learning_rate": 1.1571433574647362e-06,
      "loss": 0.5501,
      "step": 9441
    },
    {
      "epoch": 0.8500754012019177,
      "grad_norm": 1.255774909492682,
      "learning_rate": 1.1557820374648176e-06,
      "loss": 0.5332,
      "step": 9442
    },
    {
      "epoch": 0.8501654324877895,
      "grad_norm": 1.085451045082506,
      "learning_rate": 1.154421469582685e-06,
      "loss": 0.4212,
      "step": 9443
    },
    {
      "epoch": 0.8502554637736613,
      "grad_norm": 1.5353871820026193,
      "learning_rate": 1.1530616539340456e-06,
      "loss": 0.4667,
      "step": 9444
    },
    {
      "epoch": 0.8503454950595332,
      "grad_norm": 1.3433138827862476,
      "learning_rate": 1.1517025906345325e-06,
      "loss": 0.5478,
      "step": 9445
    },
    {
      "epoch": 0.850435526345405,
      "grad_norm": 1.338121224024975,
      "learning_rate": 1.15034427979973e-06,
      "loss": 0.4595,
      "step": 9446
    },
    {
      "epoch": 0.8505255576312769,
      "grad_norm": 1.1225792900082043,
      "learning_rate": 1.1489867215451433e-06,
      "loss": 0.5457,
      "step": 9447
    },
    {
      "epoch": 0.8506155889171487,
      "grad_norm": 1.44802003990738,
      "learning_rate": 1.1476299159862204e-06,
      "loss": 0.4501,
      "step": 9448
    },
    {
      "epoch": 0.8507056202030205,
      "grad_norm": 1.8779392807237403,
      "learning_rate": 1.1462738632383475e-06,
      "loss": 0.5456,
      "step": 9449
    },
    {
      "epoch": 0.8507956514888924,
      "grad_norm": 1.1192171530439952,
      "learning_rate": 1.1449185634168424e-06,
      "loss": 0.54,
      "step": 9450
    },
    {
      "epoch": 0.8508856827747643,
      "grad_norm": 1.4778073778346217,
      "learning_rate": 1.143564016636961e-06,
      "loss": 0.5372,
      "step": 9451
    },
    {
      "epoch": 0.8509757140606361,
      "grad_norm": 1.2200036914582952,
      "learning_rate": 1.1422102230138977e-06,
      "loss": 0.5253,
      "step": 9452
    },
    {
      "epoch": 0.8510657453465079,
      "grad_norm": 1.3791008812996106,
      "learning_rate": 1.1408571826627745e-06,
      "loss": 0.5464,
      "step": 9453
    },
    {
      "epoch": 0.8511557766323797,
      "grad_norm": 0.9849840885696275,
      "learning_rate": 1.1395048956986577e-06,
      "loss": 0.5418,
      "step": 9454
    },
    {
      "epoch": 0.8512458079182516,
      "grad_norm": 1.0695962464531579,
      "learning_rate": 1.138153362236546e-06,
      "loss": 0.6211,
      "step": 9455
    },
    {
      "epoch": 0.8513358392041235,
      "grad_norm": 1.434038460769862,
      "learning_rate": 1.1368025823913753e-06,
      "loss": 0.4827,
      "step": 9456
    },
    {
      "epoch": 0.8514258704899953,
      "grad_norm": 1.71043582955123,
      "learning_rate": 1.1354525562780172e-06,
      "loss": 0.5311,
      "step": 9457
    },
    {
      "epoch": 0.8515159017758671,
      "grad_norm": 1.217155233234848,
      "learning_rate": 1.1341032840112786e-06,
      "loss": 0.4816,
      "step": 9458
    },
    {
      "epoch": 0.8516059330617389,
      "grad_norm": 1.695292851211551,
      "learning_rate": 1.132754765705899e-06,
      "loss": 0.3942,
      "step": 9459
    },
    {
      "epoch": 0.8516959643476107,
      "grad_norm": 2.79919455463255,
      "learning_rate": 1.1314070014765644e-06,
      "loss": 0.4722,
      "step": 9460
    },
    {
      "epoch": 0.8517859956334827,
      "grad_norm": 1.1349444901617178,
      "learning_rate": 1.130059991437883e-06,
      "loss": 0.5597,
      "step": 9461
    },
    {
      "epoch": 0.8518760269193545,
      "grad_norm": 1.0868425143684532,
      "learning_rate": 1.1287137357044075e-06,
      "loss": 0.5172,
      "step": 9462
    },
    {
      "epoch": 0.8519660582052263,
      "grad_norm": 1.6115858917255508,
      "learning_rate": 1.1273682343906245e-06,
      "loss": 0.4814,
      "step": 9463
    },
    {
      "epoch": 0.8520560894910981,
      "grad_norm": 1.5251070785549707,
      "learning_rate": 1.1260234876109567e-06,
      "loss": 0.5654,
      "step": 9464
    },
    {
      "epoch": 0.85214612077697,
      "grad_norm": 1.445667632997125,
      "learning_rate": 1.1246794954797612e-06,
      "loss": 0.4281,
      "step": 9465
    },
    {
      "epoch": 0.8522361520628419,
      "grad_norm": 0.9948503191522513,
      "learning_rate": 1.123336258111335e-06,
      "loss": 0.5668,
      "step": 9466
    },
    {
      "epoch": 0.8523261833487137,
      "grad_norm": 1.7213013414519631,
      "learning_rate": 1.1219937756199017e-06,
      "loss": 0.5549,
      "step": 9467
    },
    {
      "epoch": 0.8524162146345855,
      "grad_norm": 0.985792318129554,
      "learning_rate": 1.1206520481196326e-06,
      "loss": 0.5429,
      "step": 9468
    },
    {
      "epoch": 0.8525062459204573,
      "grad_norm": 0.9971470609469291,
      "learning_rate": 1.1193110757246251e-06,
      "loss": 0.4922,
      "step": 9469
    },
    {
      "epoch": 0.8525962772063292,
      "grad_norm": 1.0571018530900294,
      "learning_rate": 1.1179708585489192e-06,
      "loss": 0.4763,
      "step": 9470
    },
    {
      "epoch": 0.8526863084922011,
      "grad_norm": 1.185280899034321,
      "learning_rate": 1.1166313967064869e-06,
      "loss": 0.434,
      "step": 9471
    },
    {
      "epoch": 0.8527763397780729,
      "grad_norm": 1.641439768905651,
      "learning_rate": 1.1152926903112381e-06,
      "loss": 0.5176,
      "step": 9472
    },
    {
      "epoch": 0.8528663710639447,
      "grad_norm": 1.3772821563266617,
      "learning_rate": 1.1139547394770122e-06,
      "loss": 0.4632,
      "step": 9473
    },
    {
      "epoch": 0.8529564023498165,
      "grad_norm": 2.220765933358094,
      "learning_rate": 1.1126175443175968e-06,
      "loss": 0.4998,
      "step": 9474
    },
    {
      "epoch": 0.8530464336356884,
      "grad_norm": 1.2349951020355758,
      "learning_rate": 1.111281104946701e-06,
      "loss": 0.5778,
      "step": 9475
    },
    {
      "epoch": 0.8531364649215603,
      "grad_norm": 1.5377886546735413,
      "learning_rate": 1.1099454214779803e-06,
      "loss": 0.5416,
      "step": 9476
    },
    {
      "epoch": 0.8532264962074321,
      "grad_norm": 1.060611301684957,
      "learning_rate": 1.108610494025022e-06,
      "loss": 0.5861,
      "step": 9477
    },
    {
      "epoch": 0.8533165274933039,
      "grad_norm": 1.2178157139535333,
      "learning_rate": 1.1072763227013471e-06,
      "loss": 0.4728,
      "step": 9478
    },
    {
      "epoch": 0.8534065587791758,
      "grad_norm": 1.997020022509889,
      "learning_rate": 1.1059429076204154e-06,
      "loss": 0.5942,
      "step": 9479
    },
    {
      "epoch": 0.8534965900650476,
      "grad_norm": 1.5651485855899936,
      "learning_rate": 1.1046102488956246e-06,
      "loss": 0.5875,
      "step": 9480
    },
    {
      "epoch": 0.8535866213509195,
      "grad_norm": 1.1745770240242759,
      "learning_rate": 1.1032783466402996e-06,
      "loss": 0.5862,
      "step": 9481
    },
    {
      "epoch": 0.8536766526367913,
      "grad_norm": 1.3054351340800148,
      "learning_rate": 1.101947200967708e-06,
      "loss": 0.4869,
      "step": 9482
    },
    {
      "epoch": 0.8537666839226631,
      "grad_norm": 1.728523862679028,
      "learning_rate": 1.100616811991051e-06,
      "loss": 0.5268,
      "step": 9483
    },
    {
      "epoch": 0.853856715208535,
      "grad_norm": 1.5091372908457554,
      "learning_rate": 1.0992871798234672e-06,
      "loss": 0.4296,
      "step": 9484
    },
    {
      "epoch": 0.8539467464944068,
      "grad_norm": 1.053363378845149,
      "learning_rate": 1.0979583045780285e-06,
      "loss": 0.5224,
      "step": 9485
    },
    {
      "epoch": 0.8540367777802786,
      "grad_norm": 1.1741101598221593,
      "learning_rate": 1.0966301863677443e-06,
      "loss": 0.5545,
      "step": 9486
    },
    {
      "epoch": 0.8541268090661505,
      "grad_norm": 1.3626680568259468,
      "learning_rate": 1.0953028253055541e-06,
      "loss": 0.5341,
      "step": 9487
    },
    {
      "epoch": 0.8542168403520223,
      "grad_norm": 1.3123588475910557,
      "learning_rate": 1.0939762215043449e-06,
      "loss": 0.5743,
      "step": 9488
    },
    {
      "epoch": 0.8543068716378942,
      "grad_norm": 1.8990383730785032,
      "learning_rate": 1.092650375076927e-06,
      "loss": 0.5443,
      "step": 9489
    },
    {
      "epoch": 0.854396902923766,
      "grad_norm": 1.2572136264374851,
      "learning_rate": 1.09132528613605e-06,
      "loss": 0.4942,
      "step": 9490
    },
    {
      "epoch": 0.8544869342096378,
      "grad_norm": 1.8662637647138955,
      "learning_rate": 1.0900009547944047e-06,
      "loss": 0.4205,
      "step": 9491
    },
    {
      "epoch": 0.8545769654955097,
      "grad_norm": 2.0520009273929856,
      "learning_rate": 1.088677381164609e-06,
      "loss": 0.6102,
      "step": 9492
    },
    {
      "epoch": 0.8546669967813816,
      "grad_norm": 1.5430131729286174,
      "learning_rate": 1.0873545653592233e-06,
      "loss": 0.5788,
      "step": 9493
    },
    {
      "epoch": 0.8547570280672534,
      "grad_norm": 1.3412306841807555,
      "learning_rate": 1.0860325074907407e-06,
      "loss": 0.4275,
      "step": 9494
    },
    {
      "epoch": 0.8548470593531252,
      "grad_norm": 1.9042352766687058,
      "learning_rate": 1.084711207671587e-06,
      "loss": 0.5118,
      "step": 9495
    },
    {
      "epoch": 0.854937090638997,
      "grad_norm": 1.3041578215291094,
      "learning_rate": 1.083390666014129e-06,
      "loss": 0.6317,
      "step": 9496
    },
    {
      "epoch": 0.8550271219248688,
      "grad_norm": 1.6274642406596216,
      "learning_rate": 1.0820708826306648e-06,
      "loss": 0.5352,
      "step": 9497
    },
    {
      "epoch": 0.8551171532107408,
      "grad_norm": 1.2322898944883252,
      "learning_rate": 1.0807518576334297e-06,
      "loss": 0.5129,
      "step": 9498
    },
    {
      "epoch": 0.8552071844966126,
      "grad_norm": 1.125916076241423,
      "learning_rate": 1.0794335911345954e-06,
      "loss": 0.5105,
      "step": 9499
    },
    {
      "epoch": 0.8552972157824844,
      "grad_norm": 1.2563031404748446,
      "learning_rate": 1.07811608324627e-06,
      "loss": 0.5178,
      "step": 9500
    },
    {
      "epoch": 0.8553872470683562,
      "grad_norm": 1.126595350519738,
      "learning_rate": 1.0767993340804884e-06,
      "loss": 0.4446,
      "step": 9501
    },
    {
      "epoch": 0.855477278354228,
      "grad_norm": 1.3437386777338085,
      "learning_rate": 1.0754833437492362e-06,
      "loss": 0.5467,
      "step": 9502
    },
    {
      "epoch": 0.8555673096401,
      "grad_norm": 1.1571639969567786,
      "learning_rate": 1.074168112364421e-06,
      "loss": 0.4341,
      "step": 9503
    },
    {
      "epoch": 0.8556573409259718,
      "grad_norm": 3.0885855633119053,
      "learning_rate": 1.0728536400378909e-06,
      "loss": 0.5714,
      "step": 9504
    },
    {
      "epoch": 0.8557473722118436,
      "grad_norm": 1.5326154979303275,
      "learning_rate": 1.0715399268814308e-06,
      "loss": 0.4597,
      "step": 9505
    },
    {
      "epoch": 0.8558374034977154,
      "grad_norm": 1.5716216971690353,
      "learning_rate": 1.0702269730067594e-06,
      "loss": 0.4525,
      "step": 9506
    },
    {
      "epoch": 0.8559274347835873,
      "grad_norm": 0.9935701348680034,
      "learning_rate": 1.0689147785255315e-06,
      "loss": 0.4648,
      "step": 9507
    },
    {
      "epoch": 0.8560174660694592,
      "grad_norm": 1.3397745556053093,
      "learning_rate": 1.0676033435493372e-06,
      "loss": 0.5259,
      "step": 9508
    },
    {
      "epoch": 0.856107497355331,
      "grad_norm": 0.933676905780507,
      "learning_rate": 1.0662926681897002e-06,
      "loss": 0.473,
      "step": 9509
    },
    {
      "epoch": 0.8561975286412028,
      "grad_norm": 1.1719257629098905,
      "learning_rate": 1.0649827525580826e-06,
      "loss": 0.6052,
      "step": 9510
    },
    {
      "epoch": 0.8562875599270746,
      "grad_norm": 1.5998835052485725,
      "learning_rate": 1.0636735967658785e-06,
      "loss": 0.622,
      "step": 9511
    },
    {
      "epoch": 0.8563775912129465,
      "grad_norm": 1.7133975765176406,
      "learning_rate": 1.0623652009244212e-06,
      "loss": 0.4761,
      "step": 9512
    },
    {
      "epoch": 0.8564676224988184,
      "grad_norm": 1.4225642564917749,
      "learning_rate": 1.0610575651449773e-06,
      "loss": 0.4569,
      "step": 9513
    },
    {
      "epoch": 0.8565576537846902,
      "grad_norm": 1.3160969187432037,
      "learning_rate": 1.05975068953875e-06,
      "loss": 0.4551,
      "step": 9514
    },
    {
      "epoch": 0.856647685070562,
      "grad_norm": 1.207579319302308,
      "learning_rate": 1.0584445742168714e-06,
      "loss": 0.4352,
      "step": 9515
    },
    {
      "epoch": 0.8567377163564338,
      "grad_norm": 1.3781211399695843,
      "learning_rate": 1.057139219290423e-06,
      "loss": 0.4811,
      "step": 9516
    },
    {
      "epoch": 0.8568277476423057,
      "grad_norm": 1.3705873332957428,
      "learning_rate": 1.0558346248704055e-06,
      "loss": 0.492,
      "step": 9517
    },
    {
      "epoch": 0.8569177789281776,
      "grad_norm": 1.2035489057141198,
      "learning_rate": 1.054530791067766e-06,
      "loss": 0.4536,
      "step": 9518
    },
    {
      "epoch": 0.8570078102140494,
      "grad_norm": 0.9992560250819983,
      "learning_rate": 1.0532277179933824e-06,
      "loss": 0.5061,
      "step": 9519
    },
    {
      "epoch": 0.8570978414999212,
      "grad_norm": 1.220050033385598,
      "learning_rate": 1.0519254057580697e-06,
      "loss": 0.5621,
      "step": 9520
    },
    {
      "epoch": 0.8571878727857931,
      "grad_norm": 2.284602286691106,
      "learning_rate": 1.0506238544725767e-06,
      "loss": 0.5114,
      "step": 9521
    },
    {
      "epoch": 0.8572779040716649,
      "grad_norm": 0.9992638798632899,
      "learning_rate": 1.0493230642475893e-06,
      "loss": 0.4638,
      "step": 9522
    },
    {
      "epoch": 0.8573679353575367,
      "grad_norm": 1.1294555540762536,
      "learning_rate": 1.0480230351937249e-06,
      "loss": 0.5332,
      "step": 9523
    },
    {
      "epoch": 0.8574579666434086,
      "grad_norm": 1.1117693829235926,
      "learning_rate": 1.0467237674215403e-06,
      "loss": 0.558,
      "step": 9524
    },
    {
      "epoch": 0.8575479979292804,
      "grad_norm": 1.10333260156718,
      "learning_rate": 1.045425261041525e-06,
      "loss": 0.5366,
      "step": 9525
    },
    {
      "epoch": 0.8576380292151523,
      "grad_norm": 1.2410576292470135,
      "learning_rate": 1.0441275161641073e-06,
      "loss": 0.4225,
      "step": 9526
    },
    {
      "epoch": 0.8577280605010241,
      "grad_norm": 1.3644326670078735,
      "learning_rate": 1.0428305328996458e-06,
      "loss": 0.5593,
      "step": 9527
    },
    {
      "epoch": 0.8578180917868959,
      "grad_norm": 1.3175812338795332,
      "learning_rate": 1.0415343113584397e-06,
      "loss": 0.4316,
      "step": 9528
    },
    {
      "epoch": 0.8579081230727678,
      "grad_norm": 1.839870952169221,
      "learning_rate": 1.0402388516507144e-06,
      "loss": 0.4397,
      "step": 9529
    },
    {
      "epoch": 0.8579981543586396,
      "grad_norm": 1.061641617103102,
      "learning_rate": 1.0389441538866452e-06,
      "loss": 0.4854,
      "step": 9530
    },
    {
      "epoch": 0.8580881856445115,
      "grad_norm": 1.697010453080901,
      "learning_rate": 1.0376502181763271e-06,
      "loss": 0.4433,
      "step": 9531
    },
    {
      "epoch": 0.8581782169303833,
      "grad_norm": 1.6170448137371418,
      "learning_rate": 1.0363570446297999e-06,
      "loss": 0.4994,
      "step": 9532
    },
    {
      "epoch": 0.8582682482162551,
      "grad_norm": 1.3658256761054395,
      "learning_rate": 1.0350646333570347e-06,
      "loss": 0.5442,
      "step": 9533
    },
    {
      "epoch": 0.858358279502127,
      "grad_norm": 0.9307917229375101,
      "learning_rate": 1.03377298446794e-06,
      "loss": 0.4148,
      "step": 9534
    },
    {
      "epoch": 0.8584483107879989,
      "grad_norm": 1.4559082275241833,
      "learning_rate": 1.0324820980723594e-06,
      "loss": 0.5344,
      "step": 9535
    },
    {
      "epoch": 0.8585383420738707,
      "grad_norm": 1.731213532948238,
      "learning_rate": 1.0311919742800703e-06,
      "loss": 0.4951,
      "step": 9536
    },
    {
      "epoch": 0.8586283733597425,
      "grad_norm": 1.2961623691859294,
      "learning_rate": 1.0299026132007827e-06,
      "loss": 0.4039,
      "step": 9537
    },
    {
      "epoch": 0.8587184046456143,
      "grad_norm": 1.4849596985601432,
      "learning_rate": 1.028614014944147e-06,
      "loss": 0.4499,
      "step": 9538
    },
    {
      "epoch": 0.8588084359314861,
      "grad_norm": 1.1515442029230183,
      "learning_rate": 1.0273261796197464e-06,
      "loss": 0.5068,
      "step": 9539
    },
    {
      "epoch": 0.8588984672173581,
      "grad_norm": 1.4574761983685496,
      "learning_rate": 1.026039107337099e-06,
      "loss": 0.5208,
      "step": 9540
    },
    {
      "epoch": 0.8589884985032299,
      "grad_norm": 1.0859756428522012,
      "learning_rate": 1.0247527982056581e-06,
      "loss": 0.4719,
      "step": 9541
    },
    {
      "epoch": 0.8590785297891017,
      "grad_norm": 2.3115441538865893,
      "learning_rate": 1.0234672523348144e-06,
      "loss": 0.4111,
      "step": 9542
    },
    {
      "epoch": 0.8591685610749735,
      "grad_norm": 1.279665129517191,
      "learning_rate": 1.0221824698338855e-06,
      "loss": 0.505,
      "step": 9543
    },
    {
      "epoch": 0.8592585923608453,
      "grad_norm": 1.0354387588757645,
      "learning_rate": 1.0208984508121379e-06,
      "loss": 0.514,
      "step": 9544
    },
    {
      "epoch": 0.8593486236467173,
      "grad_norm": 1.7919174409207463,
      "learning_rate": 1.0196151953787602e-06,
      "loss": 0.585,
      "step": 9545
    },
    {
      "epoch": 0.8594386549325891,
      "grad_norm": 1.341271165286338,
      "learning_rate": 1.0183327036428815e-06,
      "loss": 0.5238,
      "step": 9546
    },
    {
      "epoch": 0.8595286862184609,
      "grad_norm": 1.073029423440455,
      "learning_rate": 1.0170509757135671e-06,
      "loss": 0.4938,
      "step": 9547
    },
    {
      "epoch": 0.8596187175043327,
      "grad_norm": 1.8357484491579659,
      "learning_rate": 1.015770011699816e-06,
      "loss": 0.4835,
      "step": 9548
    },
    {
      "epoch": 0.8597087487902046,
      "grad_norm": 1.161932484904259,
      "learning_rate": 1.0144898117105617e-06,
      "loss": 0.4745,
      "step": 9549
    },
    {
      "epoch": 0.8597987800760765,
      "grad_norm": 1.539265031274888,
      "learning_rate": 1.0132103758546752e-06,
      "loss": 0.5205,
      "step": 9550
    },
    {
      "epoch": 0.8598888113619483,
      "grad_norm": 1.2213619716782689,
      "learning_rate": 1.0119317042409572e-06,
      "loss": 0.4929,
      "step": 9551
    },
    {
      "epoch": 0.8599788426478201,
      "grad_norm": 1.4062757556555103,
      "learning_rate": 1.010653796978147e-06,
      "loss": 0.4806,
      "step": 9552
    },
    {
      "epoch": 0.8600688739336919,
      "grad_norm": 0.8864553242720098,
      "learning_rate": 1.0093766541749206e-06,
      "loss": 0.4698,
      "step": 9553
    },
    {
      "epoch": 0.8601589052195638,
      "grad_norm": 1.0869532209082304,
      "learning_rate": 1.0081002759398862e-06,
      "loss": 0.4887,
      "step": 9554
    },
    {
      "epoch": 0.8602489365054357,
      "grad_norm": 1.043297904503446,
      "learning_rate": 1.006824662381587e-06,
      "loss": 0.514,
      "step": 9555
    },
    {
      "epoch": 0.8603389677913075,
      "grad_norm": 1.584135043507142,
      "learning_rate": 1.0055498136085052e-06,
      "loss": 0.4384,
      "step": 9556
    },
    {
      "epoch": 0.8604289990771793,
      "grad_norm": 2.1043186369399436,
      "learning_rate": 1.0042757297290485e-06,
      "loss": 0.4834,
      "step": 9557
    },
    {
      "epoch": 0.8605190303630511,
      "grad_norm": 1.1337969194790152,
      "learning_rate": 1.0030024108515735e-06,
      "loss": 0.4718,
      "step": 9558
    },
    {
      "epoch": 0.860609061648923,
      "grad_norm": 1.060249201766883,
      "learning_rate": 1.0017298570843582e-06,
      "loss": 0.5575,
      "step": 9559
    },
    {
      "epoch": 0.8606990929347949,
      "grad_norm": 1.5006314421103133,
      "learning_rate": 1.0004580685356225e-06,
      "loss": 0.5127,
      "step": 9560
    },
    {
      "epoch": 0.8607891242206667,
      "grad_norm": 1.3072879920200626,
      "learning_rate": 9.991870453135223e-07,
      "loss": 0.514,
      "step": 9561
    },
    {
      "epoch": 0.8608791555065385,
      "grad_norm": 1.1491809352460873,
      "learning_rate": 9.97916787526143e-07,
      "loss": 0.5954,
      "step": 9562
    },
    {
      "epoch": 0.8609691867924104,
      "grad_norm": 1.3273305829353805,
      "learning_rate": 9.96647295281511e-07,
      "loss": 0.451,
      "step": 9563
    },
    {
      "epoch": 0.8610592180782822,
      "grad_norm": 1.5801926500141033,
      "learning_rate": 9.95378568687585e-07,
      "loss": 0.5042,
      "step": 9564
    },
    {
      "epoch": 0.861149249364154,
      "grad_norm": 1.2520390470985638,
      "learning_rate": 9.941106078522555e-07,
      "loss": 0.4513,
      "step": 9565
    },
    {
      "epoch": 0.8612392806500259,
      "grad_norm": 1.640174580074148,
      "learning_rate": 9.92843412883352e-07,
      "loss": 0.5563,
      "step": 9566
    },
    {
      "epoch": 0.8613293119358977,
      "grad_norm": 1.1815157188479435,
      "learning_rate": 9.915769838886369e-07,
      "loss": 0.5033,
      "step": 9567
    },
    {
      "epoch": 0.8614193432217696,
      "grad_norm": 1.4262019497353067,
      "learning_rate": 9.903113209758098e-07,
      "loss": 0.5908,
      "step": 9568
    },
    {
      "epoch": 0.8615093745076414,
      "grad_norm": 1.262438563349095,
      "learning_rate": 9.89046424252501e-07,
      "loss": 0.5412,
      "step": 9569
    },
    {
      "epoch": 0.8615994057935132,
      "grad_norm": 1.448767503653795,
      "learning_rate": 9.877822938262827e-07,
      "loss": 0.4941,
      "step": 9570
    },
    {
      "epoch": 0.8616894370793851,
      "grad_norm": 1.3804586310110578,
      "learning_rate": 9.865189298046495e-07,
      "loss": 0.468,
      "step": 9571
    },
    {
      "epoch": 0.8617794683652569,
      "grad_norm": 0.8990644106890567,
      "learning_rate": 9.852563322950482e-07,
      "loss": 0.4973,
      "step": 9572
    },
    {
      "epoch": 0.8618694996511288,
      "grad_norm": 1.502723066867125,
      "learning_rate": 9.839945014048435e-07,
      "loss": 0.5296,
      "step": 9573
    },
    {
      "epoch": 0.8619595309370006,
      "grad_norm": 1.1379528332340831,
      "learning_rate": 9.827334372413444e-07,
      "loss": 0.5366,
      "step": 9574
    },
    {
      "epoch": 0.8620495622228724,
      "grad_norm": 1.5690657610125789,
      "learning_rate": 9.814731399117938e-07,
      "loss": 0.495,
      "step": 9575
    },
    {
      "epoch": 0.8621395935087443,
      "grad_norm": 1.2428138818241878,
      "learning_rate": 9.802136095233671e-07,
      "loss": 0.493,
      "step": 9576
    },
    {
      "epoch": 0.8622296247946162,
      "grad_norm": 1.5825699410432545,
      "learning_rate": 9.789548461831755e-07,
      "loss": 0.51,
      "step": 9577
    },
    {
      "epoch": 0.862319656080488,
      "grad_norm": 1.5233006458488263,
      "learning_rate": 9.776968499982676e-07,
      "loss": 0.5596,
      "step": 9578
    },
    {
      "epoch": 0.8624096873663598,
      "grad_norm": 1.3958190805199726,
      "learning_rate": 9.7643962107562e-07,
      "loss": 0.523,
      "step": 9579
    },
    {
      "epoch": 0.8624997186522316,
      "grad_norm": 1.043305285882466,
      "learning_rate": 9.751831595221505e-07,
      "loss": 0.5248,
      "step": 9580
    },
    {
      "epoch": 0.8625897499381034,
      "grad_norm": 1.5794458410239616,
      "learning_rate": 9.739274654447085e-07,
      "loss": 0.5031,
      "step": 9581
    },
    {
      "epoch": 0.8626797812239754,
      "grad_norm": 1.2684209962534068,
      "learning_rate": 9.72672538950079e-07,
      "loss": 0.5058,
      "step": 9582
    },
    {
      "epoch": 0.8627698125098472,
      "grad_norm": 1.3680398843716173,
      "learning_rate": 9.714183801449827e-07,
      "loss": 0.5137,
      "step": 9583
    },
    {
      "epoch": 0.862859843795719,
      "grad_norm": 1.3118037724435891,
      "learning_rate": 9.701649891360765e-07,
      "loss": 0.4486,
      "step": 9584
    },
    {
      "epoch": 0.8629498750815908,
      "grad_norm": 1.427108474305272,
      "learning_rate": 9.689123660299415e-07,
      "loss": 0.4944,
      "step": 9585
    },
    {
      "epoch": 0.8630399063674626,
      "grad_norm": 1.328664138733849,
      "learning_rate": 9.676605109331105e-07,
      "loss": 0.5063,
      "step": 9586
    },
    {
      "epoch": 0.8631299376533346,
      "grad_norm": 1.1975949000384483,
      "learning_rate": 9.664094239520372e-07,
      "loss": 0.494,
      "step": 9587
    },
    {
      "epoch": 0.8632199689392064,
      "grad_norm": 1.1454918766579958,
      "learning_rate": 9.651591051931153e-07,
      "loss": 0.5399,
      "step": 9588
    },
    {
      "epoch": 0.8633100002250782,
      "grad_norm": 1.6648532212420173,
      "learning_rate": 9.63909554762673e-07,
      "loss": 0.5662,
      "step": 9589
    },
    {
      "epoch": 0.86340003151095,
      "grad_norm": 2.0237586999313955,
      "learning_rate": 9.626607727669734e-07,
      "loss": 0.5326,
      "step": 9590
    },
    {
      "epoch": 0.8634900627968219,
      "grad_norm": 1.2494149674420436,
      "learning_rate": 9.614127593122136e-07,
      "loss": 0.3677,
      "step": 9591
    },
    {
      "epoch": 0.8635800940826938,
      "grad_norm": 1.100890522264209,
      "learning_rate": 9.601655145045263e-07,
      "loss": 0.4691,
      "step": 9592
    },
    {
      "epoch": 0.8636701253685656,
      "grad_norm": 1.5070424725804403,
      "learning_rate": 9.589190384499747e-07,
      "loss": 0.4134,
      "step": 9593
    },
    {
      "epoch": 0.8637601566544374,
      "grad_norm": 1.412907165477194,
      "learning_rate": 9.576733312545627e-07,
      "loss": 0.5022,
      "step": 9594
    },
    {
      "epoch": 0.8638501879403092,
      "grad_norm": 1.1655854498634703,
      "learning_rate": 9.564283930242258e-07,
      "loss": 0.4414,
      "step": 9595
    },
    {
      "epoch": 0.8639402192261811,
      "grad_norm": 1.6637978914601392,
      "learning_rate": 9.551842238648334e-07,
      "loss": 0.5333,
      "step": 9596
    },
    {
      "epoch": 0.864030250512053,
      "grad_norm": 1.3830770424342254,
      "learning_rate": 9.539408238821912e-07,
      "loss": 0.5743,
      "step": 9597
    },
    {
      "epoch": 0.8641202817979248,
      "grad_norm": 1.218540722876184,
      "learning_rate": 9.5269819318204e-07,
      "loss": 0.536,
      "step": 9598
    },
    {
      "epoch": 0.8642103130837966,
      "grad_norm": 1.1003579578144953,
      "learning_rate": 9.514563318700487e-07,
      "loss": 0.4123,
      "step": 9599
    },
    {
      "epoch": 0.8643003443696684,
      "grad_norm": 1.9492287215239865,
      "learning_rate": 9.502152400518327e-07,
      "loss": 0.5554,
      "step": 9600
    },
    {
      "epoch": 0.8643903756555403,
      "grad_norm": 1.3315143303445767,
      "learning_rate": 9.48974917832931e-07,
      "loss": 0.5224,
      "step": 9601
    },
    {
      "epoch": 0.8644804069414121,
      "grad_norm": 1.121767260262339,
      "learning_rate": 9.477353653188226e-07,
      "loss": 0.5286,
      "step": 9602
    },
    {
      "epoch": 0.864570438227284,
      "grad_norm": 2.1527709708127687,
      "learning_rate": 9.464965826149187e-07,
      "loss": 0.5305,
      "step": 9603
    },
    {
      "epoch": 0.8646604695131558,
      "grad_norm": 0.9247053115565164,
      "learning_rate": 9.452585698265681e-07,
      "loss": 0.4967,
      "step": 9604
    },
    {
      "epoch": 0.8647505007990277,
      "grad_norm": 1.431014066088787,
      "learning_rate": 9.440213270590503e-07,
      "loss": 0.5778,
      "step": 9605
    },
    {
      "epoch": 0.8648405320848995,
      "grad_norm": 1.2882173984270804,
      "learning_rate": 9.427848544175844e-07,
      "loss": 0.5072,
      "step": 9606
    },
    {
      "epoch": 0.8649305633707713,
      "grad_norm": 1.4102692111579713,
      "learning_rate": 9.415491520073172e-07,
      "loss": 0.4625,
      "step": 9607
    },
    {
      "epoch": 0.8650205946566432,
      "grad_norm": 1.3020373093423376,
      "learning_rate": 9.403142199333337e-07,
      "loss": 0.5435,
      "step": 9608
    },
    {
      "epoch": 0.865110625942515,
      "grad_norm": 1.6668140090793573,
      "learning_rate": 9.390800583006554e-07,
      "loss": 0.5418,
      "step": 9609
    },
    {
      "epoch": 0.8652006572283869,
      "grad_norm": 1.1534153145805262,
      "learning_rate": 9.37846667214235e-07,
      "loss": 0.5414,
      "step": 9610
    },
    {
      "epoch": 0.8652906885142587,
      "grad_norm": 1.632966791738895,
      "learning_rate": 9.366140467789608e-07,
      "loss": 0.4873,
      "step": 9611
    },
    {
      "epoch": 0.8653807198001305,
      "grad_norm": 1.78148504256578,
      "learning_rate": 9.35382197099658e-07,
      "loss": 0.3912,
      "step": 9612
    },
    {
      "epoch": 0.8654707510860024,
      "grad_norm": 1.551122640815091,
      "learning_rate": 9.34151118281078e-07,
      "loss": 0.469,
      "step": 9613
    },
    {
      "epoch": 0.8655607823718742,
      "grad_norm": 1.3324551693253293,
      "learning_rate": 9.329208104279197e-07,
      "loss": 0.4819,
      "step": 9614
    },
    {
      "epoch": 0.8656508136577461,
      "grad_norm": 1.4650977200644175,
      "learning_rate": 9.316912736448047e-07,
      "loss": 0.4143,
      "step": 9615
    },
    {
      "epoch": 0.8657408449436179,
      "grad_norm": 1.2459988006210643,
      "learning_rate": 9.304625080362939e-07,
      "loss": 0.5061,
      "step": 9616
    },
    {
      "epoch": 0.8658308762294897,
      "grad_norm": 1.1507951891011041,
      "learning_rate": 9.292345137068837e-07,
      "loss": 0.4818,
      "step": 9617
    },
    {
      "epoch": 0.8659209075153615,
      "grad_norm": 1.650819357808824,
      "learning_rate": 9.280072907610027e-07,
      "loss": 0.5223,
      "step": 9618
    },
    {
      "epoch": 0.8660109388012335,
      "grad_norm": 1.0817938558034093,
      "learning_rate": 9.267808393030142e-07,
      "loss": 0.5336,
      "step": 9619
    },
    {
      "epoch": 0.8661009700871053,
      "grad_norm": 2.2405792082943825,
      "learning_rate": 9.255551594372192e-07,
      "loss": 0.4567,
      "step": 9620
    },
    {
      "epoch": 0.8661910013729771,
      "grad_norm": 3.7110605962470866,
      "learning_rate": 9.243302512678476e-07,
      "loss": 0.5156,
      "step": 9621
    },
    {
      "epoch": 0.8662810326588489,
      "grad_norm": 1.8343180344839882,
      "learning_rate": 9.23106114899065e-07,
      "loss": 0.5448,
      "step": 9622
    },
    {
      "epoch": 0.8663710639447207,
      "grad_norm": 1.767502936599451,
      "learning_rate": 9.218827504349759e-07,
      "loss": 0.5238,
      "step": 9623
    },
    {
      "epoch": 0.8664610952305927,
      "grad_norm": 1.6231828963702994,
      "learning_rate": 9.206601579796137e-07,
      "loss": 0.4811,
      "step": 9624
    },
    {
      "epoch": 0.8665511265164645,
      "grad_norm": 1.8858819989248279,
      "learning_rate": 9.194383376369509e-07,
      "loss": 0.6066,
      "step": 9625
    },
    {
      "epoch": 0.8666411578023363,
      "grad_norm": 1.0395234079394322,
      "learning_rate": 9.18217289510891e-07,
      "loss": 0.6331,
      "step": 9626
    },
    {
      "epoch": 0.8667311890882081,
      "grad_norm": 1.5725147879537646,
      "learning_rate": 9.169970137052686e-07,
      "loss": 0.5056,
      "step": 9627
    },
    {
      "epoch": 0.86682122037408,
      "grad_norm": 1.1497001014016375,
      "learning_rate": 9.157775103238642e-07,
      "loss": 0.3859,
      "step": 9628
    },
    {
      "epoch": 0.8669112516599519,
      "grad_norm": 1.6117718629850648,
      "learning_rate": 9.145587794703781e-07,
      "loss": 0.5452,
      "step": 9629
    },
    {
      "epoch": 0.8670012829458237,
      "grad_norm": 1.7750794145779079,
      "learning_rate": 9.133408212484562e-07,
      "loss": 0.5394,
      "step": 9630
    },
    {
      "epoch": 0.8670913142316955,
      "grad_norm": 1.2474783201809165,
      "learning_rate": 9.121236357616726e-07,
      "loss": 0.5476,
      "step": 9631
    },
    {
      "epoch": 0.8671813455175673,
      "grad_norm": 1.2970088857449165,
      "learning_rate": 9.109072231135385e-07,
      "loss": 0.4196,
      "step": 9632
    },
    {
      "epoch": 0.8672713768034392,
      "grad_norm": 1.0879180563810398,
      "learning_rate": 9.096915834074971e-07,
      "loss": 0.5054,
      "step": 9633
    },
    {
      "epoch": 0.8673614080893111,
      "grad_norm": 1.7303124212644638,
      "learning_rate": 9.08476716746931e-07,
      "loss": 0.5179,
      "step": 9634
    },
    {
      "epoch": 0.8674514393751829,
      "grad_norm": 1.154315606124519,
      "learning_rate": 9.072626232351478e-07,
      "loss": 0.4784,
      "step": 9635
    },
    {
      "epoch": 0.8675414706610547,
      "grad_norm": 1.1784718304993453,
      "learning_rate": 9.06049302975397e-07,
      "loss": 0.4928,
      "step": 9636
    },
    {
      "epoch": 0.8676315019469265,
      "grad_norm": 1.203368188377239,
      "learning_rate": 9.048367560708604e-07,
      "loss": 0.4693,
      "step": 9637
    },
    {
      "epoch": 0.8677215332327984,
      "grad_norm": 1.2923112307723192,
      "learning_rate": 9.036249826246535e-07,
      "loss": 0.5108,
      "step": 9638
    },
    {
      "epoch": 0.8678115645186703,
      "grad_norm": 1.163137373419493,
      "learning_rate": 9.024139827398271e-07,
      "loss": 0.5136,
      "step": 9639
    },
    {
      "epoch": 0.8679015958045421,
      "grad_norm": 1.5836693735531282,
      "learning_rate": 9.012037565193655e-07,
      "loss": 0.5812,
      "step": 9640
    },
    {
      "epoch": 0.8679916270904139,
      "grad_norm": 1.3574109480095582,
      "learning_rate": 8.99994304066183e-07,
      "loss": 0.4744,
      "step": 9641
    },
    {
      "epoch": 0.8680816583762858,
      "grad_norm": 1.0721903853931276,
      "learning_rate": 8.987856254831396e-07,
      "loss": 0.5372,
      "step": 9642
    },
    {
      "epoch": 0.8681716896621576,
      "grad_norm": 1.878644866535804,
      "learning_rate": 8.975777208730152e-07,
      "loss": 0.4706,
      "step": 9643
    },
    {
      "epoch": 0.8682617209480294,
      "grad_norm": 1.1188953180065646,
      "learning_rate": 8.963705903385344e-07,
      "loss": 0.4199,
      "step": 9644
    },
    {
      "epoch": 0.8683517522339013,
      "grad_norm": 1.596502899687253,
      "learning_rate": 8.951642339823508e-07,
      "loss": 0.5384,
      "step": 9645
    },
    {
      "epoch": 0.8684417835197731,
      "grad_norm": 1.3686885975451681,
      "learning_rate": 8.939586519070542e-07,
      "loss": 0.4774,
      "step": 9646
    },
    {
      "epoch": 0.868531814805645,
      "grad_norm": 1.8352012666272188,
      "learning_rate": 8.927538442151684e-07,
      "loss": 0.5387,
      "step": 9647
    },
    {
      "epoch": 0.8686218460915168,
      "grad_norm": 1.3019619450300262,
      "learning_rate": 8.915498110091536e-07,
      "loss": 0.5881,
      "step": 9648
    },
    {
      "epoch": 0.8687118773773886,
      "grad_norm": 1.346378216988473,
      "learning_rate": 8.903465523913957e-07,
      "loss": 0.5387,
      "step": 9649
    },
    {
      "epoch": 0.8688019086632605,
      "grad_norm": 1.7289059319036102,
      "learning_rate": 8.891440684642239e-07,
      "loss": 0.5779,
      "step": 9650
    },
    {
      "epoch": 0.8688919399491323,
      "grad_norm": 1.3729021982567147,
      "learning_rate": 8.879423593298974e-07,
      "loss": 0.518,
      "step": 9651
    },
    {
      "epoch": 0.8689819712350042,
      "grad_norm": 0.9928141155016508,
      "learning_rate": 8.867414250906115e-07,
      "loss": 0.4733,
      "step": 9652
    },
    {
      "epoch": 0.869072002520876,
      "grad_norm": 1.2044264009176826,
      "learning_rate": 8.85541265848493e-07,
      "loss": 0.4479,
      "step": 9653
    },
    {
      "epoch": 0.8691620338067478,
      "grad_norm": 1.3093606258793717,
      "learning_rate": 8.84341881705606e-07,
      "loss": 0.5172,
      "step": 9654
    },
    {
      "epoch": 0.8692520650926197,
      "grad_norm": 1.335280319343531,
      "learning_rate": 8.831432727639433e-07,
      "loss": 0.4795,
      "step": 9655
    },
    {
      "epoch": 0.8693420963784916,
      "grad_norm": 1.826865982801929,
      "learning_rate": 8.819454391254412e-07,
      "loss": 0.477,
      "step": 9656
    },
    {
      "epoch": 0.8694321276643634,
      "grad_norm": 1.2254359872386098,
      "learning_rate": 8.807483808919581e-07,
      "loss": 0.5484,
      "step": 9657
    },
    {
      "epoch": 0.8695221589502352,
      "grad_norm": 1.0962196785056983,
      "learning_rate": 8.79552098165296e-07,
      "loss": 0.5144,
      "step": 9658
    },
    {
      "epoch": 0.869612190236107,
      "grad_norm": 1.3176346602690694,
      "learning_rate": 8.783565910471859e-07,
      "loss": 0.519,
      "step": 9659
    },
    {
      "epoch": 0.8697022215219788,
      "grad_norm": 1.0617065787897872,
      "learning_rate": 8.771618596392961e-07,
      "loss": 0.6155,
      "step": 9660
    },
    {
      "epoch": 0.8697922528078508,
      "grad_norm": 1.6525678769565197,
      "learning_rate": 8.759679040432267e-07,
      "loss": 0.4887,
      "step": 9661
    },
    {
      "epoch": 0.8698822840937226,
      "grad_norm": 1.3425604904229813,
      "learning_rate": 8.747747243605132e-07,
      "loss": 0.5872,
      "step": 9662
    },
    {
      "epoch": 0.8699723153795944,
      "grad_norm": 1.7260733273947653,
      "learning_rate": 8.735823206926219e-07,
      "loss": 0.4831,
      "step": 9663
    },
    {
      "epoch": 0.8700623466654662,
      "grad_norm": 1.6771538184670918,
      "learning_rate": 8.723906931409576e-07,
      "loss": 0.4772,
      "step": 9664
    },
    {
      "epoch": 0.870152377951338,
      "grad_norm": 1.483348219776002,
      "learning_rate": 8.711998418068557e-07,
      "loss": 0.5716,
      "step": 9665
    },
    {
      "epoch": 0.87024240923721,
      "grad_norm": 1.9806782804848613,
      "learning_rate": 8.700097667915886e-07,
      "loss": 0.519,
      "step": 9666
    },
    {
      "epoch": 0.8703324405230818,
      "grad_norm": 1.5259032009670235,
      "learning_rate": 8.688204681963586e-07,
      "loss": 0.4497,
      "step": 9667
    },
    {
      "epoch": 0.8704224718089536,
      "grad_norm": 1.2059889279326144,
      "learning_rate": 8.676319461223092e-07,
      "loss": 0.5487,
      "step": 9668
    },
    {
      "epoch": 0.8705125030948254,
      "grad_norm": 1.58142135140513,
      "learning_rate": 8.664442006705054e-07,
      "loss": 0.5124,
      "step": 9669
    },
    {
      "epoch": 0.8706025343806973,
      "grad_norm": 1.383255698469698,
      "learning_rate": 8.652572319419617e-07,
      "loss": 0.5431,
      "step": 9670
    },
    {
      "epoch": 0.8706925656665692,
      "grad_norm": 1.1167324078926428,
      "learning_rate": 8.64071040037614e-07,
      "loss": 0.4898,
      "step": 9671
    },
    {
      "epoch": 0.870782596952441,
      "grad_norm": 1.672554283024613,
      "learning_rate": 8.628856250583373e-07,
      "loss": 0.5379,
      "step": 9672
    },
    {
      "epoch": 0.8708726282383128,
      "grad_norm": 1.5721793872642633,
      "learning_rate": 8.617009871049408e-07,
      "loss": 0.4961,
      "step": 9673
    },
    {
      "epoch": 0.8709626595241846,
      "grad_norm": 1.2243099600109029,
      "learning_rate": 8.605171262781676e-07,
      "loss": 0.5158,
      "step": 9674
    },
    {
      "epoch": 0.8710526908100565,
      "grad_norm": 1.4484331676424536,
      "learning_rate": 8.593340426786922e-07,
      "loss": 0.4553,
      "step": 9675
    },
    {
      "epoch": 0.8711427220959284,
      "grad_norm": 1.7877864898113738,
      "learning_rate": 8.581517364071267e-07,
      "loss": 0.5524,
      "step": 9676
    },
    {
      "epoch": 0.8712327533818002,
      "grad_norm": 1.0855013632497403,
      "learning_rate": 8.569702075640141e-07,
      "loss": 0.4875,
      "step": 9677
    },
    {
      "epoch": 0.871322784667672,
      "grad_norm": 1.2822077150822702,
      "learning_rate": 8.557894562498314e-07,
      "loss": 0.5885,
      "step": 9678
    },
    {
      "epoch": 0.8714128159535438,
      "grad_norm": 1.3404467517756198,
      "learning_rate": 8.546094825649909e-07,
      "loss": 0.4496,
      "step": 9679
    },
    {
      "epoch": 0.8715028472394157,
      "grad_norm": 1.4013641704633182,
      "learning_rate": 8.534302866098388e-07,
      "loss": 0.5826,
      "step": 9680
    },
    {
      "epoch": 0.8715928785252876,
      "grad_norm": 1.7498022044704618,
      "learning_rate": 8.522518684846559e-07,
      "loss": 0.517,
      "step": 9681
    },
    {
      "epoch": 0.8716829098111594,
      "grad_norm": 1.6700332119109325,
      "learning_rate": 8.510742282896545e-07,
      "loss": 0.4814,
      "step": 9682
    },
    {
      "epoch": 0.8717729410970312,
      "grad_norm": 1.475967198743083,
      "learning_rate": 8.498973661249787e-07,
      "loss": 0.4832,
      "step": 9683
    },
    {
      "epoch": 0.8718629723829031,
      "grad_norm": 1.5599979763369245,
      "learning_rate": 8.487212820907164e-07,
      "loss": 0.5784,
      "step": 9684
    },
    {
      "epoch": 0.8719530036687749,
      "grad_norm": 1.782214483226706,
      "learning_rate": 8.475459762868766e-07,
      "loss": 0.5859,
      "step": 9685
    },
    {
      "epoch": 0.8720430349546467,
      "grad_norm": 1.615845349130801,
      "learning_rate": 8.463714488134112e-07,
      "loss": 0.4224,
      "step": 9686
    },
    {
      "epoch": 0.8721330662405186,
      "grad_norm": 1.4192530458060044,
      "learning_rate": 8.451976997702005e-07,
      "loss": 0.5431,
      "step": 9687
    },
    {
      "epoch": 0.8722230975263904,
      "grad_norm": 2.817051397049948,
      "learning_rate": 8.440247292570624e-07,
      "loss": 0.4928,
      "step": 9688
    },
    {
      "epoch": 0.8723131288122623,
      "grad_norm": 1.3639767539419037,
      "learning_rate": 8.428525373737462e-07,
      "loss": 0.5696,
      "step": 9689
    },
    {
      "epoch": 0.8724031600981341,
      "grad_norm": 1.3619025669858351,
      "learning_rate": 8.416811242199385e-07,
      "loss": 0.6216,
      "step": 9690
    },
    {
      "epoch": 0.8724931913840059,
      "grad_norm": 1.2986089073826934,
      "learning_rate": 8.405104898952532e-07,
      "loss": 0.5355,
      "step": 9691
    },
    {
      "epoch": 0.8725832226698778,
      "grad_norm": 2.1163307525093353,
      "learning_rate": 8.393406344992428e-07,
      "loss": 0.4799,
      "step": 9692
    },
    {
      "epoch": 0.8726732539557496,
      "grad_norm": 1.6487997924095559,
      "learning_rate": 8.381715581313932e-07,
      "loss": 0.4599,
      "step": 9693
    },
    {
      "epoch": 0.8727632852416215,
      "grad_norm": 1.4264049821992402,
      "learning_rate": 8.370032608911227e-07,
      "loss": 0.6373,
      "step": 9694
    },
    {
      "epoch": 0.8728533165274933,
      "grad_norm": 1.0970943478789537,
      "learning_rate": 8.358357428777853e-07,
      "loss": 0.5848,
      "step": 9695
    },
    {
      "epoch": 0.8729433478133651,
      "grad_norm": 1.1898891277191588,
      "learning_rate": 8.346690041906691e-07,
      "loss": 0.4597,
      "step": 9696
    },
    {
      "epoch": 0.873033379099237,
      "grad_norm": 1.2934147313312647,
      "learning_rate": 8.335030449289872e-07,
      "loss": 0.5267,
      "step": 9697
    },
    {
      "epoch": 0.8731234103851089,
      "grad_norm": 1.4843538976293966,
      "learning_rate": 8.323378651919023e-07,
      "loss": 0.5118,
      "step": 9698
    },
    {
      "epoch": 0.8732134416709807,
      "grad_norm": 1.9021063336789783,
      "learning_rate": 8.311734650784964e-07,
      "loss": 0.5014,
      "step": 9699
    },
    {
      "epoch": 0.8733034729568525,
      "grad_norm": 1.0541560497335898,
      "learning_rate": 8.300098446877925e-07,
      "loss": 0.4549,
      "step": 9700
    },
    {
      "epoch": 0.8733935042427243,
      "grad_norm": 1.2297164708021107,
      "learning_rate": 8.288470041187446e-07,
      "loss": 0.5411,
      "step": 9701
    },
    {
      "epoch": 0.8734835355285961,
      "grad_norm": 1.3605439010501228,
      "learning_rate": 8.276849434702428e-07,
      "loss": 0.5226,
      "step": 9702
    },
    {
      "epoch": 0.8735735668144681,
      "grad_norm": 1.0849472323554439,
      "learning_rate": 8.265236628411088e-07,
      "loss": 0.448,
      "step": 9703
    },
    {
      "epoch": 0.8736635981003399,
      "grad_norm": 1.1972775040592618,
      "learning_rate": 8.253631623301007e-07,
      "loss": 0.5297,
      "step": 9704
    },
    {
      "epoch": 0.8737536293862117,
      "grad_norm": 1.471298806358601,
      "learning_rate": 8.242034420359035e-07,
      "loss": 0.4924,
      "step": 9705
    },
    {
      "epoch": 0.8738436606720835,
      "grad_norm": 1.3720281821872493,
      "learning_rate": 8.230445020571443e-07,
      "loss": 0.5449,
      "step": 9706
    },
    {
      "epoch": 0.8739336919579553,
      "grad_norm": 1.152072468442183,
      "learning_rate": 8.218863424923784e-07,
      "loss": 0.5095,
      "step": 9707
    },
    {
      "epoch": 0.8740237232438273,
      "grad_norm": 2.000878504740575,
      "learning_rate": 8.207289634400972e-07,
      "loss": 0.4699,
      "step": 9708
    },
    {
      "epoch": 0.8741137545296991,
      "grad_norm": 1.6114256903884039,
      "learning_rate": 8.195723649987253e-07,
      "loss": 0.5005,
      "step": 9709
    },
    {
      "epoch": 0.8742037858155709,
      "grad_norm": 1.5995222808546925,
      "learning_rate": 8.184165472666205e-07,
      "loss": 0.5027,
      "step": 9710
    },
    {
      "epoch": 0.8742938171014427,
      "grad_norm": 1.3857321400484808,
      "learning_rate": 8.172615103420711e-07,
      "loss": 0.5131,
      "step": 9711
    },
    {
      "epoch": 0.8743838483873146,
      "grad_norm": 1.252401078840919,
      "learning_rate": 8.161072543233084e-07,
      "loss": 0.4399,
      "step": 9712
    },
    {
      "epoch": 0.8744738796731865,
      "grad_norm": 1.2323960929112527,
      "learning_rate": 8.149537793084849e-07,
      "loss": 0.4456,
      "step": 9713
    },
    {
      "epoch": 0.8745639109590583,
      "grad_norm": 2.1148645457942776,
      "learning_rate": 8.138010853956968e-07,
      "loss": 0.5578,
      "step": 9714
    },
    {
      "epoch": 0.8746539422449301,
      "grad_norm": 1.947247104965628,
      "learning_rate": 8.126491726829666e-07,
      "loss": 0.4916,
      "step": 9715
    },
    {
      "epoch": 0.8747439735308019,
      "grad_norm": 1.7747317911058287,
      "learning_rate": 8.114980412682571e-07,
      "loss": 0.4868,
      "step": 9716
    },
    {
      "epoch": 0.8748340048166738,
      "grad_norm": 2.6168821442717998,
      "learning_rate": 8.10347691249459e-07,
      "loss": 0.4378,
      "step": 9717
    },
    {
      "epoch": 0.8749240361025457,
      "grad_norm": 1.7505705514832008,
      "learning_rate": 8.091981227244017e-07,
      "loss": 0.4479,
      "step": 9718
    },
    {
      "epoch": 0.8750140673884175,
      "grad_norm": 1.8348818774680342,
      "learning_rate": 8.080493357908403e-07,
      "loss": 0.5306,
      "step": 9719
    },
    {
      "epoch": 0.8751040986742893,
      "grad_norm": 1.6889122493192874,
      "learning_rate": 8.069013305464723e-07,
      "loss": 0.4486,
      "step": 9720
    },
    {
      "epoch": 0.8751941299601611,
      "grad_norm": 1.713006517587658,
      "learning_rate": 8.057541070889229e-07,
      "loss": 0.5008,
      "step": 9721
    },
    {
      "epoch": 0.875284161246033,
      "grad_norm": 1.3367634292669064,
      "learning_rate": 8.046076655157531e-07,
      "loss": 0.4728,
      "step": 9722
    },
    {
      "epoch": 0.8753741925319048,
      "grad_norm": 1.728465245898103,
      "learning_rate": 8.03462005924458e-07,
      "loss": 0.5149,
      "step": 9723
    },
    {
      "epoch": 0.8754642238177767,
      "grad_norm": 1.7604968438212498,
      "learning_rate": 8.023171284124654e-07,
      "loss": 0.425,
      "step": 9724
    },
    {
      "epoch": 0.8755542551036485,
      "grad_norm": 1.1746242301779253,
      "learning_rate": 8.011730330771317e-07,
      "loss": 0.5251,
      "step": 9725
    },
    {
      "epoch": 0.8756442863895204,
      "grad_norm": 1.1704294696118718,
      "learning_rate": 8.000297200157591e-07,
      "loss": 0.5835,
      "step": 9726
    },
    {
      "epoch": 0.8757343176753922,
      "grad_norm": 1.4417939442272714,
      "learning_rate": 7.988871893255712e-07,
      "loss": 0.4923,
      "step": 9727
    },
    {
      "epoch": 0.875824348961264,
      "grad_norm": 1.5668717327997304,
      "learning_rate": 7.977454411037289e-07,
      "loss": 0.4437,
      "step": 9728
    },
    {
      "epoch": 0.8759143802471359,
      "grad_norm": 2.469120635839503,
      "learning_rate": 7.966044754473279e-07,
      "loss": 0.5119,
      "step": 9729
    },
    {
      "epoch": 0.8760044115330077,
      "grad_norm": 1.6292226990744296,
      "learning_rate": 7.954642924533995e-07,
      "loss": 0.5507,
      "step": 9730
    },
    {
      "epoch": 0.8760944428188796,
      "grad_norm": 1.5687703981760286,
      "learning_rate": 7.943248922188995e-07,
      "loss": 0.5621,
      "step": 9731
    },
    {
      "epoch": 0.8761844741047514,
      "grad_norm": 1.8881387552652127,
      "learning_rate": 7.931862748407304e-07,
      "loss": 0.4692,
      "step": 9732
    },
    {
      "epoch": 0.8762745053906232,
      "grad_norm": 1.5498871884421659,
      "learning_rate": 7.920484404157158e-07,
      "loss": 0.513,
      "step": 9733
    },
    {
      "epoch": 0.876364536676495,
      "grad_norm": 1.0802379727075853,
      "learning_rate": 7.909113890406195e-07,
      "loss": 0.5188,
      "step": 9734
    },
    {
      "epoch": 0.8764545679623669,
      "grad_norm": 1.5178749972172347,
      "learning_rate": 7.897751208121363e-07,
      "loss": 0.5329,
      "step": 9735
    },
    {
      "epoch": 0.8765445992482388,
      "grad_norm": 1.6098342727440296,
      "learning_rate": 7.886396358268966e-07,
      "loss": 0.3982,
      "step": 9736
    },
    {
      "epoch": 0.8766346305341106,
      "grad_norm": 1.7072191510073993,
      "learning_rate": 7.875049341814611e-07,
      "loss": 0.4795,
      "step": 9737
    },
    {
      "epoch": 0.8767246618199824,
      "grad_norm": 1.4498115261304076,
      "learning_rate": 7.863710159723292e-07,
      "loss": 0.4521,
      "step": 9738
    },
    {
      "epoch": 0.8768146931058542,
      "grad_norm": 1.957880612368475,
      "learning_rate": 7.852378812959227e-07,
      "loss": 0.465,
      "step": 9739
    },
    {
      "epoch": 0.8769047243917262,
      "grad_norm": 1.3579207734653433,
      "learning_rate": 7.841055302486134e-07,
      "loss": 0.5667,
      "step": 9740
    },
    {
      "epoch": 0.876994755677598,
      "grad_norm": 1.4200052433196575,
      "learning_rate": 7.829739629266897e-07,
      "loss": 0.5172,
      "step": 9741
    },
    {
      "epoch": 0.8770847869634698,
      "grad_norm": 1.4251457436973942,
      "learning_rate": 7.818431794263837e-07,
      "loss": 0.5099,
      "step": 9742
    },
    {
      "epoch": 0.8771748182493416,
      "grad_norm": 1.2770858701285204,
      "learning_rate": 7.807131798438572e-07,
      "loss": 0.4973,
      "step": 9743
    },
    {
      "epoch": 0.8772648495352134,
      "grad_norm": 2.7162180152944035,
      "learning_rate": 7.79583964275209e-07,
      "loss": 0.4159,
      "step": 9744
    },
    {
      "epoch": 0.8773548808210854,
      "grad_norm": 1.3893133328524017,
      "learning_rate": 7.784555328164622e-07,
      "loss": 0.4588,
      "step": 9745
    },
    {
      "epoch": 0.8774449121069572,
      "grad_norm": 1.1376500832639391,
      "learning_rate": 7.773278855635857e-07,
      "loss": 0.549,
      "step": 9746
    },
    {
      "epoch": 0.877534943392829,
      "grad_norm": 1.3107132389256193,
      "learning_rate": 7.762010226124716e-07,
      "loss": 0.4972,
      "step": 9747
    },
    {
      "epoch": 0.8776249746787008,
      "grad_norm": 1.4075766323802696,
      "learning_rate": 7.750749440589489e-07,
      "loss": 0.5383,
      "step": 9748
    },
    {
      "epoch": 0.8777150059645726,
      "grad_norm": 2.1796082604399984,
      "learning_rate": 7.73949649998782e-07,
      "loss": 0.5065,
      "step": 9749
    },
    {
      "epoch": 0.8778050372504446,
      "grad_norm": 1.1824247760767834,
      "learning_rate": 7.728251405276666e-07,
      "loss": 0.4331,
      "step": 9750
    },
    {
      "epoch": 0.8778950685363164,
      "grad_norm": 1.270492190031137,
      "learning_rate": 7.717014157412294e-07,
      "loss": 0.601,
      "step": 9751
    },
    {
      "epoch": 0.8779850998221882,
      "grad_norm": 1.3888126501373004,
      "learning_rate": 7.705784757350354e-07,
      "loss": 0.5174,
      "step": 9752
    },
    {
      "epoch": 0.87807513110806,
      "grad_norm": 1.4806083044742826,
      "learning_rate": 7.694563206045769e-07,
      "loss": 0.4543,
      "step": 9753
    },
    {
      "epoch": 0.8781651623939319,
      "grad_norm": 1.159065067436556,
      "learning_rate": 7.683349504452863e-07,
      "loss": 0.4778,
      "step": 9754
    },
    {
      "epoch": 0.8782551936798038,
      "grad_norm": 1.4416095801854734,
      "learning_rate": 7.672143653525232e-07,
      "loss": 0.4015,
      "step": 9755
    },
    {
      "epoch": 0.8783452249656756,
      "grad_norm": 1.3664981912085712,
      "learning_rate": 7.660945654215835e-07,
      "loss": 0.5662,
      "step": 9756
    },
    {
      "epoch": 0.8784352562515474,
      "grad_norm": 1.885756306046582,
      "learning_rate": 7.649755507476953e-07,
      "loss": 0.5124,
      "step": 9757
    },
    {
      "epoch": 0.8785252875374192,
      "grad_norm": 1.154903555739633,
      "learning_rate": 7.638573214260225e-07,
      "loss": 0.5526,
      "step": 9758
    },
    {
      "epoch": 0.8786153188232911,
      "grad_norm": 1.3733369746253157,
      "learning_rate": 7.627398775516548e-07,
      "loss": 0.5114,
      "step": 9759
    },
    {
      "epoch": 0.878705350109163,
      "grad_norm": 1.4912944730410889,
      "learning_rate": 7.616232192196272e-07,
      "loss": 0.485,
      "step": 9760
    },
    {
      "epoch": 0.8787953813950348,
      "grad_norm": 1.1024491964413106,
      "learning_rate": 7.605073465248958e-07,
      "loss": 0.5009,
      "step": 9761
    },
    {
      "epoch": 0.8788854126809066,
      "grad_norm": 1.6756357256796581,
      "learning_rate": 7.593922595623571e-07,
      "loss": 0.5533,
      "step": 9762
    },
    {
      "epoch": 0.8789754439667784,
      "grad_norm": 1.6580418126980343,
      "learning_rate": 7.582779584268374e-07,
      "loss": 0.4654,
      "step": 9763
    },
    {
      "epoch": 0.8790654752526503,
      "grad_norm": 1.5024052730614277,
      "learning_rate": 7.571644432130998e-07,
      "loss": 0.4841,
      "step": 9764
    },
    {
      "epoch": 0.8791555065385221,
      "grad_norm": 1.1727618353547886,
      "learning_rate": 7.560517140158374e-07,
      "loss": 0.5443,
      "step": 9765
    },
    {
      "epoch": 0.879245537824394,
      "grad_norm": 1.408503080967842,
      "learning_rate": 7.54939770929678e-07,
      "loss": 0.4304,
      "step": 9766
    },
    {
      "epoch": 0.8793355691102658,
      "grad_norm": 1.5667801473032978,
      "learning_rate": 7.538286140491779e-07,
      "loss": 0.4752,
      "step": 9767
    },
    {
      "epoch": 0.8794256003961377,
      "grad_norm": 1.254582043192072,
      "learning_rate": 7.527182434688374e-07,
      "loss": 0.442,
      "step": 9768
    },
    {
      "epoch": 0.8795156316820095,
      "grad_norm": 1.4691620153553884,
      "learning_rate": 7.516086592830785e-07,
      "loss": 0.4629,
      "step": 9769
    },
    {
      "epoch": 0.8796056629678813,
      "grad_norm": 1.88896519429418,
      "learning_rate": 7.504998615862613e-07,
      "loss": 0.4235,
      "step": 9770
    },
    {
      "epoch": 0.8796956942537532,
      "grad_norm": 1.5665233576199644,
      "learning_rate": 7.493918504726794e-07,
      "loss": 0.5196,
      "step": 9771
    },
    {
      "epoch": 0.879785725539625,
      "grad_norm": 1.4020784904901116,
      "learning_rate": 7.482846260365595e-07,
      "loss": 0.5586,
      "step": 9772
    },
    {
      "epoch": 0.8798757568254969,
      "grad_norm": 1.402294284702261,
      "learning_rate": 7.471781883720575e-07,
      "loss": 0.519,
      "step": 9773
    },
    {
      "epoch": 0.8799657881113687,
      "grad_norm": 1.4759349885889048,
      "learning_rate": 7.460725375732703e-07,
      "loss": 0.5431,
      "step": 9774
    },
    {
      "epoch": 0.8800558193972405,
      "grad_norm": 1.2008625101455077,
      "learning_rate": 7.449676737342193e-07,
      "loss": 0.5464,
      "step": 9775
    },
    {
      "epoch": 0.8801458506831124,
      "grad_norm": 1.3082191268551304,
      "learning_rate": 7.43863596948865e-07,
      "loss": 0.5394,
      "step": 9776
    },
    {
      "epoch": 0.8802358819689842,
      "grad_norm": 1.2582477110532682,
      "learning_rate": 7.427603073110967e-07,
      "loss": 0.4847,
      "step": 9777
    },
    {
      "epoch": 0.8803259132548561,
      "grad_norm": 1.268347855156659,
      "learning_rate": 7.416578049147405e-07,
      "loss": 0.5331,
      "step": 9778
    },
    {
      "epoch": 0.8804159445407279,
      "grad_norm": 1.8409717041909686,
      "learning_rate": 7.405560898535535e-07,
      "loss": 0.5745,
      "step": 9779
    },
    {
      "epoch": 0.8805059758265997,
      "grad_norm": 1.727280147526373,
      "learning_rate": 7.394551622212288e-07,
      "loss": 0.5217,
      "step": 9780
    },
    {
      "epoch": 0.8805960071124715,
      "grad_norm": 1.358991619025799,
      "learning_rate": 7.383550221113822e-07,
      "loss": 0.4943,
      "step": 9781
    },
    {
      "epoch": 0.8806860383983435,
      "grad_norm": 1.119492678643226,
      "learning_rate": 7.372556696175803e-07,
      "loss": 0.422,
      "step": 9782
    },
    {
      "epoch": 0.8807760696842153,
      "grad_norm": 1.484301835828632,
      "learning_rate": 7.36157104833305e-07,
      "loss": 0.4916,
      "step": 9783
    },
    {
      "epoch": 0.8808661009700871,
      "grad_norm": 1.4209091576519222,
      "learning_rate": 7.350593278519824e-07,
      "loss": 0.4803,
      "step": 9784
    },
    {
      "epoch": 0.8809561322559589,
      "grad_norm": 1.3854405179242593,
      "learning_rate": 7.339623387669681e-07,
      "loss": 0.5249,
      "step": 9785
    },
    {
      "epoch": 0.8810461635418307,
      "grad_norm": 1.6502984034572203,
      "learning_rate": 7.328661376715517e-07,
      "loss": 0.5845,
      "step": 9786
    },
    {
      "epoch": 0.8811361948277027,
      "grad_norm": 2.1537223233321736,
      "learning_rate": 7.31770724658949e-07,
      "loss": 0.5181,
      "step": 9787
    },
    {
      "epoch": 0.8812262261135745,
      "grad_norm": 1.3651269691955064,
      "learning_rate": 7.306760998223228e-07,
      "loss": 0.4639,
      "step": 9788
    },
    {
      "epoch": 0.8813162573994463,
      "grad_norm": 1.1401227661490059,
      "learning_rate": 7.295822632547555e-07,
      "loss": 0.4808,
      "step": 9789
    },
    {
      "epoch": 0.8814062886853181,
      "grad_norm": 1.5978497012478932,
      "learning_rate": 7.284892150492684e-07,
      "loss": 0.5417,
      "step": 9790
    },
    {
      "epoch": 0.8814963199711899,
      "grad_norm": 1.4535066041822506,
      "learning_rate": 7.273969552988158e-07,
      "loss": 0.5092,
      "step": 9791
    },
    {
      "epoch": 0.8815863512570619,
      "grad_norm": 1.740403558724562,
      "learning_rate": 7.263054840962847e-07,
      "loss": 0.4402,
      "step": 9792
    },
    {
      "epoch": 0.8816763825429337,
      "grad_norm": 1.6488492529347882,
      "learning_rate": 7.252148015344929e-07,
      "loss": 0.5038,
      "step": 9793
    },
    {
      "epoch": 0.8817664138288055,
      "grad_norm": 1.513498061568066,
      "learning_rate": 7.24124907706194e-07,
      "loss": 0.5192,
      "step": 9794
    },
    {
      "epoch": 0.8818564451146773,
      "grad_norm": 1.325189712265026,
      "learning_rate": 7.230358027040707e-07,
      "loss": 0.5145,
      "step": 9795
    },
    {
      "epoch": 0.8819464764005492,
      "grad_norm": 1.3130906753106673,
      "learning_rate": 7.219474866207465e-07,
      "loss": 0.4976,
      "step": 9796
    },
    {
      "epoch": 0.8820365076864211,
      "grad_norm": 1.0009759008736867,
      "learning_rate": 7.208599595487675e-07,
      "loss": 0.4857,
      "step": 9797
    },
    {
      "epoch": 0.8821265389722929,
      "grad_norm": 2.621654172904053,
      "learning_rate": 7.197732215806186e-07,
      "loss": 0.4575,
      "step": 9798
    },
    {
      "epoch": 0.8822165702581647,
      "grad_norm": 1.512191557539455,
      "learning_rate": 7.186872728087179e-07,
      "loss": 0.4729,
      "step": 9799
    },
    {
      "epoch": 0.8823066015440365,
      "grad_norm": 2.2671044508205465,
      "learning_rate": 7.176021133254162e-07,
      "loss": 0.5452,
      "step": 9800
    },
    {
      "epoch": 0.8823966328299084,
      "grad_norm": 1.1191682599804755,
      "learning_rate": 7.165177432229919e-07,
      "loss": 0.5647,
      "step": 9801
    },
    {
      "epoch": 0.8824866641157803,
      "grad_norm": 1.136884460993508,
      "learning_rate": 7.154341625936656e-07,
      "loss": 0.5271,
      "step": 9802
    },
    {
      "epoch": 0.8825766954016521,
      "grad_norm": 1.2920524798181223,
      "learning_rate": 7.143513715295825e-07,
      "loss": 0.5066,
      "step": 9803
    },
    {
      "epoch": 0.8826667266875239,
      "grad_norm": 1.780400851192309,
      "learning_rate": 7.132693701228244e-07,
      "loss": 0.5793,
      "step": 9804
    },
    {
      "epoch": 0.8827567579733957,
      "grad_norm": 1.3054491147598797,
      "learning_rate": 7.121881584654056e-07,
      "loss": 0.4886,
      "step": 9805
    },
    {
      "epoch": 0.8828467892592676,
      "grad_norm": 1.2851069501704717,
      "learning_rate": 7.111077366492724e-07,
      "loss": 0.4759,
      "step": 9806
    },
    {
      "epoch": 0.8829368205451394,
      "grad_norm": 1.3535218107524596,
      "learning_rate": 7.10028104766306e-07,
      "loss": 0.5042,
      "step": 9807
    },
    {
      "epoch": 0.8830268518310113,
      "grad_norm": 1.236353085111122,
      "learning_rate": 7.089492629083195e-07,
      "loss": 0.5773,
      "step": 9808
    },
    {
      "epoch": 0.8831168831168831,
      "grad_norm": 1.249570475180143,
      "learning_rate": 7.078712111670527e-07,
      "loss": 0.5653,
      "step": 9809
    },
    {
      "epoch": 0.883206914402755,
      "grad_norm": 1.4057171268535784,
      "learning_rate": 7.067939496341924e-07,
      "loss": 0.5052,
      "step": 9810
    },
    {
      "epoch": 0.8832969456886268,
      "grad_norm": 1.5263661036903977,
      "learning_rate": 7.057174784013432e-07,
      "loss": 0.4965,
      "step": 9811
    },
    {
      "epoch": 0.8833869769744986,
      "grad_norm": 1.5296694272573288,
      "learning_rate": 7.046417975600506e-07,
      "loss": 0.577,
      "step": 9812
    },
    {
      "epoch": 0.8834770082603705,
      "grad_norm": 1.330036217033937,
      "learning_rate": 7.035669072017915e-07,
      "loss": 0.4069,
      "step": 9813
    },
    {
      "epoch": 0.8835670395462423,
      "grad_norm": 2.3570880018654665,
      "learning_rate": 7.024928074179771e-07,
      "loss": 0.5786,
      "step": 9814
    },
    {
      "epoch": 0.8836570708321142,
      "grad_norm": 1.50581127535147,
      "learning_rate": 7.014194982999433e-07,
      "loss": 0.4864,
      "step": 9815
    },
    {
      "epoch": 0.883747102117986,
      "grad_norm": 2.695274207501404,
      "learning_rate": 7.00346979938974e-07,
      "loss": 0.4704,
      "step": 9816
    },
    {
      "epoch": 0.8838371334038578,
      "grad_norm": 1.4869711847323412,
      "learning_rate": 6.992752524262692e-07,
      "loss": 0.4717,
      "step": 9817
    },
    {
      "epoch": 0.8839271646897296,
      "grad_norm": 1.5561554712233283,
      "learning_rate": 6.982043158529728e-07,
      "loss": 0.4971,
      "step": 9818
    },
    {
      "epoch": 0.8840171959756015,
      "grad_norm": 1.419831887633267,
      "learning_rate": 6.971341703101564e-07,
      "loss": 0.493,
      "step": 9819
    },
    {
      "epoch": 0.8841072272614734,
      "grad_norm": 0.894343414880215,
      "learning_rate": 6.960648158888284e-07,
      "loss": 0.4668,
      "step": 9820
    },
    {
      "epoch": 0.8841972585473452,
      "grad_norm": 1.43304593754211,
      "learning_rate": 6.949962526799237e-07,
      "loss": 0.4865,
      "step": 9821
    },
    {
      "epoch": 0.884287289833217,
      "grad_norm": 1.0456120440240366,
      "learning_rate": 6.939284807743185e-07,
      "loss": 0.4369,
      "step": 9822
    },
    {
      "epoch": 0.8843773211190888,
      "grad_norm": 1.5380558533413304,
      "learning_rate": 6.928615002628102e-07,
      "loss": 0.4836,
      "step": 9823
    },
    {
      "epoch": 0.8844673524049608,
      "grad_norm": 1.6048982575320292,
      "learning_rate": 6.91795311236142e-07,
      "loss": 0.5271,
      "step": 9824
    },
    {
      "epoch": 0.8845573836908326,
      "grad_norm": 1.0340258277989047,
      "learning_rate": 6.907299137849798e-07,
      "loss": 0.5084,
      "step": 9825
    },
    {
      "epoch": 0.8846474149767044,
      "grad_norm": 1.8060298628160398,
      "learning_rate": 6.896653079999249e-07,
      "loss": 0.5621,
      "step": 9826
    },
    {
      "epoch": 0.8847374462625762,
      "grad_norm": 1.1272963298330587,
      "learning_rate": 6.886014939715147e-07,
      "loss": 0.4191,
      "step": 9827
    },
    {
      "epoch": 0.884827477548448,
      "grad_norm": 1.0928436452149894,
      "learning_rate": 6.875384717902156e-07,
      "loss": 0.5835,
      "step": 9828
    },
    {
      "epoch": 0.88491750883432,
      "grad_norm": 1.0788281252885057,
      "learning_rate": 6.864762415464254e-07,
      "loss": 0.4643,
      "step": 9829
    },
    {
      "epoch": 0.8850075401201918,
      "grad_norm": 1.9433979934989827,
      "learning_rate": 6.854148033304809e-07,
      "loss": 0.5223,
      "step": 9830
    },
    {
      "epoch": 0.8850975714060636,
      "grad_norm": 1.5396374107516977,
      "learning_rate": 6.84354157232644e-07,
      "loss": 0.4194,
      "step": 9831
    },
    {
      "epoch": 0.8851876026919354,
      "grad_norm": 2.059483445843285,
      "learning_rate": 6.83294303343115e-07,
      "loss": 0.6178,
      "step": 9832
    },
    {
      "epoch": 0.8852776339778073,
      "grad_norm": 1.999809987751749,
      "learning_rate": 6.822352417520228e-07,
      "loss": 0.4702,
      "step": 9833
    },
    {
      "epoch": 0.8853676652636792,
      "grad_norm": 1.2878118034910802,
      "learning_rate": 6.81176972549431e-07,
      "loss": 0.497,
      "step": 9834
    },
    {
      "epoch": 0.885457696549551,
      "grad_norm": 1.2100680722021337,
      "learning_rate": 6.801194958253366e-07,
      "loss": 0.5731,
      "step": 9835
    },
    {
      "epoch": 0.8855477278354228,
      "grad_norm": 1.8573613701920182,
      "learning_rate": 6.790628116696685e-07,
      "loss": 0.5143,
      "step": 9836
    },
    {
      "epoch": 0.8856377591212946,
      "grad_norm": 1.5173287792454115,
      "learning_rate": 6.780069201722839e-07,
      "loss": 0.5422,
      "step": 9837
    },
    {
      "epoch": 0.8857277904071665,
      "grad_norm": 2.208508217957738,
      "learning_rate": 6.76951821422982e-07,
      "loss": 0.5427,
      "step": 9838
    },
    {
      "epoch": 0.8858178216930384,
      "grad_norm": 1.1070844938011541,
      "learning_rate": 6.758975155114856e-07,
      "loss": 0.5194,
      "step": 9839
    },
    {
      "epoch": 0.8859078529789102,
      "grad_norm": 1.0193390281097823,
      "learning_rate": 6.748440025274527e-07,
      "loss": 0.5279,
      "step": 9840
    },
    {
      "epoch": 0.885997884264782,
      "grad_norm": 2.752640368858462,
      "learning_rate": 6.737912825604775e-07,
      "loss": 0.4648,
      "step": 9841
    },
    {
      "epoch": 0.8860879155506538,
      "grad_norm": 1.581080262214097,
      "learning_rate": 6.727393557000838e-07,
      "loss": 0.5643,
      "step": 9842
    },
    {
      "epoch": 0.8861779468365257,
      "grad_norm": 1.1296699228523186,
      "learning_rate": 6.716882220357235e-07,
      "loss": 0.4887,
      "step": 9843
    },
    {
      "epoch": 0.8862679781223975,
      "grad_norm": 1.945883753753311,
      "learning_rate": 6.706378816567927e-07,
      "loss": 0.5054,
      "step": 9844
    },
    {
      "epoch": 0.8863580094082694,
      "grad_norm": 1.7973247147041544,
      "learning_rate": 6.695883346526078e-07,
      "loss": 0.4946,
      "step": 9845
    },
    {
      "epoch": 0.8864480406941412,
      "grad_norm": 1.5938563831437051,
      "learning_rate": 6.68539581112424e-07,
      "loss": 0.4728,
      "step": 9846
    },
    {
      "epoch": 0.8865380719800131,
      "grad_norm": 1.2737974944463366,
      "learning_rate": 6.67491621125429e-07,
      "loss": 0.5518,
      "step": 9847
    },
    {
      "epoch": 0.8866281032658849,
      "grad_norm": 1.2505190866968383,
      "learning_rate": 6.664444547807414e-07,
      "loss": 0.5189,
      "step": 9848
    },
    {
      "epoch": 0.8867181345517567,
      "grad_norm": 1.3188456376128088,
      "learning_rate": 6.653980821674134e-07,
      "loss": 0.5046,
      "step": 9849
    },
    {
      "epoch": 0.8868081658376286,
      "grad_norm": 1.3265396644312246,
      "learning_rate": 6.643525033744303e-07,
      "loss": 0.4526,
      "step": 9850
    },
    {
      "epoch": 0.8868981971235004,
      "grad_norm": 2.5524618675830295,
      "learning_rate": 6.633077184907033e-07,
      "loss": 0.5694,
      "step": 9851
    },
    {
      "epoch": 0.8869882284093723,
      "grad_norm": 1.4132869585158285,
      "learning_rate": 6.622637276050891e-07,
      "loss": 0.4993,
      "step": 9852
    },
    {
      "epoch": 0.8870782596952441,
      "grad_norm": 1.6395181981666338,
      "learning_rate": 6.612205308063646e-07,
      "loss": 0.4857,
      "step": 9853
    },
    {
      "epoch": 0.8871682909811159,
      "grad_norm": 1.3321925284169265,
      "learning_rate": 6.601781281832443e-07,
      "loss": 0.5057,
      "step": 9854
    },
    {
      "epoch": 0.8872583222669878,
      "grad_norm": 1.1727256070526624,
      "learning_rate": 6.59136519824376e-07,
      "loss": 0.4334,
      "step": 9855
    },
    {
      "epoch": 0.8873483535528596,
      "grad_norm": 1.3240555582512776,
      "learning_rate": 6.58095705818339e-07,
      "loss": 0.5555,
      "step": 9856
    },
    {
      "epoch": 0.8874383848387315,
      "grad_norm": 1.4699420870237765,
      "learning_rate": 6.570556862536415e-07,
      "loss": 0.4698,
      "step": 9857
    },
    {
      "epoch": 0.8875284161246033,
      "grad_norm": 1.217624387944136,
      "learning_rate": 6.560164612187325e-07,
      "loss": 0.5326,
      "step": 9858
    },
    {
      "epoch": 0.8876184474104751,
      "grad_norm": 1.2880197871857295,
      "learning_rate": 6.549780308019837e-07,
      "loss": 0.5019,
      "step": 9859
    },
    {
      "epoch": 0.887708478696347,
      "grad_norm": 1.2168382740746844,
      "learning_rate": 6.539403950917067e-07,
      "loss": 0.5249,
      "step": 9860
    },
    {
      "epoch": 0.8877985099822189,
      "grad_norm": 1.488838134537303,
      "learning_rate": 6.529035541761419e-07,
      "loss": 0.5704,
      "step": 9861
    },
    {
      "epoch": 0.8878885412680907,
      "grad_norm": 1.2480247197848715,
      "learning_rate": 6.518675081434622e-07,
      "loss": 0.3844,
      "step": 9862
    },
    {
      "epoch": 0.8879785725539625,
      "grad_norm": 1.5241603541681126,
      "learning_rate": 6.508322570817749e-07,
      "loss": 0.4986,
      "step": 9863
    },
    {
      "epoch": 0.8880686038398343,
      "grad_norm": 1.071567668409949,
      "learning_rate": 6.497978010791195e-07,
      "loss": 0.5353,
      "step": 9864
    },
    {
      "epoch": 0.8881586351257061,
      "grad_norm": 1.4774044513327729,
      "learning_rate": 6.487641402234612e-07,
      "loss": 0.4302,
      "step": 9865
    },
    {
      "epoch": 0.8882486664115781,
      "grad_norm": 1.6151684124648755,
      "learning_rate": 6.477312746027109e-07,
      "loss": 0.4926,
      "step": 9866
    },
    {
      "epoch": 0.8883386976974499,
      "grad_norm": 1.2431333639964877,
      "learning_rate": 6.466992043046983e-07,
      "loss": 0.5143,
      "step": 9867
    },
    {
      "epoch": 0.8884287289833217,
      "grad_norm": 1.6998431459254186,
      "learning_rate": 6.45667929417193e-07,
      "loss": 0.503,
      "step": 9868
    },
    {
      "epoch": 0.8885187602691935,
      "grad_norm": 1.3892012242278817,
      "learning_rate": 6.446374500278952e-07,
      "loss": 0.5372,
      "step": 9869
    },
    {
      "epoch": 0.8886087915550653,
      "grad_norm": 1.5652180454704459,
      "learning_rate": 6.4360776622444e-07,
      "loss": 0.419,
      "step": 9870
    },
    {
      "epoch": 0.8886988228409373,
      "grad_norm": 0.995568272403623,
      "learning_rate": 6.425788780943865e-07,
      "loss": 0.5003,
      "step": 9871
    },
    {
      "epoch": 0.8887888541268091,
      "grad_norm": 1.4476235648257565,
      "learning_rate": 6.415507857252389e-07,
      "loss": 0.4966,
      "step": 9872
    },
    {
      "epoch": 0.8888788854126809,
      "grad_norm": 2.1199098271729313,
      "learning_rate": 6.405234892044221e-07,
      "loss": 0.4886,
      "step": 9873
    },
    {
      "epoch": 0.8889689166985527,
      "grad_norm": 1.8171247329514282,
      "learning_rate": 6.394969886193004e-07,
      "loss": 0.5174,
      "step": 9874
    },
    {
      "epoch": 0.8890589479844246,
      "grad_norm": 1.2837994543813072,
      "learning_rate": 6.384712840571661e-07,
      "loss": 0.4916,
      "step": 9875
    },
    {
      "epoch": 0.8891489792702965,
      "grad_norm": 1.1639239651866249,
      "learning_rate": 6.374463756052462e-07,
      "loss": 0.5076,
      "step": 9876
    },
    {
      "epoch": 0.8892390105561683,
      "grad_norm": 1.2299653009425549,
      "learning_rate": 6.364222633507011e-07,
      "loss": 0.5766,
      "step": 9877
    },
    {
      "epoch": 0.8893290418420401,
      "grad_norm": 1.6709116622761238,
      "learning_rate": 6.353989473806222e-07,
      "loss": 0.5298,
      "step": 9878
    },
    {
      "epoch": 0.8894190731279119,
      "grad_norm": 1.2698835359143765,
      "learning_rate": 6.343764277820285e-07,
      "loss": 0.4691,
      "step": 9879
    },
    {
      "epoch": 0.8895091044137838,
      "grad_norm": 1.298390608497526,
      "learning_rate": 6.333547046418819e-07,
      "loss": 0.5354,
      "step": 9880
    },
    {
      "epoch": 0.8895991356996557,
      "grad_norm": 1.3281724692659358,
      "learning_rate": 6.323337780470662e-07,
      "loss": 0.5037,
      "step": 9881
    },
    {
      "epoch": 0.8896891669855275,
      "grad_norm": 1.2550264580126869,
      "learning_rate": 6.313136480844018e-07,
      "loss": 0.4933,
      "step": 9882
    },
    {
      "epoch": 0.8897791982713993,
      "grad_norm": 1.420874787204263,
      "learning_rate": 6.302943148406437e-07,
      "loss": 0.5888,
      "step": 9883
    },
    {
      "epoch": 0.8898692295572711,
      "grad_norm": 1.3346230622869208,
      "learning_rate": 6.292757784024761e-07,
      "loss": 0.5512,
      "step": 9884
    },
    {
      "epoch": 0.889959260843143,
      "grad_norm": 1.3525248560563905,
      "learning_rate": 6.282580388565107e-07,
      "loss": 0.5562,
      "step": 9885
    },
    {
      "epoch": 0.8900492921290148,
      "grad_norm": 1.1300734663414274,
      "learning_rate": 6.27241096289305e-07,
      "loss": 0.4456,
      "step": 9886
    },
    {
      "epoch": 0.8901393234148867,
      "grad_norm": 1.1657728916429344,
      "learning_rate": 6.262249507873352e-07,
      "loss": 0.5164,
      "step": 9887
    },
    {
      "epoch": 0.8902293547007585,
      "grad_norm": 2.201458237320749,
      "learning_rate": 6.252096024370158e-07,
      "loss": 0.5477,
      "step": 9888
    },
    {
      "epoch": 0.8903193859866304,
      "grad_norm": 1.598131989749786,
      "learning_rate": 6.241950513246931e-07,
      "loss": 0.5032,
      "step": 9889
    },
    {
      "epoch": 0.8904094172725022,
      "grad_norm": 1.5081759236470937,
      "learning_rate": 6.231812975366458e-07,
      "loss": 0.5,
      "step": 9890
    },
    {
      "epoch": 0.890499448558374,
      "grad_norm": 2.8532689617514997,
      "learning_rate": 6.221683411590818e-07,
      "loss": 0.5345,
      "step": 9891
    },
    {
      "epoch": 0.8905894798442459,
      "grad_norm": 1.737186489797182,
      "learning_rate": 6.211561822781476e-07,
      "loss": 0.6096,
      "step": 9892
    },
    {
      "epoch": 0.8906795111301177,
      "grad_norm": 1.6595603779844714,
      "learning_rate": 6.201448209799121e-07,
      "loss": 0.4606,
      "step": 9893
    },
    {
      "epoch": 0.8907695424159896,
      "grad_norm": 1.370825116126821,
      "learning_rate": 6.191342573503889e-07,
      "loss": 0.4991,
      "step": 9894
    },
    {
      "epoch": 0.8908595737018614,
      "grad_norm": 1.183605012544036,
      "learning_rate": 6.181244914755113e-07,
      "loss": 0.5547,
      "step": 9895
    },
    {
      "epoch": 0.8909496049877332,
      "grad_norm": 1.5895801593385628,
      "learning_rate": 6.17115523441153e-07,
      "loss": 0.5386,
      "step": 9896
    },
    {
      "epoch": 0.891039636273605,
      "grad_norm": 2.311777404328578,
      "learning_rate": 6.161073533331174e-07,
      "loss": 0.6076,
      "step": 9897
    },
    {
      "epoch": 0.8911296675594769,
      "grad_norm": 1.3396529799315144,
      "learning_rate": 6.150999812371395e-07,
      "loss": 0.4086,
      "step": 9898
    },
    {
      "epoch": 0.8912196988453488,
      "grad_norm": 1.9638284832678443,
      "learning_rate": 6.140934072388849e-07,
      "loss": 0.5215,
      "step": 9899
    },
    {
      "epoch": 0.8913097301312206,
      "grad_norm": 1.7288627105306307,
      "learning_rate": 6.130876314239564e-07,
      "loss": 0.572,
      "step": 9900
    },
    {
      "epoch": 0.8913997614170924,
      "grad_norm": 1.381832914193253,
      "learning_rate": 6.120826538778845e-07,
      "loss": 0.5731,
      "step": 9901
    },
    {
      "epoch": 0.8914897927029642,
      "grad_norm": 1.9180769812346812,
      "learning_rate": 6.110784746861321e-07,
      "loss": 0.4198,
      "step": 9902
    },
    {
      "epoch": 0.8915798239888362,
      "grad_norm": 1.3231595248667527,
      "learning_rate": 6.100750939340971e-07,
      "loss": 0.4868,
      "step": 9903
    },
    {
      "epoch": 0.891669855274708,
      "grad_norm": 2.4500209827151416,
      "learning_rate": 6.090725117071061e-07,
      "loss": 0.6156,
      "step": 9904
    },
    {
      "epoch": 0.8917598865605798,
      "grad_norm": 1.2909741952522091,
      "learning_rate": 6.080707280904197e-07,
      "loss": 0.5447,
      "step": 9905
    },
    {
      "epoch": 0.8918499178464516,
      "grad_norm": 1.8490122573133647,
      "learning_rate": 6.07069743169233e-07,
      "loss": 0.5072,
      "step": 9906
    },
    {
      "epoch": 0.8919399491323234,
      "grad_norm": 1.4220702858747907,
      "learning_rate": 6.060695570286634e-07,
      "loss": 0.5382,
      "step": 9907
    },
    {
      "epoch": 0.8920299804181954,
      "grad_norm": 1.2269664309474915,
      "learning_rate": 6.050701697537753e-07,
      "loss": 0.5758,
      "step": 9908
    },
    {
      "epoch": 0.8921200117040672,
      "grad_norm": 2.3299412591731947,
      "learning_rate": 6.040715814295527e-07,
      "loss": 0.5442,
      "step": 9909
    },
    {
      "epoch": 0.892210042989939,
      "grad_norm": 1.7673094301867,
      "learning_rate": 6.030737921409169e-07,
      "loss": 0.5071,
      "step": 9910
    },
    {
      "epoch": 0.8923000742758108,
      "grad_norm": 1.8399420987526467,
      "learning_rate": 6.020768019727208e-07,
      "loss": 0.6093,
      "step": 9911
    },
    {
      "epoch": 0.8923901055616826,
      "grad_norm": 3.4495842910047667,
      "learning_rate": 6.0108061100975e-07,
      "loss": 0.4302,
      "step": 9912
    },
    {
      "epoch": 0.8924801368475546,
      "grad_norm": 1.9846132865882307,
      "learning_rate": 6.00085219336718e-07,
      "loss": 0.6333,
      "step": 9913
    },
    {
      "epoch": 0.8925701681334264,
      "grad_norm": 1.3445190878211322,
      "learning_rate": 5.990906270382779e-07,
      "loss": 0.4596,
      "step": 9914
    },
    {
      "epoch": 0.8926601994192982,
      "grad_norm": 4.710481677976455,
      "learning_rate": 5.980968341990068e-07,
      "loss": 0.5396,
      "step": 9915
    },
    {
      "epoch": 0.89275023070517,
      "grad_norm": 1.740867641453104,
      "learning_rate": 5.971038409034202e-07,
      "loss": 0.545,
      "step": 9916
    },
    {
      "epoch": 0.8928402619910419,
      "grad_norm": 1.177935935428146,
      "learning_rate": 5.961116472359607e-07,
      "loss": 0.4443,
      "step": 9917
    },
    {
      "epoch": 0.8929302932769138,
      "grad_norm": 1.1168948397103782,
      "learning_rate": 5.951202532810052e-07,
      "loss": 0.4171,
      "step": 9918
    },
    {
      "epoch": 0.8930203245627856,
      "grad_norm": 1.1220967596873985,
      "learning_rate": 5.94129659122864e-07,
      "loss": 0.4143,
      "step": 9919
    },
    {
      "epoch": 0.8931103558486574,
      "grad_norm": 1.5497335711985831,
      "learning_rate": 5.931398648457787e-07,
      "loss": 0.5457,
      "step": 9920
    },
    {
      "epoch": 0.8932003871345292,
      "grad_norm": 1.1841443927186863,
      "learning_rate": 5.921508705339174e-07,
      "loss": 0.5551,
      "step": 9921
    },
    {
      "epoch": 0.8932904184204011,
      "grad_norm": 1.5733453672673303,
      "learning_rate": 5.911626762713918e-07,
      "loss": 0.5786,
      "step": 9922
    },
    {
      "epoch": 0.893380449706273,
      "grad_norm": 1.2469577333931212,
      "learning_rate": 5.901752821422313e-07,
      "loss": 0.4926,
      "step": 9923
    },
    {
      "epoch": 0.8934704809921448,
      "grad_norm": 1.367425276077087,
      "learning_rate": 5.891886882304088e-07,
      "loss": 0.5254,
      "step": 9924
    },
    {
      "epoch": 0.8935605122780166,
      "grad_norm": 1.2580089577405877,
      "learning_rate": 5.882028946198237e-07,
      "loss": 0.5495,
      "step": 9925
    },
    {
      "epoch": 0.8936505435638884,
      "grad_norm": 1.158409128613996,
      "learning_rate": 5.872179013943102e-07,
      "loss": 0.5409,
      "step": 9926
    },
    {
      "epoch": 0.8937405748497603,
      "grad_norm": 1.8136816365893136,
      "learning_rate": 5.86233708637628e-07,
      "loss": 0.4997,
      "step": 9927
    },
    {
      "epoch": 0.8938306061356321,
      "grad_norm": 1.5717731435611648,
      "learning_rate": 5.852503164334789e-07,
      "loss": 0.4351,
      "step": 9928
    },
    {
      "epoch": 0.893920637421504,
      "grad_norm": 1.163911496139357,
      "learning_rate": 5.842677248654882e-07,
      "loss": 0.5209,
      "step": 9929
    },
    {
      "epoch": 0.8940106687073758,
      "grad_norm": 2.1606183221542077,
      "learning_rate": 5.83285934017217e-07,
      "loss": 0.4859,
      "step": 9930
    },
    {
      "epoch": 0.8941006999932477,
      "grad_norm": 1.5618079771778821,
      "learning_rate": 5.823049439721562e-07,
      "loss": 0.5382,
      "step": 9931
    },
    {
      "epoch": 0.8941907312791195,
      "grad_norm": 1.3964665497837636,
      "learning_rate": 5.813247548137313e-07,
      "loss": 0.5867,
      "step": 9932
    },
    {
      "epoch": 0.8942807625649913,
      "grad_norm": 1.8271740491910569,
      "learning_rate": 5.803453666252978e-07,
      "loss": 0.6264,
      "step": 9933
    },
    {
      "epoch": 0.8943707938508632,
      "grad_norm": 4.302369648758222,
      "learning_rate": 5.793667794901459e-07,
      "loss": 0.4843,
      "step": 9934
    },
    {
      "epoch": 0.894460825136735,
      "grad_norm": 1.281633880791182,
      "learning_rate": 5.783889934914877e-07,
      "loss": 0.528,
      "step": 9935
    },
    {
      "epoch": 0.8945508564226069,
      "grad_norm": 1.3265259637881672,
      "learning_rate": 5.774120087124846e-07,
      "loss": 0.496,
      "step": 9936
    },
    {
      "epoch": 0.8946408877084787,
      "grad_norm": 1.5763514863361479,
      "learning_rate": 5.764358252362123e-07,
      "loss": 0.4829,
      "step": 9937
    },
    {
      "epoch": 0.8947309189943505,
      "grad_norm": 1.4682495991153046,
      "learning_rate": 5.754604431456901e-07,
      "loss": 0.4478,
      "step": 9938
    },
    {
      "epoch": 0.8948209502802223,
      "grad_norm": 1.3536793917770922,
      "learning_rate": 5.744858625238625e-07,
      "loss": 0.4636,
      "step": 9939
    },
    {
      "epoch": 0.8949109815660942,
      "grad_norm": 1.0896372244117227,
      "learning_rate": 5.735120834536123e-07,
      "loss": 0.5104,
      "step": 9940
    },
    {
      "epoch": 0.8950010128519661,
      "grad_norm": 1.0833712660660375,
      "learning_rate": 5.725391060177454e-07,
      "loss": 0.5031,
      "step": 9941
    },
    {
      "epoch": 0.8950910441378379,
      "grad_norm": 2.1858390545900064,
      "learning_rate": 5.71566930299009e-07,
      "loss": 0.5471,
      "step": 9942
    },
    {
      "epoch": 0.8951810754237097,
      "grad_norm": 1.4184945319654996,
      "learning_rate": 5.705955563800736e-07,
      "loss": 0.5855,
      "step": 9943
    },
    {
      "epoch": 0.8952711067095815,
      "grad_norm": 1.0899624077493266,
      "learning_rate": 5.696249843435475e-07,
      "loss": 0.4505,
      "step": 9944
    },
    {
      "epoch": 0.8953611379954535,
      "grad_norm": 1.2768584741995581,
      "learning_rate": 5.686552142719693e-07,
      "loss": 0.609,
      "step": 9945
    },
    {
      "epoch": 0.8954511692813253,
      "grad_norm": 1.62304330468861,
      "learning_rate": 5.676862462478061e-07,
      "loss": 0.5451,
      "step": 9946
    },
    {
      "epoch": 0.8955412005671971,
      "grad_norm": 2.0592096066371646,
      "learning_rate": 5.667180803534633e-07,
      "loss": 0.4741,
      "step": 9947
    },
    {
      "epoch": 0.8956312318530689,
      "grad_norm": 1.4426740440808015,
      "learning_rate": 5.657507166712728e-07,
      "loss": 0.5801,
      "step": 9948
    },
    {
      "epoch": 0.8957212631389407,
      "grad_norm": 1.2066628807272395,
      "learning_rate": 5.647841552834965e-07,
      "loss": 0.5506,
      "step": 9949
    },
    {
      "epoch": 0.8958112944248127,
      "grad_norm": 1.2120855805110982,
      "learning_rate": 5.638183962723387e-07,
      "loss": 0.4266,
      "step": 9950
    },
    {
      "epoch": 0.8959013257106845,
      "grad_norm": 1.5956443926246648,
      "learning_rate": 5.628534397199215e-07,
      "loss": 0.4496,
      "step": 9951
    },
    {
      "epoch": 0.8959913569965563,
      "grad_norm": 1.632501587936279,
      "learning_rate": 5.618892857083069e-07,
      "loss": 0.4525,
      "step": 9952
    },
    {
      "epoch": 0.8960813882824281,
      "grad_norm": 2.2402063097927316,
      "learning_rate": 5.609259343194884e-07,
      "loss": 0.4938,
      "step": 9953
    },
    {
      "epoch": 0.8961714195682999,
      "grad_norm": 1.3755003594847266,
      "learning_rate": 5.599633856353915e-07,
      "loss": 0.4979,
      "step": 9954
    },
    {
      "epoch": 0.8962614508541719,
      "grad_norm": 1.2605439430243501,
      "learning_rate": 5.590016397378672e-07,
      "loss": 0.4818,
      "step": 9955
    },
    {
      "epoch": 0.8963514821400437,
      "grad_norm": 1.1941506321928457,
      "learning_rate": 5.580406967087081e-07,
      "loss": 0.5867,
      "step": 9956
    },
    {
      "epoch": 0.8964415134259155,
      "grad_norm": 1.7680056405996485,
      "learning_rate": 5.5708055662963e-07,
      "loss": 0.5662,
      "step": 9957
    },
    {
      "epoch": 0.8965315447117873,
      "grad_norm": 1.6865896170989145,
      "learning_rate": 5.561212195822841e-07,
      "loss": 0.524,
      "step": 9958
    },
    {
      "epoch": 0.8966215759976592,
      "grad_norm": 1.2511357894322321,
      "learning_rate": 5.55162685648255e-07,
      "loss": 0.5189,
      "step": 9959
    },
    {
      "epoch": 0.896711607283531,
      "grad_norm": 2.0226715469861656,
      "learning_rate": 5.542049549090545e-07,
      "loss": 0.5441,
      "step": 9960
    },
    {
      "epoch": 0.8968016385694029,
      "grad_norm": 1.7349528332818318,
      "learning_rate": 5.532480274461305e-07,
      "loss": 0.4308,
      "step": 9961
    },
    {
      "epoch": 0.8968916698552747,
      "grad_norm": 2.4105855233057993,
      "learning_rate": 5.522919033408624e-07,
      "loss": 0.491,
      "step": 9962
    },
    {
      "epoch": 0.8969817011411465,
      "grad_norm": 1.606966772783052,
      "learning_rate": 5.513365826745532e-07,
      "loss": 0.487,
      "step": 9963
    },
    {
      "epoch": 0.8970717324270184,
      "grad_norm": 1.4187899757903946,
      "learning_rate": 5.50382065528452e-07,
      "loss": 0.5421,
      "step": 9964
    },
    {
      "epoch": 0.8971617637128902,
      "grad_norm": 1.090713302386119,
      "learning_rate": 5.494283519837251e-07,
      "loss": 0.4593,
      "step": 9965
    },
    {
      "epoch": 0.8972517949987621,
      "grad_norm": 1.460897514816365,
      "learning_rate": 5.4847544212148e-07,
      "loss": 0.5295,
      "step": 9966
    },
    {
      "epoch": 0.8973418262846339,
      "grad_norm": 1.4884334878860677,
      "learning_rate": 5.475233360227516e-07,
      "loss": 0.4479,
      "step": 9967
    },
    {
      "epoch": 0.8974318575705057,
      "grad_norm": 1.2421041942916022,
      "learning_rate": 5.46572033768511e-07,
      "loss": 0.4633,
      "step": 9968
    },
    {
      "epoch": 0.8975218888563776,
      "grad_norm": 1.4507471401782346,
      "learning_rate": 5.456215354396511e-07,
      "loss": 0.448,
      "step": 9969
    },
    {
      "epoch": 0.8976119201422494,
      "grad_norm": 1.0193522077890238,
      "learning_rate": 5.446718411170083e-07,
      "loss": 0.5271,
      "step": 9970
    },
    {
      "epoch": 0.8977019514281213,
      "grad_norm": 1.3274390437641084,
      "learning_rate": 5.437229508813424e-07,
      "loss": 0.5517,
      "step": 9971
    },
    {
      "epoch": 0.8977919827139931,
      "grad_norm": 1.3613042013923802,
      "learning_rate": 5.427748648133491e-07,
      "loss": 0.4287,
      "step": 9972
    },
    {
      "epoch": 0.897882013999865,
      "grad_norm": 1.4783335235643953,
      "learning_rate": 5.418275829936537e-07,
      "loss": 0.4475,
      "step": 9973
    },
    {
      "epoch": 0.8979720452857368,
      "grad_norm": 2.0930732893506905,
      "learning_rate": 5.408811055028129e-07,
      "loss": 0.4489,
      "step": 9974
    },
    {
      "epoch": 0.8980620765716086,
      "grad_norm": 1.2757281537870098,
      "learning_rate": 5.39935432421318e-07,
      "loss": 0.423,
      "step": 9975
    },
    {
      "epoch": 0.8981521078574805,
      "grad_norm": 1.5458313595642423,
      "learning_rate": 5.389905638295878e-07,
      "loss": 0.555,
      "step": 9976
    },
    {
      "epoch": 0.8982421391433523,
      "grad_norm": 0.9148222593667135,
      "learning_rate": 5.380464998079726e-07,
      "loss": 0.5326,
      "step": 9977
    },
    {
      "epoch": 0.8983321704292242,
      "grad_norm": 1.2357650426731985,
      "learning_rate": 5.371032404367615e-07,
      "loss": 0.4907,
      "step": 9978
    },
    {
      "epoch": 0.898422201715096,
      "grad_norm": 1.1255332468327617,
      "learning_rate": 5.361607857961648e-07,
      "loss": 0.4386,
      "step": 9979
    },
    {
      "epoch": 0.8985122330009678,
      "grad_norm": 1.0699549085598463,
      "learning_rate": 5.352191359663316e-07,
      "loss": 0.5136,
      "step": 9980
    },
    {
      "epoch": 0.8986022642868396,
      "grad_norm": 1.0654722535580785,
      "learning_rate": 5.342782910273402e-07,
      "loss": 0.5758,
      "step": 9981
    },
    {
      "epoch": 0.8986922955727115,
      "grad_norm": 1.3706268976838685,
      "learning_rate": 5.333382510592022e-07,
      "loss": 0.4588,
      "step": 9982
    },
    {
      "epoch": 0.8987823268585834,
      "grad_norm": 1.4010371901046477,
      "learning_rate": 5.323990161418536e-07,
      "loss": 0.5761,
      "step": 9983
    },
    {
      "epoch": 0.8988723581444552,
      "grad_norm": 1.1104720237807708,
      "learning_rate": 5.314605863551758e-07,
      "loss": 0.559,
      "step": 9984
    },
    {
      "epoch": 0.898962389430327,
      "grad_norm": 1.1459090168047281,
      "learning_rate": 5.305229617789676e-07,
      "loss": 0.4492,
      "step": 9985
    },
    {
      "epoch": 0.8990524207161988,
      "grad_norm": 1.5997563488443456,
      "learning_rate": 5.29586142492966e-07,
      "loss": 0.4512,
      "step": 9986
    },
    {
      "epoch": 0.8991424520020708,
      "grad_norm": 2.4637087636084845,
      "learning_rate": 5.286501285768398e-07,
      "loss": 0.6062,
      "step": 9987
    },
    {
      "epoch": 0.8992324832879426,
      "grad_norm": 1.6970829927498332,
      "learning_rate": 5.277149201101872e-07,
      "loss": 0.544,
      "step": 9988
    },
    {
      "epoch": 0.8993225145738144,
      "grad_norm": 1.400684940845076,
      "learning_rate": 5.267805171725404e-07,
      "loss": 0.4507,
      "step": 9989
    },
    {
      "epoch": 0.8994125458596862,
      "grad_norm": 1.4094331571244465,
      "learning_rate": 5.258469198433613e-07,
      "loss": 0.5629,
      "step": 9990
    },
    {
      "epoch": 0.899502577145558,
      "grad_norm": 1.687413108298643,
      "learning_rate": 5.249141282020409e-07,
      "loss": 0.481,
      "step": 9991
    },
    {
      "epoch": 0.89959260843143,
      "grad_norm": 1.701332791222222,
      "learning_rate": 5.23982142327909e-07,
      "loss": 0.5194,
      "step": 9992
    },
    {
      "epoch": 0.8996826397173018,
      "grad_norm": 1.1526296916611278,
      "learning_rate": 5.230509623002189e-07,
      "loss": 0.4669,
      "step": 9993
    },
    {
      "epoch": 0.8997726710031736,
      "grad_norm": 1.5414903575845855,
      "learning_rate": 5.221205881981594e-07,
      "loss": 0.5052,
      "step": 9994
    },
    {
      "epoch": 0.8998627022890454,
      "grad_norm": 1.8288983564483252,
      "learning_rate": 5.211910201008497e-07,
      "loss": 0.4559,
      "step": 9995
    },
    {
      "epoch": 0.8999527335749172,
      "grad_norm": 1.5545333943999604,
      "learning_rate": 5.202622580873429e-07,
      "loss": 0.4523,
      "step": 9996
    },
    {
      "epoch": 0.9000427648607892,
      "grad_norm": 1.4642667633411848,
      "learning_rate": 5.193343022366182e-07,
      "loss": 0.5246,
      "step": 9997
    },
    {
      "epoch": 0.900132796146661,
      "grad_norm": 1.0939431318665267,
      "learning_rate": 5.184071526275936e-07,
      "loss": 0.3994,
      "step": 9998
    },
    {
      "epoch": 0.9002228274325328,
      "grad_norm": 1.7051557221662976,
      "learning_rate": 5.174808093391104e-07,
      "loss": 0.4067,
      "step": 9999
    },
    {
      "epoch": 0.9003128587184046,
      "grad_norm": 1.409167878109883,
      "learning_rate": 5.165552724499478e-07,
      "loss": 0.5367,
      "step": 10000
    },
    {
      "epoch": 0.9004028900042765,
      "grad_norm": 1.0111918902388861,
      "learning_rate": 5.15630542038813e-07,
      "loss": 0.4397,
      "step": 10001
    },
    {
      "epoch": 0.9004929212901484,
      "grad_norm": 1.6714212932828922,
      "learning_rate": 5.147066181843474e-07,
      "loss": 0.5883,
      "step": 10002
    },
    {
      "epoch": 0.9005829525760202,
      "grad_norm": 1.2581045642756779,
      "learning_rate": 5.137835009651193e-07,
      "loss": 0.4065,
      "step": 10003
    },
    {
      "epoch": 0.900672983861892,
      "grad_norm": 1.7897892597906624,
      "learning_rate": 5.128611904596348e-07,
      "loss": 0.48,
      "step": 10004
    },
    {
      "epoch": 0.9007630151477638,
      "grad_norm": 1.531390329189773,
      "learning_rate": 5.119396867463234e-07,
      "loss": 0.483,
      "step": 10005
    },
    {
      "epoch": 0.9008530464336357,
      "grad_norm": 1.5294120113865055,
      "learning_rate": 5.110189899035545e-07,
      "loss": 0.556,
      "step": 10006
    },
    {
      "epoch": 0.9009430777195075,
      "grad_norm": 1.5947513368704884,
      "learning_rate": 5.100991000096211e-07,
      "loss": 0.4895,
      "step": 10007
    },
    {
      "epoch": 0.9010331090053794,
      "grad_norm": 1.0487708057009615,
      "learning_rate": 5.091800171427541e-07,
      "loss": 0.4541,
      "step": 10008
    },
    {
      "epoch": 0.9011231402912512,
      "grad_norm": 1.7229699657549753,
      "learning_rate": 5.082617413811108e-07,
      "loss": 0.4826,
      "step": 10009
    },
    {
      "epoch": 0.901213171577123,
      "grad_norm": 1.4656469207118976,
      "learning_rate": 5.073442728027844e-07,
      "loss": 0.543,
      "step": 10010
    },
    {
      "epoch": 0.9013032028629949,
      "grad_norm": 2.0466753956616026,
      "learning_rate": 5.064276114857925e-07,
      "loss": 0.5815,
      "step": 10011
    },
    {
      "epoch": 0.9013932341488667,
      "grad_norm": 1.183701478390426,
      "learning_rate": 5.055117575080948e-07,
      "loss": 0.4789,
      "step": 10012
    },
    {
      "epoch": 0.9014832654347386,
      "grad_norm": 1.6093284848648628,
      "learning_rate": 5.045967109475713e-07,
      "loss": 0.5607,
      "step": 10013
    },
    {
      "epoch": 0.9015732967206104,
      "grad_norm": 1.3580861224720224,
      "learning_rate": 5.036824718820388e-07,
      "loss": 0.5364,
      "step": 10014
    },
    {
      "epoch": 0.9016633280064823,
      "grad_norm": 1.5992676384586646,
      "learning_rate": 5.027690403892461e-07,
      "loss": 0.5588,
      "step": 10015
    },
    {
      "epoch": 0.9017533592923541,
      "grad_norm": 1.0146922195086385,
      "learning_rate": 5.01856416546872e-07,
      "loss": 0.4775,
      "step": 10016
    },
    {
      "epoch": 0.9018433905782259,
      "grad_norm": 1.497090372925061,
      "learning_rate": 5.009446004325247e-07,
      "loss": 0.4408,
      "step": 10017
    },
    {
      "epoch": 0.9019334218640978,
      "grad_norm": 1.1281157264435016,
      "learning_rate": 5.000335921237498e-07,
      "loss": 0.5118,
      "step": 10018
    },
    {
      "epoch": 0.9020234531499696,
      "grad_norm": 0.9780447758553273,
      "learning_rate": 4.991233916980131e-07,
      "loss": 0.5092,
      "step": 10019
    },
    {
      "epoch": 0.9021134844358415,
      "grad_norm": 1.875962774725243,
      "learning_rate": 4.982139992327261e-07,
      "loss": 0.5454,
      "step": 10020
    },
    {
      "epoch": 0.9022035157217133,
      "grad_norm": 1.3401734807697165,
      "learning_rate": 4.973054148052192e-07,
      "loss": 0.4987,
      "step": 10021
    },
    {
      "epoch": 0.9022935470075851,
      "grad_norm": 1.3544383432701006,
      "learning_rate": 4.963976384927605e-07,
      "loss": 0.4921,
      "step": 10022
    },
    {
      "epoch": 0.902383578293457,
      "grad_norm": 1.4827570336680638,
      "learning_rate": 4.954906703725471e-07,
      "loss": 0.4455,
      "step": 10023
    },
    {
      "epoch": 0.9024736095793288,
      "grad_norm": 2.077061920747296,
      "learning_rate": 4.945845105217118e-07,
      "loss": 0.4404,
      "step": 10024
    },
    {
      "epoch": 0.9025636408652007,
      "grad_norm": 1.3925528358069896,
      "learning_rate": 4.936791590173073e-07,
      "loss": 0.5674,
      "step": 10025
    },
    {
      "epoch": 0.9026536721510725,
      "grad_norm": 1.6724783383420014,
      "learning_rate": 4.927746159363345e-07,
      "loss": 0.4708,
      "step": 10026
    },
    {
      "epoch": 0.9027437034369443,
      "grad_norm": 1.3307166439508489,
      "learning_rate": 4.918708813557094e-07,
      "loss": 0.5651,
      "step": 10027
    },
    {
      "epoch": 0.9028337347228161,
      "grad_norm": 1.2041127963278784,
      "learning_rate": 4.909679553522884e-07,
      "loss": 0.6079,
      "step": 10028
    },
    {
      "epoch": 0.9029237660086881,
      "grad_norm": 2.229382771965379,
      "learning_rate": 4.900658380028578e-07,
      "loss": 0.4908,
      "step": 10029
    },
    {
      "epoch": 0.9030137972945599,
      "grad_norm": 2.1510635579651356,
      "learning_rate": 4.891645293841329e-07,
      "loss": 0.5092,
      "step": 10030
    },
    {
      "epoch": 0.9031038285804317,
      "grad_norm": 0.9373688090069826,
      "learning_rate": 4.882640295727625e-07,
      "loss": 0.5454,
      "step": 10031
    },
    {
      "epoch": 0.9031938598663035,
      "grad_norm": 1.5361992805097222,
      "learning_rate": 4.873643386453263e-07,
      "loss": 0.3842,
      "step": 10032
    },
    {
      "epoch": 0.9032838911521753,
      "grad_norm": 0.9279131095557209,
      "learning_rate": 4.864654566783312e-07,
      "loss": 0.5091,
      "step": 10033
    },
    {
      "epoch": 0.9033739224380473,
      "grad_norm": 1.1318515151104271,
      "learning_rate": 4.855673837482233e-07,
      "loss": 0.4755,
      "step": 10034
    },
    {
      "epoch": 0.9034639537239191,
      "grad_norm": 2.0888480656070962,
      "learning_rate": 4.846701199313719e-07,
      "loss": 0.4414,
      "step": 10035
    },
    {
      "epoch": 0.9035539850097909,
      "grad_norm": 1.2119311490043616,
      "learning_rate": 4.837736653040825e-07,
      "loss": 0.5453,
      "step": 10036
    },
    {
      "epoch": 0.9036440162956627,
      "grad_norm": 1.6173294666155376,
      "learning_rate": 4.828780199425887e-07,
      "loss": 0.4826,
      "step": 10037
    },
    {
      "epoch": 0.9037340475815346,
      "grad_norm": 1.9673874249396435,
      "learning_rate": 4.819831839230593e-07,
      "loss": 0.5537,
      "step": 10038
    },
    {
      "epoch": 0.9038240788674065,
      "grad_norm": 1.4527157008513485,
      "learning_rate": 4.810891573215871e-07,
      "loss": 0.5173,
      "step": 10039
    },
    {
      "epoch": 0.9039141101532783,
      "grad_norm": 1.627079445074746,
      "learning_rate": 4.801959402142064e-07,
      "loss": 0.5354,
      "step": 10040
    },
    {
      "epoch": 0.9040041414391501,
      "grad_norm": 1.236830699775374,
      "learning_rate": 4.793035326768736e-07,
      "loss": 0.5406,
      "step": 10041
    },
    {
      "epoch": 0.9040941727250219,
      "grad_norm": 1.0483496860075225,
      "learning_rate": 4.784119347854798e-07,
      "loss": 0.4639,
      "step": 10042
    },
    {
      "epoch": 0.9041842040108938,
      "grad_norm": 2.1481409473465596,
      "learning_rate": 4.775211466158469e-07,
      "loss": 0.5053,
      "step": 10043
    },
    {
      "epoch": 0.9042742352967656,
      "grad_norm": 1.268365949369773,
      "learning_rate": 4.7663116824372966e-07,
      "loss": 0.455,
      "step": 10044
    },
    {
      "epoch": 0.9043642665826375,
      "grad_norm": 1.3976903674754977,
      "learning_rate": 4.7574199974480983e-07,
      "loss": 0.438,
      "step": 10045
    },
    {
      "epoch": 0.9044542978685093,
      "grad_norm": 1.4251166864331708,
      "learning_rate": 4.748536411947069e-07,
      "loss": 0.4755,
      "step": 10046
    },
    {
      "epoch": 0.9045443291543811,
      "grad_norm": 1.312018132500221,
      "learning_rate": 4.7396609266896154e-07,
      "loss": 0.5946,
      "step": 10047
    },
    {
      "epoch": 0.904634360440253,
      "grad_norm": 1.4528312397392051,
      "learning_rate": 4.7307935424305784e-07,
      "loss": 0.513,
      "step": 10048
    },
    {
      "epoch": 0.9047243917261248,
      "grad_norm": 1.2716486360267327,
      "learning_rate": 4.7219342599240104e-07,
      "loss": 0.4928,
      "step": 10049
    },
    {
      "epoch": 0.9048144230119967,
      "grad_norm": 1.4299135978788784,
      "learning_rate": 4.7130830799232975e-07,
      "loss": 0.5751,
      "step": 10050
    },
    {
      "epoch": 0.9049044542978685,
      "grad_norm": 2.400421160813326,
      "learning_rate": 4.704240003181182e-07,
      "loss": 0.4802,
      "step": 10051
    },
    {
      "epoch": 0.9049944855837404,
      "grad_norm": 2.175911399417019,
      "learning_rate": 4.6954050304496737e-07,
      "loss": 0.4247,
      "step": 10052
    },
    {
      "epoch": 0.9050845168696122,
      "grad_norm": 1.8409913820555528,
      "learning_rate": 4.686578162480071e-07,
      "loss": 0.5013,
      "step": 10053
    },
    {
      "epoch": 0.905174548155484,
      "grad_norm": 1.6363825323962309,
      "learning_rate": 4.6777594000230855e-07,
      "loss": 0.4687,
      "step": 10054
    },
    {
      "epoch": 0.9052645794413559,
      "grad_norm": 1.708380422617657,
      "learning_rate": 4.668948743828605e-07,
      "loss": 0.5615,
      "step": 10055
    },
    {
      "epoch": 0.9053546107272277,
      "grad_norm": 1.7748961138320791,
      "learning_rate": 4.66014619464592e-07,
      "loss": 0.5245,
      "step": 10056
    },
    {
      "epoch": 0.9054446420130996,
      "grad_norm": 1.2129221983739567,
      "learning_rate": 4.6513517532236096e-07,
      "loss": 0.4329,
      "step": 10057
    },
    {
      "epoch": 0.9055346732989714,
      "grad_norm": 1.559610376421188,
      "learning_rate": 4.6425654203095414e-07,
      "loss": 0.5175,
      "step": 10058
    },
    {
      "epoch": 0.9056247045848432,
      "grad_norm": 1.530296877274961,
      "learning_rate": 4.6337871966509294e-07,
      "loss": 0.5025,
      "step": 10059
    },
    {
      "epoch": 0.905714735870715,
      "grad_norm": 1.1133054111472103,
      "learning_rate": 4.6250170829942876e-07,
      "loss": 0.5002,
      "step": 10060
    },
    {
      "epoch": 0.9058047671565869,
      "grad_norm": 1.9871448182929954,
      "learning_rate": 4.6162550800853856e-07,
      "loss": 0.5825,
      "step": 10061
    },
    {
      "epoch": 0.9058947984424588,
      "grad_norm": 1.8720082260971647,
      "learning_rate": 4.6075011886693944e-07,
      "loss": 0.5045,
      "step": 10062
    },
    {
      "epoch": 0.9059848297283306,
      "grad_norm": 2.100192663544416,
      "learning_rate": 4.5987554094907403e-07,
      "loss": 0.3708,
      "step": 10063
    },
    {
      "epoch": 0.9060748610142024,
      "grad_norm": 2.4763188364471693,
      "learning_rate": 4.5900177432931513e-07,
      "loss": 0.4603,
      "step": 10064
    },
    {
      "epoch": 0.9061648923000742,
      "grad_norm": 2.0960167808174552,
      "learning_rate": 4.581288190819699e-07,
      "loss": 0.4978,
      "step": 10065
    },
    {
      "epoch": 0.9062549235859462,
      "grad_norm": 2.0481437273791947,
      "learning_rate": 4.5725667528127684e-07,
      "loss": 0.5147,
      "step": 10066
    },
    {
      "epoch": 0.906344954871818,
      "grad_norm": 1.4407254386412598,
      "learning_rate": 4.563853430013987e-07,
      "loss": 0.6033,
      "step": 10067
    },
    {
      "epoch": 0.9064349861576898,
      "grad_norm": 1.5356395108060539,
      "learning_rate": 4.555148223164396e-07,
      "loss": 0.513,
      "step": 10068
    },
    {
      "epoch": 0.9065250174435616,
      "grad_norm": 1.335793860486281,
      "learning_rate": 4.546451133004248e-07,
      "loss": 0.4782,
      "step": 10069
    },
    {
      "epoch": 0.9066150487294334,
      "grad_norm": 1.3539731470021363,
      "learning_rate": 4.5377621602731736e-07,
      "loss": 0.482,
      "step": 10070
    },
    {
      "epoch": 0.9067050800153054,
      "grad_norm": 1.175623479534199,
      "learning_rate": 4.5290813057100814e-07,
      "loss": 0.4571,
      "step": 10071
    },
    {
      "epoch": 0.9067951113011772,
      "grad_norm": 2.395202193709613,
      "learning_rate": 4.520408570053192e-07,
      "loss": 0.4045,
      "step": 10072
    },
    {
      "epoch": 0.906885142587049,
      "grad_norm": 1.12534296290749,
      "learning_rate": 4.5117439540400485e-07,
      "loss": 0.5194,
      "step": 10073
    },
    {
      "epoch": 0.9069751738729208,
      "grad_norm": 1.2529813263344907,
      "learning_rate": 4.503087458407507e-07,
      "loss": 0.5543,
      "step": 10074
    },
    {
      "epoch": 0.9070652051587926,
      "grad_norm": 1.1960051522988835,
      "learning_rate": 4.494439083891677e-07,
      "loss": 0.4796,
      "step": 10075
    },
    {
      "epoch": 0.9071552364446646,
      "grad_norm": 1.4123517186648806,
      "learning_rate": 4.485798831228072e-07,
      "loss": 0.4662,
      "step": 10076
    },
    {
      "epoch": 0.9072452677305364,
      "grad_norm": 1.585081421322886,
      "learning_rate": 4.477166701151436e-07,
      "loss": 0.4847,
      "step": 10077
    },
    {
      "epoch": 0.9073352990164082,
      "grad_norm": 1.100027356687901,
      "learning_rate": 4.468542694395861e-07,
      "loss": 0.5921,
      "step": 10078
    },
    {
      "epoch": 0.90742533030228,
      "grad_norm": 1.3453253280292015,
      "learning_rate": 4.459926811694726e-07,
      "loss": 0.5511,
      "step": 10079
    },
    {
      "epoch": 0.9075153615881519,
      "grad_norm": 1.718884666576331,
      "learning_rate": 4.451319053780767e-07,
      "loss": 0.5517,
      "step": 10080
    },
    {
      "epoch": 0.9076053928740238,
      "grad_norm": 1.4729036688669348,
      "learning_rate": 4.4427194213859216e-07,
      "loss": 0.524,
      "step": 10081
    },
    {
      "epoch": 0.9076954241598956,
      "grad_norm": 1.248414294861551,
      "learning_rate": 4.4341279152415927e-07,
      "loss": 0.5681,
      "step": 10082
    },
    {
      "epoch": 0.9077854554457674,
      "grad_norm": 1.2546960841004717,
      "learning_rate": 4.4255445360783413e-07,
      "loss": 0.4929,
      "step": 10083
    },
    {
      "epoch": 0.9078754867316392,
      "grad_norm": 1.3250274552772567,
      "learning_rate": 4.416969284626138e-07,
      "loss": 0.5598,
      "step": 10084
    },
    {
      "epoch": 0.9079655180175111,
      "grad_norm": 1.5404497615243353,
      "learning_rate": 4.4084021616142226e-07,
      "loss": 0.5085,
      "step": 10085
    },
    {
      "epoch": 0.908055549303383,
      "grad_norm": 1.5518525983765454,
      "learning_rate": 4.399843167771134e-07,
      "loss": 0.5799,
      "step": 10086
    },
    {
      "epoch": 0.9081455805892548,
      "grad_norm": 0.8911318780480508,
      "learning_rate": 4.3912923038247455e-07,
      "loss": 0.5518,
      "step": 10087
    },
    {
      "epoch": 0.9082356118751266,
      "grad_norm": 1.1808502179899698,
      "learning_rate": 4.382749570502243e-07,
      "loss": 0.519,
      "step": 10088
    },
    {
      "epoch": 0.9083256431609984,
      "grad_norm": 1.6753056463459153,
      "learning_rate": 4.3742149685300663e-07,
      "loss": 0.5093,
      "step": 10089
    },
    {
      "epoch": 0.9084156744468703,
      "grad_norm": 1.8876768411445173,
      "learning_rate": 4.365688498634046e-07,
      "loss": 0.4841,
      "step": 10090
    },
    {
      "epoch": 0.9085057057327421,
      "grad_norm": 1.455057554911661,
      "learning_rate": 4.357170161539248e-07,
      "loss": 0.4669,
      "step": 10091
    },
    {
      "epoch": 0.908595737018614,
      "grad_norm": 2.080627687483248,
      "learning_rate": 4.3486599579700915e-07,
      "loss": 0.5202,
      "step": 10092
    },
    {
      "epoch": 0.9086857683044858,
      "grad_norm": 2.605929171910218,
      "learning_rate": 4.3401578886502873e-07,
      "loss": 0.5937,
      "step": 10093
    },
    {
      "epoch": 0.9087757995903577,
      "grad_norm": 1.4454087984458086,
      "learning_rate": 4.331663954302867e-07,
      "loss": 0.5229,
      "step": 10094
    },
    {
      "epoch": 0.9088658308762295,
      "grad_norm": 1.3363145820138547,
      "learning_rate": 4.3231781556501207e-07,
      "loss": 0.5196,
      "step": 10095
    },
    {
      "epoch": 0.9089558621621013,
      "grad_norm": 2.0315127519450242,
      "learning_rate": 4.314700493413737e-07,
      "loss": 0.5249,
      "step": 10096
    },
    {
      "epoch": 0.9090458934479732,
      "grad_norm": 1.595140294531733,
      "learning_rate": 4.306230968314629e-07,
      "loss": 0.5848,
      "step": 10097
    },
    {
      "epoch": 0.909135924733845,
      "grad_norm": 2.0218253325121043,
      "learning_rate": 4.2977695810730526e-07,
      "loss": 0.5964,
      "step": 10098
    },
    {
      "epoch": 0.9092259560197169,
      "grad_norm": 1.314765224088363,
      "learning_rate": 4.2893163324085886e-07,
      "loss": 0.4962,
      "step": 10099
    },
    {
      "epoch": 0.9093159873055887,
      "grad_norm": 1.666445883148213,
      "learning_rate": 4.280871223040084e-07,
      "loss": 0.5962,
      "step": 10100
    },
    {
      "epoch": 0.9094060185914605,
      "grad_norm": 1.3554427447120998,
      "learning_rate": 4.272434253685731e-07,
      "loss": 0.4889,
      "step": 10101
    },
    {
      "epoch": 0.9094960498773323,
      "grad_norm": 1.3678961367235378,
      "learning_rate": 4.2640054250630337e-07,
      "loss": 0.4869,
      "step": 10102
    },
    {
      "epoch": 0.9095860811632042,
      "grad_norm": 1.7983355584573868,
      "learning_rate": 4.2555847378887183e-07,
      "loss": 0.4466,
      "step": 10103
    },
    {
      "epoch": 0.9096761124490761,
      "grad_norm": 2.707043881553709,
      "learning_rate": 4.247172192878968e-07,
      "loss": 0.4396,
      "step": 10104
    },
    {
      "epoch": 0.9097661437349479,
      "grad_norm": 1.6770796631470917,
      "learning_rate": 4.238767790749132e-07,
      "loss": 0.5188,
      "step": 10105
    },
    {
      "epoch": 0.9098561750208197,
      "grad_norm": 2.741762043337388,
      "learning_rate": 4.2303715322139393e-07,
      "loss": 0.4608,
      "step": 10106
    },
    {
      "epoch": 0.9099462063066915,
      "grad_norm": 2.776279686188122,
      "learning_rate": 4.22198341798743e-07,
      "loss": 0.5165,
      "step": 10107
    },
    {
      "epoch": 0.9100362375925635,
      "grad_norm": 2.225285023586549,
      "learning_rate": 4.2136034487829327e-07,
      "loss": 0.6036,
      "step": 10108
    },
    {
      "epoch": 0.9101262688784353,
      "grad_norm": 1.3620196933947766,
      "learning_rate": 4.205231625313044e-07,
      "loss": 0.508,
      "step": 10109
    },
    {
      "epoch": 0.9102163001643071,
      "grad_norm": 1.2544313947890398,
      "learning_rate": 4.196867948289773e-07,
      "loss": 0.5049,
      "step": 10110
    },
    {
      "epoch": 0.9103063314501789,
      "grad_norm": 1.9871663323878157,
      "learning_rate": 4.1885124184243284e-07,
      "loss": 0.4792,
      "step": 10111
    },
    {
      "epoch": 0.9103963627360507,
      "grad_norm": 1.2291724689636172,
      "learning_rate": 4.180165036427275e-07,
      "loss": 0.4861,
      "step": 10112
    },
    {
      "epoch": 0.9104863940219227,
      "grad_norm": 1.7424188813676673,
      "learning_rate": 4.171825803008489e-07,
      "loss": 0.5288,
      "step": 10113
    },
    {
      "epoch": 0.9105764253077945,
      "grad_norm": 1.9006010913132048,
      "learning_rate": 4.163494718877137e-07,
      "loss": 0.5064,
      "step": 10114
    },
    {
      "epoch": 0.9106664565936663,
      "grad_norm": 2.1963409056053096,
      "learning_rate": 4.155171784741707e-07,
      "loss": 0.5233,
      "step": 10115
    },
    {
      "epoch": 0.9107564878795381,
      "grad_norm": 2.416774707417701,
      "learning_rate": 4.146857001310001e-07,
      "loss": 0.4104,
      "step": 10116
    },
    {
      "epoch": 0.9108465191654099,
      "grad_norm": 1.7096529172314838,
      "learning_rate": 4.1385503692890627e-07,
      "loss": 0.5013,
      "step": 10117
    },
    {
      "epoch": 0.9109365504512819,
      "grad_norm": 1.6465827636675832,
      "learning_rate": 4.1302518893853506e-07,
      "loss": 0.4435,
      "step": 10118
    },
    {
      "epoch": 0.9110265817371537,
      "grad_norm": 1.4584329794066495,
      "learning_rate": 4.121961562304533e-07,
      "loss": 0.5745,
      "step": 10119
    },
    {
      "epoch": 0.9111166130230255,
      "grad_norm": 1.6634762836951742,
      "learning_rate": 4.113679388751635e-07,
      "loss": 0.504,
      "step": 10120
    },
    {
      "epoch": 0.9112066443088973,
      "grad_norm": 1.6868677981018976,
      "learning_rate": 4.1054053694309925e-07,
      "loss": 0.4686,
      "step": 10121
    },
    {
      "epoch": 0.9112966755947692,
      "grad_norm": 1.184103480850195,
      "learning_rate": 4.097139505046233e-07,
      "loss": 0.5816,
      "step": 10122
    },
    {
      "epoch": 0.911386706880641,
      "grad_norm": 1.4338473770691498,
      "learning_rate": 4.088881796300248e-07,
      "loss": 0.4363,
      "step": 10123
    },
    {
      "epoch": 0.9114767381665129,
      "grad_norm": 1.5086078719989562,
      "learning_rate": 4.0806322438953437e-07,
      "loss": 0.5657,
      "step": 10124
    },
    {
      "epoch": 0.9115667694523847,
      "grad_norm": 1.287851739750125,
      "learning_rate": 4.072390848533014e-07,
      "loss": 0.457,
      "step": 10125
    },
    {
      "epoch": 0.9116568007382565,
      "grad_norm": 1.46903311914829,
      "learning_rate": 4.064157610914132e-07,
      "loss": 0.4873,
      "step": 10126
    },
    {
      "epoch": 0.9117468320241284,
      "grad_norm": 1.0915283124084088,
      "learning_rate": 4.05593253173886e-07,
      "loss": 0.4348,
      "step": 10127
    },
    {
      "epoch": 0.9118368633100002,
      "grad_norm": 1.7464274838489389,
      "learning_rate": 4.0477156117066596e-07,
      "loss": 0.5133,
      "step": 10128
    },
    {
      "epoch": 0.9119268945958721,
      "grad_norm": 2.9643104517326786,
      "learning_rate": 4.039506851516306e-07,
      "loss": 0.5804,
      "step": 10129
    },
    {
      "epoch": 0.9120169258817439,
      "grad_norm": 1.267661330628761,
      "learning_rate": 4.0313062518658963e-07,
      "loss": 0.4059,
      "step": 10130
    },
    {
      "epoch": 0.9121069571676157,
      "grad_norm": 1.4790492468220715,
      "learning_rate": 4.02311381345275e-07,
      "loss": 0.5519,
      "step": 10131
    },
    {
      "epoch": 0.9121969884534876,
      "grad_norm": 1.494674962736782,
      "learning_rate": 4.014929536973644e-07,
      "loss": 0.4836,
      "step": 10132
    },
    {
      "epoch": 0.9122870197393594,
      "grad_norm": 1.3029347206973325,
      "learning_rate": 4.00675342312451e-07,
      "loss": 0.6193,
      "step": 10133
    },
    {
      "epoch": 0.9123770510252313,
      "grad_norm": 1.3757941893899752,
      "learning_rate": 3.998585472600669e-07,
      "loss": 0.5895,
      "step": 10134
    },
    {
      "epoch": 0.9124670823111031,
      "grad_norm": 1.9016123352128924,
      "learning_rate": 3.9904256860967436e-07,
      "loss": 0.5495,
      "step": 10135
    },
    {
      "epoch": 0.912557113596975,
      "grad_norm": 2.7901490532871893,
      "learning_rate": 3.9822740643066347e-07,
      "loss": 0.4593,
      "step": 10136
    },
    {
      "epoch": 0.9126471448828468,
      "grad_norm": 1.394755107015852,
      "learning_rate": 3.974130607923532e-07,
      "loss": 0.5878,
      "step": 10137
    },
    {
      "epoch": 0.9127371761687186,
      "grad_norm": 1.5671154653929078,
      "learning_rate": 3.965995317640026e-07,
      "loss": 0.4913,
      "step": 10138
    },
    {
      "epoch": 0.9128272074545905,
      "grad_norm": 1.5935986049551718,
      "learning_rate": 3.9578681941478957e-07,
      "loss": 0.5797,
      "step": 10139
    },
    {
      "epoch": 0.9129172387404623,
      "grad_norm": 1.7235498134483795,
      "learning_rate": 3.9497492381382784e-07,
      "loss": 0.4819,
      "step": 10140
    },
    {
      "epoch": 0.9130072700263342,
      "grad_norm": 1.7432940024943608,
      "learning_rate": 3.941638450301644e-07,
      "loss": 0.5681,
      "step": 10141
    },
    {
      "epoch": 0.913097301312206,
      "grad_norm": 2.175902121170377,
      "learning_rate": 3.933535831327706e-07,
      "loss": 0.5385,
      "step": 10142
    },
    {
      "epoch": 0.9131873325980778,
      "grad_norm": 2.0430869582896913,
      "learning_rate": 3.925441381905548e-07,
      "loss": 0.5463,
      "step": 10143
    },
    {
      "epoch": 0.9132773638839496,
      "grad_norm": 1.151364608521926,
      "learning_rate": 3.917355102723519e-07,
      "loss": 0.3996,
      "step": 10144
    },
    {
      "epoch": 0.9133673951698215,
      "grad_norm": 1.1156247816781406,
      "learning_rate": 3.9092769944692464e-07,
      "loss": 0.5345,
      "step": 10145
    },
    {
      "epoch": 0.9134574264556934,
      "grad_norm": 1.7821877404452566,
      "learning_rate": 3.901207057829748e-07,
      "loss": 0.513,
      "step": 10146
    },
    {
      "epoch": 0.9135474577415652,
      "grad_norm": 1.3304568683306315,
      "learning_rate": 3.893145293491263e-07,
      "loss": 0.4287,
      "step": 10147
    },
    {
      "epoch": 0.913637489027437,
      "grad_norm": 1.6567314843566223,
      "learning_rate": 3.8850917021393764e-07,
      "loss": 0.4871,
      "step": 10148
    },
    {
      "epoch": 0.9137275203133088,
      "grad_norm": 2.3646912393641046,
      "learning_rate": 3.877046284458974e-07,
      "loss": 0.4741,
      "step": 10149
    },
    {
      "epoch": 0.9138175515991808,
      "grad_norm": 1.1478609849942019,
      "learning_rate": 3.869009041134264e-07,
      "loss": 0.5485,
      "step": 10150
    },
    {
      "epoch": 0.9139075828850526,
      "grad_norm": 1.744826169956846,
      "learning_rate": 3.8609799728486886e-07,
      "loss": 0.4882,
      "step": 10151
    },
    {
      "epoch": 0.9139976141709244,
      "grad_norm": 1.7821799025404421,
      "learning_rate": 3.8529590802850903e-07,
      "loss": 0.4834,
      "step": 10152
    },
    {
      "epoch": 0.9140876454567962,
      "grad_norm": 1.5459895782034911,
      "learning_rate": 3.844946364125546e-07,
      "loss": 0.5405,
      "step": 10153
    },
    {
      "epoch": 0.914177676742668,
      "grad_norm": 1.4029289992927199,
      "learning_rate": 3.8369418250514657e-07,
      "loss": 0.4487,
      "step": 10154
    },
    {
      "epoch": 0.91426770802854,
      "grad_norm": 1.2648651341622692,
      "learning_rate": 3.828945463743572e-07,
      "loss": 0.5119,
      "step": 10155
    },
    {
      "epoch": 0.9143577393144118,
      "grad_norm": 0.9812292005774143,
      "learning_rate": 3.8209572808818653e-07,
      "loss": 0.4921,
      "step": 10156
    },
    {
      "epoch": 0.9144477706002836,
      "grad_norm": 1.3999751273827803,
      "learning_rate": 3.8129772771456797e-07,
      "loss": 0.4893,
      "step": 10157
    },
    {
      "epoch": 0.9145378018861554,
      "grad_norm": 1.1789923020636375,
      "learning_rate": 3.8050054532136395e-07,
      "loss": 0.4269,
      "step": 10158
    },
    {
      "epoch": 0.9146278331720272,
      "grad_norm": 1.1708918115599973,
      "learning_rate": 3.797041809763635e-07,
      "loss": 0.5124,
      "step": 10159
    },
    {
      "epoch": 0.9147178644578992,
      "grad_norm": 1.6528328813863395,
      "learning_rate": 3.78908634747297e-07,
      "loss": 0.4522,
      "step": 10160
    },
    {
      "epoch": 0.914807895743771,
      "grad_norm": 1.8551771018803738,
      "learning_rate": 3.7811390670181245e-07,
      "loss": 0.4938,
      "step": 10161
    },
    {
      "epoch": 0.9148979270296428,
      "grad_norm": 2.40645677201402,
      "learning_rate": 3.773199969074959e-07,
      "loss": 0.5197,
      "step": 10162
    },
    {
      "epoch": 0.9149879583155146,
      "grad_norm": 1.8927845803534968,
      "learning_rate": 3.765269054318621e-07,
      "loss": 0.4634,
      "step": 10163
    },
    {
      "epoch": 0.9150779896013865,
      "grad_norm": 1.2436306445330594,
      "learning_rate": 3.757346323423572e-07,
      "loss": 0.4989,
      "step": 10164
    },
    {
      "epoch": 0.9151680208872583,
      "grad_norm": 1.4840795522728292,
      "learning_rate": 3.749431777063528e-07,
      "loss": 0.487,
      "step": 10165
    },
    {
      "epoch": 0.9152580521731302,
      "grad_norm": 1.379862751573643,
      "learning_rate": 3.7415254159115955e-07,
      "loss": 0.5704,
      "step": 10166
    },
    {
      "epoch": 0.915348083459002,
      "grad_norm": 1.9966972920818886,
      "learning_rate": 3.733627240640103e-07,
      "loss": 0.4497,
      "step": 10167
    },
    {
      "epoch": 0.9154381147448738,
      "grad_norm": 1.4859249479507195,
      "learning_rate": 3.7257372519207247e-07,
      "loss": 0.5377,
      "step": 10168
    },
    {
      "epoch": 0.9155281460307457,
      "grad_norm": 1.456384979479272,
      "learning_rate": 3.7178554504244345e-07,
      "loss": 0.5097,
      "step": 10169
    },
    {
      "epoch": 0.9156181773166175,
      "grad_norm": 1.3433002482127934,
      "learning_rate": 3.7099818368214967e-07,
      "loss": 0.5273,
      "step": 10170
    },
    {
      "epoch": 0.9157082086024894,
      "grad_norm": 1.0886939971220178,
      "learning_rate": 3.7021164117814977e-07,
      "loss": 0.5811,
      "step": 10171
    },
    {
      "epoch": 0.9157982398883612,
      "grad_norm": 1.3448155741996857,
      "learning_rate": 3.694259175973325e-07,
      "loss": 0.4557,
      "step": 10172
    },
    {
      "epoch": 0.915888271174233,
      "grad_norm": 2.0253047968038373,
      "learning_rate": 3.686410130065121e-07,
      "loss": 0.55,
      "step": 10173
    },
    {
      "epoch": 0.9159783024601049,
      "grad_norm": 1.6191811854886866,
      "learning_rate": 3.6785692747244304e-07,
      "loss": 0.4834,
      "step": 10174
    },
    {
      "epoch": 0.9160683337459767,
      "grad_norm": 1.4547380353372272,
      "learning_rate": 3.6707366106180197e-07,
      "loss": 0.4487,
      "step": 10175
    },
    {
      "epoch": 0.9161583650318486,
      "grad_norm": 1.4715993145327981,
      "learning_rate": 3.662912138411967e-07,
      "loss": 0.475,
      "step": 10176
    },
    {
      "epoch": 0.9162483963177204,
      "grad_norm": 1.5171270210729673,
      "learning_rate": 3.6550958587716956e-07,
      "loss": 0.5342,
      "step": 10177
    },
    {
      "epoch": 0.9163384276035923,
      "grad_norm": 1.3651166262865673,
      "learning_rate": 3.647287772361907e-07,
      "loss": 0.436,
      "step": 10178
    },
    {
      "epoch": 0.9164284588894641,
      "grad_norm": 1.102895491133913,
      "learning_rate": 3.6394878798465703e-07,
      "loss": 0.4557,
      "step": 10179
    },
    {
      "epoch": 0.9165184901753359,
      "grad_norm": 1.4178908052610653,
      "learning_rate": 3.6316961818890437e-07,
      "loss": 0.4728,
      "step": 10180
    },
    {
      "epoch": 0.9166085214612077,
      "grad_norm": 2.278347324127619,
      "learning_rate": 3.6239126791518977e-07,
      "loss": 0.5552,
      "step": 10181
    },
    {
      "epoch": 0.9166985527470796,
      "grad_norm": 1.2860464530606963,
      "learning_rate": 3.6161373722970684e-07,
      "loss": 0.5984,
      "step": 10182
    },
    {
      "epoch": 0.9167885840329515,
      "grad_norm": 1.6293786436857625,
      "learning_rate": 3.608370261985761e-07,
      "loss": 0.5815,
      "step": 10183
    },
    {
      "epoch": 0.9168786153188233,
      "grad_norm": 1.2569858300925856,
      "learning_rate": 3.6006113488785024e-07,
      "loss": 0.5083,
      "step": 10184
    },
    {
      "epoch": 0.9169686466046951,
      "grad_norm": 1.7934088701152777,
      "learning_rate": 3.592860633635109e-07,
      "loss": 0.5239,
      "step": 10185
    },
    {
      "epoch": 0.9170586778905669,
      "grad_norm": 1.4589960579024563,
      "learning_rate": 3.5851181169147207e-07,
      "loss": 0.4785,
      "step": 10186
    },
    {
      "epoch": 0.9171487091764388,
      "grad_norm": 1.4411424085067623,
      "learning_rate": 3.5773837993757444e-07,
      "loss": 0.4761,
      "step": 10187
    },
    {
      "epoch": 0.9172387404623107,
      "grad_norm": 1.0896902692228505,
      "learning_rate": 3.569657681675931e-07,
      "loss": 0.5632,
      "step": 10188
    },
    {
      "epoch": 0.9173287717481825,
      "grad_norm": 1.2967205638773123,
      "learning_rate": 3.5619397644723e-07,
      "loss": 0.4695,
      "step": 10189
    },
    {
      "epoch": 0.9174188030340543,
      "grad_norm": 1.4004490362713444,
      "learning_rate": 3.5542300484212035e-07,
      "loss": 0.4425,
      "step": 10190
    },
    {
      "epoch": 0.9175088343199261,
      "grad_norm": 1.8671028902873068,
      "learning_rate": 3.546528534178262e-07,
      "loss": 0.5134,
      "step": 10191
    },
    {
      "epoch": 0.9175988656057981,
      "grad_norm": 1.5795075773958978,
      "learning_rate": 3.538835222398429e-07,
      "loss": 0.5948,
      "step": 10192
    },
    {
      "epoch": 0.9176888968916699,
      "grad_norm": 1.483105143126968,
      "learning_rate": 3.531150113735937e-07,
      "loss": 0.5598,
      "step": 10193
    },
    {
      "epoch": 0.9177789281775417,
      "grad_norm": 1.8209592254001856,
      "learning_rate": 3.5234732088443524e-07,
      "loss": 0.4588,
      "step": 10194
    },
    {
      "epoch": 0.9178689594634135,
      "grad_norm": 1.5839018054953709,
      "learning_rate": 3.515804508376508e-07,
      "loss": 0.5729,
      "step": 10195
    },
    {
      "epoch": 0.9179589907492853,
      "grad_norm": 1.118276938916029,
      "learning_rate": 3.50814401298456e-07,
      "loss": 0.5407,
      "step": 10196
    },
    {
      "epoch": 0.9180490220351573,
      "grad_norm": 1.5200677644473513,
      "learning_rate": 3.500491723319965e-07,
      "loss": 0.4656,
      "step": 10197
    },
    {
      "epoch": 0.9181390533210291,
      "grad_norm": 2.008381039177196,
      "learning_rate": 3.4928476400334587e-07,
      "loss": 0.5587,
      "step": 10198
    },
    {
      "epoch": 0.9182290846069009,
      "grad_norm": 1.9098484918811518,
      "learning_rate": 3.4852117637751314e-07,
      "loss": 0.4205,
      "step": 10199
    },
    {
      "epoch": 0.9183191158927727,
      "grad_norm": 2.471290185954733,
      "learning_rate": 3.477584095194331e-07,
      "loss": 0.4709,
      "step": 10200
    },
    {
      "epoch": 0.9184091471786445,
      "grad_norm": 1.3746056680222487,
      "learning_rate": 3.4699646349396823e-07,
      "loss": 0.5073,
      "step": 10201
    },
    {
      "epoch": 0.9184991784645165,
      "grad_norm": 1.566952118011521,
      "learning_rate": 3.462353383659212e-07,
      "loss": 0.5449,
      "step": 10202
    },
    {
      "epoch": 0.9185892097503883,
      "grad_norm": 1.6248052944197577,
      "learning_rate": 3.454750342000146e-07,
      "loss": 0.4852,
      "step": 10203
    },
    {
      "epoch": 0.9186792410362601,
      "grad_norm": 1.6854355951874016,
      "learning_rate": 3.4471555106090573e-07,
      "loss": 0.4255,
      "step": 10204
    },
    {
      "epoch": 0.9187692723221319,
      "grad_norm": 1.477800632255388,
      "learning_rate": 3.439568890131817e-07,
      "loss": 0.4528,
      "step": 10205
    },
    {
      "epoch": 0.9188593036080038,
      "grad_norm": 1.8663502702420551,
      "learning_rate": 3.4319904812136097e-07,
      "loss": 0.6022,
      "step": 10206
    },
    {
      "epoch": 0.9189493348938756,
      "grad_norm": 1.0872732038633202,
      "learning_rate": 3.424420284498864e-07,
      "loss": 0.4408,
      "step": 10207
    },
    {
      "epoch": 0.9190393661797475,
      "grad_norm": 1.4661561688314146,
      "learning_rate": 3.4168583006314207e-07,
      "loss": 0.5038,
      "step": 10208
    },
    {
      "epoch": 0.9191293974656193,
      "grad_norm": 1.5602546594619664,
      "learning_rate": 3.409304530254298e-07,
      "loss": 0.4766,
      "step": 10209
    },
    {
      "epoch": 0.9192194287514911,
      "grad_norm": 1.4234925172965027,
      "learning_rate": 3.4017589740098943e-07,
      "loss": 0.4477,
      "step": 10210
    },
    {
      "epoch": 0.919309460037363,
      "grad_norm": 1.6250859845713224,
      "learning_rate": 3.3942216325398957e-07,
      "loss": 0.5009,
      "step": 10211
    },
    {
      "epoch": 0.9193994913232348,
      "grad_norm": 1.5908798765127763,
      "learning_rate": 3.3866925064852784e-07,
      "loss": 0.4723,
      "step": 10212
    },
    {
      "epoch": 0.9194895226091067,
      "grad_norm": 1.523308886240293,
      "learning_rate": 3.379171596486319e-07,
      "loss": 0.3902,
      "step": 10213
    },
    {
      "epoch": 0.9195795538949785,
      "grad_norm": 1.3296330227724333,
      "learning_rate": 3.3716589031826063e-07,
      "loss": 0.4769,
      "step": 10214
    },
    {
      "epoch": 0.9196695851808503,
      "grad_norm": 1.160328895641997,
      "learning_rate": 3.364154427213007e-07,
      "loss": 0.4509,
      "step": 10215
    },
    {
      "epoch": 0.9197596164667222,
      "grad_norm": 1.6516623013715968,
      "learning_rate": 3.356658169215743e-07,
      "loss": 0.5385,
      "step": 10216
    },
    {
      "epoch": 0.919849647752594,
      "grad_norm": 2.112519572188335,
      "learning_rate": 3.349170129828272e-07,
      "loss": 0.4587,
      "step": 10217
    },
    {
      "epoch": 0.9199396790384659,
      "grad_norm": 1.1170859628071463,
      "learning_rate": 3.341690309687373e-07,
      "loss": 0.5138,
      "step": 10218
    },
    {
      "epoch": 0.9200297103243377,
      "grad_norm": 1.1046118141181642,
      "learning_rate": 3.33421870942916e-07,
      "loss": 0.5492,
      "step": 10219
    },
    {
      "epoch": 0.9201197416102096,
      "grad_norm": 2.2383123861628365,
      "learning_rate": 3.3267553296890244e-07,
      "loss": 0.4577,
      "step": 10220
    },
    {
      "epoch": 0.9202097728960814,
      "grad_norm": 1.5765904463661935,
      "learning_rate": 3.319300171101614e-07,
      "loss": 0.5049,
      "step": 10221
    },
    {
      "epoch": 0.9202998041819532,
      "grad_norm": 1.5448340789138244,
      "learning_rate": 3.3118532343009767e-07,
      "loss": 0.6187,
      "step": 10222
    },
    {
      "epoch": 0.920389835467825,
      "grad_norm": 1.201716344520772,
      "learning_rate": 3.3044145199203626e-07,
      "loss": 0.5406,
      "step": 10223
    },
    {
      "epoch": 0.9204798667536969,
      "grad_norm": 1.7334508161455249,
      "learning_rate": 3.296984028592387e-07,
      "loss": 0.509,
      "step": 10224
    },
    {
      "epoch": 0.9205698980395688,
      "grad_norm": 1.6427810374987566,
      "learning_rate": 3.2895617609489337e-07,
      "loss": 0.4738,
      "step": 10225
    },
    {
      "epoch": 0.9206599293254406,
      "grad_norm": 1.6859217570423481,
      "learning_rate": 3.282147717621198e-07,
      "loss": 0.572,
      "step": 10226
    },
    {
      "epoch": 0.9207499606113124,
      "grad_norm": 1.9214977398343798,
      "learning_rate": 3.2747418992396753e-07,
      "loss": 0.598,
      "step": 10227
    },
    {
      "epoch": 0.9208399918971842,
      "grad_norm": 1.1339509495946418,
      "learning_rate": 3.2673443064341723e-07,
      "loss": 0.443,
      "step": 10228
    },
    {
      "epoch": 0.9209300231830561,
      "grad_norm": 1.6420918387231611,
      "learning_rate": 3.259954939833765e-07,
      "loss": 0.4419,
      "step": 10229
    },
    {
      "epoch": 0.921020054468928,
      "grad_norm": 1.514227808317019,
      "learning_rate": 3.252573800066872e-07,
      "loss": 0.4397,
      "step": 10230
    },
    {
      "epoch": 0.9211100857547998,
      "grad_norm": 1.5550369244683182,
      "learning_rate": 3.2452008877611686e-07,
      "loss": 0.4693,
      "step": 10231
    },
    {
      "epoch": 0.9212001170406716,
      "grad_norm": 1.629697182280031,
      "learning_rate": 3.237836203543654e-07,
      "loss": 0.4782,
      "step": 10232
    },
    {
      "epoch": 0.9212901483265434,
      "grad_norm": 1.3325473773907244,
      "learning_rate": 3.230479748040638e-07,
      "loss": 0.5523,
      "step": 10233
    },
    {
      "epoch": 0.9213801796124154,
      "grad_norm": 1.2463852662867496,
      "learning_rate": 3.2231315218777204e-07,
      "loss": 0.4071,
      "step": 10234
    },
    {
      "epoch": 0.9214702108982872,
      "grad_norm": 1.9717474473107341,
      "learning_rate": 3.215791525679779e-07,
      "loss": 0.4406,
      "step": 10235
    },
    {
      "epoch": 0.921560242184159,
      "grad_norm": 1.122468956841226,
      "learning_rate": 3.208459760071037e-07,
      "loss": 0.5056,
      "step": 10236
    },
    {
      "epoch": 0.9216502734700308,
      "grad_norm": 1.4132074978907334,
      "learning_rate": 3.2011362256749723e-07,
      "loss": 0.4607,
      "step": 10237
    },
    {
      "epoch": 0.9217403047559026,
      "grad_norm": 1.321144005599186,
      "learning_rate": 3.193820923114388e-07,
      "loss": 0.5025,
      "step": 10238
    },
    {
      "epoch": 0.9218303360417746,
      "grad_norm": 1.625912107337108,
      "learning_rate": 3.186513853011375e-07,
      "loss": 0.4802,
      "step": 10239
    },
    {
      "epoch": 0.9219203673276464,
      "grad_norm": 1.3723021300399987,
      "learning_rate": 3.179215015987347e-07,
      "loss": 0.4433,
      "step": 10240
    },
    {
      "epoch": 0.9220103986135182,
      "grad_norm": 1.287130816145217,
      "learning_rate": 3.1719244126629965e-07,
      "loss": 0.4774,
      "step": 10241
    },
    {
      "epoch": 0.92210042989939,
      "grad_norm": 1.265047416343829,
      "learning_rate": 3.164642043658328e-07,
      "loss": 0.4271,
      "step": 10242
    },
    {
      "epoch": 0.9221904611852619,
      "grad_norm": 1.090935513283865,
      "learning_rate": 3.1573679095926015e-07,
      "loss": 0.6064,
      "step": 10243
    },
    {
      "epoch": 0.9222804924711338,
      "grad_norm": 1.9692577789811274,
      "learning_rate": 3.150102011084477e-07,
      "loss": 0.5066,
      "step": 10244
    },
    {
      "epoch": 0.9223705237570056,
      "grad_norm": 1.228876391860875,
      "learning_rate": 3.1428443487517945e-07,
      "loss": 0.5933,
      "step": 10245
    },
    {
      "epoch": 0.9224605550428774,
      "grad_norm": 1.295775524198426,
      "learning_rate": 3.135594923211771e-07,
      "loss": 0.4726,
      "step": 10246
    },
    {
      "epoch": 0.9225505863287492,
      "grad_norm": 1.6642913057665263,
      "learning_rate": 3.128353735080902e-07,
      "loss": 0.4718,
      "step": 10247
    },
    {
      "epoch": 0.9226406176146211,
      "grad_norm": 2.7050857977852987,
      "learning_rate": 3.121120784974996e-07,
      "loss": 0.4885,
      "step": 10248
    },
    {
      "epoch": 0.922730648900493,
      "grad_norm": 1.3440027439134843,
      "learning_rate": 3.113896073509115e-07,
      "loss": 0.4957,
      "step": 10249
    },
    {
      "epoch": 0.9228206801863648,
      "grad_norm": 1.1123790979562387,
      "learning_rate": 3.106679601297691e-07,
      "loss": 0.5038,
      "step": 10250
    },
    {
      "epoch": 0.9229107114722366,
      "grad_norm": 1.622923466817097,
      "learning_rate": 3.0994713689543874e-07,
      "loss": 0.5195,
      "step": 10251
    },
    {
      "epoch": 0.9230007427581084,
      "grad_norm": 1.3026670425753273,
      "learning_rate": 3.0922713770922155e-07,
      "loss": 0.5961,
      "step": 10252
    },
    {
      "epoch": 0.9230907740439803,
      "grad_norm": 1.3396817987584442,
      "learning_rate": 3.0850796263234506e-07,
      "loss": 0.5582,
      "step": 10253
    },
    {
      "epoch": 0.9231808053298521,
      "grad_norm": 1.5671723366884918,
      "learning_rate": 3.0778961172596933e-07,
      "loss": 0.4923,
      "step": 10254
    },
    {
      "epoch": 0.923270836615724,
      "grad_norm": 1.2641950135395381,
      "learning_rate": 3.0707208505118435e-07,
      "loss": 0.4281,
      "step": 10255
    },
    {
      "epoch": 0.9233608679015958,
      "grad_norm": 1.2675049964063168,
      "learning_rate": 3.06355382669008e-07,
      "loss": 0.4977,
      "step": 10256
    },
    {
      "epoch": 0.9234508991874677,
      "grad_norm": 1.4789351463942646,
      "learning_rate": 3.0563950464038706e-07,
      "loss": 0.4412,
      "step": 10257
    },
    {
      "epoch": 0.9235409304733395,
      "grad_norm": 1.3584096581137373,
      "learning_rate": 3.0492445102620503e-07,
      "loss": 0.4967,
      "step": 10258
    },
    {
      "epoch": 0.9236309617592113,
      "grad_norm": 1.2061600876034442,
      "learning_rate": 3.042102218872656e-07,
      "loss": 0.5457,
      "step": 10259
    },
    {
      "epoch": 0.9237209930450831,
      "grad_norm": 1.5137809944813914,
      "learning_rate": 3.0349681728431114e-07,
      "loss": 0.4961,
      "step": 10260
    },
    {
      "epoch": 0.923811024330955,
      "grad_norm": 1.393033289469765,
      "learning_rate": 3.0278423727800657e-07,
      "loss": 0.4299,
      "step": 10261
    },
    {
      "epoch": 0.9239010556168269,
      "grad_norm": 1.4769455858870275,
      "learning_rate": 3.0207248192895443e-07,
      "loss": 0.4708,
      "step": 10262
    },
    {
      "epoch": 0.9239910869026987,
      "grad_norm": 2.0443392886706024,
      "learning_rate": 3.013615512976764e-07,
      "loss": 0.4255,
      "step": 10263
    },
    {
      "epoch": 0.9240811181885705,
      "grad_norm": 1.7222866796706402,
      "learning_rate": 3.0065144544463743e-07,
      "loss": 0.4376,
      "step": 10264
    },
    {
      "epoch": 0.9241711494744423,
      "grad_norm": 1.3217302056759797,
      "learning_rate": 2.999421644302214e-07,
      "loss": 0.5104,
      "step": 10265
    },
    {
      "epoch": 0.9242611807603142,
      "grad_norm": 1.3789737884606375,
      "learning_rate": 2.992337083147467e-07,
      "loss": 0.4709,
      "step": 10266
    },
    {
      "epoch": 0.9243512120461861,
      "grad_norm": 1.3676445938759036,
      "learning_rate": 2.9852607715846194e-07,
      "loss": 0.5232,
      "step": 10267
    },
    {
      "epoch": 0.9244412433320579,
      "grad_norm": 1.6792919303229663,
      "learning_rate": 2.9781927102154217e-07,
      "loss": 0.5388,
      "step": 10268
    },
    {
      "epoch": 0.9245312746179297,
      "grad_norm": 1.1794412078810161,
      "learning_rate": 2.9711328996409715e-07,
      "loss": 0.5455,
      "step": 10269
    },
    {
      "epoch": 0.9246213059038015,
      "grad_norm": 1.0706777001248826,
      "learning_rate": 2.964081340461633e-07,
      "loss": 0.4397,
      "step": 10270
    },
    {
      "epoch": 0.9247113371896735,
      "grad_norm": 1.4872648172504217,
      "learning_rate": 2.957038033277049e-07,
      "loss": 0.4652,
      "step": 10271
    },
    {
      "epoch": 0.9248013684755453,
      "grad_norm": 1.4480404696459221,
      "learning_rate": 2.950002978686228e-07,
      "loss": 0.5906,
      "step": 10272
    },
    {
      "epoch": 0.9248913997614171,
      "grad_norm": 1.6182641741091615,
      "learning_rate": 2.9429761772874153e-07,
      "loss": 0.5642,
      "step": 10273
    },
    {
      "epoch": 0.9249814310472889,
      "grad_norm": 1.7556050578408475,
      "learning_rate": 2.935957629678166e-07,
      "loss": 0.5029,
      "step": 10274
    },
    {
      "epoch": 0.9250714623331607,
      "grad_norm": 3.071205587491221,
      "learning_rate": 2.9289473364553566e-07,
      "loss": 0.5019,
      "step": 10275
    },
    {
      "epoch": 0.9251614936190327,
      "grad_norm": 1.0611359316175575,
      "learning_rate": 2.9219452982151565e-07,
      "loss": 0.4554,
      "step": 10276
    },
    {
      "epoch": 0.9252515249049045,
      "grad_norm": 1.546934483351073,
      "learning_rate": 2.914951515552977e-07,
      "loss": 0.5041,
      "step": 10277
    },
    {
      "epoch": 0.9253415561907763,
      "grad_norm": 1.275034380719066,
      "learning_rate": 2.9079659890636324e-07,
      "loss": 0.5021,
      "step": 10278
    },
    {
      "epoch": 0.9254315874766481,
      "grad_norm": 1.4499342335591432,
      "learning_rate": 2.9009887193411356e-07,
      "loss": 0.5232,
      "step": 10279
    },
    {
      "epoch": 0.9255216187625199,
      "grad_norm": 1.2340962378940386,
      "learning_rate": 2.8940197069788456e-07,
      "loss": 0.5515,
      "step": 10280
    },
    {
      "epoch": 0.9256116500483919,
      "grad_norm": 1.3765605459984362,
      "learning_rate": 2.8870589525694213e-07,
      "loss": 0.4746,
      "step": 10281
    },
    {
      "epoch": 0.9257016813342637,
      "grad_norm": 1.4055627230367571,
      "learning_rate": 2.880106456704812e-07,
      "loss": 0.617,
      "step": 10282
    },
    {
      "epoch": 0.9257917126201355,
      "grad_norm": 1.4268073290255385,
      "learning_rate": 2.8731622199762446e-07,
      "loss": 0.4547,
      "step": 10283
    },
    {
      "epoch": 0.9258817439060073,
      "grad_norm": 2.321069970010309,
      "learning_rate": 2.8662262429742905e-07,
      "loss": 0.5204,
      "step": 10284
    },
    {
      "epoch": 0.9259717751918792,
      "grad_norm": 1.0164561706488904,
      "learning_rate": 2.8592985262887454e-07,
      "loss": 0.4602,
      "step": 10285
    },
    {
      "epoch": 0.926061806477751,
      "grad_norm": 1.1598699704842965,
      "learning_rate": 2.8523790705087927e-07,
      "loss": 0.5628,
      "step": 10286
    },
    {
      "epoch": 0.9261518377636229,
      "grad_norm": 1.5469497480629404,
      "learning_rate": 2.8454678762228407e-07,
      "loss": 0.4954,
      "step": 10287
    },
    {
      "epoch": 0.9262418690494947,
      "grad_norm": 1.34451522381472,
      "learning_rate": 2.838564944018618e-07,
      "loss": 0.5902,
      "step": 10288
    },
    {
      "epoch": 0.9263319003353665,
      "grad_norm": 1.1034185313226967,
      "learning_rate": 2.831670274483178e-07,
      "loss": 0.4937,
      "step": 10289
    },
    {
      "epoch": 0.9264219316212384,
      "grad_norm": 1.3257615166185592,
      "learning_rate": 2.8247838682028406e-07,
      "loss": 0.4783,
      "step": 10290
    },
    {
      "epoch": 0.9265119629071102,
      "grad_norm": 1.7689443644219616,
      "learning_rate": 2.8179057257631925e-07,
      "loss": 0.5225,
      "step": 10291
    },
    {
      "epoch": 0.9266019941929821,
      "grad_norm": 1.8064239865655953,
      "learning_rate": 2.8110358477492104e-07,
      "loss": 0.5502,
      "step": 10292
    },
    {
      "epoch": 0.9266920254788539,
      "grad_norm": 1.9555479154028101,
      "learning_rate": 2.8041742347450827e-07,
      "loss": 0.4918,
      "step": 10293
    },
    {
      "epoch": 0.9267820567647257,
      "grad_norm": 1.2398856189635614,
      "learning_rate": 2.7973208873343316e-07,
      "loss": 0.5634,
      "step": 10294
    },
    {
      "epoch": 0.9268720880505976,
      "grad_norm": 2.989607976347268,
      "learning_rate": 2.790475806099768e-07,
      "loss": 0.4966,
      "step": 10295
    },
    {
      "epoch": 0.9269621193364694,
      "grad_norm": 1.8959374759200476,
      "learning_rate": 2.783638991623505e-07,
      "loss": 0.5291,
      "step": 10296
    },
    {
      "epoch": 0.9270521506223413,
      "grad_norm": 1.1802087157228676,
      "learning_rate": 2.776810444486944e-07,
      "loss": 0.5418,
      "step": 10297
    },
    {
      "epoch": 0.9271421819082131,
      "grad_norm": 1.1783960711188959,
      "learning_rate": 2.7699901652708085e-07,
      "loss": 0.5203,
      "step": 10298
    },
    {
      "epoch": 0.927232213194085,
      "grad_norm": 1.1306154095722278,
      "learning_rate": 2.763178154555057e-07,
      "loss": 0.4859,
      "step": 10299
    },
    {
      "epoch": 0.9273222444799568,
      "grad_norm": 1.1296513896465945,
      "learning_rate": 2.7563744129190384e-07,
      "loss": 0.443,
      "step": 10300
    },
    {
      "epoch": 0.9274122757658286,
      "grad_norm": 1.8603299692545283,
      "learning_rate": 2.749578940941311e-07,
      "loss": 0.4864,
      "step": 10301
    },
    {
      "epoch": 0.9275023070517004,
      "grad_norm": 1.5861199320212147,
      "learning_rate": 2.7427917391997795e-07,
      "loss": 0.533,
      "step": 10302
    },
    {
      "epoch": 0.9275923383375723,
      "grad_norm": 1.8104898566011884,
      "learning_rate": 2.7360128082716267e-07,
      "loss": 0.5625,
      "step": 10303
    },
    {
      "epoch": 0.9276823696234442,
      "grad_norm": 1.4155205752244473,
      "learning_rate": 2.7292421487333467e-07,
      "loss": 0.4748,
      "step": 10304
    },
    {
      "epoch": 0.927772400909316,
      "grad_norm": 2.1531592079855453,
      "learning_rate": 2.722479761160701e-07,
      "loss": 0.5184,
      "step": 10305
    },
    {
      "epoch": 0.9278624321951878,
      "grad_norm": 1.3123477617950856,
      "learning_rate": 2.715725646128786e-07,
      "loss": 0.5527,
      "step": 10306
    },
    {
      "epoch": 0.9279524634810596,
      "grad_norm": 2.0350324082013618,
      "learning_rate": 2.708979804211975e-07,
      "loss": 0.4616,
      "step": 10307
    },
    {
      "epoch": 0.9280424947669315,
      "grad_norm": 1.4880509568728748,
      "learning_rate": 2.7022422359839316e-07,
      "loss": 0.4692,
      "step": 10308
    },
    {
      "epoch": 0.9281325260528034,
      "grad_norm": 1.3918161581659463,
      "learning_rate": 2.6955129420176193e-07,
      "loss": 0.438,
      "step": 10309
    },
    {
      "epoch": 0.9282225573386752,
      "grad_norm": 1.2037470660637222,
      "learning_rate": 2.6887919228853144e-07,
      "loss": 0.454,
      "step": 10310
    },
    {
      "epoch": 0.928312588624547,
      "grad_norm": 1.2239675790541313,
      "learning_rate": 2.6820791791585586e-07,
      "loss": 0.4725,
      "step": 10311
    },
    {
      "epoch": 0.9284026199104188,
      "grad_norm": 1.3510583755831223,
      "learning_rate": 2.6753747114082294e-07,
      "loss": 0.5617,
      "step": 10312
    },
    {
      "epoch": 0.9284926511962908,
      "grad_norm": 1.247629757683242,
      "learning_rate": 2.6686785202044594e-07,
      "loss": 0.408,
      "step": 10313
    },
    {
      "epoch": 0.9285826824821626,
      "grad_norm": 0.9984388620229199,
      "learning_rate": 2.661990606116716e-07,
      "loss": 0.4153,
      "step": 10314
    },
    {
      "epoch": 0.9286727137680344,
      "grad_norm": 1.0802946588792819,
      "learning_rate": 2.65531096971372e-07,
      "loss": 0.4683,
      "step": 10315
    },
    {
      "epoch": 0.9287627450539062,
      "grad_norm": 1.394222805407795,
      "learning_rate": 2.6486396115635304e-07,
      "loss": 0.5483,
      "step": 10316
    },
    {
      "epoch": 0.928852776339778,
      "grad_norm": 2.2201371095166973,
      "learning_rate": 2.6419765322334814e-07,
      "loss": 0.4404,
      "step": 10317
    },
    {
      "epoch": 0.92894280762565,
      "grad_norm": 1.6363196664928161,
      "learning_rate": 2.635321732290208e-07,
      "loss": 0.573,
      "step": 10318
    },
    {
      "epoch": 0.9290328389115218,
      "grad_norm": 1.1460421060375008,
      "learning_rate": 2.6286752122996026e-07,
      "loss": 0.5847,
      "step": 10319
    },
    {
      "epoch": 0.9291228701973936,
      "grad_norm": 1.6783059943419771,
      "learning_rate": 2.622036972826947e-07,
      "loss": 0.595,
      "step": 10320
    },
    {
      "epoch": 0.9292129014832654,
      "grad_norm": 1.457021620182642,
      "learning_rate": 2.6154070144367326e-07,
      "loss": 0.4871,
      "step": 10321
    },
    {
      "epoch": 0.9293029327691372,
      "grad_norm": 1.7559890995515148,
      "learning_rate": 2.608785337692765e-07,
      "loss": 0.5766,
      "step": 10322
    },
    {
      "epoch": 0.9293929640550092,
      "grad_norm": 1.1197133664461754,
      "learning_rate": 2.6021719431581604e-07,
      "loss": 0.516,
      "step": 10323
    },
    {
      "epoch": 0.929482995340881,
      "grad_norm": 1.3480658599336173,
      "learning_rate": 2.5955668313953466e-07,
      "loss": 0.5007,
      "step": 10324
    },
    {
      "epoch": 0.9295730266267528,
      "grad_norm": 1.3326209529493191,
      "learning_rate": 2.588970002966007e-07,
      "loss": 0.486,
      "step": 10325
    },
    {
      "epoch": 0.9296630579126246,
      "grad_norm": 2.4675049067838652,
      "learning_rate": 2.582381458431149e-07,
      "loss": 0.5621,
      "step": 10326
    },
    {
      "epoch": 0.9297530891984965,
      "grad_norm": 1.7470355447547579,
      "learning_rate": 2.575801198351047e-07,
      "loss": 0.6072,
      "step": 10327
    },
    {
      "epoch": 0.9298431204843683,
      "grad_norm": 1.4976272461355393,
      "learning_rate": 2.56922922328533e-07,
      "loss": 0.4369,
      "step": 10328
    },
    {
      "epoch": 0.9299331517702402,
      "grad_norm": 1.2667990062235768,
      "learning_rate": 2.562665533792841e-07,
      "loss": 0.6303,
      "step": 10329
    },
    {
      "epoch": 0.930023183056112,
      "grad_norm": 1.7851956306446313,
      "learning_rate": 2.556110130431788e-07,
      "loss": 0.4866,
      "step": 10330
    },
    {
      "epoch": 0.9301132143419838,
      "grad_norm": 1.3815564095476043,
      "learning_rate": 2.549563013759626e-07,
      "loss": 0.4726,
      "step": 10331
    },
    {
      "epoch": 0.9302032456278557,
      "grad_norm": 1.811781492451002,
      "learning_rate": 2.543024184333154e-07,
      "loss": 0.5266,
      "step": 10332
    },
    {
      "epoch": 0.9302932769137275,
      "grad_norm": 1.660585450096157,
      "learning_rate": 2.536493642708393e-07,
      "loss": 0.5237,
      "step": 10333
    },
    {
      "epoch": 0.9303833081995994,
      "grad_norm": 1.4523031944975808,
      "learning_rate": 2.529971389440755e-07,
      "loss": 0.5945,
      "step": 10334
    },
    {
      "epoch": 0.9304733394854712,
      "grad_norm": 1.2819353366214117,
      "learning_rate": 2.5234574250848634e-07,
      "loss": 0.5582,
      "step": 10335
    },
    {
      "epoch": 0.930563370771343,
      "grad_norm": 1.3935275749128353,
      "learning_rate": 2.516951750194685e-07,
      "loss": 0.5912,
      "step": 10336
    },
    {
      "epoch": 0.9306534020572149,
      "grad_norm": 1.3419142130868331,
      "learning_rate": 2.5104543653234557e-07,
      "loss": 0.4883,
      "step": 10337
    },
    {
      "epoch": 0.9307434333430867,
      "grad_norm": 2.7308032951942147,
      "learning_rate": 2.5039652710237226e-07,
      "loss": 0.4653,
      "step": 10338
    },
    {
      "epoch": 0.9308334646289586,
      "grad_norm": 1.9699887788578927,
      "learning_rate": 2.497484467847322e-07,
      "loss": 0.4728,
      "step": 10339
    },
    {
      "epoch": 0.9309234959148304,
      "grad_norm": 2.009716684863621,
      "learning_rate": 2.4910119563453903e-07,
      "loss": 0.4941,
      "step": 10340
    },
    {
      "epoch": 0.9310135272007023,
      "grad_norm": 1.4210646554438031,
      "learning_rate": 2.4845477370683436e-07,
      "loss": 0.4977,
      "step": 10341
    },
    {
      "epoch": 0.9311035584865741,
      "grad_norm": 1.4978632800643654,
      "learning_rate": 2.478091810565919e-07,
      "loss": 0.4498,
      "step": 10342
    },
    {
      "epoch": 0.9311935897724459,
      "grad_norm": 1.48641877351841,
      "learning_rate": 2.471644177387111e-07,
      "loss": 0.5914,
      "step": 10343
    },
    {
      "epoch": 0.9312836210583177,
      "grad_norm": 1.610553985603744,
      "learning_rate": 2.465204838080248e-07,
      "loss": 0.4779,
      "step": 10344
    },
    {
      "epoch": 0.9313736523441896,
      "grad_norm": 1.476083974990566,
      "learning_rate": 2.4587737931929256e-07,
      "loss": 0.5449,
      "step": 10345
    },
    {
      "epoch": 0.9314636836300615,
      "grad_norm": 1.8353178602382578,
      "learning_rate": 2.45235104327205e-07,
      "loss": 0.5557,
      "step": 10346
    },
    {
      "epoch": 0.9315537149159333,
      "grad_norm": 1.1314394939419676,
      "learning_rate": 2.445936588863806e-07,
      "loss": 0.5454,
      "step": 10347
    },
    {
      "epoch": 0.9316437462018051,
      "grad_norm": 1.3010622635110072,
      "learning_rate": 2.4395304305137036e-07,
      "loss": 0.4452,
      "step": 10348
    },
    {
      "epoch": 0.9317337774876769,
      "grad_norm": 1.5492367998066658,
      "learning_rate": 2.4331325687665054e-07,
      "loss": 0.4717,
      "step": 10349
    },
    {
      "epoch": 0.9318238087735488,
      "grad_norm": 1.3982271536865645,
      "learning_rate": 2.4267430041662874e-07,
      "loss": 0.5512,
      "step": 10350
    },
    {
      "epoch": 0.9319138400594207,
      "grad_norm": 1.3800448430331467,
      "learning_rate": 2.420361737256438e-07,
      "loss": 0.6158,
      "step": 10351
    },
    {
      "epoch": 0.9320038713452925,
      "grad_norm": 1.4634907163940774,
      "learning_rate": 2.4139887685796224e-07,
      "loss": 0.5335,
      "step": 10352
    },
    {
      "epoch": 0.9320939026311643,
      "grad_norm": 1.3044505506226607,
      "learning_rate": 2.4076240986777964e-07,
      "loss": 0.5176,
      "step": 10353
    },
    {
      "epoch": 0.9321839339170361,
      "grad_norm": 1.6077983517695928,
      "learning_rate": 2.4012677280922157e-07,
      "loss": 0.4613,
      "step": 10354
    },
    {
      "epoch": 0.9322739652029081,
      "grad_norm": 1.171923381066238,
      "learning_rate": 2.394919657363426e-07,
      "loss": 0.4557,
      "step": 10355
    },
    {
      "epoch": 0.9323639964887799,
      "grad_norm": 1.0589781297477268,
      "learning_rate": 2.3885798870312837e-07,
      "loss": 0.4195,
      "step": 10356
    },
    {
      "epoch": 0.9324540277746517,
      "grad_norm": 1.6047352958117573,
      "learning_rate": 2.3822484176349137e-07,
      "loss": 0.5486,
      "step": 10357
    },
    {
      "epoch": 0.9325440590605235,
      "grad_norm": 1.336585341612461,
      "learning_rate": 2.3759252497127515e-07,
      "loss": 0.5699,
      "step": 10358
    },
    {
      "epoch": 0.9326340903463953,
      "grad_norm": 2.5707082892078965,
      "learning_rate": 2.369610383802523e-07,
      "loss": 0.5122,
      "step": 10359
    },
    {
      "epoch": 0.9327241216322673,
      "grad_norm": 1.8843964342062984,
      "learning_rate": 2.363303820441265e-07,
      "loss": 0.478,
      "step": 10360
    },
    {
      "epoch": 0.9328141529181391,
      "grad_norm": 2.253541555301352,
      "learning_rate": 2.3570055601652596e-07,
      "loss": 0.4799,
      "step": 10361
    },
    {
      "epoch": 0.9329041842040109,
      "grad_norm": 1.6383949321833429,
      "learning_rate": 2.3507156035101452e-07,
      "loss": 0.5665,
      "step": 10362
    },
    {
      "epoch": 0.9329942154898827,
      "grad_norm": 1.33120215230819,
      "learning_rate": 2.344433951010805e-07,
      "loss": 0.4698,
      "step": 10363
    },
    {
      "epoch": 0.9330842467757545,
      "grad_norm": 1.4053927196397826,
      "learning_rate": 2.3381606032014337e-07,
      "loss": 0.4491,
      "step": 10364
    },
    {
      "epoch": 0.9331742780616264,
      "grad_norm": 1.361607057470782,
      "learning_rate": 2.331895560615538e-07,
      "loss": 0.6197,
      "step": 10365
    },
    {
      "epoch": 0.9332643093474983,
      "grad_norm": 2.044749800394203,
      "learning_rate": 2.3256388237858806e-07,
      "loss": 0.5262,
      "step": 10366
    },
    {
      "epoch": 0.9333543406333701,
      "grad_norm": 1.4656224884301636,
      "learning_rate": 2.319390393244547e-07,
      "loss": 0.4986,
      "step": 10367
    },
    {
      "epoch": 0.9334443719192419,
      "grad_norm": 2.7705269595908337,
      "learning_rate": 2.3131502695229235e-07,
      "loss": 0.4662,
      "step": 10368
    },
    {
      "epoch": 0.9335344032051138,
      "grad_norm": 1.2029575734658098,
      "learning_rate": 2.3069184531516408e-07,
      "loss": 0.5064,
      "step": 10369
    },
    {
      "epoch": 0.9336244344909856,
      "grad_norm": 1.3688777366245295,
      "learning_rate": 2.3006949446606864e-07,
      "loss": 0.5119,
      "step": 10370
    },
    {
      "epoch": 0.9337144657768575,
      "grad_norm": 1.3549940305323267,
      "learning_rate": 2.2944797445792921e-07,
      "loss": 0.5307,
      "step": 10371
    },
    {
      "epoch": 0.9338044970627293,
      "grad_norm": 1.4934949369112522,
      "learning_rate": 2.2882728534360131e-07,
      "loss": 0.4492,
      "step": 10372
    },
    {
      "epoch": 0.9338945283486011,
      "grad_norm": 2.046257256779721,
      "learning_rate": 2.282074271758683e-07,
      "loss": 0.5472,
      "step": 10373
    },
    {
      "epoch": 0.933984559634473,
      "grad_norm": 1.5766199223799142,
      "learning_rate": 2.2758840000744463e-07,
      "loss": 0.465,
      "step": 10374
    },
    {
      "epoch": 0.9340745909203448,
      "grad_norm": 1.1245572109649336,
      "learning_rate": 2.2697020389096936e-07,
      "loss": 0.5249,
      "step": 10375
    },
    {
      "epoch": 0.9341646222062167,
      "grad_norm": 1.7787166247105985,
      "learning_rate": 2.2635283887901817e-07,
      "loss": 0.5023,
      "step": 10376
    },
    {
      "epoch": 0.9342546534920885,
      "grad_norm": 1.3732694213123062,
      "learning_rate": 2.2573630502408906e-07,
      "loss": 0.492,
      "step": 10377
    },
    {
      "epoch": 0.9343446847779603,
      "grad_norm": 1.5267222643617275,
      "learning_rate": 2.2512060237861455e-07,
      "loss": 0.4783,
      "step": 10378
    },
    {
      "epoch": 0.9344347160638322,
      "grad_norm": 1.208827856648326,
      "learning_rate": 2.2450573099495387e-07,
      "loss": 0.4524,
      "step": 10379
    },
    {
      "epoch": 0.934524747349704,
      "grad_norm": 2.074964834008492,
      "learning_rate": 2.238916909253952e-07,
      "loss": 0.5407,
      "step": 10380
    },
    {
      "epoch": 0.9346147786355758,
      "grad_norm": 1.227095078027101,
      "learning_rate": 2.2327848222215787e-07,
      "loss": 0.4522,
      "step": 10381
    },
    {
      "epoch": 0.9347048099214477,
      "grad_norm": 1.3461784512575674,
      "learning_rate": 2.2266610493738905e-07,
      "loss": 0.4851,
      "step": 10382
    },
    {
      "epoch": 0.9347948412073196,
      "grad_norm": 1.5850051026961565,
      "learning_rate": 2.2205455912316598e-07,
      "loss": 0.4816,
      "step": 10383
    },
    {
      "epoch": 0.9348848724931914,
      "grad_norm": 1.2510415011102778,
      "learning_rate": 2.2144384483149484e-07,
      "loss": 0.6097,
      "step": 10384
    },
    {
      "epoch": 0.9349749037790632,
      "grad_norm": 1.2261073502598343,
      "learning_rate": 2.2083396211431075e-07,
      "loss": 0.4717,
      "step": 10385
    },
    {
      "epoch": 0.935064935064935,
      "grad_norm": 2.1420358721307795,
      "learning_rate": 2.2022491102347888e-07,
      "loss": 0.502,
      "step": 10386
    },
    {
      "epoch": 0.9351549663508069,
      "grad_norm": 1.5828506708617456,
      "learning_rate": 2.1961669161079336e-07,
      "loss": 0.5141,
      "step": 10387
    },
    {
      "epoch": 0.9352449976366788,
      "grad_norm": 1.72059578603397,
      "learning_rate": 2.1900930392797837e-07,
      "loss": 0.5139,
      "step": 10388
    },
    {
      "epoch": 0.9353350289225506,
      "grad_norm": 1.672975094115083,
      "learning_rate": 2.1840274802668372e-07,
      "loss": 0.5435,
      "step": 10389
    },
    {
      "epoch": 0.9354250602084224,
      "grad_norm": 1.2988865670141128,
      "learning_rate": 2.1779702395849479e-07,
      "loss": 0.4475,
      "step": 10390
    },
    {
      "epoch": 0.9355150914942942,
      "grad_norm": 1.7068906417281424,
      "learning_rate": 2.1719213177492038e-07,
      "loss": 0.4261,
      "step": 10391
    },
    {
      "epoch": 0.935605122780166,
      "grad_norm": 0.9984366891415578,
      "learning_rate": 2.1658807152740157e-07,
      "loss": 0.4945,
      "step": 10392
    },
    {
      "epoch": 0.935695154066038,
      "grad_norm": 0.9520931082782647,
      "learning_rate": 2.159848432673084e-07,
      "loss": 0.466,
      "step": 10393
    },
    {
      "epoch": 0.9357851853519098,
      "grad_norm": 1.4616012260121398,
      "learning_rate": 2.153824470459387e-07,
      "loss": 0.4521,
      "step": 10394
    },
    {
      "epoch": 0.9358752166377816,
      "grad_norm": 1.363829297233515,
      "learning_rate": 2.1478088291452148e-07,
      "loss": 0.4577,
      "step": 10395
    },
    {
      "epoch": 0.9359652479236534,
      "grad_norm": 0.9801329872760118,
      "learning_rate": 2.1418015092421585e-07,
      "loss": 0.5716,
      "step": 10396
    },
    {
      "epoch": 0.9360552792095254,
      "grad_norm": 1.1986945161579945,
      "learning_rate": 2.1358025112610425e-07,
      "loss": 0.4043,
      "step": 10397
    },
    {
      "epoch": 0.9361453104953972,
      "grad_norm": 1.916492624902627,
      "learning_rate": 2.12981183571207e-07,
      "loss": 0.4047,
      "step": 10398
    },
    {
      "epoch": 0.936235341781269,
      "grad_norm": 1.4032345141965685,
      "learning_rate": 2.1238294831046558e-07,
      "loss": 0.5536,
      "step": 10399
    },
    {
      "epoch": 0.9363253730671408,
      "grad_norm": 1.3929454986741918,
      "learning_rate": 2.1178554539475593e-07,
      "loss": 0.6601,
      "step": 10400
    },
    {
      "epoch": 0.9364154043530126,
      "grad_norm": 1.761755069636614,
      "learning_rate": 2.111889748748819e-07,
      "loss": 0.4947,
      "step": 10401
    },
    {
      "epoch": 0.9365054356388846,
      "grad_norm": 1.4842931368803487,
      "learning_rate": 2.1059323680157618e-07,
      "loss": 0.5761,
      "step": 10402
    },
    {
      "epoch": 0.9365954669247564,
      "grad_norm": 1.2089357442990296,
      "learning_rate": 2.0999833122549828e-07,
      "loss": 0.6205,
      "step": 10403
    },
    {
      "epoch": 0.9366854982106282,
      "grad_norm": 2.8049933170839396,
      "learning_rate": 2.094042581972433e-07,
      "loss": 0.5748,
      "step": 10404
    },
    {
      "epoch": 0.9367755294965,
      "grad_norm": 1.4099613932876205,
      "learning_rate": 2.088110177673297e-07,
      "loss": 0.5139,
      "step": 10405
    },
    {
      "epoch": 0.9368655607823718,
      "grad_norm": 1.7770766329010403,
      "learning_rate": 2.0821860998620602e-07,
      "loss": 0.562,
      "step": 10406
    },
    {
      "epoch": 0.9369555920682437,
      "grad_norm": 1.2474239347376839,
      "learning_rate": 2.0762703490425084e-07,
      "loss": 0.5719,
      "step": 10407
    },
    {
      "epoch": 0.9370456233541156,
      "grad_norm": 1.5314021072239465,
      "learning_rate": 2.0703629257177503e-07,
      "loss": 0.5001,
      "step": 10408
    },
    {
      "epoch": 0.9371356546399874,
      "grad_norm": 1.4603057080265145,
      "learning_rate": 2.064463830390129e-07,
      "loss": 0.4073,
      "step": 10409
    },
    {
      "epoch": 0.9372256859258592,
      "grad_norm": 2.415502093719365,
      "learning_rate": 2.05857306356132e-07,
      "loss": 0.4577,
      "step": 10410
    },
    {
      "epoch": 0.9373157172117311,
      "grad_norm": 1.4010627931456816,
      "learning_rate": 2.0526906257322565e-07,
      "loss": 0.447,
      "step": 10411
    },
    {
      "epoch": 0.9374057484976029,
      "grad_norm": 1.4185378220911125,
      "learning_rate": 2.0468165174032274e-07,
      "loss": 0.5288,
      "step": 10412
    },
    {
      "epoch": 0.9374957797834748,
      "grad_norm": 1.5527048436916375,
      "learning_rate": 2.0409507390737327e-07,
      "loss": 0.5597,
      "step": 10413
    },
    {
      "epoch": 0.9375858110693466,
      "grad_norm": 1.8368967922502586,
      "learning_rate": 2.035093291242607e-07,
      "loss": 0.4596,
      "step": 10414
    },
    {
      "epoch": 0.9376758423552184,
      "grad_norm": 1.6767890311054052,
      "learning_rate": 2.029244174407985e-07,
      "loss": 0.5861,
      "step": 10415
    },
    {
      "epoch": 0.9377658736410903,
      "grad_norm": 2.988445598139934,
      "learning_rate": 2.02340338906728e-07,
      "loss": 0.5417,
      "step": 10416
    },
    {
      "epoch": 0.9378559049269621,
      "grad_norm": 1.537296554858624,
      "learning_rate": 2.0175709357171724e-07,
      "loss": 0.5071,
      "step": 10417
    },
    {
      "epoch": 0.937945936212834,
      "grad_norm": 2.0841306963892534,
      "learning_rate": 2.011746814853699e-07,
      "loss": 0.5309,
      "step": 10418
    },
    {
      "epoch": 0.9380359674987058,
      "grad_norm": 1.7238058886070629,
      "learning_rate": 2.0059310269720965e-07,
      "loss": 0.5037,
      "step": 10419
    },
    {
      "epoch": 0.9381259987845776,
      "grad_norm": 1.2077639612834083,
      "learning_rate": 2.000123572566981e-07,
      "loss": 0.4959,
      "step": 10420
    },
    {
      "epoch": 0.9382160300704495,
      "grad_norm": 1.9641772263889512,
      "learning_rate": 1.9943244521322014e-07,
      "loss": 0.4872,
      "step": 10421
    },
    {
      "epoch": 0.9383060613563213,
      "grad_norm": 1.6625341771942956,
      "learning_rate": 1.98853366616093e-07,
      "loss": 0.5413,
      "step": 10422
    },
    {
      "epoch": 0.9383960926421931,
      "grad_norm": 2.1949399020182327,
      "learning_rate": 1.9827512151456175e-07,
      "loss": 0.5704,
      "step": 10423
    },
    {
      "epoch": 0.938486123928065,
      "grad_norm": 1.7136435100234715,
      "learning_rate": 1.9769770995780035e-07,
      "loss": 0.5293,
      "step": 10424
    },
    {
      "epoch": 0.9385761552139369,
      "grad_norm": 1.986483797704447,
      "learning_rate": 1.9712113199491178e-07,
      "loss": 0.5427,
      "step": 10425
    },
    {
      "epoch": 0.9386661864998087,
      "grad_norm": 1.1535906234679079,
      "learning_rate": 1.9654538767493014e-07,
      "loss": 0.5057,
      "step": 10426
    },
    {
      "epoch": 0.9387562177856805,
      "grad_norm": 2.0216876408066087,
      "learning_rate": 1.9597047704681626e-07,
      "loss": 0.5811,
      "step": 10427
    },
    {
      "epoch": 0.9388462490715523,
      "grad_norm": 1.8751123629991049,
      "learning_rate": 1.9539640015945994e-07,
      "loss": 0.5447,
      "step": 10428
    },
    {
      "epoch": 0.9389362803574242,
      "grad_norm": 1.4895962312854,
      "learning_rate": 1.9482315706168098e-07,
      "loss": 0.4167,
      "step": 10429
    },
    {
      "epoch": 0.9390263116432961,
      "grad_norm": 1.994180256791142,
      "learning_rate": 1.9425074780223151e-07,
      "loss": 0.422,
      "step": 10430
    },
    {
      "epoch": 0.9391163429291679,
      "grad_norm": 1.616474159042691,
      "learning_rate": 1.9367917242978485e-07,
      "loss": 0.5854,
      "step": 10431
    },
    {
      "epoch": 0.9392063742150397,
      "grad_norm": 1.9012931950523086,
      "learning_rate": 1.9310843099295206e-07,
      "loss": 0.4848,
      "step": 10432
    },
    {
      "epoch": 0.9392964055009115,
      "grad_norm": 1.5121135538626287,
      "learning_rate": 1.925385235402677e-07,
      "loss": 0.4759,
      "step": 10433
    },
    {
      "epoch": 0.9393864367867834,
      "grad_norm": 5.459158509970004,
      "learning_rate": 1.9196945012019629e-07,
      "loss": 0.4767,
      "step": 10434
    },
    {
      "epoch": 0.9394764680726553,
      "grad_norm": 1.288349432512523,
      "learning_rate": 1.914012107811336e-07,
      "loss": 0.4958,
      "step": 10435
    },
    {
      "epoch": 0.9395664993585271,
      "grad_norm": 1.6385553906496213,
      "learning_rate": 1.9083380557140208e-07,
      "loss": 0.4752,
      "step": 10436
    },
    {
      "epoch": 0.9396565306443989,
      "grad_norm": 1.9775319561271374,
      "learning_rate": 1.9026723453925534e-07,
      "loss": 0.488,
      "step": 10437
    },
    {
      "epoch": 0.9397465619302707,
      "grad_norm": 1.2327815428776534,
      "learning_rate": 1.8970149773287484e-07,
      "loss": 0.3733,
      "step": 10438
    },
    {
      "epoch": 0.9398365932161427,
      "grad_norm": 3.08468882204994,
      "learning_rate": 1.8913659520036876e-07,
      "loss": 0.4724,
      "step": 10439
    },
    {
      "epoch": 0.9399266245020145,
      "grad_norm": 1.798659038527902,
      "learning_rate": 1.885725269897809e-07,
      "loss": 0.5621,
      "step": 10440
    },
    {
      "epoch": 0.9400166557878863,
      "grad_norm": 1.0798392677619892,
      "learning_rate": 1.8800929314907624e-07,
      "loss": 0.506,
      "step": 10441
    },
    {
      "epoch": 0.9401066870737581,
      "grad_norm": 1.1539529564509137,
      "learning_rate": 1.874468937261531e-07,
      "loss": 0.5453,
      "step": 10442
    },
    {
      "epoch": 0.9401967183596299,
      "grad_norm": 2.12588459856618,
      "learning_rate": 1.8688532876883992e-07,
      "loss": 0.4444,
      "step": 10443
    },
    {
      "epoch": 0.9402867496455019,
      "grad_norm": 1.9428773477711436,
      "learning_rate": 1.863245983248929e-07,
      "loss": 0.5625,
      "step": 10444
    },
    {
      "epoch": 0.9403767809313737,
      "grad_norm": 1.2615754765655987,
      "learning_rate": 1.8576470244199397e-07,
      "loss": 0.4894,
      "step": 10445
    },
    {
      "epoch": 0.9404668122172455,
      "grad_norm": 1.9201955523526066,
      "learning_rate": 1.8520564116776053e-07,
      "loss": 0.5844,
      "step": 10446
    },
    {
      "epoch": 0.9405568435031173,
      "grad_norm": 1.9752819839055076,
      "learning_rate": 1.8464741454973345e-07,
      "loss": 0.4916,
      "step": 10447
    },
    {
      "epoch": 0.9406468747889892,
      "grad_norm": 1.2343203231277284,
      "learning_rate": 1.8409002263538366e-07,
      "loss": 0.5389,
      "step": 10448
    },
    {
      "epoch": 0.940736906074861,
      "grad_norm": 1.6661572313899682,
      "learning_rate": 1.8353346547211438e-07,
      "loss": 0.5999,
      "step": 10449
    },
    {
      "epoch": 0.9408269373607329,
      "grad_norm": 1.5742351866160122,
      "learning_rate": 1.8297774310725436e-07,
      "loss": 0.4853,
      "step": 10450
    },
    {
      "epoch": 0.9409169686466047,
      "grad_norm": 1.2552441704287225,
      "learning_rate": 1.824228555880625e-07,
      "loss": 0.5319,
      "step": 10451
    },
    {
      "epoch": 0.9410069999324765,
      "grad_norm": 1.4010408991712548,
      "learning_rate": 1.8186880296172883e-07,
      "loss": 0.5326,
      "step": 10452
    },
    {
      "epoch": 0.9410970312183484,
      "grad_norm": 1.9102597714809513,
      "learning_rate": 1.8131558527536674e-07,
      "loss": 0.4796,
      "step": 10453
    },
    {
      "epoch": 0.9411870625042202,
      "grad_norm": 1.632868303449485,
      "learning_rate": 1.8076320257602532e-07,
      "loss": 0.4768,
      "step": 10454
    },
    {
      "epoch": 0.9412770937900921,
      "grad_norm": 1.67886019980938,
      "learning_rate": 1.8021165491067804e-07,
      "loss": 0.568,
      "step": 10455
    },
    {
      "epoch": 0.9413671250759639,
      "grad_norm": 1.4366272101892963,
      "learning_rate": 1.7966094232622856e-07,
      "loss": 0.4567,
      "step": 10456
    },
    {
      "epoch": 0.9414571563618357,
      "grad_norm": 2.2401257717693754,
      "learning_rate": 1.7911106486951158e-07,
      "loss": 0.4734,
      "step": 10457
    },
    {
      "epoch": 0.9415471876477076,
      "grad_norm": 1.4039353256655671,
      "learning_rate": 1.785620225872875e-07,
      "loss": 0.4782,
      "step": 10458
    },
    {
      "epoch": 0.9416372189335794,
      "grad_norm": 1.4827024833259095,
      "learning_rate": 1.7801381552624565e-07,
      "loss": 0.4486,
      "step": 10459
    },
    {
      "epoch": 0.9417272502194513,
      "grad_norm": 1.439124270021527,
      "learning_rate": 1.774664437330109e-07,
      "loss": 0.5134,
      "step": 10460
    },
    {
      "epoch": 0.9418172815053231,
      "grad_norm": 1.8807967326318153,
      "learning_rate": 1.7691990725412723e-07,
      "loss": 0.5022,
      "step": 10461
    },
    {
      "epoch": 0.941907312791195,
      "grad_norm": 2.416478019039699,
      "learning_rate": 1.7637420613607404e-07,
      "loss": 0.5699,
      "step": 10462
    },
    {
      "epoch": 0.9419973440770668,
      "grad_norm": 1.3161917670892649,
      "learning_rate": 1.7582934042525868e-07,
      "loss": 0.4973,
      "step": 10463
    },
    {
      "epoch": 0.9420873753629386,
      "grad_norm": 2.0067196854103773,
      "learning_rate": 1.752853101680163e-07,
      "loss": 0.5105,
      "step": 10464
    },
    {
      "epoch": 0.9421774066488104,
      "grad_norm": 1.6000708283964633,
      "learning_rate": 1.7474211541061104e-07,
      "loss": 0.5232,
      "step": 10465
    },
    {
      "epoch": 0.9422674379346823,
      "grad_norm": 1.287754813438684,
      "learning_rate": 1.7419975619923922e-07,
      "loss": 0.5634,
      "step": 10466
    },
    {
      "epoch": 0.9423574692205542,
      "grad_norm": 1.982492777156636,
      "learning_rate": 1.7365823258001958e-07,
      "loss": 0.4852,
      "step": 10467
    },
    {
      "epoch": 0.942447500506426,
      "grad_norm": 1.812582007456146,
      "learning_rate": 1.731175445990063e-07,
      "loss": 0.5318,
      "step": 10468
    },
    {
      "epoch": 0.9425375317922978,
      "grad_norm": 2.6631883006414308,
      "learning_rate": 1.7257769230217826e-07,
      "loss": 0.5623,
      "step": 10469
    },
    {
      "epoch": 0.9426275630781696,
      "grad_norm": 1.634154950209053,
      "learning_rate": 1.7203867573544642e-07,
      "loss": 0.5466,
      "step": 10470
    },
    {
      "epoch": 0.9427175943640415,
      "grad_norm": 1.4033297326776217,
      "learning_rate": 1.715004949446475e-07,
      "loss": 0.4773,
      "step": 10471
    },
    {
      "epoch": 0.9428076256499134,
      "grad_norm": 1.1943541922455,
      "learning_rate": 1.709631499755504e-07,
      "loss": 0.5903,
      "step": 10472
    },
    {
      "epoch": 0.9428976569357852,
      "grad_norm": 1.544184142280764,
      "learning_rate": 1.7042664087384865e-07,
      "loss": 0.5213,
      "step": 10473
    },
    {
      "epoch": 0.942987688221657,
      "grad_norm": 1.469710463672529,
      "learning_rate": 1.698909676851701e-07,
      "loss": 0.5375,
      "step": 10474
    },
    {
      "epoch": 0.9430777195075288,
      "grad_norm": 1.7674259566173436,
      "learning_rate": 1.6935613045506728e-07,
      "loss": 0.4977,
      "step": 10475
    },
    {
      "epoch": 0.9431677507934008,
      "grad_norm": 1.193146811856391,
      "learning_rate": 1.6882212922902264e-07,
      "loss": 0.4695,
      "step": 10476
    },
    {
      "epoch": 0.9432577820792726,
      "grad_norm": 2.236500324270772,
      "learning_rate": 1.6828896405244988e-07,
      "loss": 0.4721,
      "step": 10477
    },
    {
      "epoch": 0.9433478133651444,
      "grad_norm": 1.3858250743211709,
      "learning_rate": 1.6775663497068828e-07,
      "loss": 0.5635,
      "step": 10478
    },
    {
      "epoch": 0.9434378446510162,
      "grad_norm": 1.6889061373069076,
      "learning_rate": 1.672251420290072e-07,
      "loss": 0.5199,
      "step": 10479
    },
    {
      "epoch": 0.943527875936888,
      "grad_norm": 1.2819200796897925,
      "learning_rate": 1.6669448527260602e-07,
      "loss": 0.5212,
      "step": 10480
    },
    {
      "epoch": 0.94361790722276,
      "grad_norm": 1.577151933456169,
      "learning_rate": 1.6616466474661087e-07,
      "loss": 0.515,
      "step": 10481
    },
    {
      "epoch": 0.9437079385086318,
      "grad_norm": 1.5594165806252982,
      "learning_rate": 1.6563568049608125e-07,
      "loss": 0.5012,
      "step": 10482
    },
    {
      "epoch": 0.9437979697945036,
      "grad_norm": 1.4508631811187176,
      "learning_rate": 1.6510753256599787e-07,
      "loss": 0.4611,
      "step": 10483
    },
    {
      "epoch": 0.9438880010803754,
      "grad_norm": 1.858906868736992,
      "learning_rate": 1.6458022100127702e-07,
      "loss": 0.5757,
      "step": 10484
    },
    {
      "epoch": 0.9439780323662472,
      "grad_norm": 1.705670730606616,
      "learning_rate": 1.640537458467617e-07,
      "loss": 0.4863,
      "step": 10485
    },
    {
      "epoch": 0.9440680636521191,
      "grad_norm": 2.1140657902388105,
      "learning_rate": 1.6352810714722388e-07,
      "loss": 0.4508,
      "step": 10486
    },
    {
      "epoch": 0.944158094937991,
      "grad_norm": 1.6048865612875738,
      "learning_rate": 1.6300330494736226e-07,
      "loss": 0.5879,
      "step": 10487
    },
    {
      "epoch": 0.9442481262238628,
      "grad_norm": 1.541034912373876,
      "learning_rate": 1.6247933929181004e-07,
      "loss": 0.523,
      "step": 10488
    },
    {
      "epoch": 0.9443381575097346,
      "grad_norm": 1.6037164631778744,
      "learning_rate": 1.6195621022512152e-07,
      "loss": 0.5952,
      "step": 10489
    },
    {
      "epoch": 0.9444281887956065,
      "grad_norm": 1.7124855363694955,
      "learning_rate": 1.6143391779178563e-07,
      "loss": 0.3984,
      "step": 10490
    },
    {
      "epoch": 0.9445182200814783,
      "grad_norm": 1.0621363784944584,
      "learning_rate": 1.60912462036219e-07,
      "loss": 0.4944,
      "step": 10491
    },
    {
      "epoch": 0.9446082513673502,
      "grad_norm": 1.2776125006366583,
      "learning_rate": 1.6039184300276623e-07,
      "loss": 0.5356,
      "step": 10492
    },
    {
      "epoch": 0.944698282653222,
      "grad_norm": 1.4462410922606408,
      "learning_rate": 1.598720607357007e-07,
      "loss": 0.5798,
      "step": 10493
    },
    {
      "epoch": 0.9447883139390938,
      "grad_norm": 1.251504547801965,
      "learning_rate": 1.5935311527922714e-07,
      "loss": 0.5199,
      "step": 10494
    },
    {
      "epoch": 0.9448783452249657,
      "grad_norm": 1.4396443266909202,
      "learning_rate": 1.5883500667747244e-07,
      "loss": 0.515,
      "step": 10495
    },
    {
      "epoch": 0.9449683765108375,
      "grad_norm": 1.1255853683613424,
      "learning_rate": 1.583177349745013e-07,
      "loss": 0.5028,
      "step": 10496
    },
    {
      "epoch": 0.9450584077967094,
      "grad_norm": 1.3571392381708098,
      "learning_rate": 1.5780130021429973e-07,
      "loss": 0.5647,
      "step": 10497
    },
    {
      "epoch": 0.9451484390825812,
      "grad_norm": 2.409787573715788,
      "learning_rate": 1.572857024407881e-07,
      "loss": 0.3988,
      "step": 10498
    },
    {
      "epoch": 0.945238470368453,
      "grad_norm": 1.2980691340798851,
      "learning_rate": 1.5677094169781138e-07,
      "loss": 0.4647,
      "step": 10499
    },
    {
      "epoch": 0.9453285016543249,
      "grad_norm": 1.4633524436168173,
      "learning_rate": 1.5625701802914673e-07,
      "loss": 0.5115,
      "step": 10500
    },
    {
      "epoch": 0.9454185329401967,
      "grad_norm": 2.246837679356722,
      "learning_rate": 1.5574393147849697e-07,
      "loss": 0.49,
      "step": 10501
    },
    {
      "epoch": 0.9455085642260685,
      "grad_norm": 1.614574754613904,
      "learning_rate": 1.5523168208949613e-07,
      "loss": 0.4964,
      "step": 10502
    },
    {
      "epoch": 0.9455985955119404,
      "grad_norm": 1.6623889757648418,
      "learning_rate": 1.54720269905706e-07,
      "loss": 0.5158,
      "step": 10503
    },
    {
      "epoch": 0.9456886267978123,
      "grad_norm": 1.0329560673497133,
      "learning_rate": 1.5420969497061733e-07,
      "loss": 0.4562,
      "step": 10504
    },
    {
      "epoch": 0.9457786580836841,
      "grad_norm": 1.711768278819222,
      "learning_rate": 1.5369995732765096e-07,
      "loss": 0.5719,
      "step": 10505
    },
    {
      "epoch": 0.9458686893695559,
      "grad_norm": 2.414535965478684,
      "learning_rate": 1.5319105702015224e-07,
      "loss": 0.567,
      "step": 10506
    },
    {
      "epoch": 0.9459587206554277,
      "grad_norm": 1.4991940363416005,
      "learning_rate": 1.5268299409140208e-07,
      "loss": 0.5473,
      "step": 10507
    },
    {
      "epoch": 0.9460487519412996,
      "grad_norm": 1.753905298250297,
      "learning_rate": 1.521757685846048e-07,
      "loss": 0.4604,
      "step": 10508
    },
    {
      "epoch": 0.9461387832271715,
      "grad_norm": 1.4817635585546136,
      "learning_rate": 1.516693805428926e-07,
      "loss": 0.4888,
      "step": 10509
    },
    {
      "epoch": 0.9462288145130433,
      "grad_norm": 1.5332375447774131,
      "learning_rate": 1.511638300093343e-07,
      "loss": 0.5622,
      "step": 10510
    },
    {
      "epoch": 0.9463188457989151,
      "grad_norm": 1.7644754491180354,
      "learning_rate": 1.5065911702691895e-07,
      "loss": 0.5146,
      "step": 10511
    },
    {
      "epoch": 0.9464088770847869,
      "grad_norm": 1.3364459990056448,
      "learning_rate": 1.5015524163856653e-07,
      "loss": 0.5079,
      "step": 10512
    },
    {
      "epoch": 0.9464989083706588,
      "grad_norm": 1.601454018562005,
      "learning_rate": 1.4965220388712955e-07,
      "loss": 0.5189,
      "step": 10513
    },
    {
      "epoch": 0.9465889396565307,
      "grad_norm": 0.9812175726997,
      "learning_rate": 1.4915000381538702e-07,
      "loss": 0.4299,
      "step": 10514
    },
    {
      "epoch": 0.9466789709424025,
      "grad_norm": 1.4288400688245417,
      "learning_rate": 1.4864864146604263e-07,
      "loss": 0.5204,
      "step": 10515
    },
    {
      "epoch": 0.9467690022282743,
      "grad_norm": 1.5438532594112435,
      "learning_rate": 1.481481168817367e-07,
      "loss": 0.5422,
      "step": 10516
    },
    {
      "epoch": 0.9468590335141461,
      "grad_norm": 1.3258167016925912,
      "learning_rate": 1.4764843010503182e-07,
      "loss": 0.5254,
      "step": 10517
    },
    {
      "epoch": 0.9469490648000181,
      "grad_norm": 1.3658852275232363,
      "learning_rate": 1.471495811784218e-07,
      "loss": 0.457,
      "step": 10518
    },
    {
      "epoch": 0.9470390960858899,
      "grad_norm": 1.480049977440918,
      "learning_rate": 1.466515701443294e-07,
      "loss": 0.4975,
      "step": 10519
    },
    {
      "epoch": 0.9471291273717617,
      "grad_norm": 1.544277307598935,
      "learning_rate": 1.4615439704510515e-07,
      "loss": 0.5226,
      "step": 10520
    },
    {
      "epoch": 0.9472191586576335,
      "grad_norm": 3.1628208985479294,
      "learning_rate": 1.4565806192303078e-07,
      "loss": 0.5472,
      "step": 10521
    },
    {
      "epoch": 0.9473091899435053,
      "grad_norm": 1.664024922292637,
      "learning_rate": 1.4516256482031477e-07,
      "loss": 0.4336,
      "step": 10522
    },
    {
      "epoch": 0.9473992212293773,
      "grad_norm": 1.5548605339802257,
      "learning_rate": 1.4466790577909117e-07,
      "loss": 0.5163,
      "step": 10523
    },
    {
      "epoch": 0.9474892525152491,
      "grad_norm": 1.2416615369913864,
      "learning_rate": 1.4417408484142968e-07,
      "loss": 0.5477,
      "step": 10524
    },
    {
      "epoch": 0.9475792838011209,
      "grad_norm": 1.6886078468820902,
      "learning_rate": 1.436811020493234e-07,
      "loss": 0.4839,
      "step": 10525
    },
    {
      "epoch": 0.9476693150869927,
      "grad_norm": 1.1988705343228752,
      "learning_rate": 1.4318895744469762e-07,
      "loss": 0.471,
      "step": 10526
    },
    {
      "epoch": 0.9477593463728645,
      "grad_norm": 1.2396129295166365,
      "learning_rate": 1.4269765106940226e-07,
      "loss": 0.4601,
      "step": 10527
    },
    {
      "epoch": 0.9478493776587364,
      "grad_norm": 1.4933934081206715,
      "learning_rate": 1.422071829652205e-07,
      "loss": 0.5325,
      "step": 10528
    },
    {
      "epoch": 0.9479394089446083,
      "grad_norm": 1.5067929518558036,
      "learning_rate": 1.4171755317385904e-07,
      "loss": 0.5048,
      "step": 10529
    },
    {
      "epoch": 0.9480294402304801,
      "grad_norm": 1.706464888716342,
      "learning_rate": 1.412287617369601e-07,
      "loss": 0.551,
      "step": 10530
    },
    {
      "epoch": 0.9481194715163519,
      "grad_norm": 1.5066847173706182,
      "learning_rate": 1.407408086960882e-07,
      "loss": 0.5593,
      "step": 10531
    },
    {
      "epoch": 0.9482095028022238,
      "grad_norm": 1.510522168528959,
      "learning_rate": 1.4025369409274014e-07,
      "loss": 0.4814,
      "step": 10532
    },
    {
      "epoch": 0.9482995340880956,
      "grad_norm": 1.0510524509515178,
      "learning_rate": 1.397674179683406e-07,
      "loss": 0.4135,
      "step": 10533
    },
    {
      "epoch": 0.9483895653739675,
      "grad_norm": 2.7813343689275896,
      "learning_rate": 1.39281980364242e-07,
      "loss": 0.5651,
      "step": 10534
    },
    {
      "epoch": 0.9484795966598393,
      "grad_norm": 1.8366774306662927,
      "learning_rate": 1.3879738132172694e-07,
      "loss": 0.5483,
      "step": 10535
    },
    {
      "epoch": 0.9485696279457111,
      "grad_norm": 1.629779323819535,
      "learning_rate": 1.3831362088200683e-07,
      "loss": 0.4196,
      "step": 10536
    },
    {
      "epoch": 0.948659659231583,
      "grad_norm": 1.6452520560836519,
      "learning_rate": 1.3783069908621772e-07,
      "loss": 0.3958,
      "step": 10537
    },
    {
      "epoch": 0.9487496905174548,
      "grad_norm": 1.3742726789558732,
      "learning_rate": 1.3734861597543115e-07,
      "loss": 0.5013,
      "step": 10538
    },
    {
      "epoch": 0.9488397218033267,
      "grad_norm": 1.4123467200609485,
      "learning_rate": 1.368673715906421e-07,
      "loss": 0.426,
      "step": 10539
    },
    {
      "epoch": 0.9489297530891985,
      "grad_norm": 1.2497341226796195,
      "learning_rate": 1.3638696597277678e-07,
      "loss": 0.5888,
      "step": 10540
    },
    {
      "epoch": 0.9490197843750703,
      "grad_norm": 1.607430817337586,
      "learning_rate": 1.3590739916268802e-07,
      "loss": 0.6164,
      "step": 10541
    },
    {
      "epoch": 0.9491098156609422,
      "grad_norm": 1.5093287500969583,
      "learning_rate": 1.3542867120115988e-07,
      "loss": 0.3916,
      "step": 10542
    },
    {
      "epoch": 0.949199846946814,
      "grad_norm": 1.5561763755886564,
      "learning_rate": 1.349507821289009e-07,
      "loss": 0.5607,
      "step": 10543
    },
    {
      "epoch": 0.9492898782326858,
      "grad_norm": 1.7784424166876083,
      "learning_rate": 1.3447373198655523e-07,
      "loss": 0.4443,
      "step": 10544
    },
    {
      "epoch": 0.9493799095185577,
      "grad_norm": 1.3052454108632447,
      "learning_rate": 1.3399752081468819e-07,
      "loss": 0.4431,
      "step": 10545
    },
    {
      "epoch": 0.9494699408044296,
      "grad_norm": 1.1842564265874498,
      "learning_rate": 1.3352214865379853e-07,
      "loss": 0.4752,
      "step": 10546
    },
    {
      "epoch": 0.9495599720903014,
      "grad_norm": 1.369519268031824,
      "learning_rate": 1.3304761554431168e-07,
      "loss": 0.5036,
      "step": 10547
    },
    {
      "epoch": 0.9496500033761732,
      "grad_norm": 1.657443357598314,
      "learning_rate": 1.32573921526582e-07,
      "loss": 0.4861,
      "step": 10548
    },
    {
      "epoch": 0.949740034662045,
      "grad_norm": 1.5876147184936609,
      "learning_rate": 1.3210106664089395e-07,
      "loss": 0.5738,
      "step": 10549
    },
    {
      "epoch": 0.9498300659479169,
      "grad_norm": 1.678621257818734,
      "learning_rate": 1.316290509274587e-07,
      "loss": 0.4915,
      "step": 10550
    },
    {
      "epoch": 0.9499200972337888,
      "grad_norm": 2.686767039283792,
      "learning_rate": 1.3115787442641527e-07,
      "loss": 0.5513,
      "step": 10551
    },
    {
      "epoch": 0.9500101285196606,
      "grad_norm": 2.637379981251793,
      "learning_rate": 1.3068753717783488e-07,
      "loss": 0.4477,
      "step": 10552
    },
    {
      "epoch": 0.9501001598055324,
      "grad_norm": 2.041475792551311,
      "learning_rate": 1.302180392217156e-07,
      "loss": 0.4586,
      "step": 10553
    },
    {
      "epoch": 0.9501901910914042,
      "grad_norm": 2.1139903663923816,
      "learning_rate": 1.2974938059798102e-07,
      "loss": 0.4351,
      "step": 10554
    },
    {
      "epoch": 0.950280222377276,
      "grad_norm": 1.7385005809876324,
      "learning_rate": 1.2928156134648928e-07,
      "loss": 0.497,
      "step": 10555
    },
    {
      "epoch": 0.950370253663148,
      "grad_norm": 1.961193634945332,
      "learning_rate": 1.2881458150702298e-07,
      "loss": 0.5595,
      "step": 10556
    },
    {
      "epoch": 0.9504602849490198,
      "grad_norm": 2.1547371065097347,
      "learning_rate": 1.2834844111929256e-07,
      "loss": 0.5453,
      "step": 10557
    },
    {
      "epoch": 0.9505503162348916,
      "grad_norm": 1.8927084879553586,
      "learning_rate": 1.2788314022294192e-07,
      "loss": 0.4402,
      "step": 10558
    },
    {
      "epoch": 0.9506403475207634,
      "grad_norm": 1.3327318212791577,
      "learning_rate": 1.2741867885753823e-07,
      "loss": 0.4275,
      "step": 10559
    },
    {
      "epoch": 0.9507303788066354,
      "grad_norm": 1.9600992133192927,
      "learning_rate": 1.2695505706258105e-07,
      "loss": 0.5465,
      "step": 10560
    },
    {
      "epoch": 0.9508204100925072,
      "grad_norm": 1.9779761487743295,
      "learning_rate": 1.264922748774955e-07,
      "loss": 0.5895,
      "step": 10561
    },
    {
      "epoch": 0.950910441378379,
      "grad_norm": 1.4895653252992622,
      "learning_rate": 1.2603033234163897e-07,
      "loss": 0.4489,
      "step": 10562
    },
    {
      "epoch": 0.9510004726642508,
      "grad_norm": 1.9865424866077583,
      "learning_rate": 1.255692294942934e-07,
      "loss": 0.4806,
      "step": 10563
    },
    {
      "epoch": 0.9510905039501226,
      "grad_norm": 1.542981286578916,
      "learning_rate": 1.2510896637467184e-07,
      "loss": 0.5812,
      "step": 10564
    },
    {
      "epoch": 0.9511805352359946,
      "grad_norm": 2.1556642764405387,
      "learning_rate": 1.2464954302191635e-07,
      "loss": 0.3891,
      "step": 10565
    },
    {
      "epoch": 0.9512705665218664,
      "grad_norm": 1.666424829198634,
      "learning_rate": 1.2419095947509452e-07,
      "loss": 0.4913,
      "step": 10566
    },
    {
      "epoch": 0.9513605978077382,
      "grad_norm": 1.7580993785809882,
      "learning_rate": 1.237332157732063e-07,
      "loss": 0.4707,
      "step": 10567
    },
    {
      "epoch": 0.95145062909361,
      "grad_norm": 1.5188441179734407,
      "learning_rate": 1.2327631195517832e-07,
      "loss": 0.578,
      "step": 10568
    },
    {
      "epoch": 0.9515406603794818,
      "grad_norm": 1.648533700237221,
      "learning_rate": 1.2282024805986504e-07,
      "loss": 0.5169,
      "step": 10569
    },
    {
      "epoch": 0.9516306916653537,
      "grad_norm": 2.2289632289011845,
      "learning_rate": 1.223650241260521e-07,
      "loss": 0.4938,
      "step": 10570
    },
    {
      "epoch": 0.9517207229512256,
      "grad_norm": 0.991790559175245,
      "learning_rate": 1.219106401924497e-07,
      "loss": 0.4511,
      "step": 10571
    },
    {
      "epoch": 0.9518107542370974,
      "grad_norm": 2.255956555106343,
      "learning_rate": 1.214570962977013e-07,
      "loss": 0.4579,
      "step": 10572
    },
    {
      "epoch": 0.9519007855229692,
      "grad_norm": 1.7131156706945216,
      "learning_rate": 1.21004392480375e-07,
      "loss": 0.5075,
      "step": 10573
    },
    {
      "epoch": 0.9519908168088411,
      "grad_norm": 1.8224866532963688,
      "learning_rate": 1.2055252877896884e-07,
      "loss": 0.6111,
      "step": 10574
    },
    {
      "epoch": 0.9520808480947129,
      "grad_norm": 1.590657831637001,
      "learning_rate": 1.201015052319099e-07,
      "loss": 0.559,
      "step": 10575
    },
    {
      "epoch": 0.9521708793805848,
      "grad_norm": 1.145815596925264,
      "learning_rate": 1.1965132187755524e-07,
      "loss": 0.5709,
      "step": 10576
    },
    {
      "epoch": 0.9522609106664566,
      "grad_norm": 1.7025976014833317,
      "learning_rate": 1.1920197875418649e-07,
      "loss": 0.5118,
      "step": 10577
    },
    {
      "epoch": 0.9523509419523284,
      "grad_norm": 1.142396085720499,
      "learning_rate": 1.1875347590001751e-07,
      "loss": 0.3761,
      "step": 10578
    },
    {
      "epoch": 0.9524409732382003,
      "grad_norm": 1.0922869916292117,
      "learning_rate": 1.183058133531878e-07,
      "loss": 0.4198,
      "step": 10579
    },
    {
      "epoch": 0.9525310045240721,
      "grad_norm": 1.2363130831514333,
      "learning_rate": 1.1785899115176802e-07,
      "loss": 0.5276,
      "step": 10580
    },
    {
      "epoch": 0.952621035809944,
      "grad_norm": 1.5572186973388822,
      "learning_rate": 1.1741300933375555e-07,
      "loss": 0.4704,
      "step": 10581
    },
    {
      "epoch": 0.9527110670958158,
      "grad_norm": 2.186836849696371,
      "learning_rate": 1.1696786793707782e-07,
      "loss": 0.428,
      "step": 10582
    },
    {
      "epoch": 0.9528010983816876,
      "grad_norm": 1.2209125876960696,
      "learning_rate": 1.16523566999589e-07,
      "loss": 0.5184,
      "step": 10583
    },
    {
      "epoch": 0.9528911296675595,
      "grad_norm": 1.430555779912854,
      "learning_rate": 1.1608010655907332e-07,
      "loss": 0.5462,
      "step": 10584
    },
    {
      "epoch": 0.9529811609534313,
      "grad_norm": 0.8476578461394686,
      "learning_rate": 1.156374866532417e-07,
      "loss": 0.5647,
      "step": 10585
    },
    {
      "epoch": 0.9530711922393031,
      "grad_norm": 1.4731067013735382,
      "learning_rate": 1.1519570731973739e-07,
      "loss": 0.5552,
      "step": 10586
    },
    {
      "epoch": 0.953161223525175,
      "grad_norm": 1.8714205895929783,
      "learning_rate": 1.147547685961281e-07,
      "loss": 0.4145,
      "step": 10587
    },
    {
      "epoch": 0.9532512548110469,
      "grad_norm": 1.704624786875218,
      "learning_rate": 1.1431467051991053e-07,
      "loss": 0.5472,
      "step": 10588
    },
    {
      "epoch": 0.9533412860969187,
      "grad_norm": 1.4506261828485572,
      "learning_rate": 1.1387541312851136e-07,
      "loss": 0.421,
      "step": 10589
    },
    {
      "epoch": 0.9534313173827905,
      "grad_norm": 1.525337350052537,
      "learning_rate": 1.134369964592863e-07,
      "loss": 0.4491,
      "step": 10590
    },
    {
      "epoch": 0.9535213486686623,
      "grad_norm": 2.8813240298406098,
      "learning_rate": 1.1299942054951885e-07,
      "loss": 0.4729,
      "step": 10591
    },
    {
      "epoch": 0.9536113799545342,
      "grad_norm": 1.693369022331454,
      "learning_rate": 1.1256268543641924e-07,
      "loss": 0.4994,
      "step": 10592
    },
    {
      "epoch": 0.9537014112404061,
      "grad_norm": 2.3600447490063052,
      "learning_rate": 1.1212679115712888e-07,
      "loss": 0.5652,
      "step": 10593
    },
    {
      "epoch": 0.9537914425262779,
      "grad_norm": 1.8406360284175567,
      "learning_rate": 1.1169173774871478e-07,
      "loss": 0.5459,
      "step": 10594
    },
    {
      "epoch": 0.9538814738121497,
      "grad_norm": 1.5345987539077557,
      "learning_rate": 1.1125752524817623e-07,
      "loss": 0.5078,
      "step": 10595
    },
    {
      "epoch": 0.9539715050980215,
      "grad_norm": 1.4381653251895803,
      "learning_rate": 1.1082415369243816e-07,
      "loss": 0.5447,
      "step": 10596
    },
    {
      "epoch": 0.9540615363838933,
      "grad_norm": 1.3950717255206695,
      "learning_rate": 1.1039162311835439e-07,
      "loss": 0.4393,
      "step": 10597
    },
    {
      "epoch": 0.9541515676697653,
      "grad_norm": 1.8888209563280036,
      "learning_rate": 1.0995993356270774e-07,
      "loss": 0.4661,
      "step": 10598
    },
    {
      "epoch": 0.9542415989556371,
      "grad_norm": 1.4872748985378759,
      "learning_rate": 1.0952908506220883e-07,
      "loss": 0.4417,
      "step": 10599
    },
    {
      "epoch": 0.9543316302415089,
      "grad_norm": 1.320207537570196,
      "learning_rate": 1.0909907765349948e-07,
      "loss": 0.4856,
      "step": 10600
    },
    {
      "epoch": 0.9544216615273807,
      "grad_norm": 1.868195960215731,
      "learning_rate": 1.0866991137314598e-07,
      "loss": 0.4526,
      "step": 10601
    },
    {
      "epoch": 0.9545116928132527,
      "grad_norm": 2.072754267444287,
      "learning_rate": 1.0824158625764469e-07,
      "loss": 0.557,
      "step": 10602
    },
    {
      "epoch": 0.9546017240991245,
      "grad_norm": 1.3539819163209248,
      "learning_rate": 1.0781410234342093e-07,
      "loss": 0.4693,
      "step": 10603
    },
    {
      "epoch": 0.9546917553849963,
      "grad_norm": 1.2854186910112675,
      "learning_rate": 1.0738745966682784e-07,
      "loss": 0.4931,
      "step": 10604
    },
    {
      "epoch": 0.9547817866708681,
      "grad_norm": 1.4085651799395291,
      "learning_rate": 1.0696165826414862e-07,
      "loss": 0.4529,
      "step": 10605
    },
    {
      "epoch": 0.9548718179567399,
      "grad_norm": 1.0599550868645746,
      "learning_rate": 1.0653669817159429e-07,
      "loss": 0.52,
      "step": 10606
    },
    {
      "epoch": 0.9549618492426118,
      "grad_norm": 1.6070123447877656,
      "learning_rate": 1.061125794253004e-07,
      "loss": 0.616,
      "step": 10607
    },
    {
      "epoch": 0.9550518805284837,
      "grad_norm": 1.9090520512512357,
      "learning_rate": 1.0568930206133699e-07,
      "loss": 0.4511,
      "step": 10608
    },
    {
      "epoch": 0.9551419118143555,
      "grad_norm": 1.3220421778669917,
      "learning_rate": 1.052668661156997e-07,
      "loss": 0.6118,
      "step": 10609
    },
    {
      "epoch": 0.9552319431002273,
      "grad_norm": 1.585380252307956,
      "learning_rate": 1.048452716243109e-07,
      "loss": 0.4576,
      "step": 10610
    },
    {
      "epoch": 0.9553219743860991,
      "grad_norm": 1.3218645574407557,
      "learning_rate": 1.0442451862302527e-07,
      "loss": 0.4963,
      "step": 10611
    },
    {
      "epoch": 0.955412005671971,
      "grad_norm": 1.1857061537366624,
      "learning_rate": 1.0400460714762417e-07,
      "loss": 0.4491,
      "step": 10612
    },
    {
      "epoch": 0.9555020369578429,
      "grad_norm": 1.3802784477762091,
      "learning_rate": 1.0358553723381348e-07,
      "loss": 0.5781,
      "step": 10613
    },
    {
      "epoch": 0.9555920682437147,
      "grad_norm": 1.5954917254952048,
      "learning_rate": 1.0316730891723692e-07,
      "loss": 0.5731,
      "step": 10614
    },
    {
      "epoch": 0.9556820995295865,
      "grad_norm": 1.7318691148765708,
      "learning_rate": 1.0274992223345603e-07,
      "loss": 0.5231,
      "step": 10615
    },
    {
      "epoch": 0.9557721308154584,
      "grad_norm": 1.2841571049146037,
      "learning_rate": 1.0233337721796688e-07,
      "loss": 0.6387,
      "step": 10616
    },
    {
      "epoch": 0.9558621621013302,
      "grad_norm": 1.8074576987858544,
      "learning_rate": 1.0191767390619444e-07,
      "loss": 0.4577,
      "step": 10617
    },
    {
      "epoch": 0.955952193387202,
      "grad_norm": 1.5966856850466238,
      "learning_rate": 1.0150281233348824e-07,
      "loss": 0.6063,
      "step": 10618
    },
    {
      "epoch": 0.9560422246730739,
      "grad_norm": 1.0899282112355504,
      "learning_rate": 1.0108879253513004e-07,
      "loss": 0.4728,
      "step": 10619
    },
    {
      "epoch": 0.9561322559589457,
      "grad_norm": 1.876514593752088,
      "learning_rate": 1.0067561454632724e-07,
      "loss": 0.5734,
      "step": 10620
    },
    {
      "epoch": 0.9562222872448176,
      "grad_norm": 1.682599407233281,
      "learning_rate": 1.0026327840221728e-07,
      "loss": 0.6023,
      "step": 10621
    },
    {
      "epoch": 0.9563123185306894,
      "grad_norm": 2.4567275741330237,
      "learning_rate": 9.985178413786434e-08,
      "loss": 0.4249,
      "step": 10622
    },
    {
      "epoch": 0.9564023498165612,
      "grad_norm": 1.740134809867639,
      "learning_rate": 9.944113178826376e-08,
      "loss": 0.4659,
      "step": 10623
    },
    {
      "epoch": 0.9564923811024331,
      "grad_norm": 1.3115942785697194,
      "learning_rate": 9.90313213883376e-08,
      "loss": 0.4788,
      "step": 10624
    },
    {
      "epoch": 0.9565824123883049,
      "grad_norm": 1.7980738294388459,
      "learning_rate": 9.862235297293465e-08,
      "loss": 0.545,
      "step": 10625
    },
    {
      "epoch": 0.9566724436741768,
      "grad_norm": 1.1729248509201482,
      "learning_rate": 9.8214226576836e-08,
      "loss": 0.5923,
      "step": 10626
    },
    {
      "epoch": 0.9567624749600486,
      "grad_norm": 1.1800329994272496,
      "learning_rate": 9.780694223474719e-08,
      "loss": 0.435,
      "step": 10627
    },
    {
      "epoch": 0.9568525062459204,
      "grad_norm": 1.6432449591865865,
      "learning_rate": 9.740049998130496e-08,
      "loss": 0.5554,
      "step": 10628
    },
    {
      "epoch": 0.9569425375317923,
      "grad_norm": 1.5514240032076325,
      "learning_rate": 9.699489985107391e-08,
      "loss": 0.4944,
      "step": 10629
    },
    {
      "epoch": 0.9570325688176642,
      "grad_norm": 1.448677572127457,
      "learning_rate": 9.65901418785442e-08,
      "loss": 0.4352,
      "step": 10630
    },
    {
      "epoch": 0.957122600103536,
      "grad_norm": 1.6021103404793027,
      "learning_rate": 9.618622609813943e-08,
      "loss": 0.553,
      "step": 10631
    },
    {
      "epoch": 0.9572126313894078,
      "grad_norm": 1.6755117366589425,
      "learning_rate": 9.578315254420767e-08,
      "loss": 0.5641,
      "step": 10632
    },
    {
      "epoch": 0.9573026626752796,
      "grad_norm": 1.2593854786655672,
      "learning_rate": 9.538092125102593e-08,
      "loss": 0.496,
      "step": 10633
    },
    {
      "epoch": 0.9573926939611515,
      "grad_norm": 1.4352417598499672,
      "learning_rate": 9.497953225280131e-08,
      "loss": 0.5538,
      "step": 10634
    },
    {
      "epoch": 0.9574827252470234,
      "grad_norm": 1.359968920729486,
      "learning_rate": 9.457898558366763e-08,
      "loss": 0.5537,
      "step": 10635
    },
    {
      "epoch": 0.9575727565328952,
      "grad_norm": 1.7058776570408194,
      "learning_rate": 9.41792812776876e-08,
      "loss": 0.5345,
      "step": 10636
    },
    {
      "epoch": 0.957662787818767,
      "grad_norm": 1.6595305941915308,
      "learning_rate": 9.378041936885296e-08,
      "loss": 0.5827,
      "step": 10637
    },
    {
      "epoch": 0.9577528191046388,
      "grad_norm": 2.7105910637848654,
      "learning_rate": 9.338239989108211e-08,
      "loss": 0.5175,
      "step": 10638
    },
    {
      "epoch": 0.9578428503905106,
      "grad_norm": 1.5063155033646238,
      "learning_rate": 9.298522287822354e-08,
      "loss": 0.5288,
      "step": 10639
    },
    {
      "epoch": 0.9579328816763826,
      "grad_norm": 1.012424438256765,
      "learning_rate": 9.258888836405356e-08,
      "loss": 0.4741,
      "step": 10640
    },
    {
      "epoch": 0.9580229129622544,
      "grad_norm": 1.336258661785849,
      "learning_rate": 9.21933963822752e-08,
      "loss": 0.5238,
      "step": 10641
    },
    {
      "epoch": 0.9581129442481262,
      "grad_norm": 1.7368749807759738,
      "learning_rate": 9.17987469665238e-08,
      "loss": 0.4981,
      "step": 10642
    },
    {
      "epoch": 0.958202975533998,
      "grad_norm": 1.9243054569923095,
      "learning_rate": 9.140494015035917e-08,
      "loss": 0.4674,
      "step": 10643
    },
    {
      "epoch": 0.95829300681987,
      "grad_norm": 1.0518629652914868,
      "learning_rate": 9.101197596727118e-08,
      "loss": 0.4417,
      "step": 10644
    },
    {
      "epoch": 0.9583830381057418,
      "grad_norm": 2.031081266771051,
      "learning_rate": 9.061985445067756e-08,
      "loss": 0.3777,
      "step": 10645
    },
    {
      "epoch": 0.9584730693916136,
      "grad_norm": 1.1726010821102784,
      "learning_rate": 9.022857563392384e-08,
      "loss": 0.5164,
      "step": 10646
    },
    {
      "epoch": 0.9585631006774854,
      "grad_norm": 1.1921663575387187,
      "learning_rate": 8.983813955028675e-08,
      "loss": 0.5177,
      "step": 10647
    },
    {
      "epoch": 0.9586531319633572,
      "grad_norm": 2.6977629426658116,
      "learning_rate": 8.944854623296751e-08,
      "loss": 0.5531,
      "step": 10648
    },
    {
      "epoch": 0.9587431632492291,
      "grad_norm": 2.4665507124298123,
      "learning_rate": 8.905979571509738e-08,
      "loss": 0.4652,
      "step": 10649
    },
    {
      "epoch": 0.958833194535101,
      "grad_norm": 1.2215383407848823,
      "learning_rate": 8.867188802973658e-08,
      "loss": 0.6359,
      "step": 10650
    },
    {
      "epoch": 0.9589232258209728,
      "grad_norm": 1.995169109763734,
      "learning_rate": 8.82848232098732e-08,
      "loss": 0.5862,
      "step": 10651
    },
    {
      "epoch": 0.9590132571068446,
      "grad_norm": 1.398100316748792,
      "learning_rate": 8.789860128842198e-08,
      "loss": 0.5151,
      "step": 10652
    },
    {
      "epoch": 0.9591032883927165,
      "grad_norm": 1.5130458607657382,
      "learning_rate": 8.75132222982289e-08,
      "loss": 0.5075,
      "step": 10653
    },
    {
      "epoch": 0.9591933196785883,
      "grad_norm": 1.0136558572305472,
      "learning_rate": 8.712868627206772e-08,
      "loss": 0.5674,
      "step": 10654
    },
    {
      "epoch": 0.9592833509644602,
      "grad_norm": 1.1591143811268572,
      "learning_rate": 8.674499324263674e-08,
      "loss": 0.523,
      "step": 10655
    },
    {
      "epoch": 0.959373382250332,
      "grad_norm": 1.7267937704316851,
      "learning_rate": 8.636214324256875e-08,
      "loss": 0.548,
      "step": 10656
    },
    {
      "epoch": 0.9594634135362038,
      "grad_norm": 1.3193005519626333,
      "learning_rate": 8.598013630441882e-08,
      "loss": 0.5728,
      "step": 10657
    },
    {
      "epoch": 0.9595534448220757,
      "grad_norm": 1.5277857860702577,
      "learning_rate": 8.559897246067316e-08,
      "loss": 0.4754,
      "step": 10658
    },
    {
      "epoch": 0.9596434761079475,
      "grad_norm": 0.9319494347798463,
      "learning_rate": 8.521865174374921e-08,
      "loss": 0.4555,
      "step": 10659
    },
    {
      "epoch": 0.9597335073938194,
      "grad_norm": 1.2174255106415572,
      "learning_rate": 8.483917418598553e-08,
      "loss": 0.5361,
      "step": 10660
    },
    {
      "epoch": 0.9598235386796912,
      "grad_norm": 1.630363530136882,
      "learning_rate": 8.44605398196563e-08,
      "loss": 0.518,
      "step": 10661
    },
    {
      "epoch": 0.959913569965563,
      "grad_norm": 1.4605310366368758,
      "learning_rate": 8.40827486769602e-08,
      "loss": 0.5247,
      "step": 10662
    },
    {
      "epoch": 0.9600036012514349,
      "grad_norm": 1.4780044543455455,
      "learning_rate": 8.370580079002378e-08,
      "loss": 0.4843,
      "step": 10663
    },
    {
      "epoch": 0.9600936325373067,
      "grad_norm": 1.9721820184252528,
      "learning_rate": 8.332969619090359e-08,
      "loss": 0.5792,
      "step": 10664
    },
    {
      "epoch": 0.9601836638231785,
      "grad_norm": 1.7580954513045635,
      "learning_rate": 8.295443491158406e-08,
      "loss": 0.5589,
      "step": 10665
    },
    {
      "epoch": 0.9602736951090504,
      "grad_norm": 1.1277541092562935,
      "learning_rate": 8.258001698397744e-08,
      "loss": 0.5834,
      "step": 10666
    },
    {
      "epoch": 0.9603637263949223,
      "grad_norm": 1.3534377154875639,
      "learning_rate": 8.220644243992382e-08,
      "loss": 0.4571,
      "step": 10667
    },
    {
      "epoch": 0.9604537576807941,
      "grad_norm": 1.7745466141949813,
      "learning_rate": 8.183371131119333e-08,
      "loss": 0.5605,
      "step": 10668
    },
    {
      "epoch": 0.9605437889666659,
      "grad_norm": 1.5375054545830946,
      "learning_rate": 8.146182362948063e-08,
      "loss": 0.6352,
      "step": 10669
    },
    {
      "epoch": 0.9606338202525377,
      "grad_norm": 1.4083384737822315,
      "learning_rate": 8.109077942641596e-08,
      "loss": 0.549,
      "step": 10670
    },
    {
      "epoch": 0.9607238515384096,
      "grad_norm": 1.4843600978816969,
      "learning_rate": 8.072057873354855e-08,
      "loss": 0.5673,
      "step": 10671
    },
    {
      "epoch": 0.9608138828242815,
      "grad_norm": 1.3570664728716801,
      "learning_rate": 8.035122158236319e-08,
      "loss": 0.374,
      "step": 10672
    },
    {
      "epoch": 0.9609039141101533,
      "grad_norm": 1.7748992440762013,
      "learning_rate": 7.998270800426921e-08,
      "loss": 0.5249,
      "step": 10673
    },
    {
      "epoch": 0.9609939453960251,
      "grad_norm": 1.7284538118576271,
      "learning_rate": 7.9615038030606e-08,
      "loss": 0.4433,
      "step": 10674
    },
    {
      "epoch": 0.9610839766818969,
      "grad_norm": 1.3106231995009072,
      "learning_rate": 7.924821169263963e-08,
      "loss": 0.4641,
      "step": 10675
    },
    {
      "epoch": 0.9611740079677688,
      "grad_norm": 1.7681667527354565,
      "learning_rate": 7.888222902156517e-08,
      "loss": 0.4918,
      "step": 10676
    },
    {
      "epoch": 0.9612640392536407,
      "grad_norm": 1.699449316364314,
      "learning_rate": 7.851709004850772e-08,
      "loss": 0.5064,
      "step": 10677
    },
    {
      "epoch": 0.9613540705395125,
      "grad_norm": 1.6782356520759472,
      "learning_rate": 7.815279480451576e-08,
      "loss": 0.5022,
      "step": 10678
    },
    {
      "epoch": 0.9614441018253843,
      "grad_norm": 1.5388364212258288,
      "learning_rate": 7.778934332057231e-08,
      "loss": 0.4894,
      "step": 10679
    },
    {
      "epoch": 0.9615341331112561,
      "grad_norm": 1.902680555487057,
      "learning_rate": 7.742673562758373e-08,
      "loss": 0.5043,
      "step": 10680
    },
    {
      "epoch": 0.9616241643971281,
      "grad_norm": 1.0345075583878882,
      "learning_rate": 7.70649717563865e-08,
      "loss": 0.4881,
      "step": 10681
    },
    {
      "epoch": 0.9617141956829999,
      "grad_norm": 1.4979094640608275,
      "learning_rate": 7.670405173774709e-08,
      "loss": 0.4486,
      "step": 10682
    },
    {
      "epoch": 0.9618042269688717,
      "grad_norm": 1.5473856350215343,
      "learning_rate": 7.63439756023554e-08,
      "loss": 0.4769,
      "step": 10683
    },
    {
      "epoch": 0.9618942582547435,
      "grad_norm": 1.521109076577567,
      "learning_rate": 7.598474338083473e-08,
      "loss": 0.4876,
      "step": 10684
    },
    {
      "epoch": 0.9619842895406153,
      "grad_norm": 1.5936573688413362,
      "learning_rate": 7.562635510373284e-08,
      "loss": 0.5432,
      "step": 10685
    },
    {
      "epoch": 0.9620743208264873,
      "grad_norm": 1.4014830405374243,
      "learning_rate": 7.526881080152982e-08,
      "loss": 0.4489,
      "step": 10686
    },
    {
      "epoch": 0.9621643521123591,
      "grad_norm": 1.0550182316261132,
      "learning_rate": 7.491211050462798e-08,
      "loss": 0.5342,
      "step": 10687
    },
    {
      "epoch": 0.9622543833982309,
      "grad_norm": 1.0956982970264173,
      "learning_rate": 7.455625424336422e-08,
      "loss": 0.4446,
      "step": 10688
    },
    {
      "epoch": 0.9623444146841027,
      "grad_norm": 1.513401929853354,
      "learning_rate": 7.420124204799983e-08,
      "loss": 0.5209,
      "step": 10689
    },
    {
      "epoch": 0.9624344459699745,
      "grad_norm": 1.3745273413244614,
      "learning_rate": 7.384707394872514e-08,
      "loss": 0.4843,
      "step": 10690
    },
    {
      "epoch": 0.9625244772558464,
      "grad_norm": 1.3083026974651835,
      "learning_rate": 7.349374997565827e-08,
      "loss": 0.4474,
      "step": 10691
    },
    {
      "epoch": 0.9626145085417183,
      "grad_norm": 1.8921954009775386,
      "learning_rate": 7.314127015884742e-08,
      "loss": 0.438,
      "step": 10692
    },
    {
      "epoch": 0.9627045398275901,
      "grad_norm": 2.3325915151316607,
      "learning_rate": 7.278963452826638e-08,
      "loss": 0.4626,
      "step": 10693
    },
    {
      "epoch": 0.9627945711134619,
      "grad_norm": 2.2814390767495745,
      "learning_rate": 7.243884311381898e-08,
      "loss": 0.5206,
      "step": 10694
    },
    {
      "epoch": 0.9628846023993338,
      "grad_norm": 1.4449722736441986,
      "learning_rate": 7.208889594533696e-08,
      "loss": 0.4985,
      "step": 10695
    },
    {
      "epoch": 0.9629746336852056,
      "grad_norm": 1.4121577895473092,
      "learning_rate": 7.173979305258094e-08,
      "loss": 0.5491,
      "step": 10696
    },
    {
      "epoch": 0.9630646649710775,
      "grad_norm": 1.4699334424538741,
      "learning_rate": 7.139153446523606e-08,
      "loss": 0.5414,
      "step": 10697
    },
    {
      "epoch": 0.9631546962569493,
      "grad_norm": 1.2444918299289924,
      "learning_rate": 7.104412021292084e-08,
      "loss": 0.515,
      "step": 10698
    },
    {
      "epoch": 0.9632447275428211,
      "grad_norm": 1.4661230032113564,
      "learning_rate": 7.069755032517944e-08,
      "loss": 0.5592,
      "step": 10699
    },
    {
      "epoch": 0.963334758828693,
      "grad_norm": 0.9271064719048084,
      "learning_rate": 7.035182483148272e-08,
      "loss": 0.5011,
      "step": 10700
    },
    {
      "epoch": 0.9634247901145648,
      "grad_norm": 1.6278453322991449,
      "learning_rate": 7.000694376123385e-08,
      "loss": 0.4871,
      "step": 10701
    },
    {
      "epoch": 0.9635148214004366,
      "grad_norm": 1.3775160629849406,
      "learning_rate": 6.966290714375934e-08,
      "loss": 0.4881,
      "step": 10702
    },
    {
      "epoch": 0.9636048526863085,
      "grad_norm": 1.4894311612892994,
      "learning_rate": 6.931971500831802e-08,
      "loss": 0.486,
      "step": 10703
    },
    {
      "epoch": 0.9636948839721803,
      "grad_norm": 1.4303476623939577,
      "learning_rate": 6.897736738409545e-08,
      "loss": 0.4784,
      "step": 10704
    },
    {
      "epoch": 0.9637849152580522,
      "grad_norm": 1.7535716996021828,
      "learning_rate": 6.863586430020275e-08,
      "loss": 0.6023,
      "step": 10705
    },
    {
      "epoch": 0.963874946543924,
      "grad_norm": 1.6815948578229774,
      "learning_rate": 6.829520578568338e-08,
      "loss": 0.5169,
      "step": 10706
    },
    {
      "epoch": 0.9639649778297958,
      "grad_norm": 1.0660470887268503,
      "learning_rate": 6.795539186950639e-08,
      "loss": 0.4576,
      "step": 10707
    },
    {
      "epoch": 0.9640550091156677,
      "grad_norm": 2.2376239906544115,
      "learning_rate": 6.761642258056977e-08,
      "loss": 0.5088,
      "step": 10708
    },
    {
      "epoch": 0.9641450404015396,
      "grad_norm": 1.9217053906640915,
      "learning_rate": 6.727829794770158e-08,
      "loss": 0.6279,
      "step": 10709
    },
    {
      "epoch": 0.9642350716874114,
      "grad_norm": 1.323147274652443,
      "learning_rate": 6.694101799965325e-08,
      "loss": 0.4782,
      "step": 10710
    },
    {
      "epoch": 0.9643251029732832,
      "grad_norm": 1.7694961490291805,
      "learning_rate": 6.66045827651085e-08,
      "loss": 0.5334,
      "step": 10711
    },
    {
      "epoch": 0.964415134259155,
      "grad_norm": 1.4090399379971137,
      "learning_rate": 6.626899227267892e-08,
      "loss": 0.4715,
      "step": 10712
    },
    {
      "epoch": 0.9645051655450269,
      "grad_norm": 1.3995817329657243,
      "learning_rate": 6.593424655090274e-08,
      "loss": 0.5754,
      "step": 10713
    },
    {
      "epoch": 0.9645951968308988,
      "grad_norm": 1.4036492049496907,
      "learning_rate": 6.560034562824502e-08,
      "loss": 0.4969,
      "step": 10714
    },
    {
      "epoch": 0.9646852281167706,
      "grad_norm": 1.3907476190957515,
      "learning_rate": 6.526728953310413e-08,
      "loss": 0.5156,
      "step": 10715
    },
    {
      "epoch": 0.9647752594026424,
      "grad_norm": 1.6375398971391593,
      "learning_rate": 6.493507829380075e-08,
      "loss": 0.511,
      "step": 10716
    },
    {
      "epoch": 0.9648652906885142,
      "grad_norm": 1.3680019610755785,
      "learning_rate": 6.460371193858895e-08,
      "loss": 0.5674,
      "step": 10717
    },
    {
      "epoch": 0.964955321974386,
      "grad_norm": 1.5290976332294732,
      "learning_rate": 6.427319049564617e-08,
      "loss": 0.573,
      "step": 10718
    },
    {
      "epoch": 0.965045353260258,
      "grad_norm": 1.2721808651641398,
      "learning_rate": 6.394351399307997e-08,
      "loss": 0.4381,
      "step": 10719
    },
    {
      "epoch": 0.9651353845461298,
      "grad_norm": 1.6388672671262912,
      "learning_rate": 6.361468245892788e-08,
      "loss": 0.5071,
      "step": 10720
    },
    {
      "epoch": 0.9652254158320016,
      "grad_norm": 1.4048474651950242,
      "learning_rate": 6.328669592115199e-08,
      "loss": 0.5171,
      "step": 10721
    },
    {
      "epoch": 0.9653154471178734,
      "grad_norm": 1.5276501534521167,
      "learning_rate": 6.295955440764668e-08,
      "loss": 0.4919,
      "step": 10722
    },
    {
      "epoch": 0.9654054784037454,
      "grad_norm": 1.926733685956648,
      "learning_rate": 6.263325794623076e-08,
      "loss": 0.46,
      "step": 10723
    },
    {
      "epoch": 0.9654955096896172,
      "grad_norm": 2.5364591630294195,
      "learning_rate": 6.23078065646543e-08,
      "loss": 0.4667,
      "step": 10724
    },
    {
      "epoch": 0.965585540975489,
      "grad_norm": 1.2794164172791693,
      "learning_rate": 6.198320029059068e-08,
      "loss": 0.5002,
      "step": 10725
    },
    {
      "epoch": 0.9656755722613608,
      "grad_norm": 1.8720596698536576,
      "learning_rate": 6.165943915164784e-08,
      "loss": 0.5062,
      "step": 10726
    },
    {
      "epoch": 0.9657656035472326,
      "grad_norm": 1.8240289070977624,
      "learning_rate": 6.13365231753571e-08,
      "loss": 0.3821,
      "step": 10727
    },
    {
      "epoch": 0.9658556348331045,
      "grad_norm": 1.491523708157607,
      "learning_rate": 6.10144523891798e-08,
      "loss": 0.5928,
      "step": 10728
    },
    {
      "epoch": 0.9659456661189764,
      "grad_norm": 1.5984857184005201,
      "learning_rate": 6.069322682050516e-08,
      "loss": 0.5752,
      "step": 10729
    },
    {
      "epoch": 0.9660356974048482,
      "grad_norm": 2.174530250132261,
      "learning_rate": 6.03728464966502e-08,
      "loss": 0.5104,
      "step": 10730
    },
    {
      "epoch": 0.96612572869072,
      "grad_norm": 1.6608894837175745,
      "learning_rate": 6.005331144485982e-08,
      "loss": 0.4856,
      "step": 10731
    },
    {
      "epoch": 0.9662157599765918,
      "grad_norm": 1.0609244739778045,
      "learning_rate": 5.973462169230892e-08,
      "loss": 0.4885,
      "step": 10732
    },
    {
      "epoch": 0.9663057912624637,
      "grad_norm": 1.6060297171703943,
      "learning_rate": 5.941677726609807e-08,
      "loss": 0.452,
      "step": 10733
    },
    {
      "epoch": 0.9663958225483356,
      "grad_norm": 1.5543181929133933,
      "learning_rate": 5.909977819325674e-08,
      "loss": 0.4292,
      "step": 10734
    },
    {
      "epoch": 0.9664858538342074,
      "grad_norm": 1.7549190818666491,
      "learning_rate": 5.8783624500742265e-08,
      "loss": 0.544,
      "step": 10735
    },
    {
      "epoch": 0.9665758851200792,
      "grad_norm": 2.230075521136842,
      "learning_rate": 5.846831621544202e-08,
      "loss": 0.5705,
      "step": 10736
    },
    {
      "epoch": 0.9666659164059511,
      "grad_norm": 1.398647644943564,
      "learning_rate": 5.815385336416901e-08,
      "loss": 0.4541,
      "step": 10737
    },
    {
      "epoch": 0.9667559476918229,
      "grad_norm": 1.367457061402567,
      "learning_rate": 5.784023597366628e-08,
      "loss": 0.5228,
      "step": 10738
    },
    {
      "epoch": 0.9668459789776948,
      "grad_norm": 0.9290334688513691,
      "learning_rate": 5.752746407060139e-08,
      "loss": 0.5478,
      "step": 10739
    },
    {
      "epoch": 0.9669360102635666,
      "grad_norm": 2.640487381523102,
      "learning_rate": 5.7215537681577504e-08,
      "loss": 0.4141,
      "step": 10740
    },
    {
      "epoch": 0.9670260415494384,
      "grad_norm": 1.2971191202968206,
      "learning_rate": 5.690445683311563e-08,
      "loss": 0.5377,
      "step": 10741
    },
    {
      "epoch": 0.9671160728353103,
      "grad_norm": 1.1504672289270534,
      "learning_rate": 5.65942215516746e-08,
      "loss": 0.4882,
      "step": 10742
    },
    {
      "epoch": 0.9672061041211821,
      "grad_norm": 1.3192997462633385,
      "learning_rate": 5.628483186363443e-08,
      "loss": 0.4973,
      "step": 10743
    },
    {
      "epoch": 0.967296135407054,
      "grad_norm": 1.718294091880426,
      "learning_rate": 5.5976287795306286e-08,
      "loss": 0.5391,
      "step": 10744
    },
    {
      "epoch": 0.9673861666929258,
      "grad_norm": 1.6643408859783768,
      "learning_rate": 5.5668589372930295e-08,
      "loss": 0.4915,
      "step": 10745
    },
    {
      "epoch": 0.9674761979787976,
      "grad_norm": 1.6460227604504893,
      "learning_rate": 5.536173662267219e-08,
      "loss": 0.5434,
      "step": 10746
    },
    {
      "epoch": 0.9675662292646695,
      "grad_norm": 1.4071470919600837,
      "learning_rate": 5.505572957062666e-08,
      "loss": 0.4453,
      "step": 10747
    },
    {
      "epoch": 0.9676562605505413,
      "grad_norm": 1.0118550786727938,
      "learning_rate": 5.4750568242816215e-08,
      "loss": 0.4706,
      "step": 10748
    },
    {
      "epoch": 0.9677462918364131,
      "grad_norm": 1.3860344706229728,
      "learning_rate": 5.4446252665194545e-08,
      "loss": 0.4444,
      "step": 10749
    },
    {
      "epoch": 0.967836323122285,
      "grad_norm": 1.791457456425591,
      "learning_rate": 5.414278286363761e-08,
      "loss": 0.5288,
      "step": 10750
    },
    {
      "epoch": 0.9679263544081569,
      "grad_norm": 1.9226341423115487,
      "learning_rate": 5.384015886395588e-08,
      "loss": 0.5078,
      "step": 10751
    },
    {
      "epoch": 0.9680163856940287,
      "grad_norm": 2.738913933185643,
      "learning_rate": 5.353838069188211e-08,
      "loss": 0.4743,
      "step": 10752
    },
    {
      "epoch": 0.9681064169799005,
      "grad_norm": 1.7759181063680711,
      "learning_rate": 5.32374483730802e-08,
      "loss": 0.4924,
      "step": 10753
    },
    {
      "epoch": 0.9681964482657723,
      "grad_norm": 1.9034407152119548,
      "learning_rate": 5.293736193314303e-08,
      "loss": 0.5238,
      "step": 10754
    },
    {
      "epoch": 0.9682864795516442,
      "grad_norm": 1.3763402125764017,
      "learning_rate": 5.263812139758795e-08,
      "loss": 0.4593,
      "step": 10755
    },
    {
      "epoch": 0.9683765108375161,
      "grad_norm": 1.2278043327601262,
      "learning_rate": 5.233972679186461e-08,
      "loss": 0.468,
      "step": 10756
    },
    {
      "epoch": 0.9684665421233879,
      "grad_norm": 2.0547372005223106,
      "learning_rate": 5.2042178141347155e-08,
      "loss": 0.4546,
      "step": 10757
    },
    {
      "epoch": 0.9685565734092597,
      "grad_norm": 1.2859409345464943,
      "learning_rate": 5.17454754713409e-08,
      "loss": 0.525,
      "step": 10758
    },
    {
      "epoch": 0.9686466046951315,
      "grad_norm": 1.785314627987343,
      "learning_rate": 5.1449618807076773e-08,
      "loss": 0.5635,
      "step": 10759
    },
    {
      "epoch": 0.9687366359810033,
      "grad_norm": 1.481618955701791,
      "learning_rate": 5.115460817371354e-08,
      "loss": 0.5527,
      "step": 10760
    },
    {
      "epoch": 0.9688266672668753,
      "grad_norm": 1.3993556432175023,
      "learning_rate": 5.0860443596341125e-08,
      "loss": 0.5721,
      "step": 10761
    },
    {
      "epoch": 0.9689166985527471,
      "grad_norm": 1.162907093725823,
      "learning_rate": 5.056712509997397e-08,
      "loss": 0.5434,
      "step": 10762
    },
    {
      "epoch": 0.9690067298386189,
      "grad_norm": 1.8761779872744886,
      "learning_rate": 5.027465270955545e-08,
      "loss": 0.5737,
      "step": 10763
    },
    {
      "epoch": 0.9690967611244907,
      "grad_norm": 1.2245226475748316,
      "learning_rate": 4.9983026449960114e-08,
      "loss": 0.4042,
      "step": 10764
    },
    {
      "epoch": 0.9691867924103627,
      "grad_norm": 1.8667375774917643,
      "learning_rate": 4.9692246345985905e-08,
      "loss": 0.5592,
      "step": 10765
    },
    {
      "epoch": 0.9692768236962345,
      "grad_norm": 1.52423876373277,
      "learning_rate": 4.940231242236193e-08,
      "loss": 0.4696,
      "step": 10766
    },
    {
      "epoch": 0.9693668549821063,
      "grad_norm": 1.7323423391419468,
      "learning_rate": 4.911322470374402e-08,
      "loss": 0.445,
      "step": 10767
    },
    {
      "epoch": 0.9694568862679781,
      "grad_norm": 1.7022591693940983,
      "learning_rate": 4.8824983214716956e-08,
      "loss": 0.4976,
      "step": 10768
    },
    {
      "epoch": 0.9695469175538499,
      "grad_norm": 1.4113680222774276,
      "learning_rate": 4.853758797979113e-08,
      "loss": 0.4987,
      "step": 10769
    },
    {
      "epoch": 0.9696369488397218,
      "grad_norm": 1.6659479878503198,
      "learning_rate": 4.825103902340922e-08,
      "loss": 0.4957,
      "step": 10770
    },
    {
      "epoch": 0.9697269801255937,
      "grad_norm": 1.7143762868402481,
      "learning_rate": 4.796533636993728e-08,
      "loss": 0.4648,
      "step": 10771
    },
    {
      "epoch": 0.9698170114114655,
      "grad_norm": 1.5142078098845375,
      "learning_rate": 4.768048004367365e-08,
      "loss": 0.4653,
      "step": 10772
    },
    {
      "epoch": 0.9699070426973373,
      "grad_norm": 1.4251885724248117,
      "learning_rate": 4.739647006884118e-08,
      "loss": 0.5521,
      "step": 10773
    },
    {
      "epoch": 0.9699970739832091,
      "grad_norm": 1.1227210329786943,
      "learning_rate": 4.7113306469593886e-08,
      "loss": 0.5005,
      "step": 10774
    },
    {
      "epoch": 0.970087105269081,
      "grad_norm": 1.5925689182384957,
      "learning_rate": 4.6830989270010285e-08,
      "loss": 0.5534,
      "step": 10775
    },
    {
      "epoch": 0.9701771365549529,
      "grad_norm": 1.374917287050685,
      "learning_rate": 4.6549518494100055e-08,
      "loss": 0.5971,
      "step": 10776
    },
    {
      "epoch": 0.9702671678408247,
      "grad_norm": 1.282480694040923,
      "learning_rate": 4.626889416579961e-08,
      "loss": 0.5209,
      "step": 10777
    },
    {
      "epoch": 0.9703571991266965,
      "grad_norm": 1.3203697547951938,
      "learning_rate": 4.598911630897318e-08,
      "loss": 0.4691,
      "step": 10778
    },
    {
      "epoch": 0.9704472304125684,
      "grad_norm": 1.5659018656741563,
      "learning_rate": 4.5710184947411754e-08,
      "loss": 0.5094,
      "step": 10779
    },
    {
      "epoch": 0.9705372616984402,
      "grad_norm": 1.907190259538161,
      "learning_rate": 4.543210010483856e-08,
      "loss": 0.5632,
      "step": 10780
    },
    {
      "epoch": 0.970627292984312,
      "grad_norm": 0.9610134903884932,
      "learning_rate": 4.515486180490025e-08,
      "loss": 0.5113,
      "step": 10781
    },
    {
      "epoch": 0.9707173242701839,
      "grad_norm": 1.2525276508064997,
      "learning_rate": 4.487847007117463e-08,
      "loss": 0.5395,
      "step": 10782
    },
    {
      "epoch": 0.9708073555560557,
      "grad_norm": 1.5408868115236842,
      "learning_rate": 4.460292492716512e-08,
      "loss": 0.4091,
      "step": 10783
    },
    {
      "epoch": 0.9708973868419276,
      "grad_norm": 2.649596686847692,
      "learning_rate": 4.4328226396304075e-08,
      "loss": 0.4968,
      "step": 10784
    },
    {
      "epoch": 0.9709874181277994,
      "grad_norm": 1.9018545800590765,
      "learning_rate": 4.405437450195282e-08,
      "loss": 0.4863,
      "step": 10785
    },
    {
      "epoch": 0.9710774494136712,
      "grad_norm": 1.3976545371783553,
      "learning_rate": 4.3781369267399396e-08,
      "loss": 0.4132,
      "step": 10786
    },
    {
      "epoch": 0.9711674806995431,
      "grad_norm": 1.3959862208592682,
      "learning_rate": 4.3509210715860787e-08,
      "loss": 0.4974,
      "step": 10787
    },
    {
      "epoch": 0.9712575119854149,
      "grad_norm": 1.9445163981289393,
      "learning_rate": 4.32378988704818e-08,
      "loss": 0.4972,
      "step": 10788
    },
    {
      "epoch": 0.9713475432712868,
      "grad_norm": 1.5175678580652079,
      "learning_rate": 4.2967433754333985e-08,
      "loss": 0.4671,
      "step": 10789
    },
    {
      "epoch": 0.9714375745571586,
      "grad_norm": 1.084567115166037,
      "learning_rate": 4.269781539041784e-08,
      "loss": 0.4395,
      "step": 10790
    },
    {
      "epoch": 0.9715276058430304,
      "grad_norm": 1.514672966092262,
      "learning_rate": 4.242904380166279e-08,
      "loss": 0.4728,
      "step": 10791
    },
    {
      "epoch": 0.9716176371289023,
      "grad_norm": 1.9501602653852086,
      "learning_rate": 4.216111901092501e-08,
      "loss": 0.4987,
      "step": 10792
    },
    {
      "epoch": 0.9717076684147742,
      "grad_norm": 1.59679841575684,
      "learning_rate": 4.189404104098849e-08,
      "loss": 0.5707,
      "step": 10793
    },
    {
      "epoch": 0.971797699700646,
      "grad_norm": 1.8055546460428142,
      "learning_rate": 4.162780991456728e-08,
      "loss": 0.5253,
      "step": 10794
    },
    {
      "epoch": 0.9718877309865178,
      "grad_norm": 1.917058407055891,
      "learning_rate": 4.1362425654299934e-08,
      "loss": 0.4248,
      "step": 10795
    },
    {
      "epoch": 0.9719777622723896,
      "grad_norm": 1.496677320174781,
      "learning_rate": 4.109788828275618e-08,
      "loss": 0.5266,
      "step": 10796
    },
    {
      "epoch": 0.9720677935582615,
      "grad_norm": 1.7613567297981667,
      "learning_rate": 4.0834197822431365e-08,
      "loss": 0.4771,
      "step": 10797
    },
    {
      "epoch": 0.9721578248441334,
      "grad_norm": 1.9618595607421985,
      "learning_rate": 4.0571354295749766e-08,
      "loss": 0.5157,
      "step": 10798
    },
    {
      "epoch": 0.9722478561300052,
      "grad_norm": 1.397182775383517,
      "learning_rate": 4.030935772506572e-08,
      "loss": 0.562,
      "step": 10799
    },
    {
      "epoch": 0.972337887415877,
      "grad_norm": 2.2057502917265106,
      "learning_rate": 4.0048208132656976e-08,
      "loss": 0.4961,
      "step": 10800
    },
    {
      "epoch": 0.9724279187017488,
      "grad_norm": 1.8216412353811897,
      "learning_rate": 3.978790554073353e-08,
      "loss": 0.516,
      "step": 10801
    },
    {
      "epoch": 0.9725179499876206,
      "grad_norm": 1.7923304302569425,
      "learning_rate": 3.9528449971432126e-08,
      "loss": 0.4836,
      "step": 10802
    },
    {
      "epoch": 0.9726079812734926,
      "grad_norm": 1.9177062001549727,
      "learning_rate": 3.9269841446816225e-08,
      "loss": 0.5733,
      "step": 10803
    },
    {
      "epoch": 0.9726980125593644,
      "grad_norm": 1.5108918804626463,
      "learning_rate": 3.9012079988877125e-08,
      "loss": 0.5805,
      "step": 10804
    },
    {
      "epoch": 0.9727880438452362,
      "grad_norm": 2.2565401230351405,
      "learning_rate": 3.8755165619536185e-08,
      "loss": 0.4843,
      "step": 10805
    },
    {
      "epoch": 0.972878075131108,
      "grad_norm": 1.4860580271431467,
      "learning_rate": 3.8499098360641474e-08,
      "loss": 0.5918,
      "step": 10806
    },
    {
      "epoch": 0.97296810641698,
      "grad_norm": 1.849211286559566,
      "learning_rate": 3.824387823396891e-08,
      "loss": 0.5159,
      "step": 10807
    },
    {
      "epoch": 0.9730581377028518,
      "grad_norm": 1.3474287797049893,
      "learning_rate": 3.7989505261223355e-08,
      "loss": 0.5543,
      "step": 10808
    },
    {
      "epoch": 0.9731481689887236,
      "grad_norm": 1.0261083052343436,
      "learning_rate": 3.7735979464035285e-08,
      "loss": 0.5558,
      "step": 10809
    },
    {
      "epoch": 0.9732382002745954,
      "grad_norm": 1.677019111000868,
      "learning_rate": 3.748330086396523e-08,
      "loss": 0.5101,
      "step": 10810
    },
    {
      "epoch": 0.9733282315604672,
      "grad_norm": 1.9050067697609863,
      "learning_rate": 3.723146948250267e-08,
      "loss": 0.5009,
      "step": 10811
    },
    {
      "epoch": 0.9734182628463391,
      "grad_norm": 1.1674362956998348,
      "learning_rate": 3.6980485341060467e-08,
      "loss": 0.4943,
      "step": 10812
    },
    {
      "epoch": 0.973508294132211,
      "grad_norm": 1.4567130513135953,
      "learning_rate": 3.6730348460986e-08,
      "loss": 0.5411,
      "step": 10813
    },
    {
      "epoch": 0.9735983254180828,
      "grad_norm": 1.79871005309288,
      "learning_rate": 3.64810588635478e-08,
      "loss": 0.4726,
      "step": 10814
    },
    {
      "epoch": 0.9736883567039546,
      "grad_norm": 1.4291962437379622,
      "learning_rate": 3.623261656994781e-08,
      "loss": 0.5094,
      "step": 10815
    },
    {
      "epoch": 0.9737783879898264,
      "grad_norm": 1.3107005237582248,
      "learning_rate": 3.5985021601313564e-08,
      "loss": 0.5293,
      "step": 10816
    },
    {
      "epoch": 0.9738684192756983,
      "grad_norm": 1.5177891541694037,
      "learning_rate": 3.573827397869933e-08,
      "loss": 0.4982,
      "step": 10817
    },
    {
      "epoch": 0.9739584505615702,
      "grad_norm": 1.3228518643196525,
      "learning_rate": 3.5492373723090554e-08,
      "loss": 0.5229,
      "step": 10818
    },
    {
      "epoch": 0.974048481847442,
      "grad_norm": 2.2711856104660404,
      "learning_rate": 3.524732085539606e-08,
      "loss": 0.5187,
      "step": 10819
    },
    {
      "epoch": 0.9741385131333138,
      "grad_norm": 1.2994743072789237,
      "learning_rate": 3.5003115396458065e-08,
      "loss": 0.438,
      "step": 10820
    },
    {
      "epoch": 0.9742285444191857,
      "grad_norm": 1.7085029613871157,
      "learning_rate": 3.475975736704218e-08,
      "loss": 0.4969,
      "step": 10821
    },
    {
      "epoch": 0.9743185757050575,
      "grad_norm": 1.3526463773308355,
      "learning_rate": 3.451724678784518e-08,
      "loss": 0.5847,
      "step": 10822
    },
    {
      "epoch": 0.9744086069909293,
      "grad_norm": 1.5314279378839002,
      "learning_rate": 3.4275583679489464e-08,
      "loss": 0.5142,
      "step": 10823
    },
    {
      "epoch": 0.9744986382768012,
      "grad_norm": 1.5664254311236074,
      "learning_rate": 3.403476806252526e-08,
      "loss": 0.4573,
      "step": 10824
    },
    {
      "epoch": 0.974588669562673,
      "grad_norm": 1.5977777081910327,
      "learning_rate": 3.3794799957433956e-08,
      "loss": 0.5324,
      "step": 10825
    },
    {
      "epoch": 0.9746787008485449,
      "grad_norm": 1.292125489756419,
      "learning_rate": 3.355567938462034e-08,
      "loss": 0.4302,
      "step": 10826
    },
    {
      "epoch": 0.9747687321344167,
      "grad_norm": 1.0241478392527232,
      "learning_rate": 3.331740636442038e-08,
      "loss": 0.4964,
      "step": 10827
    },
    {
      "epoch": 0.9748587634202885,
      "grad_norm": 1.5141203202177405,
      "learning_rate": 3.307998091709674e-08,
      "loss": 0.5181,
      "step": 10828
    },
    {
      "epoch": 0.9749487947061604,
      "grad_norm": 1.5612899591262297,
      "learning_rate": 3.284340306284106e-08,
      "loss": 0.5123,
      "step": 10829
    },
    {
      "epoch": 0.9750388259920322,
      "grad_norm": 1.1010626699789308,
      "learning_rate": 3.260767282177168e-08,
      "loss": 0.5314,
      "step": 10830
    },
    {
      "epoch": 0.9751288572779041,
      "grad_norm": 1.8019188383494227,
      "learning_rate": 3.23727902139348e-08,
      "loss": 0.5342,
      "step": 10831
    },
    {
      "epoch": 0.9752188885637759,
      "grad_norm": 1.3868533777754302,
      "learning_rate": 3.213875525930443e-08,
      "loss": 0.4884,
      "step": 10832
    },
    {
      "epoch": 0.9753089198496477,
      "grad_norm": 2.3566422408830454,
      "learning_rate": 3.1905567977783546e-08,
      "loss": 0.4793,
      "step": 10833
    },
    {
      "epoch": 0.9753989511355196,
      "grad_norm": 1.3845977898285984,
      "learning_rate": 3.167322838920406e-08,
      "loss": 0.4458,
      "step": 10834
    },
    {
      "epoch": 0.9754889824213915,
      "grad_norm": 1.398110142153546,
      "learning_rate": 3.144173651332239e-08,
      "loss": 0.5737,
      "step": 10835
    },
    {
      "epoch": 0.9755790137072633,
      "grad_norm": 1.778875867453444,
      "learning_rate": 3.1211092369825004e-08,
      "loss": 0.4529,
      "step": 10836
    },
    {
      "epoch": 0.9756690449931351,
      "grad_norm": 1.4113704240866018,
      "learning_rate": 3.098129597832622e-08,
      "loss": 0.4438,
      "step": 10837
    },
    {
      "epoch": 0.9757590762790069,
      "grad_norm": 1.0614940084228033,
      "learning_rate": 3.0752347358369295e-08,
      "loss": 0.5004,
      "step": 10838
    },
    {
      "epoch": 0.9758491075648787,
      "grad_norm": 1.9984662632627088,
      "learning_rate": 3.0524246529421984e-08,
      "loss": 0.5026,
      "step": 10839
    },
    {
      "epoch": 0.9759391388507507,
      "grad_norm": 1.4668187235706303,
      "learning_rate": 3.029699351088322e-08,
      "loss": 0.4942,
      "step": 10840
    },
    {
      "epoch": 0.9760291701366225,
      "grad_norm": 2.143940146398975,
      "learning_rate": 3.0070588322079765e-08,
      "loss": 0.4877,
      "step": 10841
    },
    {
      "epoch": 0.9761192014224943,
      "grad_norm": 1.5822579168120903,
      "learning_rate": 2.984503098226399e-08,
      "loss": 0.4739,
      "step": 10842
    },
    {
      "epoch": 0.9762092327083661,
      "grad_norm": 1.436137489137826,
      "learning_rate": 2.9620321510616114e-08,
      "loss": 0.5915,
      "step": 10843
    },
    {
      "epoch": 0.9762992639942379,
      "grad_norm": 1.3476805234490445,
      "learning_rate": 2.939645992624862e-08,
      "loss": 0.462,
      "step": 10844
    },
    {
      "epoch": 0.9763892952801099,
      "grad_norm": 1.4921334057564293,
      "learning_rate": 2.91734462481974e-08,
      "loss": 0.3983,
      "step": 10845
    },
    {
      "epoch": 0.9764793265659817,
      "grad_norm": 1.7936747734188032,
      "learning_rate": 2.895128049542728e-08,
      "loss": 0.4694,
      "step": 10846
    },
    {
      "epoch": 0.9765693578518535,
      "grad_norm": 1.739170105484138,
      "learning_rate": 2.8729962686830927e-08,
      "loss": 0.4537,
      "step": 10847
    },
    {
      "epoch": 0.9766593891377253,
      "grad_norm": 1.129898737697917,
      "learning_rate": 2.8509492841231058e-08,
      "loss": 0.5463,
      "step": 10848
    },
    {
      "epoch": 0.9767494204235972,
      "grad_norm": 1.4926615775924994,
      "learning_rate": 2.828987097737601e-08,
      "loss": 0.4944,
      "step": 10849
    },
    {
      "epoch": 0.9768394517094691,
      "grad_norm": 2.1277794468152775,
      "learning_rate": 2.807109711394085e-08,
      "loss": 0.4471,
      "step": 10850
    },
    {
      "epoch": 0.9769294829953409,
      "grad_norm": 1.5452318458341094,
      "learning_rate": 2.7853171269532908e-08,
      "loss": 0.5744,
      "step": 10851
    },
    {
      "epoch": 0.9770195142812127,
      "grad_norm": 1.1784155433106753,
      "learning_rate": 2.7636093462682923e-08,
      "loss": 0.4879,
      "step": 10852
    },
    {
      "epoch": 0.9771095455670845,
      "grad_norm": 1.393888897448723,
      "learning_rate": 2.741986371185168e-08,
      "loss": 0.4803,
      "step": 10853
    },
    {
      "epoch": 0.9771995768529564,
      "grad_norm": 2.7046599128962576,
      "learning_rate": 2.7204482035427803e-08,
      "loss": 0.4975,
      "step": 10854
    },
    {
      "epoch": 0.9772896081388283,
      "grad_norm": 1.2134034679361942,
      "learning_rate": 2.6989948451726643e-08,
      "loss": 0.4271,
      "step": 10855
    },
    {
      "epoch": 0.9773796394247001,
      "grad_norm": 2.417583278002885,
      "learning_rate": 2.6776262978993605e-08,
      "loss": 0.3774,
      "step": 10856
    },
    {
      "epoch": 0.9774696707105719,
      "grad_norm": 1.8507684064414338,
      "learning_rate": 2.6563425635399708e-08,
      "loss": 0.4245,
      "step": 10857
    },
    {
      "epoch": 0.9775597019964438,
      "grad_norm": 2.4492338424056763,
      "learning_rate": 2.635143643904492e-08,
      "loss": 0.507,
      "step": 10858
    },
    {
      "epoch": 0.9776497332823156,
      "grad_norm": 1.636216159748456,
      "learning_rate": 2.614029540795704e-08,
      "loss": 0.4656,
      "step": 10859
    },
    {
      "epoch": 0.9777397645681875,
      "grad_norm": 1.723287603414207,
      "learning_rate": 2.5930002560091704e-08,
      "loss": 0.5448,
      "step": 10860
    },
    {
      "epoch": 0.9778297958540593,
      "grad_norm": 1.2468837435673923,
      "learning_rate": 2.572055791333239e-08,
      "loss": 0.4473,
      "step": 10861
    },
    {
      "epoch": 0.9779198271399311,
      "grad_norm": 2.0313588602097155,
      "learning_rate": 2.5511961485490398e-08,
      "loss": 0.5363,
      "step": 10862
    },
    {
      "epoch": 0.978009858425803,
      "grad_norm": 1.68859960834114,
      "learning_rate": 2.530421329430488e-08,
      "loss": 0.6375,
      "step": 10863
    },
    {
      "epoch": 0.9780998897116748,
      "grad_norm": 1.776398219327212,
      "learning_rate": 2.509731335744281e-08,
      "loss": 0.434,
      "step": 10864
    },
    {
      "epoch": 0.9781899209975466,
      "grad_norm": 1.5371306943997847,
      "learning_rate": 2.4891261692497892e-08,
      "loss": 0.4759,
      "step": 10865
    },
    {
      "epoch": 0.9782799522834185,
      "grad_norm": 1.348343831212385,
      "learning_rate": 2.4686058316995e-08,
      "loss": 0.5711,
      "step": 10866
    },
    {
      "epoch": 0.9783699835692903,
      "grad_norm": 1.2731818743939343,
      "learning_rate": 2.4481703248383503e-08,
      "loss": 0.5261,
      "step": 10867
    },
    {
      "epoch": 0.9784600148551622,
      "grad_norm": 1.6918670627984393,
      "learning_rate": 2.4278196504042837e-08,
      "loss": 0.523,
      "step": 10868
    },
    {
      "epoch": 0.978550046141034,
      "grad_norm": 2.1244828197667887,
      "learning_rate": 2.407553810127805e-08,
      "loss": 0.487,
      "step": 10869
    },
    {
      "epoch": 0.9786400774269058,
      "grad_norm": 1.5114795820944333,
      "learning_rate": 2.387372805732424e-08,
      "loss": 0.4943,
      "step": 10870
    },
    {
      "epoch": 0.9787301087127777,
      "grad_norm": 2.0516328148515415,
      "learning_rate": 2.3672766389343238e-08,
      "loss": 0.4988,
      "step": 10871
    },
    {
      "epoch": 0.9788201399986496,
      "grad_norm": 1.5455613665989356,
      "learning_rate": 2.3472653114424704e-08,
      "loss": 0.4068,
      "step": 10872
    },
    {
      "epoch": 0.9789101712845214,
      "grad_norm": 1.7680528505281576,
      "learning_rate": 2.3273388249587248e-08,
      "loss": 0.5424,
      "step": 10873
    },
    {
      "epoch": 0.9790002025703932,
      "grad_norm": 1.3182515631408946,
      "learning_rate": 2.3074971811775092e-08,
      "loss": 0.5395,
      "step": 10874
    },
    {
      "epoch": 0.979090233856265,
      "grad_norm": 1.9056054059315037,
      "learning_rate": 2.287740381786252e-08,
      "loss": 0.469,
      "step": 10875
    },
    {
      "epoch": 0.9791802651421369,
      "grad_norm": 1.22553540157428,
      "learning_rate": 2.2680684284650532e-08,
      "loss": 0.4247,
      "step": 10876
    },
    {
      "epoch": 0.9792702964280088,
      "grad_norm": 1.6146313646653907,
      "learning_rate": 2.2484813228867974e-08,
      "loss": 0.5473,
      "step": 10877
    },
    {
      "epoch": 0.9793603277138806,
      "grad_norm": 1.532668511584605,
      "learning_rate": 2.228979066717374e-08,
      "loss": 0.5259,
      "step": 10878
    },
    {
      "epoch": 0.9794503589997524,
      "grad_norm": 1.65628098639369,
      "learning_rate": 2.2095616616150117e-08,
      "loss": 0.4386,
      "step": 10879
    },
    {
      "epoch": 0.9795403902856242,
      "grad_norm": 1.1224910351411896,
      "learning_rate": 2.190229109231168e-08,
      "loss": 0.5219,
      "step": 10880
    },
    {
      "epoch": 0.979630421571496,
      "grad_norm": 1.546559923497779,
      "learning_rate": 2.1709814112098604e-08,
      "loss": 0.4249,
      "step": 10881
    },
    {
      "epoch": 0.979720452857368,
      "grad_norm": 1.5686805365722956,
      "learning_rate": 2.1518185691877804e-08,
      "loss": 0.5707,
      "step": 10882
    },
    {
      "epoch": 0.9798104841432398,
      "grad_norm": 1.934692365904021,
      "learning_rate": 2.1327405847947346e-08,
      "loss": 0.4825,
      "step": 10883
    },
    {
      "epoch": 0.9799005154291116,
      "grad_norm": 1.0965776431699907,
      "learning_rate": 2.1137474596530925e-08,
      "loss": 0.4925,
      "step": 10884
    },
    {
      "epoch": 0.9799905467149834,
      "grad_norm": 1.373270705327207,
      "learning_rate": 2.0948391953778957e-08,
      "loss": 0.5175,
      "step": 10885
    },
    {
      "epoch": 0.9800805780008554,
      "grad_norm": 1.7689411618440536,
      "learning_rate": 2.0760157935771906e-08,
      "loss": 0.4395,
      "step": 10886
    },
    {
      "epoch": 0.9801706092867272,
      "grad_norm": 1.2917071662287236,
      "learning_rate": 2.0572772558518085e-08,
      "loss": 0.642,
      "step": 10887
    },
    {
      "epoch": 0.980260640572599,
      "grad_norm": 1.3086237817670283,
      "learning_rate": 2.0386235837952516e-08,
      "loss": 0.5038,
      "step": 10888
    },
    {
      "epoch": 0.9803506718584708,
      "grad_norm": 3.1049471666062187,
      "learning_rate": 2.0200547789938074e-08,
      "loss": 0.5618,
      "step": 10889
    },
    {
      "epoch": 0.9804407031443426,
      "grad_norm": 1.8743874790988988,
      "learning_rate": 2.001570843026546e-08,
      "loss": 0.4903,
      "step": 10890
    },
    {
      "epoch": 0.9805307344302145,
      "grad_norm": 2.906205184171827,
      "learning_rate": 1.9831717774654314e-08,
      "loss": 0.4271,
      "step": 10891
    },
    {
      "epoch": 0.9806207657160864,
      "grad_norm": 1.8831352031783397,
      "learning_rate": 1.9648575838751016e-08,
      "loss": 0.4994,
      "step": 10892
    },
    {
      "epoch": 0.9807107970019582,
      "grad_norm": 1.5067173109538288,
      "learning_rate": 1.9466282638129775e-08,
      "loss": 0.6028,
      "step": 10893
    },
    {
      "epoch": 0.98080082828783,
      "grad_norm": 1.400388476945049,
      "learning_rate": 1.928483818829374e-08,
      "loss": 0.4983,
      "step": 10894
    },
    {
      "epoch": 0.9808908595737018,
      "grad_norm": 1.9139038171909122,
      "learning_rate": 1.91042425046728e-08,
      "loss": 0.5211,
      "step": 10895
    },
    {
      "epoch": 0.9809808908595737,
      "grad_norm": 1.9635563919955965,
      "learning_rate": 1.892449560262355e-08,
      "loss": 0.5045,
      "step": 10896
    },
    {
      "epoch": 0.9810709221454456,
      "grad_norm": 1.978109271970167,
      "learning_rate": 1.8745597497433765e-08,
      "loss": 0.4426,
      "step": 10897
    },
    {
      "epoch": 0.9811609534313174,
      "grad_norm": 1.818703960784544,
      "learning_rate": 1.8567548204315722e-08,
      "loss": 0.4347,
      "step": 10898
    },
    {
      "epoch": 0.9812509847171892,
      "grad_norm": 1.2672153480570714,
      "learning_rate": 1.839034773841175e-08,
      "loss": 0.5219,
      "step": 10899
    },
    {
      "epoch": 0.9813410160030611,
      "grad_norm": 2.1344019862817545,
      "learning_rate": 1.82139961147898e-08,
      "loss": 0.6437,
      "step": 10900
    },
    {
      "epoch": 0.9814310472889329,
      "grad_norm": 2.224306768168788,
      "learning_rate": 1.8038493348448983e-08,
      "loss": 0.4351,
      "step": 10901
    },
    {
      "epoch": 0.9815210785748048,
      "grad_norm": 1.4977465201261846,
      "learning_rate": 1.7863839454311804e-08,
      "loss": 0.6059,
      "step": 10902
    },
    {
      "epoch": 0.9816111098606766,
      "grad_norm": 1.5428030844447203,
      "learning_rate": 1.769003444723194e-08,
      "loss": 0.5011,
      "step": 10903
    },
    {
      "epoch": 0.9817011411465484,
      "grad_norm": 0.8262598548294127,
      "learning_rate": 1.75170783419909e-08,
      "loss": 0.4908,
      "step": 10904
    },
    {
      "epoch": 0.9817911724324203,
      "grad_norm": 1.243145569202017,
      "learning_rate": 1.7344971153294697e-08,
      "loss": 0.5018,
      "step": 10905
    },
    {
      "epoch": 0.9818812037182921,
      "grad_norm": 1.7210727485209212,
      "learning_rate": 1.7173712895781625e-08,
      "loss": 0.5828,
      "step": 10906
    },
    {
      "epoch": 0.981971235004164,
      "grad_norm": 1.3336645008522126,
      "learning_rate": 1.7003303584014474e-08,
      "loss": 0.4658,
      "step": 10907
    },
    {
      "epoch": 0.9820612662900358,
      "grad_norm": 1.9729363458312403,
      "learning_rate": 1.68337432324861e-08,
      "loss": 0.6459,
      "step": 10908
    },
    {
      "epoch": 0.9821512975759076,
      "grad_norm": 1.2356156491914128,
      "learning_rate": 1.6665031855614967e-08,
      "loss": 0.447,
      "step": 10909
    },
    {
      "epoch": 0.9822413288617795,
      "grad_norm": 1.3589758911406649,
      "learning_rate": 1.6497169467748488e-08,
      "loss": 0.4688,
      "step": 10910
    },
    {
      "epoch": 0.9823313601476513,
      "grad_norm": 2.365247717678664,
      "learning_rate": 1.6330156083160796e-08,
      "loss": 0.5089,
      "step": 10911
    },
    {
      "epoch": 0.9824213914335231,
      "grad_norm": 2.427596174340841,
      "learning_rate": 1.6163991716057204e-08,
      "loss": 0.4911,
      "step": 10912
    },
    {
      "epoch": 0.982511422719395,
      "grad_norm": 1.139403820510288,
      "learning_rate": 1.5998676380566403e-08,
      "loss": 0.4911,
      "step": 10913
    },
    {
      "epoch": 0.9826014540052669,
      "grad_norm": 2.008115880985153,
      "learning_rate": 1.583421009074826e-08,
      "loss": 0.5467,
      "step": 10914
    },
    {
      "epoch": 0.9826914852911387,
      "grad_norm": 1.384334810643483,
      "learning_rate": 1.567059286058825e-08,
      "loss": 0.501,
      "step": 10915
    },
    {
      "epoch": 0.9827815165770105,
      "grad_norm": 1.5770227582026388,
      "learning_rate": 1.5507824704000806e-08,
      "loss": 0.5685,
      "step": 10916
    },
    {
      "epoch": 0.9828715478628823,
      "grad_norm": 1.4316974978824122,
      "learning_rate": 1.5345905634827073e-08,
      "loss": 0.5487,
      "step": 10917
    },
    {
      "epoch": 0.9829615791487541,
      "grad_norm": 1.5859195271708864,
      "learning_rate": 1.518483566683826e-08,
      "loss": 0.4788,
      "step": 10918
    },
    {
      "epoch": 0.9830516104346261,
      "grad_norm": 1.41887834670125,
      "learning_rate": 1.5024614813730077e-08,
      "loss": 0.4184,
      "step": 10919
    },
    {
      "epoch": 0.9831416417204979,
      "grad_norm": 3.1746018359238284,
      "learning_rate": 1.48652430891294e-08,
      "loss": 0.5038,
      "step": 10920
    },
    {
      "epoch": 0.9832316730063697,
      "grad_norm": 2.070010154364506,
      "learning_rate": 1.4706720506588723e-08,
      "loss": 0.4354,
      "step": 10921
    },
    {
      "epoch": 0.9833217042922415,
      "grad_norm": 1.2527940588607447,
      "learning_rate": 1.4549047079588374e-08,
      "loss": 0.4734,
      "step": 10922
    },
    {
      "epoch": 0.9834117355781133,
      "grad_norm": 1.3602130436024715,
      "learning_rate": 1.4392222821537627e-08,
      "loss": 0.5243,
      "step": 10923
    },
    {
      "epoch": 0.9835017668639853,
      "grad_norm": 1.3641638099419877,
      "learning_rate": 1.423624774577248e-08,
      "loss": 0.5224,
      "step": 10924
    },
    {
      "epoch": 0.9835917981498571,
      "grad_norm": 2.360883703756371,
      "learning_rate": 1.4081121865557878e-08,
      "loss": 0.4924,
      "step": 10925
    },
    {
      "epoch": 0.9836818294357289,
      "grad_norm": 2.7335594930659575,
      "learning_rate": 1.3926845194085491e-08,
      "loss": 0.514,
      "step": 10926
    },
    {
      "epoch": 0.9837718607216007,
      "grad_norm": 2.1947414170411155,
      "learning_rate": 1.3773417744474826e-08,
      "loss": 0.6253,
      "step": 10927
    },
    {
      "epoch": 0.9838618920074726,
      "grad_norm": 1.9391548457027108,
      "learning_rate": 1.3620839529773221e-08,
      "loss": 0.5324,
      "step": 10928
    },
    {
      "epoch": 0.9839519232933445,
      "grad_norm": 1.8176652229230743,
      "learning_rate": 1.3469110562956966e-08,
      "loss": 0.5338,
      "step": 10929
    },
    {
      "epoch": 0.9840419545792163,
      "grad_norm": 1.5103500803021772,
      "learning_rate": 1.331823085692796e-08,
      "loss": 0.5878,
      "step": 10930
    },
    {
      "epoch": 0.9841319858650881,
      "grad_norm": 1.5076443191650724,
      "learning_rate": 1.3168200424518163e-08,
      "loss": 0.4878,
      "step": 10931
    },
    {
      "epoch": 0.9842220171509599,
      "grad_norm": 1.3305927816672851,
      "learning_rate": 1.3019019278485145e-08,
      "loss": 0.5779,
      "step": 10932
    },
    {
      "epoch": 0.9843120484368318,
      "grad_norm": 1.8655909952474292,
      "learning_rate": 1.2870687431516537e-08,
      "loss": 0.572,
      "step": 10933
    },
    {
      "epoch": 0.9844020797227037,
      "grad_norm": 1.3284316341156408,
      "learning_rate": 1.2723204896226693e-08,
      "loss": 0.4563,
      "step": 10934
    },
    {
      "epoch": 0.9844921110085755,
      "grad_norm": 2.3858551354330233,
      "learning_rate": 1.257657168515669e-08,
      "loss": 0.4604,
      "step": 10935
    },
    {
      "epoch": 0.9845821422944473,
      "grad_norm": 1.2283232368450199,
      "learning_rate": 1.2430787810776556e-08,
      "loss": 0.5657,
      "step": 10936
    },
    {
      "epoch": 0.9846721735803191,
      "grad_norm": 1.5670062557555575,
      "learning_rate": 1.2285853285484151e-08,
      "loss": 0.4959,
      "step": 10937
    },
    {
      "epoch": 0.984762204866191,
      "grad_norm": 1.2703556456720833,
      "learning_rate": 1.2141768121604058e-08,
      "loss": 0.4606,
      "step": 10938
    },
    {
      "epoch": 0.9848522361520629,
      "grad_norm": 1.2844471883335342,
      "learning_rate": 1.1998532331389812e-08,
      "loss": 0.5394,
      "step": 10939
    },
    {
      "epoch": 0.9849422674379347,
      "grad_norm": 1.4581980023893197,
      "learning_rate": 1.1856145927022777e-08,
      "loss": 0.5687,
      "step": 10940
    },
    {
      "epoch": 0.9850322987238065,
      "grad_norm": 1.5469339140995826,
      "learning_rate": 1.1714608920611048e-08,
      "loss": 0.5573,
      "step": 10941
    },
    {
      "epoch": 0.9851223300096784,
      "grad_norm": 1.3767070299881525,
      "learning_rate": 1.1573921324190551e-08,
      "loss": 0.5351,
      "step": 10942
    },
    {
      "epoch": 0.9852123612955502,
      "grad_norm": 1.0737152376494363,
      "learning_rate": 1.1434083149727271e-08,
      "loss": 0.5363,
      "step": 10943
    },
    {
      "epoch": 0.985302392581422,
      "grad_norm": 1.7061300800120465,
      "learning_rate": 1.1295094409110585e-08,
      "loss": 0.5017,
      "step": 10944
    },
    {
      "epoch": 0.9853924238672939,
      "grad_norm": 1.998782530098566,
      "learning_rate": 1.1156955114162149e-08,
      "loss": 0.4486,
      "step": 10945
    },
    {
      "epoch": 0.9854824551531657,
      "grad_norm": 1.4868515706758478,
      "learning_rate": 1.1019665276628122e-08,
      "loss": 0.555,
      "step": 10946
    },
    {
      "epoch": 0.9855724864390376,
      "grad_norm": 1.7088002274279084,
      "learning_rate": 1.088322490818583e-08,
      "loss": 0.524,
      "step": 10947
    },
    {
      "epoch": 0.9856625177249094,
      "grad_norm": 1.5819138634585355,
      "learning_rate": 1.0747634020434882e-08,
      "loss": 0.5641,
      "step": 10948
    },
    {
      "epoch": 0.9857525490107812,
      "grad_norm": 1.4744693785963443,
      "learning_rate": 1.0612892624909387e-08,
      "loss": 0.5132,
      "step": 10949
    },
    {
      "epoch": 0.9858425802966531,
      "grad_norm": 1.6128019105046651,
      "learning_rate": 1.0479000733065737e-08,
      "loss": 0.5571,
      "step": 10950
    },
    {
      "epoch": 0.9859326115825249,
      "grad_norm": 5.086206094372392,
      "learning_rate": 1.0345958356290376e-08,
      "loss": 0.5267,
      "step": 10951
    },
    {
      "epoch": 0.9860226428683968,
      "grad_norm": 1.2645172619310006,
      "learning_rate": 1.0213765505898699e-08,
      "loss": 0.5182,
      "step": 10952
    },
    {
      "epoch": 0.9861126741542686,
      "grad_norm": 1.2802952502499885,
      "learning_rate": 1.0082422193130604e-08,
      "loss": 0.5544,
      "step": 10953
    },
    {
      "epoch": 0.9862027054401404,
      "grad_norm": 1.8452733812386857,
      "learning_rate": 9.951928429157154e-09,
      "loss": 0.445,
      "step": 10954
    },
    {
      "epoch": 0.9862927367260123,
      "grad_norm": 1.3551645796719298,
      "learning_rate": 9.822284225073919e-09,
      "loss": 0.5156,
      "step": 10955
    },
    {
      "epoch": 0.9863827680118842,
      "grad_norm": 1.7146571552637746,
      "learning_rate": 9.693489591907634e-09,
      "loss": 0.5879,
      "step": 10956
    },
    {
      "epoch": 0.986472799297756,
      "grad_norm": 1.3645131792707952,
      "learning_rate": 9.565544540610649e-09,
      "loss": 0.5369,
      "step": 10957
    },
    {
      "epoch": 0.9865628305836278,
      "grad_norm": 1.022328044065434,
      "learning_rate": 9.43844908206315e-09,
      "loss": 0.4616,
      "step": 10958
    },
    {
      "epoch": 0.9866528618694996,
      "grad_norm": 1.0517823010765355,
      "learning_rate": 9.312203227073158e-09,
      "loss": 0.5549,
      "step": 10959
    },
    {
      "epoch": 0.9867428931553714,
      "grad_norm": 1.021010765280864,
      "learning_rate": 9.186806986376528e-09,
      "loss": 0.4672,
      "step": 10960
    },
    {
      "epoch": 0.9868329244412434,
      "grad_norm": 1.363630817678175,
      "learning_rate": 9.062260370638065e-09,
      "loss": 0.4865,
      "step": 10961
    },
    {
      "epoch": 0.9869229557271152,
      "grad_norm": 1.214114325888546,
      "learning_rate": 8.938563390449295e-09,
      "loss": 0.4868,
      "step": 10962
    },
    {
      "epoch": 0.987012987012987,
      "grad_norm": 1.1803971843640118,
      "learning_rate": 8.815716056328472e-09,
      "loss": 0.6186,
      "step": 10963
    },
    {
      "epoch": 0.9871030182988588,
      "grad_norm": 1.236896564110872,
      "learning_rate": 8.693718378722793e-09,
      "loss": 0.4868,
      "step": 10964
    },
    {
      "epoch": 0.9871930495847306,
      "grad_norm": 1.5376761330653441,
      "learning_rate": 8.572570368008403e-09,
      "loss": 0.4919,
      "step": 10965
    },
    {
      "epoch": 0.9872830808706026,
      "grad_norm": 1.2598140589534461,
      "learning_rate": 8.452272034485953e-09,
      "loss": 0.4827,
      "step": 10966
    },
    {
      "epoch": 0.9873731121564744,
      "grad_norm": 1.3055208398185267,
      "learning_rate": 8.332823388386147e-09,
      "loss": 0.5457,
      "step": 10967
    },
    {
      "epoch": 0.9874631434423462,
      "grad_norm": 1.760605243792483,
      "learning_rate": 8.214224439867523e-09,
      "loss": 0.5185,
      "step": 10968
    },
    {
      "epoch": 0.987553174728218,
      "grad_norm": 1.7680031673069867,
      "learning_rate": 8.096475199016462e-09,
      "loss": 0.4853,
      "step": 10969
    },
    {
      "epoch": 0.98764320601409,
      "grad_norm": 1.2243748676124555,
      "learning_rate": 7.979575675844953e-09,
      "loss": 0.4449,
      "step": 10970
    },
    {
      "epoch": 0.9877332372999618,
      "grad_norm": 1.3621755440490138,
      "learning_rate": 7.863525880293932e-09,
      "loss": 0.4984,
      "step": 10971
    },
    {
      "epoch": 0.9878232685858336,
      "grad_norm": 1.594520227801488,
      "learning_rate": 7.748325822234392e-09,
      "loss": 0.4974,
      "step": 10972
    },
    {
      "epoch": 0.9879132998717054,
      "grad_norm": 1.4498303350875514,
      "learning_rate": 7.633975511460723e-09,
      "loss": 0.4738,
      "step": 10973
    },
    {
      "epoch": 0.9880033311575772,
      "grad_norm": 2.072848791576608,
      "learning_rate": 7.520474957699586e-09,
      "loss": 0.5797,
      "step": 10974
    },
    {
      "epoch": 0.9880933624434491,
      "grad_norm": 1.6317565308380837,
      "learning_rate": 7.407824170601041e-09,
      "loss": 0.5662,
      "step": 10975
    },
    {
      "epoch": 0.988183393729321,
      "grad_norm": 1.2292949581144552,
      "learning_rate": 7.296023159746313e-09,
      "loss": 0.491,
      "step": 10976
    },
    {
      "epoch": 0.9882734250151928,
      "grad_norm": 3.7159715004122855,
      "learning_rate": 7.185071934643351e-09,
      "loss": 0.5514,
      "step": 10977
    },
    {
      "epoch": 0.9883634563010646,
      "grad_norm": 1.710851225136557,
      "learning_rate": 7.07497050472572e-09,
      "loss": 0.5194,
      "step": 10978
    },
    {
      "epoch": 0.9884534875869364,
      "grad_norm": 1.2285673542514544,
      "learning_rate": 6.965718879358152e-09,
      "loss": 0.5356,
      "step": 10979
    },
    {
      "epoch": 0.9885435188728083,
      "grad_norm": 1.2439464153127158,
      "learning_rate": 6.857317067830993e-09,
      "loss": 0.4807,
      "step": 10980
    },
    {
      "epoch": 0.9886335501586802,
      "grad_norm": 1.8343574650788361,
      "learning_rate": 6.749765079363535e-09,
      "loss": 0.4613,
      "step": 10981
    },
    {
      "epoch": 0.988723581444552,
      "grad_norm": 2.3429028632246753,
      "learning_rate": 6.643062923099575e-09,
      "loss": 0.5243,
      "step": 10982
    },
    {
      "epoch": 0.9888136127304238,
      "grad_norm": 1.678816221625211,
      "learning_rate": 6.5372106081162955e-09,
      "loss": 0.5019,
      "step": 10983
    },
    {
      "epoch": 0.9889036440162957,
      "grad_norm": 1.7876191849077738,
      "learning_rate": 6.432208143413165e-09,
      "loss": 0.4834,
      "step": 10984
    },
    {
      "epoch": 0.9889936753021675,
      "grad_norm": 1.523955140007494,
      "learning_rate": 6.328055537920819e-09,
      "loss": 0.5595,
      "step": 10985
    },
    {
      "epoch": 0.9890837065880393,
      "grad_norm": 1.6276528270381463,
      "learning_rate": 6.224752800495504e-09,
      "loss": 0.451,
      "step": 10986
    },
    {
      "epoch": 0.9891737378739112,
      "grad_norm": 1.8919135661610693,
      "learning_rate": 6.122299939923526e-09,
      "loss": 0.4835,
      "step": 10987
    },
    {
      "epoch": 0.989263769159783,
      "grad_norm": 1.3837986676689387,
      "learning_rate": 6.020696964916806e-09,
      "loss": 0.5032,
      "step": 10988
    },
    {
      "epoch": 0.9893538004456549,
      "grad_norm": 1.2275240837187038,
      "learning_rate": 5.919943884115098e-09,
      "loss": 0.5354,
      "step": 10989
    },
    {
      "epoch": 0.9894438317315267,
      "grad_norm": 1.7545058142871133,
      "learning_rate": 5.820040706088215e-09,
      "loss": 0.5423,
      "step": 10990
    },
    {
      "epoch": 0.9895338630173985,
      "grad_norm": 1.3051200273394719,
      "learning_rate": 5.7209874393293615e-09,
      "loss": 0.5213,
      "step": 10991
    },
    {
      "epoch": 0.9896238943032704,
      "grad_norm": 1.5369471423249432,
      "learning_rate": 5.6227840922651325e-09,
      "loss": 0.5074,
      "step": 10992
    },
    {
      "epoch": 0.9897139255891422,
      "grad_norm": 2.5805470917465225,
      "learning_rate": 5.525430673244403e-09,
      "loss": 0.4716,
      "step": 10993
    },
    {
      "epoch": 0.9898039568750141,
      "grad_norm": 1.2083210150856778,
      "learning_rate": 5.428927190546107e-09,
      "loss": 0.4769,
      "step": 10994
    },
    {
      "epoch": 0.9898939881608859,
      "grad_norm": 2.264576378591847,
      "learning_rate": 5.333273652379234e-09,
      "loss": 0.454,
      "step": 10995
    },
    {
      "epoch": 0.9899840194467577,
      "grad_norm": 2.7118211696565337,
      "learning_rate": 5.238470066877277e-09,
      "loss": 0.499,
      "step": 10996
    },
    {
      "epoch": 0.9900740507326296,
      "grad_norm": 1.5143766676456247,
      "learning_rate": 5.144516442100455e-09,
      "loss": 0.4879,
      "step": 10997
    },
    {
      "epoch": 0.9901640820185015,
      "grad_norm": 1.331328346776724,
      "learning_rate": 5.051412786041265e-09,
      "loss": 0.5248,
      "step": 10998
    },
    {
      "epoch": 0.9902541133043733,
      "grad_norm": 1.4186054769137317,
      "learning_rate": 4.959159106615596e-09,
      "loss": 0.4576,
      "step": 10999
    },
    {
      "epoch": 0.9903441445902451,
      "grad_norm": 1.896290311476581,
      "learning_rate": 4.867755411670505e-09,
      "loss": 0.4877,
      "step": 11000
    },
    {
      "epoch": 0.9904341758761169,
      "grad_norm": 1.5149985166289728,
      "learning_rate": 4.7772017089764425e-09,
      "loss": 0.4657,
      "step": 11001
    },
    {
      "epoch": 0.9905242071619887,
      "grad_norm": 1.5736070131086124,
      "learning_rate": 4.687498006236135e-09,
      "loss": 0.5041,
      "step": 11002
    },
    {
      "epoch": 0.9906142384478607,
      "grad_norm": 1.4040186597345514,
      "learning_rate": 4.5986443110779265e-09,
      "loss": 0.5804,
      "step": 11003
    },
    {
      "epoch": 0.9907042697337325,
      "grad_norm": 1.8006264170949842,
      "learning_rate": 4.510640631056884e-09,
      "loss": 0.5403,
      "step": 11004
    },
    {
      "epoch": 0.9907943010196043,
      "grad_norm": 1.3869446309612907,
      "learning_rate": 4.423486973658131e-09,
      "loss": 0.4542,
      "step": 11005
    },
    {
      "epoch": 0.9908843323054761,
      "grad_norm": 1.3494209305038507,
      "learning_rate": 4.337183346292406e-09,
      "loss": 0.4804,
      "step": 11006
    },
    {
      "epoch": 0.9909743635913479,
      "grad_norm": 1.5219497075812185,
      "learning_rate": 4.2517297562993945e-09,
      "loss": 0.4684,
      "step": 11007
    },
    {
      "epoch": 0.9910643948772199,
      "grad_norm": 1.532177375456184,
      "learning_rate": 4.1671262109466145e-09,
      "loss": 0.5773,
      "step": 11008
    },
    {
      "epoch": 0.9911544261630917,
      "grad_norm": 1.3113336016300483,
      "learning_rate": 4.083372717428313e-09,
      "loss": 0.5351,
      "step": 11009
    },
    {
      "epoch": 0.9912444574489635,
      "grad_norm": 1.1126981696314302,
      "learning_rate": 4.00046928286546e-09,
      "loss": 0.5629,
      "step": 11010
    },
    {
      "epoch": 0.9913344887348353,
      "grad_norm": 1.5804478361769114,
      "learning_rate": 3.918415914311302e-09,
      "loss": 0.5274,
      "step": 11011
    },
    {
      "epoch": 0.9914245200207072,
      "grad_norm": 1.3313374466198715,
      "learning_rate": 3.8372126187413706e-09,
      "loss": 0.5283,
      "step": 11012
    },
    {
      "epoch": 0.9915145513065791,
      "grad_norm": 1.265222596121492,
      "learning_rate": 3.756859403061252e-09,
      "loss": 0.4994,
      "step": 11013
    },
    {
      "epoch": 0.9916045825924509,
      "grad_norm": 2.0561324460634705,
      "learning_rate": 3.6773562741054814e-09,
      "loss": 0.5335,
      "step": 11014
    },
    {
      "epoch": 0.9916946138783227,
      "grad_norm": 1.2023310380039272,
      "learning_rate": 3.598703238634205e-09,
      "loss": 0.4849,
      "step": 11015
    },
    {
      "epoch": 0.9917846451641945,
      "grad_norm": 2.0251355128367594,
      "learning_rate": 3.5209003033365163e-09,
      "loss": 0.4914,
      "step": 11016
    },
    {
      "epoch": 0.9918746764500664,
      "grad_norm": 1.748836109838061,
      "learning_rate": 3.4439474748293455e-09,
      "loss": 0.4939,
      "step": 11017
    },
    {
      "epoch": 0.9919647077359383,
      "grad_norm": 1.6974960531580392,
      "learning_rate": 3.3678447596552364e-09,
      "loss": 0.4753,
      "step": 11018
    },
    {
      "epoch": 0.9920547390218101,
      "grad_norm": 2.20436456665678,
      "learning_rate": 3.2925921642878998e-09,
      "loss": 0.5169,
      "step": 11019
    },
    {
      "epoch": 0.9921447703076819,
      "grad_norm": 1.9120453115364935,
      "learning_rate": 3.2181896951255507e-09,
      "loss": 0.571,
      "step": 11020
    },
    {
      "epoch": 0.9922348015935537,
      "grad_norm": 1.9045249907483606,
      "learning_rate": 3.1446373584953504e-09,
      "loss": 0.5643,
      "step": 11021
    },
    {
      "epoch": 0.9923248328794256,
      "grad_norm": 1.5175691934248179,
      "learning_rate": 3.071935160652295e-09,
      "loss": 0.5302,
      "step": 11022
    },
    {
      "epoch": 0.9924148641652975,
      "grad_norm": 1.5588046967296298,
      "learning_rate": 3.0000831077803273e-09,
      "loss": 0.5268,
      "step": 11023
    },
    {
      "epoch": 0.9925048954511693,
      "grad_norm": 1.4238068313675964,
      "learning_rate": 2.929081205987894e-09,
      "loss": 0.5309,
      "step": 11024
    },
    {
      "epoch": 0.9925949267370411,
      "grad_norm": 1.5718507202988783,
      "learning_rate": 2.858929461314608e-09,
      "loss": 0.4466,
      "step": 11025
    },
    {
      "epoch": 0.992684958022913,
      "grad_norm": 1.8876239693565244,
      "learning_rate": 2.7896278797256983e-09,
      "loss": 0.5262,
      "step": 11026
    },
    {
      "epoch": 0.9927749893087848,
      "grad_norm": 2.1180505284650786,
      "learning_rate": 2.7211764671142283e-09,
      "loss": 0.4888,
      "step": 11027
    },
    {
      "epoch": 0.9928650205946566,
      "grad_norm": 2.172313070059931,
      "learning_rate": 2.6535752293022077e-09,
      "loss": 0.4991,
      "step": 11028
    },
    {
      "epoch": 0.9929550518805285,
      "grad_norm": 1.6544469705255784,
      "learning_rate": 2.586824172037261e-09,
      "loss": 0.5241,
      "step": 11029
    },
    {
      "epoch": 0.9930450831664003,
      "grad_norm": 2.5410828115348347,
      "learning_rate": 2.520923300997069e-09,
      "loss": 0.4852,
      "step": 11030
    },
    {
      "epoch": 0.9931351144522722,
      "grad_norm": 1.2196384707406176,
      "learning_rate": 2.455872621784927e-09,
      "loss": 0.5404,
      "step": 11031
    },
    {
      "epoch": 0.993225145738144,
      "grad_norm": 1.5863604099903799,
      "learning_rate": 2.391672139934187e-09,
      "loss": 0.5455,
      "step": 11032
    },
    {
      "epoch": 0.9933151770240158,
      "grad_norm": 1.2430164146189375,
      "learning_rate": 2.3283218609027048e-09,
      "loss": 0.5144,
      "step": 11033
    },
    {
      "epoch": 0.9934052083098877,
      "grad_norm": 1.1542287343531528,
      "learning_rate": 2.265821790078393e-09,
      "loss": 0.4455,
      "step": 11034
    },
    {
      "epoch": 0.9934952395957595,
      "grad_norm": 1.6649650181635083,
      "learning_rate": 2.2041719327781096e-09,
      "loss": 0.5321,
      "step": 11035
    },
    {
      "epoch": 0.9935852708816314,
      "grad_norm": 1.1896055767667646,
      "learning_rate": 2.1433722942421076e-09,
      "loss": 0.5635,
      "step": 11036
    },
    {
      "epoch": 0.9936753021675032,
      "grad_norm": 1.1450537479860714,
      "learning_rate": 2.0834228796418054e-09,
      "loss": 0.5214,
      "step": 11037
    },
    {
      "epoch": 0.993765333453375,
      "grad_norm": 1.3931216025047322,
      "learning_rate": 2.0243236940753476e-09,
      "loss": 0.4549,
      "step": 11038
    },
    {
      "epoch": 0.9938553647392468,
      "grad_norm": 1.9128257867341503,
      "learning_rate": 1.966074742568713e-09,
      "loss": 0.5403,
      "step": 11039
    },
    {
      "epoch": 0.9939453960251188,
      "grad_norm": 1.8480149333268114,
      "learning_rate": 1.9086760300746075e-09,
      "loss": 0.5053,
      "step": 11040
    },
    {
      "epoch": 0.9940354273109906,
      "grad_norm": 1.6842576339168882,
      "learning_rate": 1.8521275614757917e-09,
      "loss": 0.4904,
      "step": 11041
    },
    {
      "epoch": 0.9941254585968624,
      "grad_norm": 1.2804615936277879,
      "learning_rate": 1.7964293415795309e-09,
      "loss": 0.5199,
      "step": 11042
    },
    {
      "epoch": 0.9942154898827342,
      "grad_norm": 1.4991588156879565,
      "learning_rate": 1.7415813751242571e-09,
      "loss": 0.5449,
      "step": 11043
    },
    {
      "epoch": 0.994305521168606,
      "grad_norm": 1.7454183675290256,
      "learning_rate": 1.6875836667729073e-09,
      "loss": 0.4433,
      "step": 11044
    },
    {
      "epoch": 0.994395552454478,
      "grad_norm": 1.0945757079703,
      "learning_rate": 1.6344362211173637e-09,
      "loss": 0.5058,
      "step": 11045
    },
    {
      "epoch": 0.9944855837403498,
      "grad_norm": 1.1272581234873509,
      "learning_rate": 1.5821390426773441e-09,
      "loss": 0.5881,
      "step": 11046
    },
    {
      "epoch": 0.9945756150262216,
      "grad_norm": 1.3680654112779698,
      "learning_rate": 1.5306921359015126e-09,
      "loss": 0.4828,
      "step": 11047
    },
    {
      "epoch": 0.9946656463120934,
      "grad_norm": 1.3907540601258204,
      "learning_rate": 1.4800955051641474e-09,
      "loss": 0.6081,
      "step": 11048
    },
    {
      "epoch": 0.9947556775979652,
      "grad_norm": 1.601018539063603,
      "learning_rate": 1.430349154767363e-09,
      "loss": 0.5294,
      "step": 11049
    },
    {
      "epoch": 0.9948457088838372,
      "grad_norm": 1.2151087069347495,
      "learning_rate": 1.3814530889433298e-09,
      "loss": 0.4309,
      "step": 11050
    },
    {
      "epoch": 0.994935740169709,
      "grad_norm": 1.2007206773620822,
      "learning_rate": 1.333407311847612e-09,
      "loss": 0.5525,
      "step": 11051
    },
    {
      "epoch": 0.9950257714555808,
      "grad_norm": 1.2240208306847085,
      "learning_rate": 1.2862118275680513e-09,
      "loss": 0.5124,
      "step": 11052
    },
    {
      "epoch": 0.9951158027414526,
      "grad_norm": 1.2186415684292262,
      "learning_rate": 1.2398666401181037e-09,
      "loss": 0.5088,
      "step": 11053
    },
    {
      "epoch": 0.9952058340273245,
      "grad_norm": 2.320923540196211,
      "learning_rate": 1.1943717534379506e-09,
      "loss": 0.4711,
      "step": 11054
    },
    {
      "epoch": 0.9952958653131964,
      "grad_norm": 1.311888661485817,
      "learning_rate": 1.1497271713978297e-09,
      "loss": 0.4958,
      "step": 11055
    },
    {
      "epoch": 0.9953858965990682,
      "grad_norm": 1.6155885632373492,
      "learning_rate": 1.1059328977935936e-09,
      "loss": 0.4626,
      "step": 11056
    },
    {
      "epoch": 0.99547592788494,
      "grad_norm": 1.3272703007064892,
      "learning_rate": 1.06298893634893e-09,
      "loss": 0.4673,
      "step": 11057
    },
    {
      "epoch": 0.9955659591708118,
      "grad_norm": 1.481237119985429,
      "learning_rate": 1.0208952907164726e-09,
      "loss": 0.5329,
      "step": 11058
    },
    {
      "epoch": 0.9956559904566837,
      "grad_norm": 1.9791874323388698,
      "learning_rate": 9.79651964476691e-10,
      "loss": 0.4662,
      "step": 11059
    },
    {
      "epoch": 0.9957460217425556,
      "grad_norm": 1.865642434285871,
      "learning_rate": 9.392589611356696e-10,
      "loss": 0.528,
      "step": 11060
    },
    {
      "epoch": 0.9958360530284274,
      "grad_norm": 1.4084743280124352,
      "learning_rate": 8.997162841284379e-10,
      "loss": 0.5728,
      "step": 11061
    },
    {
      "epoch": 0.9959260843142992,
      "grad_norm": 1.514050418546256,
      "learning_rate": 8.610239368178619e-10,
      "loss": 0.4477,
      "step": 11062
    },
    {
      "epoch": 0.9960161156001711,
      "grad_norm": 1.3996083267580222,
      "learning_rate": 8.231819224957527e-10,
      "loss": 0.5698,
      "step": 11063
    },
    {
      "epoch": 0.9961061468860429,
      "grad_norm": 1.4044732643450268,
      "learning_rate": 7.861902443784264e-10,
      "loss": 0.5107,
      "step": 11064
    },
    {
      "epoch": 0.9961961781719147,
      "grad_norm": 1.3916944218698373,
      "learning_rate": 7.500489056133653e-10,
      "loss": 0.4988,
      "step": 11065
    },
    {
      "epoch": 0.9962862094577866,
      "grad_norm": 2.089264681437297,
      "learning_rate": 7.147579092725565e-10,
      "loss": 0.5174,
      "step": 11066
    },
    {
      "epoch": 0.9963762407436584,
      "grad_norm": 1.1887161696989625,
      "learning_rate": 6.803172583569329e-10,
      "loss": 0.5136,
      "step": 11067
    },
    {
      "epoch": 0.9964662720295303,
      "grad_norm": 1.7611554001979641,
      "learning_rate": 6.467269557974832e-10,
      "loss": 0.5046,
      "step": 11068
    },
    {
      "epoch": 0.9965563033154021,
      "grad_norm": 1.203813028711324,
      "learning_rate": 6.139870044485907e-10,
      "loss": 0.4973,
      "step": 11069
    },
    {
      "epoch": 0.9966463346012739,
      "grad_norm": 1.0859072277216866,
      "learning_rate": 5.82097407094695e-10,
      "loss": 0.55,
      "step": 11070
    },
    {
      "epoch": 0.9967363658871458,
      "grad_norm": 1.6170497563661483,
      "learning_rate": 5.51058166449181e-10,
      "loss": 0.4511,
      "step": 11071
    },
    {
      "epoch": 0.9968263971730176,
      "grad_norm": 1.3415142351662535,
      "learning_rate": 5.208692851510489e-10,
      "loss": 0.4891,
      "step": 11072
    },
    {
      "epoch": 0.9969164284588895,
      "grad_norm": 1.2421765218255625,
      "learning_rate": 4.915307657671342e-10,
      "loss": 0.5482,
      "step": 11073
    },
    {
      "epoch": 0.9970064597447613,
      "grad_norm": 1.3937152144263523,
      "learning_rate": 4.6304261079210824e-10,
      "loss": 0.501,
      "step": 11074
    },
    {
      "epoch": 0.9970964910306331,
      "grad_norm": 1.241089019848546,
      "learning_rate": 4.354048226484775e-10,
      "loss": 0.5076,
      "step": 11075
    },
    {
      "epoch": 0.997186522316505,
      "grad_norm": 1.6643664638670574,
      "learning_rate": 4.0861740368880463e-10,
      "loss": 0.545,
      "step": 11076
    },
    {
      "epoch": 0.9972765536023769,
      "grad_norm": 1.66486489274367,
      "learning_rate": 3.8268035618793667e-10,
      "loss": 0.4304,
      "step": 11077
    },
    {
      "epoch": 0.9973665848882487,
      "grad_norm": 1.6488388964102185,
      "learning_rate": 3.5759368235410707e-10,
      "loss": 0.4816,
      "step": 11078
    },
    {
      "epoch": 0.9974566161741205,
      "grad_norm": 1.221190888206642,
      "learning_rate": 3.3335738431894416e-10,
      "loss": 0.4923,
      "step": 11079
    },
    {
      "epoch": 0.9975466474599923,
      "grad_norm": 1.6188566432808587,
      "learning_rate": 3.0997146414524225e-10,
      "loss": 0.5914,
      "step": 11080
    },
    {
      "epoch": 0.9976366787458641,
      "grad_norm": 1.205295681732709,
      "learning_rate": 2.874359238203006e-10,
      "loss": 0.5467,
      "step": 11081
    },
    {
      "epoch": 0.9977267100317361,
      "grad_norm": 1.178893298026177,
      "learning_rate": 2.6575076526036413e-10,
      "loss": 0.4777,
      "step": 11082
    },
    {
      "epoch": 0.9978167413176079,
      "grad_norm": 1.9699599062410647,
      "learning_rate": 2.4491599031173376e-10,
      "loss": 0.5116,
      "step": 11083
    },
    {
      "epoch": 0.9979067726034797,
      "grad_norm": 1.2155951105377056,
      "learning_rate": 2.249316007429947e-10,
      "loss": 0.4963,
      "step": 11084
    },
    {
      "epoch": 0.9979968038893515,
      "grad_norm": 1.4861368027674988,
      "learning_rate": 2.057975982561189e-10,
      "loss": 0.5153,
      "step": 11085
    },
    {
      "epoch": 0.9980868351752233,
      "grad_norm": 1.230553179093231,
      "learning_rate": 1.8751398447758307e-10,
      "loss": 0.508,
      "step": 11086
    },
    {
      "epoch": 0.9981768664610953,
      "grad_norm": 1.3862110441236137,
      "learning_rate": 1.7008076096280966e-10,
      "loss": 0.5238,
      "step": 11087
    },
    {
      "epoch": 0.9982668977469671,
      "grad_norm": 1.5774966862820494,
      "learning_rate": 1.5349792919283625e-10,
      "loss": 0.4148,
      "step": 11088
    },
    {
      "epoch": 0.9983569290328389,
      "grad_norm": 1.3288465327199215,
      "learning_rate": 1.377654905787562e-10,
      "loss": 0.5083,
      "step": 11089
    },
    {
      "epoch": 0.9984469603187107,
      "grad_norm": 1.4553962688344229,
      "learning_rate": 1.2288344645838835e-10,
      "loss": 0.5431,
      "step": 11090
    },
    {
      "epoch": 0.9985369916045826,
      "grad_norm": 1.7262490907625196,
      "learning_rate": 1.0885179809849711e-10,
      "loss": 0.4823,
      "step": 11091
    },
    {
      "epoch": 0.9986270228904545,
      "grad_norm": 1.7859690354127433,
      "learning_rate": 9.567054668924159e-11,
      "loss": 0.4971,
      "step": 11092
    },
    {
      "epoch": 0.9987170541763263,
      "grad_norm": 2.1673877346749273,
      "learning_rate": 8.33396933552777e-11,
      "loss": 0.5788,
      "step": 11093
    },
    {
      "epoch": 0.9988070854621981,
      "grad_norm": 1.5545456231461416,
      "learning_rate": 7.185923914243553e-11,
      "loss": 0.4743,
      "step": 11094
    },
    {
      "epoch": 0.9988971167480699,
      "grad_norm": 1.1766680191409375,
      "learning_rate": 6.122918502882158e-11,
      "loss": 0.5289,
      "step": 11095
    },
    {
      "epoch": 0.9989871480339418,
      "grad_norm": 1.4957490274016974,
      "learning_rate": 5.144953191704716e-11,
      "loss": 0.5521,
      "step": 11096
    },
    {
      "epoch": 0.9990771793198137,
      "grad_norm": 1.676527020467371,
      "learning_rate": 4.2520280639779535e-11,
      "loss": 0.5411,
      "step": 11097
    },
    {
      "epoch": 0.9991672106056855,
      "grad_norm": 1.4909907414896784,
      "learning_rate": 3.444143195530103e-11,
      "loss": 0.5578,
      "step": 11098
    },
    {
      "epoch": 0.9992572418915573,
      "grad_norm": 1.800554368760745,
      "learning_rate": 2.7212986551949926e-11,
      "loss": 0.5798,
      "step": 11099
    },
    {
      "epoch": 0.9993472731774291,
      "grad_norm": 1.338277073200609,
      "learning_rate": 2.0834945043679556e-11,
      "loss": 0.5159,
      "step": 11100
    },
    {
      "epoch": 0.999437304463301,
      "grad_norm": 1.7512825075887775,
      "learning_rate": 1.530730797227875e-11,
      "loss": 0.5116,
      "step": 11101
    },
    {
      "epoch": 0.9995273357491729,
      "grad_norm": 1.8994958970869744,
      "learning_rate": 1.06300758095923e-11,
      "loss": 0.4255,
      "step": 11102
    },
    {
      "epoch": 0.9996173670350447,
      "grad_norm": 1.640807806698307,
      "learning_rate": 6.803248951969821e-12,
      "loss": 0.5398,
      "step": 11103
    },
    {
      "epoch": 0.9997073983209165,
      "grad_norm": 2.095384020932824,
      "learning_rate": 3.826827725816884e-12,
      "loss": 0.485,
      "step": 11104
    },
    {
      "epoch": 0.9997974296067884,
      "grad_norm": 1.5194293472715046,
      "learning_rate": 1.7008123831541157e-12,
      "loss": 0.4922,
      "step": 11105
    },
    {
      "epoch": 0.9998874608926602,
      "grad_norm": 1.7766050990560183,
      "learning_rate": 4.2520310494786885e-13,
      "loss": 0.4562,
      "step": 11106
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.9627794630745277,
      "learning_rate": 0.0,
      "loss": 0.5799,
      "step": 11107
    },
    {
      "epoch": 1.0,
      "step": 11107,
      "total_flos": 8.886834119456063e+18,
      "train_loss": 0.5648524630528023,
      "train_runtime": 217082.5364,
      "train_samples_per_second": 6.549,
      "train_steps_per_second": 0.051
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 11107,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8.886834119456063e+18,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
